{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Approaches worth mentioning that were tried but finally not used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The following Sections contain descriptions and the code of the approaches that we tried but decided not to use in the final clean notebook due to MAE performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Outlier Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "\n",
    "**Our approach:**\n",
    "- We treat outliers as a **data quality + robustness problem**, not as a reason to delete rare cars.\n",
    "- We keep the process **leakage-safe** by implementing outlier handling as an sklearn transformer inside the pipeline (thresholds learned on training folds only).\n",
    "- We explicitly separate:\n",
    "  - **logical inconsistencies** found in the EDA (handled deterministically in `CarDataCleaner`),\n",
    "  - vs. **statistical extremes** (handled in `OutlierHandler`).\n",
    "- **Identification** of outliers\n",
    "  - **Voting of robust univariate detectors:**\n",
    "    - **Tukey IQR fences (1.5×IQR)**\n",
    "      - Flags a value if it lies outside:\n",
    "        - `Q1 − 1.5·IQR` or `Q3 + 1.5·IQR`\n",
    "      - Strength: non-parametric, robust, widely used baseline (boxplot rule).\n",
    "    - **Modified Z-score using Median Absolute Deviation (MAD)**\n",
    "      - Robust alternative to z-scores:\n",
    "        - uses the **median** instead of mean\n",
    "        - uses **MAD** instead of standard deviation\n",
    "      - Typical threshold: `|modified_z| > 3.5`\n",
    "      - Strength: less sensitive to extreme tails than mean/std-based z-scores.\n",
    "    - **Voting rule** (for robustness):\n",
    "      - A value is treated as an outlier only if both methods agree (`min_votes=2`).\n",
    "      - This reduces false positives compared to using only IQR fences on skewed distributions.\n",
    "- **Treatment of Outliers:**\n",
    "  - **Winsorization** (clip extreme values):\n",
    "    - We keep every car in the dataset (no row deletion).\n",
    "    - We reduce the influence of extreme values while still preserving information and rank order in the feature.\n",
    "    - We **clip** flagged values to conservative bounds (`action=\"clip\"`):\n",
    "      - For each numeric feature we compute robust lower/upper bounds (from IQR and MAD-based thresholds).\n",
    "      - Values outside those bounds are replaced by the nearest bound (winsorization).\n",
    "- **Benefits** of this approach:\n",
    "  - Keeps rare cars (no row deletion).\n",
    "  - Avoids replacing informative extremes with typical medians (which can hurt tree models).\n",
    "  - Stabilizes downstream steps (imputation, feature engineering, scaling) without collapsing signal into missingness.\n",
    "- **Model-family sensitivity:** \n",
    "  - winsorization is especially helpful for linear/SVR models (reduces leverage points) and remains safe for trees\n",
    "  - we keep one unified default pipeline for comparability.\n",
    "\n",
    "<u>Place in the pipe</u>: \n",
    "- Before imputation to use original distribution for identifying the outliers (otherwise we would inflate the distributions with the imputed values)\n",
    "- Then in imputation, fill the original gaps based on a distribution that does not includes the massive outliers (skewing the mean/median)    \n",
    "  -> kill the outliers first (set to NaN) so the imputation for everyone becomes cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Further tried but unused techniques:\n",
    "- **Drop outside 1.5*IQR:** We decided against the classical “drop rows outside 1.5×IQR” because of:\n",
    "    - The classical 1.5×IQR boxplot rule (Tukey fences) is a strong baseline, but real-world car variables (especially mileage) are often skewed / heavy-tailed, which can over-flag valid high values.\n",
    "    - Dropping rows removes rare but valid cars (e.g., very high mileage vehicles), which is undesirable for production.\n",
    "- **NaN into Imputation:** Set outliers found by Voting of robust univariate detectors to NaN and impute later with (`action=\"nan\"`), but this significantly hurt the best averaged CV MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The way it was called in the pipeline:\n",
    "preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"outliers\", OutlierHandler(\n",
    "        cols=[c for c in orig_numeric_features if c != \"mileage\"],      # only original numeric features here, no mileage because of log transform later\n",
    "        methods=(\"iqr\", \"mod_z\"),                                       # robust voting\n",
    "        min_votes=2,                                                    # outlier if both methods agree\n",
    "        iqr_k=1.5,\n",
    "        z_thresh=3.5,\n",
    "        action=\"clip\",                                                   \n",
    "        verbose=False,\n",
    "    )),\n",
    "    # [Unused Group Imputer Handling]\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", (enc_transf_scale)),\n",
    "    (\"fs\", (fs_pipe)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    ('debug_after_outliers', DebugTransformer('AFTER OUTLIER HANDLING', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Group Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**Our approach:**\n",
    "- We use a custom **hierarchical GroupImputer** to impute missing values in a way that matches the structure of the car market.\n",
    "- Instead of imputing from the full dataset only (global statistics), we first try to impute from **the most similar cars**:\n",
    "  - same `brand` and same `model` (closest peer group),\n",
    "  - otherwise same `brand`,\n",
    "  - otherwise the global dataset.\n",
    "- This is more realistic than a single global median because many variables (e.g., `engineSize`, `mpg`, `tax`) are strongly segment-dependent.\n",
    "\n",
    "**Leakage safety:**\n",
    "- The `GroupImputer` is implemented as an sklearn transformer in the pipeline.\n",
    "- Therefore, during cross-validation it learns all medians/modes **only on the training fold** in `fit()` and applies them to the validation fold in `transform()` (no leakage).\n",
    "\n",
    "---\n",
    "\n",
    "##### Place in the pipe\n",
    "\n",
    "> `CarDataCleaner` → `OutlierHandler` → `GroupImputer` → `CarFeatureEngineer` → `encoding/scaling` → `FS` → `model`\n",
    "\n",
    "**Justification:**\n",
    "- Imputation must happen on **original features** first, because feature engineering creates ratios/interactions (e.g., `miles_per_year`, `engine_per_mpg`) that would otherwise explode or become undefined when inputs are missing.\n",
    "- We impute **before** feature engineering to ensure engineered features are computed on complete, consistent base variables.\n",
    "\n",
    "---\n",
    "\n",
    "##### Why medians/modes:\n",
    "\n",
    "- **Median** is robust to skewed distributions (common in `mileage`, `tax`) and less sensitive to extreme values than the mean.\n",
    "- **Mode** is the natural robust default for categorical variables.\n",
    "\n",
    "---\n",
    "\n",
    "##### Implementation notes:\n",
    "\n",
    "- `group_cols` are used only to define groups; they themselves are **not imputed**.\n",
    "- The transformer is deterministic: ties in categorical mode are handled consistently (pandas `.mode()` → first entry).\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Hierarchical imputer for numeric + categorical features.\n",
    "\n",
    "    Idea:\n",
    "    ----\n",
    "    We have to  compute the median value for the train dataset and fill the missing values in train, validation and test set with the median from the train dataset.\n",
    "    For each row with a missing value, fill it using statistics from \"similar\" rows first, and only fall back to global statistics if needed.\n",
    "\n",
    "    Hierarchy for numeric columns (num_cols):\n",
    "        1) median per (group_cols[0], group_cols[1])    > we use brand, model\n",
    "        2) median per group_cols[0]                     > we use brand\n",
    "        3) global median across all rows\n",
    "\n",
    "    Hierarchy for categorical columns (cat_cols):\n",
    "        1) mode per (group_cols[0], group_cols[1])      > we use brand, model\n",
    "        2) mode per group_cols[0]                       > we use brand\n",
    "        3) global mode across all rows\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - `group_cols` are used only to define groups; they themselves are not imputed.\n",
    "    - `num_cols` and `cat_cols` can be given explicitly (lists of column names). If None, they are inferred from the dtypes in `fit`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, group_cols=(\"brand\", \"model\"), num_cols=None, cat_cols=None, fallback=\"__MISSING__\", verbose=False, verbose_top_n=10):\n",
    "        self.group_cols = group_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.fallback = fallback\n",
    "        self.verbose = verbose\n",
    "        self.verbose_top_n = verbose_top_n\n",
    "\n",
    "    # helpers\n",
    "    def _mode(self, s: pd.Series):\n",
    "        \"\"\"\n",
    "        Deterministic mode helper.\n",
    "\n",
    "        - Compute the most frequent non-null value.\n",
    "        - If multiple values tie, Series.mode() returns them in order, we take .iloc[0].\n",
    "        - If there is no valid mode (all NaN), return fallback token.\n",
    "        \"\"\"\n",
    "        m = s.mode(dropna=True)\n",
    "        if not m.empty:\n",
    "            return m.iloc[0]\n",
    "        return self.fallback\n",
    "\n",
    "    def _get_group_series(self, df: pd.DataFrame, col_name: str) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Get the FIRST physical column with the given label from df.\n",
    "\n",
    "        Reason\n",
    "        ------\n",
    "        - In some workflows, df.columns can contain duplicate labels\n",
    "          (e.g. \"brand\" appearing twice after some operations).\n",
    "        - df[\"brand\"] would then raise \"Grouper for 'brand' not 1-dimensional\".\n",
    "        - By using np.where(df.columns == col_name)[0] we get *positions* and\n",
    "          explicitly pick the first one.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError if no column with that name exists.\n",
    "        \"\"\"\n",
    "        matches = np.where(df.columns == col_name)[0]\n",
    "        if len(matches) == 0:\n",
    "            raise ValueError(f\"GroupImputer: grouping column '{col_name}' not found in data.\")\n",
    "        return df.iloc[:, matches[0]]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn the group-level and global statistics from the training data.\n",
    "\n",
    "        Steps\n",
    "        -----\n",
    "        1) Convert X to DataFrame and remember the original column order.\n",
    "        2) Resolve which columns are numeric/categorical to impute.\n",
    "        3) Build group keys (g0, g1) from group_cols (e.g. brand, model).\n",
    "        4) For numeric columns:\n",
    "            - compute global medians\n",
    "            - medians per g0 (e.g. per brand)\n",
    "            - medians per (g0, g1) (e.g. per brand+model)\n",
    "        5) For categorical columns:\n",
    "            - global modes\n",
    "            - modes per g0\n",
    "            - modes per (g0, g1)\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.DataFrame(X).copy()\n",
    "        self.feature_names_in_ = df.columns.to_list()\n",
    "\n",
    "        # group_cols must contain at least one column name\n",
    "        if self.group_cols is None or len(self.group_cols) == 0:\n",
    "            raise ValueError(\"GroupImputer: at least one group column must be specified.\")\n",
    "\n",
    "        self.group_cols_ = list(self.group_cols)\n",
    "\n",
    "        # Determine numeric columns to impute (internal num_cols_)\n",
    "        if self.num_cols is None:\n",
    "            # If not specified: all numeric columns except the group columns\n",
    "            num_cols_all = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "            self.num_cols_ = [c for c in num_cols_all if c not in self.group_cols_]\n",
    "        else:\n",
    "            # If specified: keep only those that exist in df\n",
    "            self.num_cols_ = [c for c in self.num_cols if c in df.columns]\n",
    "\n",
    "        # Determine categorical columns to impute (internal cat_cols_)\n",
    "        if self.cat_cols is None:\n",
    "            # If not specified: all non-group, non-numeric columns\n",
    "            self.cat_cols_ = [c for c in df.columns if c not in self.group_cols_ + self.num_cols_]\n",
    "        else:\n",
    "            # If specified: keep only those that exist in df\n",
    "            self.cat_cols_ = [c for c in self.cat_cols if c in df.columns]\n",
    "\n",
    "        # Build group key series based on the current df\n",
    "        # g0 = first grouping column (e.g. brand)\n",
    "        g0 = self._get_group_series(df, self.group_cols_[0])\n",
    "\n",
    "        # g1 = second grouping column (e.g. model), optional\n",
    "        g1 = None\n",
    "        if len(self.group_cols_) > 1:\n",
    "            g1 = self._get_group_series(df, self.group_cols_[1])\n",
    "\n",
    "        # numeric statistics\n",
    "        if self.num_cols_:\n",
    "            # Extract the numeric columns to impute\n",
    "            num_df = df[self.num_cols_].copy()\n",
    "\n",
    "            # 3) Global median per numeric column (fallback for any group with no stats)\n",
    "            self.num_global_ = num_df.median(numeric_only=True)\n",
    "\n",
    "            # 2) Median per first-level group (g0, e.g. brand)\n",
    "            num_first = num_df.copy()\n",
    "            num_first[\"_g0\"] = g0.values  # temporary group key column\n",
    "            self.num_first_ = num_first.groupby(\"_g0\", dropna=True).median(numeric_only=True)\n",
    "\n",
    "            # 1) Median per pair (g0, g1), e.g. (brand, model)\n",
    "            if g1 is not None:\n",
    "                num_pair = num_df.copy()\n",
    "                num_pair[\"_g0\"] = g0.values\n",
    "                num_pair[\"_g1\"] = g1.values\n",
    "                self.num_pair_ = num_pair.groupby([\"_g0\", \"_g1\"], dropna=True).median(numeric_only=True)\n",
    "            else:\n",
    "                self.num_pair_ = pd.DataFrame()\n",
    "        else:\n",
    "            self.num_global_ = pd.Series(dtype=\"float64\")\n",
    "            self.num_first_ = pd.DataFrame()\n",
    "            self.num_pair_ = pd.DataFrame()\n",
    "\n",
    "        # categorical statistics\n",
    "        if self.cat_cols_:\n",
    "            cat_df = df[self.cat_cols_].copy()\n",
    "\n",
    "            # 3) Global mode per categorical column\n",
    "            self.cat_global_ = pd.Series({c: self._mode(cat_df[c]) for c in self.cat_cols_}, dtype=\"object\")\n",
    "\n",
    "            # 2) Mode per first-level group (g0)\n",
    "            cat_first = cat_df.copy()\n",
    "            cat_first[\"_g0\"] = g0.values\n",
    "            self.cat_first_ = cat_first.groupby(\"_g0\", dropna=True).agg(lambda s: self._mode(s))\n",
    "\n",
    "            # 1) Mode per pair (g0, g1)\n",
    "            if g1 is not None:\n",
    "                cat_pair = cat_df.copy()\n",
    "                cat_pair[\"_g0\"] = g0.values\n",
    "                cat_pair[\"_g1\"] = g1.values\n",
    "                self.cat_pair_ = cat_pair.groupby([\"_g0\", \"_g1\"], dropna=True).agg(lambda s: self._mode(s))\n",
    "            else:\n",
    "                self.cat_pair_ = pd.DataFrame()\n",
    "        else:\n",
    "            self.cat_global_ = pd.Series(dtype=\"object\")\n",
    "            self.cat_first_ = pd.DataFrame()\n",
    "            self.cat_pair_ = pd.DataFrame()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply hierarchical imputation to new data.\n",
    "            1) Convert input to DataFrame and align columns to what fit() saw.\n",
    "            2) Rebuild group keys g0, g1 from the current data.\n",
    "            3) For each numeric column with missing values:\n",
    "                - try pair-level median (g0, g1)\n",
    "                - then brand-level median (g0)\n",
    "                - then global median\n",
    "            4) Same for categorical columns with modes.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(X).copy()\n",
    "        df = df.reindex(columns=self.feature_names_in_)\n",
    "\n",
    "        g0 = self._get_group_series(df, self.group_cols_[0])\n",
    "        g1 = None\n",
    "        if len(self.group_cols_) > 1:\n",
    "            g1 = self._get_group_series(df, self.group_cols_[1])\n",
    "\n",
    "        # NEW: audit counters\n",
    "        report = {\"num_pair\": 0, \"num_brand\": 0, \"num_global\": 0, \"cat_pair\": 0, \"cat_brand\": 0, \"cat_global\": 0}\n",
    "        per_col = Counter()\n",
    "\n",
    "        # numeric imputation\n",
    "        if hasattr(self, \"num_cols_\") and self.num_cols_:\n",
    "            df[self.num_cols_] = df[self.num_cols_].astype(\"float64\")\n",
    "            to_impute_num = [c for c in self.num_cols_ if df[c].isna().any()]\n",
    "\n",
    "            if to_impute_num:\n",
    "                # 1) pair-level imputation: per (g0, g1)\n",
    "                if g1 is not None and not self.num_pair_.empty:\n",
    "                    key_df = pd.DataFrame({\"_g0\": g0.values, \"_g1\": g1.values})\n",
    "                    med_df = self.num_pair_.reset_index()\n",
    "                    joined = key_df.merge(med_df, on=[\"_g0\", \"_g1\"], how=\"left\")\n",
    "\n",
    "                    for col in to_impute_num:\n",
    "                        if col not in self.num_pair_.columns:\n",
    "                            continue\n",
    "                        mask = df[col].isna() & joined[col].notna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"num_pair\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df.loc[mask, col] = joined.loc[mask, col]\n",
    "\n",
    "                # 2) first-level imputation: per g0 only\n",
    "                if not self.num_first_.empty:\n",
    "                    key1 = pd.DataFrame({\"_g0\": g0.values})\n",
    "                    med1 = self.num_first_.reset_index()\n",
    "                    joined1 = key1.merge(med1, on=\"_g0\", how=\"left\")\n",
    "\n",
    "                    for col in to_impute_num:\n",
    "                        if col not in self.num_first_.columns:\n",
    "                            continue\n",
    "                        mask = df[col].isna() & joined1[col].notna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"num_brand\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df.loc[mask, col] = joined1.loc[mask, col]\n",
    "\n",
    "                # 3) global median fallback\n",
    "                for col in to_impute_num:\n",
    "                    if col in self.num_global_:\n",
    "                        mask = df[col].isna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"num_global\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df[col] = df[col].fillna(self.num_global_[col])\n",
    "\n",
    "        # categorical imputation\n",
    "        if hasattr(self, \"cat_cols_\") and self.cat_cols_:\n",
    "            to_impute_cat = [c for c in self.cat_cols_ if df[c].isna().any()]\n",
    "\n",
    "            if to_impute_cat:\n",
    "                # 1) pair-level imputation: per (g0, g1)\n",
    "                if g1 is not None and not self.cat_pair_.empty:\n",
    "                    key_df = pd.DataFrame({\"_g0\": g0.values, \"_g1\": g1.values})\n",
    "                    mode_df = self.cat_pair_.reset_index()\n",
    "                    joined = key_df.merge(mode_df, on=[\"_g0\", \"_g1\"], how=\"left\")\n",
    "\n",
    "                    for col in to_impute_cat:\n",
    "                        if col not in self.cat_pair_.columns:\n",
    "                            continue\n",
    "                        mask = df[col].isna() & joined[col].notna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"cat_pair\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df.loc[mask, col] = joined.loc[mask, col]\n",
    "\n",
    "                # 2) first-level imputation: per g0 only\n",
    "                if not self.cat_first_.empty:\n",
    "                    key1 = pd.DataFrame({\"_g0\": g0.values})\n",
    "                    mode1 = self.cat_first_.reset_index()\n",
    "                    joined1 = key1.merge(mode1, on=\"_g0\", how=\"left\")\n",
    "\n",
    "                    for col in to_impute_cat:\n",
    "                        if col not in self.cat_first_.columns:\n",
    "                            continue\n",
    "                        mask = df[col].isna() & joined1[col].notna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"cat_brand\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df.loc[mask, col] = joined1.loc[mask, col]\n",
    "\n",
    "                # 3) global mode fallback (or fallback token)\n",
    "                for col in to_impute_cat:\n",
    "                    mask = df[col].isna()\n",
    "                    n = int(mask.sum())\n",
    "                    report[\"cat_global\"] += n\n",
    "                    per_col[col] += n\n",
    "                    df[col] = df[col].fillna(self.cat_global_.get(col, self.fallback))\n",
    "\n",
    "        # store report for later inspection\n",
    "        self.report_ = report\n",
    "        self.report_by_column_ = (\n",
    "            pd.DataFrame(per_col.items(), columns=[\"column\", \"values_filled\"])\n",
    "            .sort_values(\"values_filled\", ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            _print_section(\"GroupImputer report\")\n",
    "            print(\"Imputed Missing Values ( always try 'most similar cars' first):\\n\")\n",
    "            print(f\"- Numeric (Median):   (brand+model)={report['num_pair']}, brand={report['num_brand']}, global={report['num_global']}\")\n",
    "            print(f\"- Categorical (Mode): (brand+model)={report['cat_pair']}, brand={report['cat_brand']}, global={report['cat_global']}\")\n",
    "            print(\"\\nTop columns affected:\")\n",
    "            _maybe_display(self.report_by_column_, max_rows=self.verbose_top_n)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"\n",
    "        Make the transformer compatible with sklearn's get feature-name.\n",
    "\n",
    "        - If called without arguments, return the original feature names seen in fit().\n",
    "        - This is mostly useful when GroupImputer is at the top of a Pipeline and\n",
    "          later steps want to introspect feature names.\n",
    "        \"\"\"\n",
    "        if input_features is None:\n",
    "            input_features = getattr(self, \"feature_names_in_\", None)\n",
    "        return np.asarray(input_features, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The way it was called in the pipeline:\n",
    "preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    # [Unused Group Imputer Handling]\n",
    "    (\"group_imputer\", GroupImputer(\n",
    "        group_cols=(\"brand\", \"model\"),\n",
    "        num_cols=orig_numeric_features,                                 # We have to use the original features here because the others are engineered in the next step\n",
    "        # bools_cols=orig_boolean\n",
    "        cat_cols=orig_categorical_features,                             # We have to use the original features here because the others are engineered in the next step\n",
    "        fallback=\"__MISSING__\",\n",
    "    )),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", (enc_transf_scale)),\n",
    "    (\"fs\", (fs_pipe)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "SKlearn elements we also considered but decided not to use:\n",
    "- Filter Methods: & SelectPercentile\n",
    "    - SelectFwe (Family-Wise Error Rate)   \n",
    "    -> too strict and we don't want to be too conservative in our feature selection (we prefer to keep weak but useful signals)\n",
    "    - SelectKBest     \n",
    "    -> we didn't want to fix k (number of selected features)\n",
    "- Wrapper Methods: \n",
    "    - RFECV   \n",
    "    -> too expensive\n",
    "    - SequentialFeatureSelector (forward, backward selection)   \n",
    "    -> too expensive\n",
    "- Embedded:\n",
    "    - Regularization Method (Lasso)     \n",
    "    -> considers only linear relationships so discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARCHIVE: \"top-k from SHAP\" code commented out\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# The following would create a new model/pipeline based on SHAP-top-k features.\n",
    "# We keep it for reference but do not use it because:\n",
    "# - We already have a robust, leakage-safe FS pipeline (VT + majority voting).\n",
    "# - Here we want SHAP to be interpretability only.\n",
    "\n",
    "# def cv_mae_topk_from_shap(\n",
    "#     pipe,\n",
    "#     shap_importance,\n",
    "#     X,\n",
    "#     y,\n",
    "#     n_features_list,\n",
    "#     folds=5,\n",
    "#     seed=rs,\n",
    "#     model_name=None,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     For a fitted pipeline `pipe` and its SHAP importances:\n",
    "#       - Build X_proc, feat_names from the pipeline.\n",
    "#       - For each k in n_features_list:\n",
    "#           * Take top-k features by SHAP.\n",
    "#           * Run KFold CV on X_proc[:, idx] with the pipeline's final estimator.\n",
    "#       - Print MAE per k and return the best (k, model, feature list).\n",
    "#\n",
    "#     Returns:\n",
    "#       best_model: fitted estimator on full X_proc restricted to best-k features\n",
    "#       best_features: list of feature names used\n",
    "#     \"\"\"\n",
    "#     # 1) Get processed features and names\n",
    "#     X_proc, feat_names = get_pipeline_feature_matrix(pipe, X)\n",
    "#     feat_names = np.asarray(feat_names, dtype=object)\n",
    "#\n",
    "#     # 2) SHAP ranking\n",
    "#     shap_sorted = shap_importance.sort_values(\"importance\", ascending=False)\n",
    "#     shap_order = shap_sorted[\"feature\"].tolist()\n",
    "#\n",
    "#     # helper: indices of top-k by SHAP\n",
    "#     def indices_for_topk(k):\n",
    "#         top_feats = shap_order[:k]\n",
    "#         return [i for i, fname in enumerate(feat_names) if fname in top_feats]\n",
    "#\n",
    "#     kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "#     model_proto = pipe.named_steps[\"model\"]\n",
    "#     tag = model_name or model_proto.__class__.__name__\n",
    "#\n",
    "#     results = []\n",
    "#\n",
    "#     for k in n_features_list:\n",
    "#         idx = indices_for_topk(k)\n",
    "#         if len(idx) == 0:\n",
    "#             print(f\"Skipping k={k}: no matching feature indices.\")\n",
    "#             continue\n",
    "#\n",
    "#         mae_folds = []\n",
    "#\n",
    "#         for train_idx, val_idx in kf.split(X_proc):\n",
    "#             X_tr, X_val = X_proc[train_idx][:, idx], X_proc[val_idx][:, idx]\n",
    "#             y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "#\n",
    "#             est = clone(model_proto)\n",
    "#             est.fit(X_tr, y_tr)\n",
    "#             y_pred = est.predict(X_val)\n",
    "#             mae_folds.append(mean_absolute_error(y_val, y_pred))\n",
    "#\n",
    "#         mae_mean = float(np.mean(mae_folds))\n",
    "#         results.append({\"k\": k, \"mae\": mae_mean, \"idx\": idx})\n",
    "#\n",
    "#     # pick best k\n",
    "#     if not results:\n",
    "#         raise RuntimeError(\"No valid k in n_features_list produced results.\")\n",
    "#\n",
    "#     best = min(results, key=lambda r: r[\"mae\"])\n",
    "#     best_k = best[\"k\"]\n",
    "#     best_mae = best[\"mae\"]\n",
    "#     best_idx = best[\"idx\"]\n",
    "#     best_features = [feat_names[i] for i in best_idx]\n",
    "#\n",
    "#     print(f\"\\nTop-k SHAP feature CV – {tag}\")\n",
    "#     for r in results:\n",
    "#         print(f\"  k={r['k']:3d} | MAE={r['mae']:.2f}\")\n",
    "#     print(f\"Best: k={best_k} | MAE={best_mae:.2f}\")\n",
    "#\n",
    "#     # fit final estimator on full X_proc restricted to best_k features\n",
    "#     final_est = clone(model_proto)\n",
    "#     final_est.fit(X_proc[:, best_idx], y)\n",
    "#\n",
    "#     return final_est, best_features\n",
    "#\n",
    "#\n",
    "# class ShapTopKColumnSelector(BaseEstimator, TransformerMixin):\n",
    "#     \"\"\"\n",
    "#     Transformer that selects a fixed subset of columns by name.\n",
    "#\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     selected_features : list of str\n",
    "#         Feature names (after preprocessing) to keep.\n",
    "#\n",
    "#     all_feature_names : array-like of str\n",
    "#         Full list of feature names aligned with the columns of X after preprocessing.\n",
    "#         These are typically obtained from get_pipeline_feature_matrix(...).\n",
    "#     \"\"\"\n",
    "#     def __init__(self, selected_features, all_feature_names):\n",
    "#         self.selected_features = list(selected_features)\n",
    "#         self.all_feature_names = np.asarray(all_feature_names, dtype=object)\n",
    "#\n",
    "#     def fit(self, X, y=None):\n",
    "#         # Compute the column indices corresponding to selected_features\n",
    "#         name_to_idx = {name: i for i, name in enumerate(self.all_feature_names)}\n",
    "#         self.idx_ = [\n",
    "#             name_to_idx[name]\n",
    "#             for name in self.selected_features\n",
    "#             if name in name_to_idx\n",
    "#         ]\n",
    "#         if len(self.idx_) == 0:\n",
    "#             raise ValueError(\n",
    "#                 \"ShapTopKColumnSelector: none of the selected_features were found \"\n",
    "#                 \"in all_feature_names.\"\n",
    "#             )\n",
    "#         return self\n",
    "#\n",
    "#     def transform(self, X):\n",
    "#         # X is the matrix after preprocessing; select only the chosen columns\n",
    "#         return X[:, self.idx_]\n",
    "#\n",
    "#     def get_feature_names_out(self, input_features=None):\n",
    "#         # For consistency with sklearn's feature-name API\n",
    "#         return np.asarray(self.selected_features, dtype=object)\n",
    "#\n",
    "#\n",
    "# def build_shap_topk_pipeline(\n",
    "#     base_pipe,\n",
    "#     best_features,\n",
    "#     all_feature_names,\n",
    "#     step_model_name=\"model\",\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Build a final pipeline that:\n",
    "#       - reuses the preprocessing (and vt/fs if present) from `base_pipe`\n",
    "#       - inserts a ShapTopKColumnSelector to keep only `best_features`\n",
    "#       - uses a fresh clone of the base model as final estimator\n",
    "#\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     base_pipe : sklearn Pipeline\n",
    "#         Fitted pipeline with steps: 'preprocess' -> optional 'vt'/'fs' -> 'model'.\n",
    "#\n",
    "#     best_features : list of str\n",
    "#         Names of the features (after preprocessing) to keep.\n",
    "#\n",
    "#     all_feature_names : array-like of str\n",
    "#         Full list of feature names aligned with the output of preprocessing\n",
    "#         (and vt/fs if they were applied when computing SHAP).\n",
    "#\n",
    "#     step_model_name : str, default=\"model\"\n",
    "#         Name of the final estimator step in base_pipe.\n",
    "#\n",
    "#     Returns\n",
    "#     -------\n",
    "#     final_pipe : sklearn Pipeline\n",
    "#         Unfitted pipeline. Call final_pipe.fit(X, y) to train on full data.\n",
    "#     \"\"\"\n",
    "#     steps = []\n",
    "#\n",
    "#     # 1) Preprocess step (clone so we refit on full data)\n",
    "#     pre = base_pipe.named_steps[\"preprocess\"]\n",
    "#     steps.append((\"preprocess\", clone(pre)))\n",
    "#\n",
    "#     # 2) Optional VarianceThreshold\n",
    "#     if \"vt\" in base_pipe.named_steps and base_pipe.named_steps[\"vt\"] is not None:\n",
    "#         steps.append((\"vt\", clone(base_pipe.named_steps[\"vt\"])))\n",
    "#\n",
    "#     # 3) SHAP-based column selector\n",
    "#     shap_selector = ShapTopKColumnSelector(\n",
    "#         selected_features=best_features,\n",
    "#         all_feature_names=all_feature_names,\n",
    "#     )\n",
    "#     steps.append((\"shap_select\", shap_selector))\n",
    "#\n",
    "#     # 4) Final estimator – fresh clone of the base model\n",
    "#     base_model = base_pipe.named_steps[step_model_name]\n",
    "#     steps.append((\"model\", clone(base_model)))\n",
    "#\n",
    "#     final_pipe = Pipeline(steps)\n",
    "#     return final_pipe\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
