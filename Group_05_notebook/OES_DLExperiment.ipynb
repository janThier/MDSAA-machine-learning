{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Deep Learning Experiments: Pre-training vs Training from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Objective and Motivation\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "Deep neural networks often underperform tree-based models on tabular data due to heterogeneous feature types, relatively small sample sizes, and irregular, non-smooth decision boundaries that are well captured by tree ensembles (Grinsztajn et al., 2022).\n",
    "\n",
    "Meanwhile, self-supervised pre-training has revolutionized performance benchmarks, especially in natural language processing or computer vision (Devlin et al., 2019; Somepalli et al., 2021).  \n",
    "This raises the question: **Can similar pre-training strategies help neural networks close the performance gap on tabular data?**\n",
    "\n",
    "#### Approach\n",
    "We compare two training strategies:\n",
    "\n",
    "**Experiment A - From Scratch:**\n",
    "- Initialize weights randomly\n",
    "- Train end-to-end on supervised price prediction task\n",
    "- Baseline for neural network performance\n",
    "\n",
    "**Experiment B - Pre-trained:**\n",
    "- **Phase 1:** Unsupervised pre-training using an autoencoder to learn feature representations\n",
    "- **Phase 2:** Fine-tune the pre-trained encoder on the supervised price prediction task\n",
    "- Tests whether unsupervised feature learning improves performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. Task Difficulty\n",
    "\n",
    "- **Limited Training Data:** 60k samples is relatively small for deep learning\n",
    "\n",
    "- **Heterogeneous Features:** Mixed numerical and categorical features require careful preprocessing\n",
    "\n",
    "- **Non-Smooth Decision Boundaries:** Car prices exhibit discontinuities (e.g., brand premium jumps)\n",
    "\n",
    "- **Training Dynamics:** Unsupervised pre-training may learn features irrelevant to price prediction  \n",
    "\n",
    "- **Hyperparameter Sensitivity:** Learning rates, architecture depth, and regularization critically impact performance  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 3. Implementation Details\n",
    "\n",
    "### Architecture Design\n",
    "\n",
    "#### Autoencoder:\n",
    "```\n",
    "Input: 26 features → Encoder → 12 compressed features → Decoder → 26 reconstructed features\n",
    "```\n",
    "\n",
    "**Encoder:**\n",
    "- Linear layer: 26 → 12 dimensions (2.17× compression)\n",
    "- Activation: ReLU (non-linearity, prevents negative values)\n",
    "- Objective: Learn compressed representations that preserve information\n",
    "\n",
    "**Decoder:**\n",
    "- Linear layer: 12 → 26 dimensions (reconstruction)\n",
    "- No activation: Allow negative values for normalized features\n",
    "- Objective: Reconstruct original input from compressed representation\n",
    "\n",
    "**Pre-training Loss:** MSE (Mean Squared Error)\n",
    "- Minimizes: ||x - decoder(encoder(x))||²\n",
    "- Goal: Force encoder to learn informative 12-dimensional features\n",
    "\n",
    "\n",
    "\n",
    "#### Price Predictor:\n",
    "```\n",
    "Input: 26 features → Encoder (12) → Prediction Head (16) → Output: Price\n",
    "```\n",
    "\n",
    "**Encoder:** \n",
    "- Architecture: 26 → 12 (identical to autoencoder encoder)\n",
    "- Initialization: Random (Experiment A) or Pre-trained (Experiment B)\n",
    "- Training: Unfrozen - weights continue updating\n",
    "\n",
    "**Prediction Head:**\n",
    "- Layer 1: Linear(12 → 16) + ReLU + Dropout(0.3)\n",
    "- Layer 2: Linear(16 → 1)\n",
    "- Regularization: 30% dropout prevents overfitting\n",
    "\n",
    "**Supervised Loss:** L1/MAE (Mean Absolute Error)\n",
    "- Minimizes: |y_true - y_pred|\n",
    "- Robust to outliers compared to MSE\n",
    "- Directly optimizes evaluation metric\n",
    "\n",
    "\n",
    "\n",
    "### Training Configuration\n",
    "- **Phase 1: Unsupervised Pre-training (Experiment B only)** 50 epochs, MSE loss, learn feature representations without supervision\n",
    "- **Phase 2: Supervised Training (Both experiments)** 50 epochs, MAE loss, Optimize price prediction performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 4. Discussion\n",
    "\n",
    "### Discussion Themes\n",
    "\n",
    "**Performance Analysis:**\n",
    "- How does pre-training affect validation MAE?\n",
    "\n",
    "- Pre-training shows faster early convergence with lower MAE during initial epochs, but training from scratch ultimately achieves slightly better final performance after 50 epochs (MAE: £2,125 vs £2,105, 1,0% difference). This training dynamic suggests that while unsupervised pre-training provides better weight initialization, the advantage diminishes as supervised training progresses. For our tabular car price dataset, the final performance difference is not statistically significant, indicating that unsupervised feature learning provides minimal long-term benefit over end-to-end supervised learning.\n",
    "\n",
    "**Feature Learning:**\n",
    "- Does the autoencoder learn meaningful price-relevant features?\n",
    "\n",
    "- While the autoencoder successfully learns to reconstruct input features (achieving low reconstruction loss), these learned representations do not translate to improved price prediction performance. This indicates a mismatch between reconstruction objectives and regression objectives: features optimized for minimizing MSE reconstruction error are not necessarily optimal for predicting car prices. The autoencoder captures general data structure rather than price-relevant patterns.\n",
    "\n",
    "**Limitations**\n",
    "- **Single architecture tested:** Results may not generalize to deeper networks\n",
    "- **Fixed hyperparameters:** No extensive tuning performed\n",
    "- **Modest dataset size:** 60k training samples may be insufficient for neural networks to show their full potential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Comparison to State-of-the-Art Tabular Pre-training\n",
    "\n",
    "Recent literature proposes more sophisticated pre-training approaches than our simple reconstruction-based autoencoder.\n",
    "\n",
    "#### Why Advanced Methods Were Not Implemented\n",
    "\n",
    "**XTab (Zhu et al., 2023) - Cross-Table Pre-training:**\n",
    "- **Concept:** Pre-train a single transformer on 50+ diverse datasets simultaneously\n",
    "- **Benefit:** Learn general tabular patterns through cross-domain transfer\n",
    "- **Why Not Feasible:** \n",
    "  - Requires 20-50+ datasets\n",
    "  - Beyond project scope and computational resources\n",
    "\n",
    "**SAINT (Somepalli et al., 2021) - Advanced Single-Dataset Pre-training:**\n",
    "- **Concept:** Contrastive learning + intersample attention (samples attend to each other)\n",
    "- **Benefit:** Better pre-training objective than reconstruction\n",
    "- **Why Not Implemented:**\n",
    "  - Implementation complexity (intersample attention mechanism)\n",
    "  - We focused on a baseline comparison first\n",
    "  - Time constraints\n",
    "\n",
    "#### Key Insight from Literature\n",
    "\n",
    "**Reconstruction loss (our approach):**\n",
    "- Optimizes: Input similarity (minimize ||x - x̂||²)\n",
    "- Learns: General data structure\n",
    "- Problem: Not aligned with regression task\n",
    "\n",
    "**Contrastive loss (SAINT approach):**\n",
    "- Optimizes: Sample similarity (similar samples close, different samples far)\n",
    "- Learns: Task-relevant feature representations\n",
    "- Benefit: Better aligned with prediction tasks\n",
    "\n",
    "SAINT's success on some tabular benchmarks suggests that contrastive pre-training—not reconstruction—is the key to helping neural networks on tabular data.  \n",
    "Our simple autoencoder establishes the baseline: basic pre-training provides minimal benefit, motivating investigation into sophisticated objectives like contrastive learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019).  \n",
    "**BERT: Pre-training of deep bidirectional transformers for language understanding**.  \n",
    "In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019)* (pp. 4171–4186). Association for Computational Linguistics.  \n",
    "https://doi.org/10.18653/v1/N19-1423\n",
    "\n",
    "Somepalli, G., Goldblum, M., Schwarzschild, A., Bruss, C. B., & Goldstein, T. (2021).  \n",
    "**SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training**.  \n",
    "*arXiv preprint*. https://arxiv.org/abs/2106.01342\n",
    "\n",
    "Grinsztajn, L., Oyallon, E., & Varoquaux, G. (2022).  \n",
    "**Why do tree-based models still outperform deep learning on tabular data?**  \n",
    "*arXiv preprint*. https://doi.org/10.48550/arXiv.2207.08815\n",
    "\n",
    "Zhu, B., Shi, X., Erickson, N., Li, M., Karypis, G., & Shoaran, M. (2023).  \n",
    "**XTab: Cross-table pretraining for tabular transformers**.  \n",
    "In *Proceedings of the 40th International Conference on Machine Learning (ICML 2023)*.  \n",
    "*Proceedings of Machine Learning Research, 202*. https://proceedings.mlr.press/v202/zhu23o.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "  ---\n",
    "\n",
    "  ## Table of Contents\n",
    "  1. [Imports and Setup](#1-imports-and-setup)\n",
    "  2. [Data Loading and Preprocessing](#2-data-loading-and-preprocessing)\n",
    "  3. [Autoencoder Architecture](#3-autoencoder-architecture)\n",
    "  4. [Pre-training for Autoencoder](#4-pre-training-for-autoencoder)\n",
    "  5. [Price Prediction](#5-price-prediction)\n",
    "  6. [Supervised Training](#6-supervised-training)\n",
    "  7. [Experiments](#7-experiments)\n",
    "     - [A: DL From Scratch](#a-dl-from-scratch)\n",
    "     - [B: DL with pre-trained Autoencoder](#b-dl-with-pre-trained-autoencoder)\n",
    "  8. [Results and Comparison](#8-results-and-comparison)\n",
    "\n",
    "  ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Custom preprocessing pipeline\n",
    "from pipeline_functions import CarDataCleaner\n",
    "import pickle\n",
    " \n",
    "# Reproducibility\n",
    "import random\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "           Set all random seeds for reproducible results across \n",
    "       different libraries.\n",
    "           This ensures that experiments can be replicated with \n",
    "       identical results.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADING\n",
    "# Load the raw training dataset from CSV file\n",
    "df_train = pd.read_csv('train.csv')\n",
    "\n",
    "# Initialize the data cleaning pipeline\n",
    "cleaner = CarDataCleaner(handle_electric=\"other\", set_carid_index=False)\n",
    "\n",
    "# Apply data cleaning transformations\n",
    "df_train = cleaner.fit_transform(df_train)\n",
    "\n",
    "# Separate features (X) from target variable (y)\n",
    "X = df_train.drop(columns=['price'])\n",
    "y = df_train['price']\n",
    "\n",
    "# X = X.rename(columns={\n",
    "#     'Brand': 'brand'\n",
    "# })\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data loaded and cleaned\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# Load existing preprocessing pipeline\n",
    "with open('preprocessor_pipe.pkl', 'rb') as f: # The main notebook has to be run to create this file. It is not included in the zip due to its file size.\n",
    "    preprocessor = pickle.load(f)\n",
    "\n",
    "# Transform training data & validation data\n",
    "# Learns the parameters from training data and applies transformation\n",
    "X_train_transformed = preprocessor.fit_transform(X_train, y_train)\n",
    "\n",
    "# Only applies the transformation without learning new parameters (prevents data leakage)\n",
    "X_val_transformed = preprocessor.transform(X_val)\n",
    "\n",
    "# Convert data to PyTorch tensors for neural network training\n",
    "X_train_tensor = torch.FloatTensor(X_train_transformed)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1)\n",
    "X_val_tensor = torch.FloatTensor(X_val_transformed)\n",
    "y_val_tensor = torch.FloatTensor(y_val.values).reshape(-1, 1)\n",
    "\n",
    "print(\"Data preprocessed\")\n",
    "print(f\"Input dimension: {X_train_tensor.shape[1]}\")\n",
    "print(f\"Train tensor shape: {X_train_tensor.shape}\")\n",
    "print(f\"Validation tensor shape: {X_val_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 3. Autoencoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "        An autoencoder is an unsupervised learning model that learns to compress data \n",
    "        into a lower-dimensional representation (encoding) and then reconstruct the \n",
    "        original data from this compressed representation (decoding).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, encoding_dim=12):\n",
    "        \"\"\"\n",
    "            Initialize the autoencoder architecture.\n",
    "            Args: \n",
    "                input_dim (int): Number of input features (26)\n",
    "                encoding_dim (int): Size of the compressed representation (12)\n",
    "        \"\"\"\n",
    "        super(TabularAutoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim),\n",
    "            nn.ReLU() # ReLU activation adds non-linearity and prevents negative values\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim),\n",
    "            # No ReLU: Decoder needs to reconstruct negative values from normalized features\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: data flows through the autoencoder to reconstruct input features.\n",
    "        \n",
    "        This method is called automatically when you use model(input_data).\n",
    "        Data flows: input features → encoder → decoder → reconstructed features.\n",
    "        \n",
    "        Args:\n",
    "            x: Input car features (batch_size, 26)\n",
    "        Returns:\n",
    "            decoded: Reconstructed car features (batch_size, 26)\n",
    "        \"\"\"\n",
    "        encoded = self.encoder(x)       # 26 -> 12 \n",
    "        decoded = self.decoder(encoded) # 12 -> 26\n",
    "        return decoded                  # Returns decoded features\n",
    "    \n",
    "\n",
    "# Create an instance of the autoencoder\n",
    "input_dim = X_train_tensor.shape[1] # 26 features after preprocessing\n",
    "\n",
    "print(f\"Autoencoder created with input_dim={input_dim}\")\n",
    "print(f\"Encoding dimension: 12 (compression from {input_dim})\")\n",
    "print(f\"Compression ratio: {input_dim/12:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 4. Pre-training for Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-TRAINING FUNCTION\n",
    "def pretrain_autoencoder(model, X_train, X_val, epochs=50, batch_size=256, lr=0.001):\n",
    "    \"\"\"\n",
    "        Pre-training means training the autoencoder to reconstruct input data without using any target labels (prices). \n",
    "        This helps the encoder learn meaningful feature representations that can later be used for price prediction.\n",
    "\n",
    "        Args:\n",
    "            model: The autoencoder model to train\n",
    "            X_train: Training data features\n",
    "            X_val: Validation data features  \n",
    "            epochs: Number of training epochs (full passes through the data)\n",
    "            batch_size: Number of samples processed together in each batch\n",
    "            lr: Learning rate for the optimizer\n",
    "            \n",
    "        Returns:\n",
    "            train_losses: List of training losses for each epoch\n",
    "            val_losses: List of validation losses for each epoch\n",
    "\"\"\"\n",
    "    \n",
    "    \n",
    "    # Set seeds for reproducibility\n",
    "    set_seed(42)\n",
    "    \n",
    "     # Set up the optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # Adam optimizer: Adaptive learning rate algorithm\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Create batches for efficient training\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    \n",
    "    # For autoencoder, both input and target are the same\n",
    "    train_dataset = TensorDataset(X_train, X_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, generator=generator)\n",
    "    \n",
    "    val_dataset = TensorDataset(X_val, X_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Lists to store losses for plotting training progress\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Main training loop\n",
    "    for epoch in range(epochs):\n",
    "        # TRAINING PHASE\n",
    "        model.train() # Enable learning mode (dropout active, gradients computed)\n",
    "        train_loss = 0.0 # Start\n",
    "\n",
    "        # Process training data in batches\n",
    "        for batch_X, batch_X_target in train_loader:\n",
    "            optimizer.zero_grad()                           # Reset gradients from previous batch\n",
    "            reconstructed = model(batch_X)                  # Forward pass: get reconstructed data\n",
    "            loss = criterion(reconstructed, batch_X_target) # Calculate reconstruction error\n",
    "            loss.backward()                                 # Compute gradients (how to improve)\n",
    "            optimizer.step()                                # Update weights (actual learning)\n",
    "            train_loss += loss.item()                       # Accumulate batch loss\n",
    "        \n",
    "        train_loss /= len(train_loader)                     # Average loss across all batches\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # VALIDATION PHASE\n",
    "        model.eval() # Set model to evaluation mode (disables dropout, batch norm updates)\n",
    "        val_loss = 0.0 # Start\n",
    "        with torch.no_grad(): # Process validation data without computing gradients (saves memory)\n",
    "            for batch_X, batch_X_target in val_loader:\n",
    "                reconstructed = model(batch_X)\n",
    "                loss = criterion(reconstructed, batch_X_target)\n",
    "                val_loss += loss.item()  # Accumulate batch loss\n",
    "        \n",
    "        val_loss /= len(val_loader)  # Average validation loss\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "         # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "print(\"Pre-training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 5. Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "        Neural network for car price prediction using encoder features.\n",
    "        Uses an encoder to compress input features to 12 dimensions, then applies a prediction head to estimate car prices. \n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, encoding_dim=12):\n",
    "        \"\"\"\n",
    "          Initialize the price prediction model.\n",
    "          Args:\n",
    "              encoder:Encoder network (fresh or pre-trained from autoencoder)\n",
    "              encoding_dim: Dimension of encoded feature vector (default: 12)\n",
    "        \"\"\"\n",
    "        super(PricePredictor, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder             # Store the Encoder\n",
    "        \n",
    "        # Build Prediction head: encoded features (12) → price prediction\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 16),  # Expand to 16 dimensions (Layer 1)\n",
    "            nn.ReLU(),                    # Non-linear activation\n",
    "            nn.Dropout(0.3),              # Regularization (30% dropout)\n",
    "            nn.Linear(16, 1)              # Final price output (Layer 2)\n",
    "          )\n",
    "    \n",
    "    def forward(self, x):\n",
    "      \"\"\"\n",
    "      Forward pass: data flows through the network to predict car prices.\n",
    "      \n",
    "      This method is called automatically when you use model(input_data).\n",
    "      Data flows: input features → encoder → prediction head → price output.\n",
    "      \n",
    "      Args: \n",
    "        x: Input car features (batch_size, 26)\n",
    "      Returns: \n",
    "        price: Predicted car prices (batch_size, 1) \n",
    "      \"\"\"\n",
    "    \n",
    "      encoded = self.encoder(x)    # Transform the Data: (batch_size, 26) → (batch_size, 12)\n",
    "      price = self.head(encoded)   # Uses the prediction head for price estimate: (batch_size, 12) → (batch_size, 1)\n",
    "      return price\n",
    "\n",
    "print(\"PricePredictor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 6. Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predictor(model, X_train, y_train, X_val, y_val, epochs=100, batch_size=256, lr=0.001):\n",
    "    \"\"\"\n",
    "      Train the price prediction model using supervised learning.\n",
    "      \n",
    "      Performs the actual learning process: \n",
    "      repeatedly shows the model car features and their true prices, \n",
    "      then adjusts the model's weights to improve predictions.\n",
    "      \n",
    "      Args:\n",
    "          model: PricePredictor model to train\n",
    "          X_train: Training car features  \n",
    "          y_train: True training prices\n",
    "          X_val: Validation car features\n",
    "          y_val: True validation prices\n",
    "          epochs: Number of complete passes through training data\n",
    "          batch_size: Number of samples processed together\n",
    "          lr: Learning rate\n",
    "          \n",
    "      Returns:\n",
    "          train_losses, val_losses, val_maes: Training progress metrics\n",
    "      \"\"\"\n",
    "    \n",
    "    # Set seeds for reproducibility\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Set up the optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # Adam optimizer: Adaptive learning rate algorithm\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    # Create batches for efficient training\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    \n",
    "    # Training data: features → prices\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "    \n",
    "    # Validation data: features → prices\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Progress tracking: Store losses and metrics for each epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_maes = []\n",
    "    \n",
    "    \n",
    "    # TRAINING LOOP\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase:\n",
    "        model.train() # Enable learning mode (dropout active, gradients computed)\n",
    "        train_loss = 0.0 # Start\n",
    "        for batch_X, batch_y in train_loader:       # Process training data in batches\n",
    "            optimizer.zero_grad()                   # Clear previous gradients\n",
    "            predictions = model(batch_X)            # Make predictions(calls forward)\n",
    "            loss = criterion(predictions, batch_y)  # Calculate prediction error\n",
    "            loss.backward()                         # Compute gradients (how to improve)\n",
    "            optimizer.step()                        # Update weights (actual learning)\n",
    "            train_loss += loss.item()               # Accumulate batch loss\n",
    "        \n",
    "        # Calculate average training loss for this epoch\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval() # Disable learning mode (dropout off, no gradient computation)\n",
    "        val_loss = 0.0 # Start\n",
    "        all_preds = [] # all predictions for MAE\n",
    "        all_targets = [] # all true prices for MAE calculation\n",
    "        with torch.no_grad(): # Process validation data without computing gradients (saves memory)\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Store predictions and true values for metrics\n",
    "                all_preds.append(predictions.numpy())\n",
    "                all_targets.append(batch_y.numpy())\n",
    "        \n",
    "\n",
    "         # Calculate validation metrics\n",
    "        val_loss /= len(val_loader) # Average validation loss\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate Mean Absolute Error in pounds (more interpretable than loss)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        val_mae = mean_absolute_error(all_targets, all_preds)\n",
    "        val_maes.append(val_mae)\n",
    "\n",
    "        # Print progress every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.2f}, Val Loss: {val_loss:.2f}, Val MAE: £{val_mae:.2f}\")\n",
    "    \n",
    "    return train_losses, val_losses, val_maes\n",
    "\n",
    "print(\"Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### A: DL From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT A: TRAINING FROM SCRATCH (NO PRE-TRAINING)\n",
    "print(\"EXPERIMENT A: FROM SCRATCH\")\n",
    "\n",
    "# Reproducible initialization\n",
    "set_seed(42)\n",
    "\n",
    "# Create autoencoder architecture with random initialization and no pre-training)\n",
    "scratch_autoencoder = TabularAutoencoder(input_dim=input_dim, encoding_dim=12)\n",
    "\n",
    "# Build price predictor using the untrained encoder\n",
    "# Architecture: 26 input features → encoder (12 compressed features) → prediction head → price\n",
    "model_scratch = PricePredictor(scratch_autoencoder.encoder, encoding_dim=12)\n",
    "\n",
    "print(\"\\nTraining from scratch...\")\n",
    "# Train the complete pipeline end-to-end using supervised learning\n",
    "train_losses_scratch, val_losses_scratch, val_maes_scratch = train_predictor(\n",
    "    model_scratch, \n",
    "    X_train_tensor, \n",
    "    y_train_tensor, \n",
    "    X_val_tensor, \n",
    "    y_val_tensor,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# Extract best performance metrics for comparison\n",
    "best_mae_scratch = min(val_maes_scratch)\n",
    "best_epoch_scratch = np.argmin(val_maes_scratch) + 1\n",
    "print(f\"\\nBest Validation MAE (From Scratch): £{best_mae_scratch:.2f} (Epoch {best_epoch_scratch})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### B: DL with pre-trained Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT B: PRE-TRAINING PHASE\n",
    "print(\"EXPERIMENT B: WITH PRE-TRAINING\")\n",
    "\n",
    "# Reproducible initialization\n",
    "set_seed(42)\n",
    "\n",
    "# Create autoencoder with identical architecture to Experiment A\n",
    "pretrained_autoencoder = TabularAutoencoder(input_dim=input_dim, encoding_dim=12)\n",
    "\n",
    "print(\"\\nPre-training autoencoder...\")\n",
    "\n",
    "# Unsupervised pre-training - learn feature representations\n",
    "train_losses_pretrain, val_losses_pretrain = pretrain_autoencoder(\n",
    "    pretrained_autoencoder,\n",
    "    X_train_tensor,\n",
    "    X_val_tensor,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "print(\"\\nPre-training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT B: FINE-TUNING WITH PRE-TRAINED ENCODER\n",
    "# Build price predictor using the pre-trained encoder\n",
    "# The encoder weights are initialized with learned representations, not random values\n",
    "model_pretrained = PricePredictor(pretrained_autoencoder.encoder, encoding_dim=12)\n",
    "\n",
    "\n",
    "print(\"\\nFine-tuning with pre-trained encoder...\")\n",
    "# Train the complete pipeline end-to-end using supervised learning\n",
    "train_losses_pretrained, val_losses_pretrained, val_maes_pretrained = train_predictor(\n",
    "    model_pretrained,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# Extract best performance metrics for comparison\n",
    "best_mae_pretrained = min(val_maes_pretrained)\n",
    "best_epoch_pretrained = np.argmin(val_maes_pretrained) + 1\n",
    "print(f\"\\nBest Validation MAE (Pre-trained): £{best_mae_pretrained:.2f} (Epoch {best_epoch_pretrained})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 8. Results and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS SUMMARY\n",
    "\n",
    "# Compile results from both experiments \n",
    "results = {\n",
    "    'Model': ['From Scratch', 'Pre-trained'],\n",
    "    'Validation MAE (£)': [best_mae_scratch, best_mae_pretrained],\n",
    "    'Improvement vs Scratch': [\n",
    "        0.0,\n",
    "        ((best_mae_scratch - best_mae_pretrained) / best_mae_scratch) * 100\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create comparison table for clear result interpretation\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "# Calculate absolute improvement in prediction accuracy\n",
    "improvement = best_mae_scratch - best_mae_pretrained\n",
    "print(f\"Pre-training improved MAE by: £{improvement:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION: MODEL COMPARISON\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "# Data\n",
    "models = results_df['Model']\n",
    "maes = results_df['Validation MAE (£)']\n",
    "colors = ['#e74c3c', '#3498db']\n",
    "\n",
    "# Create bars\n",
    "x_pos = [0, 1]\n",
    "bars = ax.bar(x_pos, maes, color=colors, alpha=0.85, width=0.6, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Styling\n",
    "ax.set_ylabel('Validation MAE (£)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Neural Network: Pre-training vs From Scratch', fontsize=15, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(models, fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.set_ylim(0, maes.max() * 1.15)\n",
    "\n",
    "# Add MAE values on bars\n",
    "for i, (bar, mae) in enumerate(zip(bars, maes)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + maes.max()*0.015,\n",
    "            f'£{mae:.0f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add improvement info \n",
    "improvement = best_mae_scratch - best_mae_pretrained  \n",
    "improvement_pct = ((best_mae_scratch - best_mae_pretrained) / best_mae_scratch) * 100 \n",
    "\n",
    "# Highlight the improvement\n",
    "mid_y = (maes[0] + maes[1]) / 2\n",
    "if improvement > 0:\n",
    "    # Arrow showing improvement\n",
    "    ax.annotate('', xy=(1, maes[1]), xytext=(0, maes[0]),\n",
    "                arrowprops=dict(arrowstyle='<->', color='green', lw=3, ls='--'))\n",
    "    \n",
    "    # Improvement text\n",
    "    ax.text(0.5, mid_y, f'−£{improvement:.0f}\\n({improvement_pct:.1f}% better)',\n",
    "            ha='center', va='center', fontsize=11, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', \n",
    "                     edgecolor='darkgreen', linewidth=2, alpha=0.9))\n",
    "else:\n",
    "    # If pre-training made it worse\n",
    "    ax.annotate('', xy=(0, maes[0]), xytext=(1, maes[1]),\n",
    "                arrowprops=dict(arrowstyle='<->', color='red', lw=3, ls='--'))\n",
    "    \n",
    "    ax.text(0.5, mid_y, f'+£{abs(improvement):.0f}\\n({abs(improvement_pct):.1f}% worse)',\n",
    "            ha='center', va='center', fontsize=11, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='lightcoral', \n",
    "                     edgecolor='darkred', linewidth=2, alpha=0.9))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION: TRAINING HISTORY\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Pre-training\n",
    "axes[0].plot(train_losses_pretrain, label='Train Loss', color='#3498db', linewidth=2)\n",
    "axes[0].plot(val_losses_pretrain, label='Val Loss', color='#e74c3c', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Reconstruction Loss (MSE)', fontsize=12)\n",
    "axes[0].set_title('Autoencoder Pre-training', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Right plot: Fine-tuning\n",
    "axes[1].plot(val_maes_scratch, label='From Scratch', color='#e74c3c', linewidth=2)\n",
    "axes[1].plot(val_maes_pretrained, label='Pre-trained', color='#3498db', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Validation MAE (£)', fontsize=12)\n",
    "axes[1].set_title('Supervised Training Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
