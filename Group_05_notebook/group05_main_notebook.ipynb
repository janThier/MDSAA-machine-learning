{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4467ab3-7594-4e02-a19f-8c5e47fb14e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"\n",
    "    background: rgba(25, 25, 25, 0.55);\n",
    "    backdrop-filter: blur(16px) saturate(150%);\n",
    "    -webkit-backdrop-filter: blur(16px) saturate(150%);\n",
    "    border: 1px solid rgba(255, 255, 255, 0.12);\n",
    "    border-radius: 18px;\n",
    "    padding: 45px 30px;\n",
    "    text-align: center;\n",
    "    font-family: 'Inter', 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;\n",
    "    color: #e0e0e0;\n",
    "    box-shadow: 0 0 30px rgba(0, 0, 0, 0.35);\n",
    "    margin: 40px auto;\n",
    "    max-width: 800px;\n",
    "\">\n",
    "\n",
    "  <h1 style=\"\n",
    "      font-size: 2.8em;\n",
    "      font-weight: 700;\n",
    "      margin: 0 0 8px 0;\n",
    "      letter-spacing: -0.02em;\n",
    "      background: linear-gradient(90deg, #00e0ff, #9c7eff);\n",
    "      -webkit-background-clip: text;\n",
    "      -webkit-text-fill-color: transparent;\n",
    "  \">\n",
    "      Machine Learning Project\n",
    "  </h1>\n",
    "\n",
    "  <h2 style=\"\n",
    "      font-size: 1.6em;\n",
    "      font-weight: 500;\n",
    "      margin: 0 0 25px 0;\n",
    "      color: #b0b0b0;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Cars 4 You - Predicting Car Prices\n",
    "  </h2>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.25em;\n",
    "      font-weight: 500;\n",
    "      color: #c0c0c0;\n",
    "      margin-bottom: 6px;\n",
    "  \">\n",
    "      Group 5 - Lukas Belser, Samuel Braun, Elias Karle, Jan Thier\n",
    "  </p>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.05em;\n",
    "      font-weight: 400;\n",
    "      color: #8a8a8a;\n",
    "      font-style: italic;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Machine Learning End Results · 22.12.2025\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372c87aa-b7f3-4a34-a596-bb5e584db114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Table of Contents**\n",
    "\n",
    "- [0. Import Packages and Data](#0-import-packages-and-data)\n",
    "  - [0.1 Import Required Packages](#01-import-required-packages)\n",
    "  - [0.2 Load Datasets](#02-load-datasets)\n",
    "  - [0.3 Kaggle Setup](#03-kaggle-setup)\n",
    "\n",
    "- [I. Identifying Business Needs](#i-identifying-business-needs)\n",
    "\n",
    "- [II. Data Exploration and Preprocessing](#ii-data-exploration-and-preprocessing)\n",
    "  - [2.1 Data Split](#21-data-split)\n",
    "  - [2.2 Data Cleaning](#22-data-cleaning)\n",
    "  - [2.3 Outlier Handling](#23-outlier-handling)\n",
    "  - [2.4 Missing Values Handling](#24-missing-values-handling)\n",
    "  - [2.5 Feature Engineering](#25-feature-engineering)\n",
    "  - [2.6 Encoding, Transforming and Scaling](#26-encoding-transforming-and-scaling)\n",
    "  - [2.7 Create Preprocessing Pipeline](#27-create-preprocessing-pipeline)\n",
    "  - [2.8 Summary of the Pipelines Before Modeling](#28-summary-of-the-pipelines-before-modeling)\n",
    "\n",
    "- [III. Regression Benchmarking](#iii-regression-benchmarking)\n",
    "  - [3.1 Model Assessment Strategy and Metrics](#31-model-assessment-strategy-and-metrics)\n",
    "  - [3.2 Feature Selection Strategy and Results](#32-feature-selection-strategy-and-results)\n",
    "  - [3.3 Model Comparison](#33-model-comparison)\n",
    "    - [3.3.1 Model Setup (Original vs Optimized Preprocessing)](#331-model-setup-original-vs-optimized-preprocessing)\n",
    "    - [3.3.2 Run the Models](#332-run-the-models)\n",
    "    - [3.3.3 Baseline Model Comparison Discussion of Results](#333-baseline-model-comparison-discussion-of-results)\n",
    "  - [3.4 Optimization: Hyperparameter Tuning of Top Models](#34-optimization-hyperparameter-tuning-of-top-models)\n",
    "    - [3.4.1 RandomForest](#341-randomforest)\n",
    "    - [3.4.2 Extra Trees](#342-extra-trees)\n",
    "    - [3.4.3 Histogram Gradient Boosting](#343-histogram-gradient-boosting)\n",
    "    - [3.4.4 Comparison of Tuned Models](#344-comparison-of-tuned-models)\n",
    "    - [3.4.5 Optimize Best Model Further on Absolute Error](#345-optimize-best-model-further-on-absolute-error)\n",
    "  - [3.5 Visualizations and Insights of the Entire Pipeline](#35-visualizations-and-insights-of-the-entire-pipeline)\n",
    "    - [3.5.1 Start](#351-start)\n",
    "    - [3.5.2 Cleaning](#352-cleaning)\n",
    "    - [3.5.3 [Unused] Outlier Handling](#353-unused-outlier-handling)\n",
    "    - [3.5.4 Missing Values Handling](#354-missing-values-handling)\n",
    "    - [3.5.5 Feature Engineering](#355-feature-engineering)\n",
    "    - [3.5.6 Transformation, Scaling, Encoding](#356-transformation-scaling-encoding)\n",
    "    - [3.5.7 Feature Selection](#357-feature-selection)\n",
    "    - [3.5.8 Visualize Entire Pipeline Before Adding the Model](#358-visualize-entire-pipeline-before-adding-the-model)\n",
    "    - [3.5.9 Model](#359-model)\n",
    "\n",
    "- [IV. Open-Ended Section](#iv-open-ended-section)\n",
    "  - [4.1 SHAP Interpretability for Our Final Tree Model](#41-shap-interpretability-for-our-final-tree-model)\n",
    "  - [4.2 Global vs Brand- and Model-Specific Models](#42-global-vs-brand--and-model-specific-models)\n",
    "    - [4.2.1 Load Final Tuned Pipeline, Define Key Columns](#421-load-final-tuned-pipeline-define-key-columns)\n",
    "    - [4.2.2 Brand-Level Comparison (Global vs Brand-Specific)](#422-brand-level-comparison-global-vs-brand-specific)\n",
    "    - [4.2.3 Brand-Model Comparison (Global vs Pair-Specific)](#423-brand-model-comparison-global-vs-pair-specific)\n",
    "  - [4.3 Price Predictor Web Application](#43-price-predictor-web-application)\n",
    "  - [4.4 Deep Learning](#44-deep-learning)\n",
    "\n",
    "- [V. Deployment and Prediction on Test Set](#v-deployment-and-prediction-on-test-set)\n",
    "\n",
    "- [VI. Discussion and Outlook](#vi-discussion-and-outlook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Group Member Contribution**    \n",
    " \n",
    "Jan (25%):\n",
    "- EDA\n",
    "- Pipeline structure (Data Split)\n",
    "- Missing Values Handling\n",
    "- Feature Engineering\n",
    "- Encoding, Transforming and Scaling\n",
    "- Feature Selection\n",
    "- Model Assessment Strategy\n",
    "- Baseline Model Comparison\n",
    "- Hyperparameter Tuning of Top Candidates\n",
    "- Comparison of Tuned Models\n",
    "- Deployment and Prediction on Test Set\n",
    "- Visualizations and Insights of the Data Preparation Pipeline\n",
    "- Discussion and Outlook\n",
    " \n",
    "Samu (25%):\n",
    "- Pipeline chart and Introduction Markdowns\n",
    "- Data Cleaning\n",
    "- Outlier Handling\n",
    "- Feature Engineering\n",
    "- Encoding, Transforming and Scaling\n",
    "- Model Assessment Strategy\n",
    "- Baseline Model Comparison\n",
    "- Deployment and Prediction on Test Set\n",
    "- Open End: SHAP Feature Importance\n",
    "- Open End: Brand-specific model comparison\n",
    " \n",
    "Lukas (25%):\n",
    "- EDA\n",
    "- Missing Values Handling\n",
    "- Feature Engineering\n",
    "- Encoding, Transforming and Scaling\n",
    "- Feature Selection\n",
    "- Model Assessment Strategy\n",
    "- Baseline Model Comparison\n",
    "- Hyperparameter Tuning of Top Candidates\n",
    "- Open End: Brand-specific model comparison\n",
    " \n",
    "Elias (25%):\n",
    "- Baseline Model Comparison\n",
    "- Hyperparameter Tuning of Top Candidates\n",
    "- Comparison of Tuned Models\n",
    "- Deployment and Prediction on Test Set\n",
    "- Visualizations and Insights of Best Model and the Data\n",
    "- Open End: SHAP Feature Importance\n",
    "- Open End: Analytical Interface\n",
    "- Open End: Deep-Learning\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract\n",
    "\n",
    "The Cars4You project aims to accelerate and standardize used-car price evaluations by replacing manual, subjective pricing with a production-ready machine learning pipeline. Our objective was to optimize predictive accuracy (MAE) on unseen cars while ensuring robustness to wrong inputs and a leakage-free evaluation.\n",
    "\n",
    "Our EDA containing univariate, bivariate and multivariate analysis showed three dominant challenges: (1) inconsistent raw entries (typos, invalid ranges, sparse categories), (2) strong segmentation effects by brand/model, and (3) heavy-tailed numeric distributions (notably mileage and price). \n",
    "\n",
    "We addressed these with a custom engineered and reproducible sklearn pipeline. It follows the state of the art pipeline architecture and consists the following transformers: \n",
    "\n",
    "Deterministic cleaning and category canonicalization with `CarDataCleaner` and hierarchical imputation with `IndividualHierarchyImputer`. We then added domain-informed feature engineering with `CarFeatureEngineer` to encode depreciation, usage intensity, efficiency/performance ratios, interaction effects, and relative positioning within brand/model segments.\n",
    "Encoding and scaling were consolidated in a `ColumnTransformer` combining selective log transforms, `RobustScaler`, one-hot encoding, mean and median target encoding for categorical structure.\n",
    "To reduce noise and improve generalization, we implemented automated feature selection as a dedicated pipeline stage: VarianceThreshold followed by majority voting across complementary selectors (Spearman relevance+redundancy, mutual information, and tree-based importance).\n",
    "\n",
    "All model selection and tuning followed a consistent 5-fold cross-validation protocol. In addition to the primary evaluation metric MAE, we also evaluated RMSE and R2. After a first run of different models on original and engineered features, further hyperparameter tuning on the best models was decided (RF, ET, HGB). The final tuned pipeline improved substantially over a naive median baseline (MAE ≈ 6.8k), achieving approximately **£1.2k MAE** in cross-validation.\n",
    "\n",
    "For detailed methodological reasoning, trade-offs and further findings refer to the respective sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual representation of the approach we followed in this notebook:\n",
    "\n",
    "<img src=\"images/process_ML.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual representation of our technical pipeline (Data Preparation & Modelling):\n",
    "\n",
    "<img src=\"images/pipeline-chart.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "\n",
    "The code is publicly available on github: https://github.com/janThier/MDSAA-machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!pip install shap\n",
    "!pip install -U scikit-learn\n",
    "!pip install category_encoders\n",
    "!pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    " \n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder, StandardScaler, FunctionTransformer, RobustScaler\n",
    "from category_encoders import QuantileEncoder\n",
    "from category_encoders.wrapper import NestedCVWrapper\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
    " \n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({\"figure.max_open_warning\": 0, \"figure.dpi\": 100})\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "from pipeline_functions import CarDataCleaner, IndividualHierarchyImputer, CarFeatureEngineer, DebugTransformer, MajorityVoteSelectorTransformer, MutualInfoThresholdSelector, SpearmanRelevancyRedundancySelector, create_model_pipe, model_hyperparameter_tuning, SetOutputCompatibleWrapper, run_quick_randomsearch\n",
    "from visualization_functions import plot_selector_agreement, plot_train_val_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_train = pd.read_csv(\"train.csv\").rename(columns={\"Brand\": \"brand\",\n",
    "                                                        \"paintQuality%\": \"paintQuality\"})\n",
    "df_cars_test = pd.read_csv(\"test.csv\").rename(columns={\"Brand\": \"brand\",\n",
    "                                                       \"paintQuality%\": \"paintQuality\"})\n",
    "\n",
    "# Check for duplicates in carID column\n",
    "print(f\"Number of duplicate carIDs in training data: {df_cars_train['carID'].duplicated().sum()}\")\n",
    "print(f\"Total rows in training data: {len(df_cars_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3 Kaggle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing kaggle.json (add kaggle.json api token)\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/Workspace/Users/X@novaims.unl.pt\"\n",
    "\n",
    "# Test\n",
    "!echo $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Identifying Business Needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context**       \n",
    "Cars 4 You is an online car-resale company that buys cars from many brands. Sellers submit car details online, and the company traditionally relies on a mechanic inspection before making a purchase offer. Company growth has increased waiting lists for inspections, which risks losing potential customers to competitors. The business need is therefore to **expedite pricing** by generating a reliable **pre-inspection price estimate** directly from user-provided inputs.\n",
    "\n",
    "**Main goals**     \n",
    "**Business goal:** provide a fast, consistent price estimate at intake to reduce inspection bottlenecks and improve conversion.\n",
    "\n",
    "**ML goal:** train a supervised **regression** model to predict `price` (GBP) from features available at form submission time. Because `paintQuality%` is filled by a mechanic during evaluation, it is treated as **non-production** input and is excluded from the deployed prediction path.\n",
    "\n",
    "**Overall process (end-to-end)**      \n",
    "1. **Data intake:** use the 2020 training dataset (features + target `price`) to develop and validate models; use the provided test dataset (features only) for final predictions and Kaggle submission.\n",
    "2. **EDA -> preprocessing decisions:** identify data inconsistencies, missingness patterns, segmentation by brand/model, and heavy-tailed variables; translate insights into pipeline steps.\n",
    "3. **Leakage-safe pipeline:** implement data cleaning, hierarchical imputation, feature engineering, encoding/scaling, and feature selection as sklearn transformers inside a single `Pipeline` so that every step is fitted only on training folds.\n",
    "4. **Model benchmarking & optimization:** compare candidate regressors on a consistent protocol. Tune the most promising models, select the most generalizable pipeline based on validation performance.\n",
    "5. **Deployment output:** fit the selected pipeline on the full training set and generate predictions for `test.csv` to produce the final submission `.csv`.\n",
    "\n",
    "**Model assessment strategy:**     \n",
    "We adopt the same **5-fold cross-validation (CV)** on the training set as the single assessment strategy used throughout benchmarking and tuning. The primary metric is **MAE** (business-interpretable error in GBP), with RMSE and R-squared reported as complementary diagnostics. The Kaggle test set functions as an **external holdout** for the final chosen pipeline (labels hidden are hidden -> performance observed via leaderboard score).\n",
    "\n",
    "For more details on the respective steps in the pipeline, refer to the specific part in the notebook below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Exploration:** For the analysis of the data received including the consequences for preprocessing, refer to notebook `group05_exploratory_data_analysis.ipynb`. These insights are then used to clean and prepare the data. In addition we redid the EDA after each preprocessing step to visualize the impact and debug the pipeline. This process can be inspected in Section 3.5.     \n",
    "\n",
    "**Top 3 EDA Key Insights:**\n",
    "- Multiple **data errors and logical inconsistencies** that will be handled in Data Cleaning (e.g. mileage < 0 -> set to NaN)\n",
    "- Some features, especially mileage and the target price, are **right-skewed** -> log-transform\n",
    "- The target has high **correlations** with year, mileage and engineSize while other features (previousOwners) have no correlation. There is also a high correlation between features (e.g. spearman of roughly -0.8 for mileage and year) -> handled by our Feature Selection\n",
    "\n",
    "**Preprocessing:** The steps taken to clean and prepare the data based on exploration are described and justified in the following respective Subsections on Data Preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7890fb35-e1ca-463b-b83b-262df6947367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c7ca8f-1b74-4848-89b9-2bac509cb4db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Train and Val**: We use `Cross-Validation` in the `sklearn pipeline` on the available training data to make use of all data while validating different approaches.    \n",
    "(We fix the random states everywhere to ensure that all models use the same split for a fair model comparison)\n",
    "\n",
    "**Test**: We use the external hold-out set from kaggle as final test set (remains completely unseen to avoid leakage)\n",
    "-> An additional hold-out set is therefore not necessary and would waste training data\n",
    "\n",
    "Final setup:\n",
    "1. **Training Set (n-1 folds from CV)**: Used to fit models.\n",
    "2. **Validation Set (1 fold from CV)**: Used to evaluate performance of models and tune hyperparameters, detect overfitting. \n",
    "3. **Test Set (Kaggle)**: Used only once at the end of the entire process to evaluate final model performance. Not considered before to prevent leakage.\n",
    "\n",
    "\n",
    "\n",
    "<u>Place in the pipe:</u> The split is decided right in the beginning because the data has to be split before the preprocessing steps to avoid data leakage. All of the following steps are part of the sklearn pipeline while the CV is not an explicit part of the pipeline but rather the technique that calls the pipeline with its separate folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c64124-1444-462c-a986-0d06cdb71d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create CV (shuffle to ensure randomness in splits, random_state to make it reproducible and comparable across models)\n",
    "rs = 5\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=rs)\n",
    "# => This cv will be passed for hyperparameter tuning later when training the models\n",
    "\n",
    "# Split features and target\n",
    "X_train = df_cars_train.drop(columns='price')\n",
    "y_train = df_cars_train['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8287784a-0d6e-4a6b-87d1-ee92ec55485f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**    \n",
    "CV achieves better results than using a single hold-out set\n",
    "\n",
    "**Consequences/Interpretation:**     \n",
    "Usage of all available data enables a better model comparison 'wasting' training data for a hold-out set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53a2a97-4662-40a3-8b24-b238dad5138e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our approach:**\n",
    "- We `clean data inconsistencies` and data entry errors that we found in the EDA\n",
    "- These columns will be `set to NaN` for that specific entry to not lose rows in the data due to removing\n",
    "- Afterwards, these values will be imputed (see Section 2.3)\n",
    "\n",
    "____\n",
    "\n",
    "**Numerical Features**\n",
    "\n",
    "| **Feature** | **Allowed thresholds** | **Reasoning** | **# filtered below threshold** | **# filtered above threshold** |\n",
    "| :--- | :--- | :--- | :---: | :---: |\n",
    "| `year` | 1886 to 2020 | The first automobile is dated to 1886 (Benz Patent Motor Car), so earlier values are implausible; the dataset is from 2020, so newer model years are logically impossible. [1] | 0 | 358 |\n",
    "| `mileage` | ≥ 0 | Negative mileage is not possible. | 369 | - |\n",
    "| `tax` | ≥ 0 | Negative tax is not possible. | 378 | - |\n",
    "| `mpg` | 5 to 150 | Lower bound 5 mpg is a conservative “sanity floor” below the least-efficient passenger car on FuelEconomy.gov’s list (Bugatti Mistral at 9 mpg combined), so we avoid removing valid low-efficiency cars while filtering implausible entries. [2] Upper bound 150 is a pragmatic cap to reduce leverage from extreme values and potential metric-mixing (e.g., MPGe-style values are defined as an energy-equivalent MPG for plug-in vehicles). [3] Reference point for high-efficiency non-EVs: Prius variants are ~50–56 mpg combined on FuelEconomy.gov. [4] | 49 | 221 |\n",
    "| `engineSize` | 0.1 to 12.7 | Practical bounds: kei-class cars are capped at 660cc (0.66L), giving a grounded “small production car” reference point. [4] Very large historical production engines reach ~12.763L (Bugatti Type 41 / Royale). [5] Lower bound reduced to 0.1L as a conservative data-sanity floor (primarily to remove obvious errors) while avoiding unnecessary loss of potentially valid small-displacement entries. | 264 | 0 |\n",
    "| `paintQuality` | 0 to 100 | Percentage values must be between 0 and 100. | 0 | 367 |\n",
    "| `previousOwners` | ≥ 0 | Negative owner counts are not possible. | 371 | - |\n",
    "| `hasDamage` | - | Only 0 and NaN values in the data -> no thresholding | - | - |\n",
    "\n",
    "\n",
    "[[1]: https://group.mercedes-benz.com/company/tradition/company-history/1885-1886.html [2]: https://www.fueleconomy.gov/feg/best-worst.shtml https://www.fueleconomy.gov/feg/PowerSearch.do?action=noform&baseModel=Prius&make=Toyota&path=1&srchtyp=ymm&year1=2020&year2=2020 [3]: https://www.epa.gov/greenvehicles/fuel-economy-and-ev-range-testing [4]: https://www.fueleconomy.gov/feg/PowerSearch.do?action=noform&baseModel=Prius&make=Toyota&path=1&srchtyp=ymm&year1=2020&year2=2020 [5]: https://www.motortrend.com/features/what-is-a-kei-car [6]: https://www.bugatti-trust.co.uk/bugatti-type-41/]\n",
    "\n",
    "----\n",
    "\n",
    "**Categorial Features**\n",
    "\n",
    "We fix data entry noise (typos, truncations, inconsistent casing) in two stages:\n",
    "\n",
    "**1) Deterministic canonicalization:**\n",
    "\n",
    "We first apply **static mapping tables** (built from our EDA findings on the training data) that collapse variants into one canonical label\n",
    "\n",
    "- normalize formatting: lowercase + trim whitespace  \n",
    "- map known variants/typos → one canonical category  \n",
    "  - example: `\"AUDI\"`, `\"udi\"`, `\"Aud\"` → `Audi`  \n",
    "  - example: `\"semi-aut\"`, `\"emi-auto\"` → `Semi-Auto`\n",
    "\n",
    "This step is **fully deterministic**.\n",
    "\n",
    "**2) Strict validity check:**\n",
    "\n",
    "After canonicalization, we apply a **valid-category filter**:\n",
    "\n",
    "- for high-cardinality `model`, we accept only **known valid model names** (our static canonical model vocabulary)\n",
    "- any model token not in that vocabulary is considered unreliable and is set to **NaN**\n",
    "- this prevents “garbage categories” (rare or malformed strings) from becoming real categories and harming generalization\n",
    "\n",
    "Examples of what gets rejected (→ NaN):\n",
    "- truncations like `\"scirocc\"` (intended: `scirocco`)\n",
    "- incomplete single-letter tokens like `\"a\"`, `\"q\"`, `\"x\"` without enough context\n",
    "\n",
    "**3) Special-rule buckets for ambiguous 1-letter tokens:**\n",
    "\n",
    "Some one-letter model tokens are ambiguous and must not be guessed:\n",
    "\n",
    "- if `brand == Audi` and `model == \"a\"` → set to `a_unknown`\n",
    "- if `brand == Audi` and `model == \"q\"` → set to `q_unknown`\n",
    "- if `brand == BMW` and `model == \"x\"` → set to `x_unknown`\n",
    "\n",
    "If the brand is missing and the token is one of `{a, q, x}`, we do **not** guess and force it to **NaN**.\n",
    "\n",
    "**4) Conservative fuzzy rescue (only for values that are still missing):**\n",
    "\n",
    "Only after steps (1)–(3), we run a very strict “rescue step” to recover obvious typos:\n",
    "\n",
    "- applied **only where the value is still NaN**\n",
    "- candidate choices are restricted to **valid canonical categories** (never invent new labels)\n",
    "- acceptance requires very high similarity (strict cutoff) and/or a unique prefix match  \n",
    "  - example: `\"pum\"` → `puma` (unique prefix, safe)\n",
    "  - example: `\"sl clas\"` → `sl class` (high similarity, safe)\n",
    "- if no safe match exists, the value remains **NaN**\n",
    "\n",
    "**5) Final handling of remaining missing categories:**\n",
    "\n",
    "Any categorical value that is still missing after all steps remains **NaN** and is filled later by our **Imputer** (Section 2.3).\n",
    "\n",
    "_____\n",
    "\n",
    "**Leakage safety:**\n",
    "All “data-driven vocabularies” used in the cleaner are learned inside `fit()` on the training fold only, so the pipeline remains leakage-safe under CV:\n",
    "- the validation fold is never used to decide which categories are “valid”\n",
    "- the same cleaning logic is applied consistently in train/validation/test\n",
    "- the static mapping includes misspellings independent of the training fold and is included for every fold to prevent unnecessary pitfalls in model learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f5aae4cf-4df3-4f71-bff7-8a736d9a5b52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the data cleaning by running it on raw df and inspect uniques\n",
    "cleaner = CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)\n",
    "\n",
    "df_cars_train_clean = cleaner.fit_transform(df_cars_train)\n",
    "df_cars_test_clean  = cleaner.transform(df_cars_test)\n",
    "\n",
    "# Print unique values for visualization after cleaning (more detailed inspection before and after Cleaning can be found in the EDA)\n",
    "print(\"CLEANED TRAIN uniques after removing data errors (example for 'brand')\")\n",
    "print(\"brand\", df_cars_train_clean['brand'].unique())\n",
    "print(\"==> No more typos in brand names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393a3fff-9a33-43c1-979f-e31b6d1c5574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**     \n",
    "The findings on how many values are handled during data cleaning are already included in the table above for an easier overview and direct comparison. Without a well-researched data cleaning approach, our first models performed significantly worse.\n",
    "\n",
    "**Consequences/Interpretation:**     \n",
    "Handling data erros is crucial for effective model training. Otherwise, the models might learn noise and try to find patterns in incorrect data. To identify these data erros, an extensive EDA is inevitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e48e30f-ff20-46b7-94e1-d318088733da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.3 Outlier Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical considerations:**\n",
    "\n",
    "We tried different outlier handling techniques (see `unused_experiments.ipynb`) but decided to not use an explicit technique because they hurt our performance.\n",
    "Besides from an explicit outlier handling step, we use the [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) to scale the features which also handles outliers by centering and scaling the data based on the median and IQR (refer to Section 2.6).\n",
    "\n",
    "<u>Place in the pipe</u>: If an explicit outlier handling strategy were implemented, it would be placed here before imputation to use original distribution for identifying the outliers (otherwise we would inflate the distributions with the imputed values). So, in imputation the original gaps would have been filled based on a distribution that does not includes the massive outliers (skewing the mean/median)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**:      \n",
    "Outlier handling significantly **hurt our best MAE**. The tried techniques and findings are descripted in the `unused_experiments.ipynb` file.\n",
    "\n",
    "**Consequences/Interpretation:**   \n",
    "Due to performance reasons we decided to use **no explicit outlier handling**.      \n",
    "The worse performance due to outlier handling can be explained by the **importance of extreme cars for the algorithm**. When winsorizing or imputing these extreme values, we remove valuable signals. The specific decision to not use outlier handling is based on our best performing tree-based models which are not as sensitive to outliers as other models. When using other approaches that are more sensitive to extreme values, outlier handling could indeed be a valuable technique to improve performance.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0bafff7-43a2-4c3c-81cf-1e54efd19ffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.4 Missing Values Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inidividual Hierarchy per Feature**\n",
    "\n",
    "To impute the NaNs in each cell, an **individual hierarchical approach** is applied for each feature. For this, we compute group statistics (median and mode) of features that are highly correlated with the feature in the missing cell. If one of the required features is not available, we use a fallback to the next level until finally the global dataset statistic (median or mode) is used as a last fallback.    \n",
    "\n",
    "Examples (for an extensive explanation refer to the `IndividualHierarchyImputer` class in pipeline_functions.py):\n",
    "- `brand`: impute the brand using the model and if the model is NaN too, we impute using the grouped mode of (fuelType, transmission) because we identified interaction between these two features and brand in the EDA.   \n",
    "- `mileage`: we impute mileage with the median of the year because of a high spearman corr identified in the EDA (~0.8). If year is not available, we fall back to the global median.\n",
    "\n",
    "The group statistics are only computed on the respective train folds and transformed on the val set to prevent leakage.   \n",
    "-> When refitting the entire model, the entire train set is used to fit and the kaggle test set is transformed using the fitted values\n",
    "\n",
    "<u>Place in the pipe:</u> The Imputation is placed here because the data has to be imputed on original values before engineering new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c48d8a2-2615-4533-9a66-0375bc994fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**       \n",
    "The `IndividualHierarchyImputer` improves performance over the best of our other tried imputers (`SimpleImputer` (mode and median), `KNN` and the custom `GroupImputer` (see unused_experiments.ipynb)) by ~30 MAE on the best baseline models.\n",
    "\n",
    "**Consequences/Interpretation:**       \n",
    "The improved performance over SimpleImputer and the custom GroupImputer show that each feature is **best approximated by a different feature (combination)**. There is **no golden rule** of imputing everything by the median or mode of the model. Instead, the **interaction** (e.g. spearman corr) of the missing feature and the other features should be regarded (e.g. impute mileage with median of year because of high spearman corr)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b198720e-b5a0-46ef-8055-4ac33b852410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.5 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d75d2e08-9cae-4186-b329-d389b4ca8a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**    \n",
    "We implement feature engineering as an sklearn transformer (`CarFeatureEngineer`) **inside the pipeline**. This makes the process **CV-safe / leakage-free**: all fold-specific statistics (e.g., model frequency, mean ages) are learned only on the training fold in `fit()` and applied to the validation fold in `transform()`.     \n",
    "We engineer features with two goals:\n",
    "  1. **Inject domain structure** (age, usage intensity, efficiency, “big engine + old car” effects).\n",
    "  2. **Create stronger signals for models** by expressing ratios and interactions that are difficult to learn reliably from raw variables (e.g. age_rel_model).\n",
    "\n",
    "\n",
    "**Input columns** (after cleaning + imputation):\n",
    "- Numeric: `year`, `mileage`, `tax`, `mpg`, `engineSize`, `previousOwners`\n",
    "- Categorical: `brand`, `model`, `transmission`, `fuelType`\n",
    "- Boolean: `hasDamage`\n",
    "\n",
    "Important design notes: Interaction features use `(age + 1)` to avoid division by zero for cars in the reference year.\n",
    "\n",
    "---\n",
    "\n",
    "| **New Feature** | **Calculation** | **Nature** | **Reasoning** |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| `age` | `ref_year - year` | Base | Captures depreciation; turns a calendar value into a meaningful pricing variable. |\n",
    "| `mpg_x_engine` | `mpg * engineSize` | Interaction (product) | Joint signal for “performance vs efficiency” patterns (high engine + low mpg vs small engine + high mpg). |\n",
    "| `engine_x_age` | `engineSize * (age + 1)` | Interaction (product) | Differentiates large engines in older cars vs newer cars; helps model capture age-dependent valuation of engine size. |\n",
    "| `mpg_x_age` | `mpg * (age + 1)` | Interaction (product) | Captures age-dependent fuel-efficiency patterns (e.g., older fleets / technology differences) that can correlate with price. |\n",
    "| `tax_x_age` | `tax * (age + 1)` | Interaction (product) | Models that tax effects can differ by car age (policy/regime + car segment composition). |\n",
    "| `miles_per_year` | `mileage / (age + 1)` | Interaction (ratio) | Normalizes mileage by lifetime: 60k miles on a 3-year car is very different from 60k on a 10-year car; reduces collinearity between `mileage` and `age`. |\n",
    "| `tax_per_mpg` | `tax / mpg` | Interaction (ratio) | “Cost pressure” proxy: high tax relative to efficiency can reflect segment / running cost patterns. |\n",
    "| `engine_per_mpg` | `engineSize / mpg` | Interaction (ratio) | Performance-style signal: high engine with low mpg tends to indicate sporty/luxury configurations. |\n",
    "| `brand_fuel` | `brand + \"_\" + fuelType` | Interaction (categorical) | Creates configuration groups for target encoding (e.g., Diesel BMW differs from Petrol BMW). |\n",
    "| `brand_trans` | `brand + \"_\" + transmission` | Interaction (categorical) | Creates configuration groups for target encoding (e.g., Automatic Mercedes vs Manual Mercedes). |\n",
    "| `model_freq` | `P(model)` from training fold | Popularity | Approximates market supply/demand stability: common models have more stable pricing; learned CV-safe in `fit()`. |\n",
    "| `age_rel_brand` | `age - mean_age(brand)` | Relative / group-stat | Measures whether a car is newer/older than typical within its brand (brand-relative positioning). |\n",
    "| `age_rel_model` | `age - mean_age(model)` | Relative / group-stat | Measures whether a car is newer/older than typical within its model (model-relative positioning). |\n",
    "| `engine_rel_model` | `engineSize / mean_engineSize(model)` | Relative / group-stat | Captures whether a car is under-/over-engined relative to its model’s typical configuration. |\n",
    "\n",
    "---\n",
    "\n",
    "Legend (feature “Nature”)\n",
    "\n",
    "- **Base Features**: derived from a single original variable (e.g. `age` from `year`)\n",
    "- **Interaction Features**: combine multiple variables to capture non-additive effects\n",
    "  - products (“amplifiers”) and ratios (“normalizers”)\n",
    "- **Popularity Features**: learned from the training fold distribution (e.g. model frequency)\n",
    "- **Relative / Group-stat Features**: compare a car to typical peers within `brand` or `model`\n",
    "  - learned in `fit()` and applied in `transform()` to avoid leakage\n",
    "\n",
    "---\n",
    "\n",
    "Relation to encoding (Target Encoded Features)\n",
    "\n",
    "We also create categorical “group keys” (`brand_fuel`, `brand_trans`) specifically so that our later target-encoding step inside the preprocessing pipeline can learn stable, configuration-specific signals.  \n",
    "This encoding is handled **after** feature engineering and is **CV-safe** because it is part of the pipeline.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a40f2b62-88b2-4530-92e3-86423cb38ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**      \n",
    "The engineered features are important drivers for performance improvement. In addition, to the improved MAE when using feature engineering in the pipeline, the analyzed feature important in the open end section on SHAP supports this insight:\n",
    "- `mpg_x_age` is the third most important feature\n",
    "- `age_rel_model` is the fifth most important feature\n",
    "\n",
    "**Consequences/Interpretation:**      \n",
    "Some relationships are better captured through engineered features than raw original features. For example age_rel_brand captures the relative age of a car compared to the age of other cars in that brand. This provides additional information for the models to learn and find new patterns to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9425a7c0-ec96-4458-a0da-8ef65acb3f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.6 Encoding, Transforming and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4385f8f4-0a8b-4b53-8350-52ee453af34a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- We combine the different treatments for different natures of features in the ColumnTransformer\n",
    "    - Numerics vs. Booleans vs. Categoricals\n",
    "- `Log-Transforming` using [FunctionTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) for skewed Numerics identified in the EDA\n",
    "- `RobustScaler` ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)) for Numerics because it performed better than [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)     \n",
    "    -> Scaling only on training data to avoid data leakage and then scale val and later test set with the fitted scaler of the training set     \n",
    "- `Encoding` for Categoricals:\n",
    "    - Low cardinality: [OHE](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) because of optimized performance on best models. We don't use drop='first' let the FS handle it (better performance for our best tree-based models).\n",
    "    - High Cardinality: [Median TE](https://contrib.scikit-learn.org/category_encoders/quantile.html) and [Mean TE](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html) for optimized performance on best models\n",
    " \n",
    "| **Feature** | Nature | Transformation | Encoding | Scaling |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| age | Numerical | - | - | Robust |\n",
    "| tax | Numerical | - | - | Robust |\n",
    "| mpg | Numerical | - | - | Robust |\n",
    "| engineSize | Numerical | - | - | Robust |\n",
    "| previousOwners | Numerical | - | - | Robust |\n",
    "| mpg_x_engine | Numerical | - | - | Robust |\n",
    "| engine_x_age | Numerical | - | - | Robust |\n",
    "| mpg_x_age | Numerical | - | - | Robust |\n",
    "| tax_x_age | Numerical | - | - | Robust |\n",
    "| engine_per_mpg | Numerical | - | - | Robust |\n",
    "| tax_per_mpg | Numerical | - | - | Robust |\n",
    "| model_freq | Numerical | - | - | Robust |\n",
    "| age_rel_brand | Numerical | - | - | Robust |\n",
    "| age_rel_model | Numerical | - | - | Robust |\n",
    "| engine_rel_model | Numerical | - | - | Robust |\n",
    "| mileage | Numerical | log1p | - | Robust |\n",
    "| miles_per_year | Numerical | log1p | - | Robust |\n",
    "| hasDamage | Boolean | - | - | - |\n",
    "| transmission | Categorical | - | OHE | - |\n",
    "| fuelType | Categorical | - | OHE | - |\n",
    "| brand | Categorical | - | Mean TE **and** Median TE | Robust |\n",
    "| model | Categorical | - | Mean TE **and** Median TE | Robust |\n",
    "| brand_fuel | Categorical | - | Mean TE **and** Median TE | Robust |\n",
    "| brand_trans | Categorical | - | Mean TE **and** Median TE | Robust |\n",
    " \n",
    " \n",
    " \n",
    "All operations are combined in a [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) which applies the different steps to the columns of the data in one unified pipeline (reproducible and prevents leakage)    \n",
    "  -> outputs a combined feature matrix\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original features\n",
    "orig_numeric_features = [\"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\", \"previousOwners\"]\n",
    "orig_boolean = [\"hasDamage\"]\n",
    "orig_categorical_features = [\"brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "\n",
    "# Original and Engineered Features\n",
    "numeric_features = [\n",
    "    \"age\", \"tax\", \"mpg\", \"engineSize\", \"previousOwners\",        \n",
    "    \"mpg_x_engine\", \"engine_x_age\", \"mpg_x_age\", \"tax_x_age\",   \n",
    "    \"engine_per_mpg\", \"tax_per_mpg\",                            \n",
    "    \"model_freq\",\n",
    "    \"age_rel_brand\", \"age_rel_model\", \"engine_rel_model\"\n",
    "]\n",
    "numeric_features_for_log = [\"mileage\", \"miles_per_year\"]\n",
    "boolean_features = [\"hasDamage\"]\n",
    "categorical_features_ohe = [\"transmission\", \"fuelType\"]\n",
    "categorical_features_te_mean = [\"brand\", \"model\", \"brand_fuel\", \"brand_trans\"]\n",
    "categorical_features_te_median = [\"brand\", \"model\", \"brand_fuel\", \"brand_trans\"]\n",
    "unused_columns = [\"year\"] # replaced by age (see Section on Feature Selection for details)\n",
    "\n",
    "all_feature_names_before_encoding = set(numeric_features + numeric_features_for_log + boolean_features + categorical_features_ohe + categorical_features_te_median)\n",
    "print(len(all_feature_names_before_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fc19882e-d4db-4c22-a2d6-7bb290b42eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enc_transf_scale = ColumnTransformer([\n",
    "    (\"log\", Pipeline([\n",
    "        (\"log\", FunctionTransformer(np.log1p, validate=False, feature_names_out=\"one-to-one\")),  # log1p handles zeros safely\n",
    "        (\"scaler\", RobustScaler()),\n",
    "    ]), numeric_features_for_log),\n",
    "\n",
    "    (\"num\", RobustScaler(), numeric_features),\n",
    "\n",
    "    (\"boolean\", \"passthrough\", boolean_features),\n",
    "\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features_ohe), # Use sparse_output=False to get dense array back (e.g. necessary for hgb)\n",
    "\n",
    "    (\"mean_te\", Pipeline([ \n",
    "        (\"encoder\", TargetEncoder(target_type='continuous', cv=5, smooth='auto', random_state=rs)),\n",
    "        (\"scaler\", RobustScaler()),\n",
    "    ]), categorical_features_te_mean),\n",
    "\n",
    "    # Smoothing (m) mitigates but doesnt eliminate leakage, so we use nested cv to work similar to the sklearn TE\n",
    "    (\"median_te\", Pipeline(steps=[\n",
    "        ('median_encoder', SetOutputCompatibleWrapper(NestedCVWrapper(QuantileEncoder(quantile=0.5, m=10.0), cv=cv, random_state=rs))), # not specifying the cols means it encodes all columns\n",
    "        ('scaler', RobustScaler()),\n",
    "    ]), categorical_features_te_median)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0f4406-27be-4204-bd85-653c8d5ac173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.7 Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a716cfa0-d1e3-4978-83f2-183c1a346b94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- We have one `baseline pipe` and one `optimized pipe` to compare basic preprocessing to optimized preprocessing\n",
    "    - The baseline pipe does the minimum for the algorithms to work cleanly (Cleaning, Imputation, Scaling/Encoding)\n",
    "    - The optimized pipe was adjusted iteratively through multiple experiments and trials during the process to optimize model performance\n",
    "- The [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) combines all steps into the preprocessing pipe (see table below)\n",
    "- Through calling the pipeline for data preparation, we ensure that the data is preprocessed independently for each training fold    \n",
    "-> prevent leakage while handling missing values, scaling, encoding, etc.\n",
    "\n",
    "##### Summary of the pipelines before modeling: Baseline (original) vs Optimized\n",
    "\n",
    "| | **Baseline** | **Optimized** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Data cleaning** | `CarDataCleaner` (Section 2.2) | `CarDataCleaner` (Section 2.2) |\n",
    "| **Outlier handling** | - | - |\n",
    "| **Imputation** | SimpleImputer median/mode <br>(simplicity; median more robust than mean)</br> | `IndividualHierarchyImputer` (Section 2.4) |\n",
    "| **Feature engineering** | - | `CarFeatureEngineer` (Section 2.5) |\n",
    "| **Transformation** | - | Log-Transform selected skewed numerics (Section 2.6) |\n",
    "| **Scaling** | StandardScaler <br>(basic choice)</br>| RobustScaler (Section 2.6) |\n",
    "| **Encoding** | OneHotEncoder <br>(simplicity, most straight-forward)</br>| OneHotEncoder + Target Encoding (Section 2.6) |\n",
    "| **Feature selection** | - | VT + `Majority voting` (added in Section 3.2) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_baseline = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    (\"transform\", ColumnTransformer([\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), orig_numeric_features),\n",
    "        (\"bool\", SimpleImputer(strategy=\"most_frequent\"), orig_boolean),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)) # dont use drop='first' for tree models, use sparse_output=False to get dense array back (e.g. necessary for hgb)\n",
    "        ]), orig_categorical_features)\n",
    "    ]))\n",
    "    # [Modeling: see Section 3.3 for details]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6949511c-a9e8-44f6-b9d4-a4d2950fff69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### The entire data preparation pipeline is build incorporating the enc_transf_scale defined in Section 2.6\n",
    "preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", enc_transf_scale),\n",
    "    # [Feature Selection: see Section 3.2 for details]\n",
    "    # [Modeling: see Section 3.3 for details]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "225688cc-6157-4613-bf28-9e430473a3a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### III. Regression Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) Model assessment strategy and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6297f034-638d-4db6-a681-597f20036432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definition of Function for Model Assessment"
    }
   },
   "source": [
    "**Performance Metrics**    \n",
    "\n",
    "Comparison on train and val set to identify potential overfitting:\n",
    "- **MAE** (Mean Absolute Error):     \n",
    "    Used as primary metric because it is the metric used for evaluating in the Kaggle competition. It is easy to interpret in pounds.\n",
    "\n",
    "- **RMSE** (Root Mean Squared Error):    \n",
    "    Used to identify large prediction errors which helps us understand model weaknesses (sensitive to outliers)\n",
    "\n",
    "- **R-squared**:     \n",
    "    Used to compare how well the models explain the underlying data compared to the mean (R-squared=0). It serves as an additional information compared to the two \"error-metrics\" MAE and RMSE.\n",
    "\n",
    "Arguably, **MAPE** could be a well-suited metric for this use-case to identify how far off an prediction is for the specific car. However, as we are focusing on optimizing MAE here, MAPE is out of scope for this experiment.\n",
    "\n",
    "=> All comparisons are based on **identical cross-validation splits** (same seed/folds). Reported values are fold-averaged metrics on train and validation.\n",
    "\n",
    "**Structured approach to identify production model**\n",
    "1. Run RandomizedSearch on Baseline Models iteratively multiple times to get first results of their performance respecting their parameterss\n",
    "2. Compare and discuss model performance\n",
    "3. Select top candidates for further optimization\n",
    "4. Run more extensive Hyperparameter tuning on top candidates\n",
    "5. Select best model and train it with best hyperparameters on minimizing the absolut_error (primary metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2) Feature Selection Strategy and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategy:**     \n",
    "\n",
    "We apply an automatic feature selection approach in addition to the previously removed features (data cleaning, feature engineering)\n",
    "- paintQuality: dropped in cleaning because filled by mechanic so not available for our predictions in production  as the car prediction skips the mechanic\n",
    "- year: dropped after feature engineering because replaced by derived feature 'age'\n",
    "\n",
    "The goal is to create a very robust feature selection approach that finds features that are irrelevant/redundant and therefore generate noise in the model that might lead to overfitting.     \n",
    "To achieve that goal, we apply **two steps**:\n",
    "1) `Variance Threshold` (Filter) to filter constant variables with threshold 0 ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html))    \n",
    "(If using a different value than threshold 0, VarianceThreshold has to be applied before scaling, because e.g. standard scaling leads to std=1)\n",
    "2) Majority voter:\n",
    "    - `Spearman` handles the clean, obvious trends and cleans up redundancy ([docu](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html))\n",
    "    - `MI` catches more complex relations that Spearman might miss ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html))\n",
    "    - `RF Feature Importance` to account for importance of the features ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#selectfrommodel))\n",
    "\n",
    "\n",
    "The different voters capture different aspects:\n",
    "| **Voter** | **Nature** | **Role & Responsibility** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Spearman Voter** <br> *(SpearmanRedundancySelector)* | Filter | **Linear/Monotonic**<br>Captures obvious, strong relationships (e.g., \"Newer cars are expensive\").<br>Also handles Redundancy by filtering out features that are exact duplicates of better ones (Maximum Relevance, Minimum Redundancy (mRMR)-style pruning).|\n",
    "| **MI Voter** <br> *(MutualInfoThresholdSelector)* | Filter | **Non-Linear**<br>Captures complex \"physics\" and non-monotonic patterns that correlation misses. |\n",
    "| **RF Voter** <br> *(SelectFromModel)* | Embedded | **Interactions**<br>Captures features that are only important in combination with others. |\n",
    "\n",
    "==> The feature selection is performed inside the pipelines cross-validation and consistent across all models, ensuring no data leakage and consistent feature selection logic.\n",
    "\n",
    "<u>Place in the pipe:</u> The Feature Selection is placed after the scaling to have the features on one scale (just like in the lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Pipeline including the fs_pipe\n",
    "fs_pipe = Pipeline([\n",
    "        (\"vt\", VarianceThreshold(threshold=0.0)),\n",
    "        ('selector', MajorityVoteSelectorTransformer(\n",
    "            selectors=[\n",
    "                SpearmanRelevancyRedundancySelector(relevance_threshold=0.05, redundancy_threshold=0.95), # If we set redundancy threshold to 1.01, this becomes similar to just relevance filtering,\n",
    "                MutualInfoThresholdSelector(threshold=0.01, n_neighbors=10), # Increasing n_neighbors makes the estimation more stable but computationally slower\n",
    "                SelectFromModel(RandomForestRegressor(n_estimators=100, max_depth=8, n_jobs=-1, random_state=rs), # max_depths not too low (miss interactions) and not too high (selecting noise -> overfitting)\n",
    "                                threshold='0.001*mean'), # threshold relative because it sums to 1 and if we have many features, many features will have a low importance but are still important],\n",
    "                # [Unused] RFE unused because of high computational cost\n",
    "                # rfecv_rf = RFECV(estimator = RandomForestRegressor(n_jobs=-1, max_depth=50), step=1, random_state=rs, cv=cv, scoring='neg_mean_absolute_error', min_features_to_select=5)\n",
    "            ],\n",
    "            min_votes=2))\n",
    "        ])\n",
    "\n",
    "\n",
    "# The entire data preparation pipeline is build incorporating the enc_transf_scale and fs-pipe defined in Section 2.6 and 3.2 respectively\n",
    "preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", enc_transf_scale),\n",
    "    (\"fs\", fs_pipe),\n",
    "])\n",
    "\n",
    "# Save preprocessor for reuse in DL experiments\n",
    "with open('preprocessor_pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor_pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the votes of each contributor, resulting in the final decision whether to keep the feature or not. Be aware that the constant feature `boolean__hasDamage` has already been **pre-filtered by the VT** before the majority voter.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Debug pipeline to visualize feature selection agreement\n",
    "enc_transf_scale.set_output(transform=\"pandas\")\n",
    "fs_pipe.set_output(transform=\"pandas\")\n",
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020, verbose=False)),\n",
    "    (\"ct\", (enc_transf_scale)),\n",
    "    (\"fs\", (fs_pipe)),\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data of the debug preprocesor to just visualize the result of a final model using this FS\n",
    "# -> The insights from here are not used for model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)\n",
    "\n",
    "# Feed the feed names after VT because VT is applied before the majority voting to remove constant features\n",
    "feature_names_after_vt = debug_preprocessor_pipe.named_steps['fs'].named_steps['vt'].get_feature_names_out()\n",
    "plot_selector_agreement(\n",
    "    majority_selector = debug_preprocessor_pipe.named_steps['fs'].named_steps['selector'], \n",
    "    feature_names = feature_names_after_vt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our findings:**\n",
    "- While trees are comparatively robust to rather useless features, applying the feature selection pipeline still slightly improves the performance\n",
    "- Using feature selection compared to not using feature selection on the exact same pipeline improved the performance by roughly 2 MAE on the best models (ET and RF)\n",
    "\n",
    "**Argumentation** why we discard certain features: \n",
    "- `boolean__hasDamage` is already pre-filtered by the VT because it is a constant variable (only 0 after imputation)\n",
    "- `num__previousOwners` is deselected because Spearman Correlation (with target in this case) and MI deselect it while the Feature Importance in the RF is above the threshold. However, two votes are enough to deselect the feature.\n",
    "- `cat__transmission_Other` and `cat__fuelTypeOther` are filtered by all three voters. This can be explained by the fact that they are redundant as none of the OHE features is dropped during creation to let the FS handle it, which happened succesfully here.\n",
    "\n",
    "**Consequences/Interpretation:**\n",
    "- The similar performance with or without feature selection is due to the few selected features to remove\n",
    "- It shows that most features are important for the models while the feature selector reliably detects useless and noisy features (e.g. hasDamage and the OHE features that were created without dropping the first dummy: cat__transmission_Other and cat__fuelType_Other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0dc6124-f736-46fb-9f73-895b930faae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.3) Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Log-transforming the target (price)` using [TransformedTargetRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html) because EDA showed that it is heavily right-skewed. The model predicts the log-price and automatically converts it back to pounds at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc50f05c-4c02-4ff6-9a48-319af5baecfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3.3.1) Model Setup (Original vs. Optimized Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**    \n",
    "\n",
    "We compare **6 models** with the baseline (median) on both preprocessing pipelines (baseline and optimized):\n",
    "- [Baseline (Median)](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html) to ensure that models predict better than the global median\n",
    "- [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to get a baseline of a linear model\n",
    "- [KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) to evaluate instance-based learning\n",
    "- [MLP](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) to evaluate performance on a neural network\n",
    "- [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) to evaluate a promising tree-based model\n",
    "- [Extra Trees](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) to check whether it outperforms random forest\n",
    "- [Histogram Gradient Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html) to evaluate a fast gradient boosting model\n",
    "\n",
    "**Fair comparison:**    \n",
    "- We **iteratively** applied **Randomized Search** to the models, starting with a broad grid and narowing it down to get results of models potential\n",
    "- After multiple iterations, we decide on which models to use for further optimizing (more extensive hyperparameter tuning).\n",
    "- Use the same random_state for reproducibility and n_jobs to speed up computations.    \n",
    "- Baseline: DummyRegressor using the median price as prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter distributions for each model with both preprocessors\n",
    "def get_all_models_both_preprocessors(preprocessor_baseline, preprocessor_pipe, rs):\n",
    "    \"\"\"\n",
    "    Define all models with both preprocessors (orig and adjusted).\n",
    "    Returns dictionary with model_name: (pipeline, param_distributions)\n",
    "    \"\"\"\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    ######### Baseline Median #########\n",
    "    baseline_median = create_model_pipe(\n",
    "        preprocessor_baseline, \n",
    "        DummyRegressor(strategy=\"median\")\n",
    "    )\n",
    "    models['Baseline_Median'] = (baseline_median, {})\n",
    "    \n",
    "    ######### Linear Regression #########\n",
    "    # No hyperparameter for Linear Regression\n",
    "    linear_reg_orig = create_model_pipe(\n",
    "        preprocessor_baseline,\n",
    "        TransformedTargetRegressor(\n",
    "            regressor=LinearRegression(),\n",
    "            func=np.log1p,\n",
    "            inverse_func=np.expm1\n",
    "        )\n",
    "    )\n",
    "    models['LinearRegression_orig'] = (linear_reg_orig, {})\n",
    "    \n",
    "    linear_reg_adjusted = create_model_pipe(\n",
    "        preprocessor_pipe,\n",
    "        TransformedTargetRegressor(\n",
    "            regressor=LinearRegression(),\n",
    "            func=np.log1p,\n",
    "            inverse_func=np.expm1\n",
    "        )\n",
    "    )\n",
    "    models['LinearRegression_adjusted'] = (linear_reg_adjusted, {})\n",
    "    \n",
    "    ######### KNN #########\n",
    "    knn_orig = create_model_pipe(\n",
    "        preprocessor_baseline,\n",
    "        TransformedTargetRegressor(\n",
    "            regressor=KNeighborsRegressor(n_jobs=-1),\n",
    "            func=np.log1p,\n",
    "            inverse_func=np.expm1\n",
    "        )\n",
    "    )\n",
    "    models['KNN_orig'] = (knn_orig, {\n",
    "        'model__regressor__n_neighbors': randint(10, 25),       # started between 3 and 30 but ~17 gave best results\n",
    "        'model__regressor__weights': ['uniform', 'distance'],   # Distance gave slightly better results but include uniform as backup\n",
    "        'model__regressor__metric': ['euclidean'],              # Even though manhatten performed slightly better, euclidean is used to speed up computations (KNN is no top candidate here anyway)\n",
    "    })\n",
    "\n",
    "    knn_adjusted = create_model_pipe(\n",
    "        preprocessor_pipe,\n",
    "        TransformedTargetRegressor(\n",
    "            regressor=KNeighborsRegressor(n_jobs=-1),\n",
    "            func=np.log1p,\n",
    "            inverse_func=np.expm1\n",
    "        )\n",
    "    )\n",
    "    models['KNN_adjusted'] = (knn_adjusted, {\n",
    "        'model__regressor__n_neighbors': randint(10, 25),       # started between 3 and 30 but ~17 gave best results\n",
    "        'model__regressor__weights': ['uniform', 'distance'],   # Distance gave slightly better results but include uniform as backup\n",
    "        'model__regressor__metric': ['euclidean'],              # Even though manhatten performed slightly better, euclidean is used to speed up computations (KNN is no top candidate here anyway)\n",
    "    })\n",
    "\n",
    "    \n",
    "    ######### Neural Network (MLP) #########\n",
    "    mlp_orig = create_model_pipe(\n",
    "        preprocessor_baseline,\n",
    "        TransformedTargetRegressor(\n",
    "            regressor=MLPRegressor(random_state=rs, max_iter=500, early_stopping=True),\n",
    "            func=np.log1p,\n",
    "            inverse_func=np.expm1\n",
    "        )\n",
    "    )\n",
    "    models['MLP_orig'] = (mlp_orig, {\n",
    "        'model__regressor__hidden_layer_sizes': [\n",
    "            (80, 40),\n",
    "            (100, 50),      # Best from previous search\n",
    "            (120, 60),\n",
    "            (100, 50, 25),  # Try adding depth\n",
    "            (100,),         # Single layer\n",
    "        ],\n",
    "        'model__regressor__alpha': uniform(0.0005, 0.004),              # narrow range around 0.0017\n",
    "        'model__regressor__activation': ['tanh', 'relu'],               # Keep best activation, but include relu as backup\n",
    "        'model__regressor__learning_rate': ['constant', 'adaptive'],    # Keep constant and adaptive\n",
    "        'model__regressor__learning_rate_init': uniform(0.0005, 0.0025),  # 0.0005 to 0.003\n",
    "    })\n",
    "\n",
    "    \n",
    "    mlp_adjusted = create_model_pipe(\n",
    "        preprocessor_pipe,\n",
    "        TransformedTargetRegressor(\n",
    "            regressor=MLPRegressor(random_state=rs, max_iter=500, early_stopping=True),\n",
    "            func=np.log1p,\n",
    "            inverse_func=np.expm1\n",
    "        )\n",
    "    )\n",
    "    models['MLP_adjusted'] = (mlp_adjusted, {\n",
    "        'model__regressor__hidden_layer_sizes': [\n",
    "            (120, 60),\n",
    "            (150, 75),      # Best from previous search\n",
    "            (180, 90),\n",
    "            (150, 75, 38),  # Try adding depth\n",
    "            (200, 100),     # Larger network\n",
    "        ],\n",
    "        'model__regressor__alpha': uniform(0.0002, 0.0018),                 # narrow range around 0.0007\n",
    "        'model__regressor__activation': ['tanh', 'relu'],                   # Keep best activation, but include relu as backup\n",
    "        'model__regressor__learning_rate': ['constant', 'adaptive'],        # Keep constant and adaptive\n",
    "        'model__regressor__learning_rate_init': uniform(0.0005, 0.0025),    # Add learning rate init after first searches without this parameter\n",
    "    })\n",
    "\n",
    "    \n",
    "    # ######### Random Forest #########\n",
    "    rf_orig = create_model_pipe(\n",
    "        preprocessor_baseline,\n",
    "        TransformedTargetRegressor(\n",
    "            regressor=RandomForestRegressor(random_state=rs, n_jobs=-1),\n",
    "            func=np.log1p,\n",
    "            inverse_func=np.expm1\n",
    "        )\n",
    "    )\n",
    "    models['RF_orig'] = (rf_orig, {                             \n",
    "        'model__regressor__n_estimators': randint(100, 300),    # Narrow down to 100-300 (performed best previously)\n",
    "        'model__regressor__max_depth': [None, 10],              # Narow down to None and 10 (performed best previously)\n",
    "        'model__regressor__min_samples_split': randint(2, 5),   # Narrow down to low values (performed best previously)\n",
    "        'model__regressor__min_samples_leaf': randint(1, 4),    # Narrow down on low values (performed best previously)\n",
    "        'model__regressor__max_features': ['sqrt'],             # 'sqrt' performed best in previous searches\n",
    "    })\n",
    "    \n",
    "    rf_adjusted = create_model_pipe(\n",
    "        preprocessor_pipe,\n",
    "        TransformedTargetRegressor(\n",
    "            regressor=RandomForestRegressor(random_state=rs, n_jobs=-1),\n",
    "            func=np.log1p,\n",
    "            inverse_func=np.expm1\n",
    "        )\n",
    "    )\n",
    "    models['RF_adjusted'] = (rf_adjusted, {\n",
    "        'model__regressor__n_estimators': randint(200, 400),    # ~300 performed best\n",
    "        'model__regressor__max_depth': [15, 20, 25],            # ~20 performed best\n",
    "        'model__regressor__min_samples_split': randint(2, 8),   # ~5 performed best\n",
    "        'model__regressor__min_samples_leaf': randint(1, 5),    # ~1 performed best\n",
    "        'model__regressor__max_features': ['sqrt'],             # 'sqrt' performed best\n",
    "    })\n",
    "\n",
    "    \n",
    "    ######### Extra Trees #########\n",
    "    et_orig = create_model_pipe(\n",
    "        preprocessor_baseline,\n",
    "        TransformedTargetRegressor(\n",
    "            regressor=ExtraTreesRegressor(random_state=rs, n_jobs=-1),\n",
    "            func=np.log1p,\n",
    "            inverse_func=np.expm1\n",
    "        )\n",
    "    )\n",
    "    models['ET_orig'] = (et_orig, {                           \n",
    "        'model__regressor__n_estimators': randint(100, 400),  # Narrow down to 100-400 (performed best previously)\n",
    "        'model__regressor__max_depth': [None, 10, 20],        # Narrow down to None, 10, and 20 (None performed best previously)\n",
    "        'model__regressor__min_samples_split': randint(2, 5), # Narrow down to low values (performed best previously)\n",
    "        'model__regressor__min_samples_leaf': randint(1, 4),  # Narrow down to low values (performed best previously)\n",
    "        'model__regressor__max_features': [0.7, 0.8, 0.9],    # 0.8 performed best previously\n",
    "    })\n",
    "    \n",
    "    et_adjusted = create_model_pipe(\n",
    "        preprocessor_pipe,\n",
    "        TransformedTargetRegressor(\n",
    "            regressor=ExtraTreesRegressor(random_state=rs, n_jobs=-1),\n",
    "            func=np.log1p,\n",
    "            inverse_func=np.expm1\n",
    "        )\n",
    "    )\n",
    "    models['ET_adjusted'] = (et_adjusted, {\n",
    "        'model__regressor__n_estimators': randint(350, 450),    # ~400 performed best\n",
    "        'model__regressor__max_depth': [18, 20, 22],            # 20 performed best previously\n",
    "        'model__regressor__min_samples_split': randint(5, 10),  # ~8 performed best\n",
    "        'model__regressor__min_samples_leaf': randint(1, 5),    # ~1 performed best\n",
    "        'model__regressor__max_features': [0.7, 0.8, 0.9],      # 0.8 performed best\n",
    "    })\n",
    "\n",
    "    \n",
    "    ######### Histogram Gradient Boosting #########\n",
    "    hgb_orig = create_model_pipe(\n",
    "        preprocessor_baseline,\n",
    "        TransformedTargetRegressor(\n",
    "            regressor=HistGradientBoostingRegressor(random_state=rs),\n",
    "            func=np.log1p,\n",
    "            inverse_func=np.expm1\n",
    "        )\n",
    "    )\n",
    "    models['HGB_orig'] = (hgb_orig, {\n",
    "        'model__regressor__learning_rate': uniform(0.05, 0.15), # ~0.1 performed best\n",
    "        'model__regressor__max_iter': randint(100, 300),        # ~200 performed best\n",
    "        'model__regressor__max_depth': [None, 5, 10],           # Keep None and lower values\n",
    "        'model__regressor__min_samples_leaf': randint(10, 25),  # ~15 performed best\n",
    "        'model__regressor__l2_regularization': uniform(0, 5),   # Keep broader range to explore\n",
    "    })\n",
    "    \n",
    "    hgb_adjusted = create_model_pipe(\n",
    "        preprocessor_pipe,\n",
    "        TransformedTargetRegressor(\n",
    "            regressor=HistGradientBoostingRegressor(random_state=rs),\n",
    "            func=np.log1p,\n",
    "            inverse_func=np.expm1\n",
    "        )\n",
    "    )\n",
    "    models['HGB_adjusted'] = (hgb_adjusted, {\n",
    "        'model__regressor__learning_rate': uniform(0.05, 0.13), # ~0.1 performed best\n",
    "        'model__regressor__max_iter': randint(400, 700),        # ~550 performed best\n",
    "        'model__regressor__max_depth': [5, 10, 15],             # 10 performed best previously\n",
    "        'model__regressor__min_samples_leaf': randint(10, 20),  # ~15 performed good\n",
    "        'model__regressor__l2_regularization': uniform(0, 2),   # Lower values performed better\n",
    "    })\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a21d8b-3aeb-4155-b6db-17475e30c3b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3.3.2) Run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all models with both preprocessors\n",
    "models_dict = get_all_models_both_preprocessors(\n",
    "    preprocessor_baseline=preprocessor_baseline,\n",
    "    preprocessor_pipe=preprocessor_pipe,\n",
    "    rs=rs\n",
    ")\n",
    "\n",
    "# Run RandomizedSearchCV on all models\n",
    "results_df, best_estimators, final_table = run_quick_randomsearch(\n",
    "    models_dict=models_dict,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    cv=cv,\n",
    "    n_iter=4,\n",
    "    random_state=rs,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Display final formatted table\n",
    "print(\"\\nFinal Results Table\")\n",
    "display(final_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.3) Baseline Model Comparison Discussion of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Comparative performance under optimized preprocessing**\n",
    "\n",
    "Under the optimized preprocessing pipeline, performance is clearly led by **tree-based ensemble models**, with a narrow but consistent separation at the top.\n",
    "\n",
    "- **ExtraTrees (ET, optimized)** is the strongest overall model, achieving  \n",
    "  **val MAE = 1212.18**, **val RMSE = 2167.02**, **val R-squared = 0.9506**.  \n",
    "  It delivers the lowest validation MAE among all models and combines this with strong explanatory power.\n",
    "\n",
    "- **RandomForest (RF, optimized)** follows closely behind with  \n",
    "  **val MAE = 1241.79**, **val RMSE = 2293.23**, **val R-squared = 0.9446**.  \n",
    "  While slightly worse than ET across all validation metrics, RF remains firmly in the top tier.\n",
    "\n",
    "- **Histogram Gradient Boosting (HGB, optimized)** achieves  \n",
    "**val MAE = 1251.38**, **val RMSE = 2122.16**, **val R-squared = 0.9526**.  \n",
    "While its RMSE is the best, its MAE is significantly worse than the MAE from ET. As it performs best in RMSE which is a good valuable in a pricing regression task, we keep it for further analysis.\n",
    "\n",
    "These models clearly outperform the remaining candidates:\n",
    "\n",
    "- **KNN (optimized)** performs moderately well but remains limited by local sensitivity and reduced robustness in higher-error regions.  \n",
    "  (**val MAE = 1497.76**, **val RMSE = 2669.17**, **val R-squared = 0.9250**)  \n",
    "\n",
    "- **Linear Regression (optimized)** underperforms substantially confirming that the pricing signal cannot be captured adequately by a global linear model.    \n",
    "  (**val MAE = 2242.25**, **val RMSE = 3925.59**, **val R-squared = 0.8374**),  \n",
    "\n",
    "- **MLP (optimized)** is competitive with KNN but also worse than the top candidates.   \n",
    "  **val MAE = 1481.24**, **val RMSE = 2561.13**, **val R-squared = 0.9309**.  \n",
    "\n",
    "---\n",
    "\n",
    "**2) Effect size of preprocessing (optimized vs original)**\n",
    "\n",
    "Optimized preprocessing yields **clear and consistent gains**, especially for ensemble-based models:\n",
    "\n",
    "- **ExtraTrees:**  \n",
    "  val MAE improves from **1310.26 → 1212.18** (Δ ≈ **-98**, ≈ **-7.5%**) -> significant positive effect of preprocessing  \n",
    "\n",
    "- **RandomForest:**  \n",
    "  val MAE improves significantly from **1624.55 → 1241.79** (Δ ≈ **-383**, ≈ **-23.6%**). However, this result is due to chance in the rather small randomized search. RF often performed relatively good on the baseline preprocessing too.\n",
    "\n",
    "- **Histogram Gradient Boosting:**  \n",
    "  val MAE also improves from **1390.51 → 1251.38** (Δ ≈ **-139**, ≈ **-10.0%**).\n",
    "\n",
    "- **KNN:**  \n",
    "  val MAE improves from **1660.33 → 1497.76** (Δ ≈ **-163**, ≈ **-9.8%**) but remains behind the ensemble methods.\n",
    "\n",
    "A key exception is **Linear Regression**, which degrades under optimized preprocessing (**val MAE = 1833.25 → 2242.25**). This aligns with the introduction of nonlinear interactions that benefit tree-based models but are detrimental to a strictly linear hypothesis class.\n",
    "\n",
    "---\n",
    "\n",
    "**3) Detailed focus: why ExtraTrees is the strongest model here**\n",
    "\n",
    "**ExtraTrees (optimized)** emerges as the strongest model by a **consistent margin** across validation metrics:\n",
    "\n",
    "- Lowest **validation MAE**\n",
    "- Competitive **RMSE**\n",
    "- High **R-squared**, indicating strong explanatory power\n",
    "\n",
    "This performance profile is theoretically well grounded:\n",
    "\n",
    "- ExtraTrees captures **high-order nonlinear interactions** without relying on greedy split optimization.\n",
    "- Randomized thresholds reduce sensitivity to noise in key pricing drivers (e.g., mileage, age, engine size).\n",
    "- Ensemble averaging controls variance while retaining expressive capacity.\n",
    "\n",
    "Compared to RandomForest, ET benefits from **additional randomness**, which appears to translate into better generalization under this feature-rich preprocessing pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "**4) Generalization and overfitting signals (train vs validation)**\n",
    "\n",
    "Train–validation comparisons provide important diagnostics:\n",
    "\n",
    "- **ExtraTrees (optimized):**  \n",
    "  train MAE ≈ **883.57** vs val MAE ≈ **1212.18**\n",
    "\n",
    "- **RandomForest (optimized):**  \n",
    "  train MAE ≈ **1023.87** vs val MAE ≈ **1241.79**\n",
    "\n",
    "These gaps reflect **high model capacity**, but are expected for ensemble learners. Crucially, validation performance remains strong which is our main focus. This indicates effective generalization rather than harmful overfitting. However, the final performance on the hold-out test set should be evaluated with keeping this in mind.\n",
    "\n",
    "Overall, the optimized preprocessing pipeline yields **plausible and stable training behavior**, with no evidence of pathological leakage or metric artefacts.\n",
    "\n",
    "---\n",
    "\n",
    "**5) Bottom-line conclusion from the evidence**\n",
    "\n",
    "The empirical evidence supports a clear conclusion:\n",
    "\n",
    "- Under optimized preprocessing, **ExtraTrees is the best-performing and most reliable model**, achieving the lowest validation MAE and strong RMSE and R-squared.\n",
    "- **RandomForest** is a close second, with slightly weaker validation performance.\n",
    "- Preprocessing delivers **significant improvements**, especially for high-capacity ensemble models.\n",
    "- Simpler or non-ensemble models either underfit (Linear Regression), or plateau earlier (KNN, MLP)\n",
    "\n",
    "Within this benchmark, **ExtraTrees combined with optimized preprocessing** represents the strongest overall solution by both **accuracy and robustness** criteria.\n",
    "\n",
    "==> We use ET, RF and HGB in further optimizing to fine-tune their hyperparameters for best performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MAE Performance` of Models on original and optimized data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MAE_gap\n",
    "final_table[\"MAE_gap\"] = final_table[\"val_MAE\"] - final_table[\"train_MAE\"]\n",
    "final_table[\"base_model\"] = final_table[\"model\"].str.replace(\"_orig\", \"\", regex=False)\n",
    "\n",
    "# Original vs Optimized preprocessing (Validation MAE)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for prep in [\"original\", \"optimized\"]:\n",
    "    subset = final_table[final_table[\"preprocessing\"] == prep]\n",
    "    plt.scatter(subset[\"base_model\"], subset[\"val_MAE\"], label=prep)\n",
    "\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.title(\"Original vs Optimized Preprocessing (Validation MAE)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Train-Val-Gap` to analyze overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation vs Train MAE (grouped bar plot)\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = range(len(final_table))\n",
    "plt.bar(x, final_table[\"train_MAE\"], width=0.4, label=\"Train MAE\")\n",
    "plt.bar(\n",
    "    [i + 0.4 for i in x],\n",
    "    final_table[\"val_MAE\"],\n",
    "    width=0.4,\n",
    "    label=\"Validation MAE\"\n",
    ")\n",
    "\n",
    "plt.xticks([i + 0.2 for i in x], final_table[\"model\"], rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"Train vs Validation MAE by Model\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalization gap (Val − Train MAE)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.bar(final_table[\"model\"], final_table[\"MAE_gap\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "\n",
    "plt.ylabel(\"MAE Gap (Validation − Train)\")\n",
    "plt.title(\"Generalization Gap by Model\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.4) Optimization: Hyperparameter Tuning of Top Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0eb17d8-4b9a-433f-98f6-26777d49fa3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Tune Top candidates with Randomized Search CV:**     \n",
    "\n",
    "After the first runs we only keep the **top candidates for further hyperparameter** tuning to focus on most promising approaches and not waste computing power.     \n",
    "We tune using [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) which calls the pipeline object for consistent preprocessing. An example by sklearn of calling the pipeline similar to this can be found [here](https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html#sphx-glr-auto-examples-compose-plot-compare-reduction-py).\n",
    "Within the process of tuning, the parameters were iteratively optimized to decrease size of the search space of the top candidates. This allowed faster runtimes and better final results compared to running one extensive search on a big hyperparameter grid.\n",
    "\n",
    "We use `squared_error` to find hyperparameters but use `absolute_error` to optimize for MAE (primary metric) for final retraining on the best Model because it is [significantly slower](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) (~5x) but optimizes our primary metric even further.\n",
    "\n",
    "To visualize the performance of different hyperparameters, we plot the 2 arguably most impactful parameters and their achieved val MAE. When analyzing the plot, one has to be aware that the performance is also influenced by features that are not included in the plot so one should focus only on rough directions instead of relying on exact performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with the best models from the random search for further analysis\n",
    "rf_pipe_adjusted = models_dict['RF_adjusted'][0]\n",
    "et_pipe_adjusted = models_dict['ET_adjusted'][0]\n",
    "hgb_pipe_adjusted = models_dict['HGB_adjusted'][0]\n",
    "\n",
    "# Determine the number of iterations for top candidates model hyperparameter tuning (This was iteratively narrowed down based on previous results)\n",
    "n_iter_on_final_models = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70de2673-0443-4cc4-b1cd-f3813a969246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3.4.1) RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "# Narrowed parameter distribution after multiple iterations\n",
    "rf_param_dist = {\n",
    "    \"model__regressor__criterion\": [\"squared_error\"],       # optimizes way faster than \"absolute_error\"\n",
    "    \"model__regressor__n_estimators\": randint(300, 350),    # number of trees\n",
    "    \"model__regressor__max_depth\": randint(18, 22),         # depth of each tree\n",
    "    \"model__regressor__min_samples_split\": randint(4, 6),   # min samples to split an internal node\n",
    "    \"model__regressor__min_samples_leaf\": randint(1, 3),    # min samples per leaf (increse to not overfit)\n",
    "    \"model__regressor__max_features\": [\"sqrt\"],             # feature sampling strategy (sqrt performed better than log2 and None in previous tests)\n",
    "    # \"model__regressor__bootstrap\": [True],                # True is default for RFs\n",
    "}\n",
    "\n",
    "rf_tuned_pipe, rf_random_search_object, rf_scores_dict = model_hyperparameter_tuning(X_train,\n",
    "                                                                                     y_train,\n",
    "                                                                                     cv,\n",
    "                                                                                     rf_pipe_adjusted,\n",
    "                                                                                     rf_param_dist,\n",
    "                                                                                     n_iter=n_iter_on_final_models,\n",
    "                                                                                     verbose_features=[[\"model__regressor__n_estimators\", \"model__regressor__max_depth\"],],\n",
    "                                                                                     verbose_metric=\"mae\",\n",
    "                                                                                     verbose_plot=True,\n",
    "                                                                                     verbose_top_n=20)\n",
    "\n",
    "joblib.dump(rf_tuned_pipe, \"rf_tuned_pipe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "917f1472-aee1-41b0-bc59-12f436c1f17f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3.4.2) Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c162067b-9019-40a6-9bb3-2977ffc615a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Narrowed parameter distribution after multiple iterations\n",
    "et_param_dist = {\n",
    "    \"model__regressor__criterion\": [\"squared_error\"],           # optimizes way faster than \"absolute_error\"\n",
    "    \"model__regressor__n_estimators\": randint(350, 420),        # number of trees\n",
    "    \"model__regressor__max_depth\": randint(23, 25),             # depth of each tree\n",
    "    \"model__regressor__min_samples_split\": randint(6, 8),       # min samples to split an internal node\n",
    "    \"model__regressor__min_samples_leaf\": randint(1, 3),        # min samples per leaf\n",
    "    \"model__regressor__max_features\": [0.8, 0.9],               # feature sampling strategy\n",
    "    # \"model__regressor__bootstrap\": [False]                    # default for ETs\n",
    "}\n",
    "\n",
    "et_tuned_pipe_squared_err, et_random_search_object, et_scores_dict = model_hyperparameter_tuning(X_train, y_train, cv,\n",
    "                                                                                    et_pipe_adjusted,\n",
    "                                                                                    et_param_dist,\n",
    "                                                                                    n_iter=n_iter_on_final_models,\n",
    "                                                                                    verbose_features=[[\"model__regressor__n_estimators\", \"model__regressor__max_depth\"],],\n",
    "                                                                                    verbose_metric=\"mae\",\n",
    "                                                                                    verbose_plot=True,\n",
    "                                                                                    verbose_top_n=20)\n",
    "\n",
    "joblib.dump(et_tuned_pipe_squared_err, \"et_tuned_pipe_squared_err.pkl\")\n",
    "# Best Model params: {'model__regressor__criterion': 'squared_error', 'model__regressor__max_depth': 23, 'model__regressor__max_features': 0.9, 'model__regressor__min_samples_leaf': 1, 'model__regressor__min_samples_split': 6, 'model__regressor__n_estimators': 410}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.3) Histogram Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrowed parameter distribution after multiple iterations\n",
    "hgb_param_dist = {\n",
    "    \"model__regressor__loss\": ['squared_error'],               # Default is squared_error, absolute_error is slower to optimize\n",
    "    \"model__regressor__max_iter\": randint(400, 800),\n",
    "    \"model__regressor__learning_rate\": uniform(0.01, 0.15),\n",
    "    \"model__regressor__min_samples_leaf\": randint(2, 20),\n",
    "    \"model__regressor__max_depth\": [None, 10],\n",
    "    \"model__regressor__l2_regularization\": uniform(0.0, 1.0),\n",
    "    \"model__regressor__early_stopping\": [True],\n",
    "    \"model__regressor__n_iter_no_change\": randint(15, 25),\n",
    "    \"model__regressor__random_state\": [rs],\n",
    "\n",
    "}\n",
    "\n",
    "hgb_tuned_pipe, hgb_random_search_object, hgb_scores_dict = model_hyperparameter_tuning(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv,\n",
    "    hgb_pipe_adjusted,\n",
    "    hgb_param_dist,\n",
    "    n_iter=n_iter_on_final_models,\n",
    "    verbose_features=[[\"model__regressor__learning_rate\", \"model__regressor__max_iter\"]],\n",
    "    verbose_plot=True\n",
    ")\n",
    "joblib.dump(hgb_tuned_pipe, \"hgb_tuned_pipe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf7394c-9f64-4866-a068-52dcd03c77e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3.4.4) Comparison of Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd683a0-76c8-469b-8530-d27d834b1c69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use object from randomizedsearch to retrieve the mean metrics of the best model (that was also refit on entire data for final predictions later)\n",
    "model_scores = {\"rf_tuned\": rf_scores_dict, \"et_tuned\": et_scores_dict, \"hgb_tuned\": hgb_scores_dict}\n",
    "\n",
    "# Convert dictionary to DataFrame (transpose to have models as rows) and sort by val_mae (primary metric)\n",
    "df_scores = pd.DataFrame(model_scores).T \n",
    "df_scores = df_scores[['val_mae', 'val_rmse', 'val_r2','train_mae', 'train_rmse', 'train_r2']]\n",
    "df_scores = df_scores.sort_values(by='val_mae')\n",
    "\n",
    "print(\"Model Comparison Table:\")\n",
    "display(df_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings: Evaluation and the expected effects from Hyperparameter changes (default -> tuned)**\n",
    "\n",
    "**RandomForest (tuned)**\n",
    "- `n_estimators=335` (100 -> 335): reduces estimator variance via stronger ensemble averaging.\n",
    "- `max_depth=20` (None -> 20): limits tree complexity and mitigates overfitting.\n",
    "- `min_samples_split=5` (2 -> 5): avoids splits supported by very few samples; smoother decision boundaries.\n",
    "- `min_samples_leaf=1` (unchanged): preserves the ability to model fine local structure.\n",
    "- `max_features=\"sqrt\"` (1.0 -> sqrt): increases tree diversity; typically improves generalization.\n",
    "- `criterion=\"squared_error\"` (unchanged): efficient optimization objective; not directly aligned with MAE but standard for RF.\n",
    "\n",
    "---\n",
    "\n",
    "**ExtraTrees (tuned)**\n",
    "- `n_estimators=410` (100 -> 410): variance reduction through increased averaging.\n",
    "- `max_depth=23` (None -> 23): controls complexity while still allowing deep interactions.\n",
    "- `min_samples_split=6` (2 -> 6): stronger regularization than RF; discourages overly specific splits.\n",
    "- `min_samples_leaf=1` (unchanged): maintains capacity for local patterns.\n",
    "- `max_features=0.9` (1.0 -> 0.9): increases randomness across trees; supports better generalization.\n",
    "\n",
    "---\n",
    "\n",
    "**HistGradientBoosting (tuned)**\n",
    "- `max_iter=713` (100 -> 713): significantly more boosting iterations for better convergence.\n",
    "- `learning_rate=0.159` (0.1 -> 0.159): slightly higher learning rate balanced by more iterations.\n",
    "- `max_depth=None` (unchanged): allows unrestricted tree depth; complexity controlled by other params.\n",
    "- `min_samples_leaf=13` (20 -> 13): slightly more flexible leaf nodes.\n",
    "- `l2_regularization=0.00078` (0 -> 0.00078): minimal L2 regularization added.\n",
    "- `early_stopping=True` (False -> True): prevents overfitting by monitoring validation performance.\n",
    "- `n_iter_no_change=23` (10 -> 23): more patient early stopping criterion.\n",
    "- `loss=\"squared_error\"` (unchanged): standard MSE-based loss for regression.\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation of the tuning choices**\n",
    "\n",
    "- All models apply standard **bias–variance trade-offs**: more trees/iterations, depth caps, stricter split criteria, and feature subsampling.\n",
    "- **ExtraTrees** is regularized more aggressively at the split level, relying on randomness and averaging rather than deep deterministic structure.\n",
    "- **HGB** uses early stopping and significantly more iterations (~7× increase) to achieve better convergence with careful regularization.\n",
    "\n",
    "---\n",
    "\n",
    "**Observed tuned performance (mean CV)**\n",
    "\n",
    "| Model      | val_MAE     | val_RMSE    | val_R²   | train_MAE   | train_RMSE  | train_R²  |\n",
    "|------------|-------------|-------------|----------|-------------|-------------|-----------|\n",
    "| **et_tuned**   | **1192.70** | **2076.87** | **0.9546** | 712.07      | 1247.27     | 0.9836    |\n",
    "| rf_tuned   | 1202.05     | 2157.16     | 0.9510   | 872.77      | 1547.35     | 0.9747    |\n",
    "| hgb_tuned  | 1228.56     | 2091.43     | 0.9539   | 1032.82     | 1701.11     | 0.9695    |\n",
    "\n",
    "**Key observations:**\n",
    "\n",
    "- **ExtraTrees achieves the best validation performance across all metrics:**\n",
    "  - **MAE** (primary objective): **1192.70** (best)\n",
    "  - **RMSE**: **2076.87** (best)\n",
    "  - **R-squared**: **0.9546** (best)\n",
    "\n",
    "- **Performance ranking:** ET > RF > HGB\n",
    "  - ET outperforms RF by **~9.4 MAE points** (~0.8% improvement)\n",
    "  - ET outperforms HGB by **~35.9 MAE points** (~2.9% improvement)\n",
    "\n",
    "- **Train–validation gaps indicate high-capacity learners:**\n",
    "  - ET: largest gap (712 → 1193 MAE), yet best validation performance\n",
    "  - RF: moderate gap (873 → 1202 MAE)\n",
    "  - HGB: smallest gap (1033 → 1229 MAE), suggesting more conservative fitting\n",
    "\n",
    "- ET's superior validation performance **despite larger train–validation gap** demonstrates effective randomization and ensemble averaging rather than brittle overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "**Supported by the results:**\n",
    "\n",
    "- `et_tuned` is the **best tuned candidate** under the chosen objective (minimize validation MAE).\n",
    "- Improvements in **RMSE and R-squared** show that the MAE gain is not an artifact of a single metric.\n",
    "- All three models show strong performance (R-squared > 0.95) -> preprocessing pipeline is effective.\n",
    "\n",
    "---\n",
    "\n",
    "**Decision**\n",
    "\n",
    "- **Final choice:** use **`et_tuned`** as the production model.\n",
    "- **Rationale:** \n",
    "  - Achieves the **lowest validation MAE** (1192.70)\n",
    "  - **Best across all metrics** (MAE, RMSE, R-squared)\n",
    "  - Consistent performance under identical CV splits\n",
    "  - Effective regularization through random splits and ensemble diversity\n",
    "- **Next steps:** Tune best model with absolute_error and valuate on hold-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf74f27d-4b15-407e-a9a1-1e52b11f071b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_train_val_comparison(df_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.5) Optimize Best Model further on absolute error (primary metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model is now trained optimizing the `absolut_error` with the best hyperparameters found when optimizing the `squared_error`. This approach is taken because optimizing the absolut_error is computationally too expensive and therefore only used for the final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best parameter distribution and run it with \"absolute_error\" to minimize MAE even further\n",
    "et_param_dist = {\n",
    "    \"model__regressor__criterion\": [\"absolute_error\"], # absolute_error for final best prediction\n",
    "    \"model__regressor__n_estimators\": [410],\n",
    "    \"model__regressor__max_depth\": [23],\n",
    "    \"model__regressor__min_samples_split\": [6],\n",
    "    \"model__regressor__min_samples_leaf\": [1],\n",
    "    \"model__regressor__max_features\": [0.9],\n",
    "}\n",
    "\n",
    "# We use the same function to get a final result CV result with \"absolut_error\" and then the model is automatically refit on all data\n",
    "et_tuned_pipe, et_random_search_object, et_scores_dict = model_hyperparameter_tuning(X_train, y_train, cv, \n",
    "                                                                                     et_pipe_adjusted,\n",
    "                                                                                     et_param_dist,\n",
    "                                                                                     n_iter=1,\n",
    "                                                                                     verbose_features=[],\n",
    "                                                                                     verbose_plot=False)\n",
    "# => MAE: 1183.5502\n",
    "\n",
    "joblib.dump(et_tuned_pipe, \"et_tuned_pipe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5) Visualizations and Insights of the entire Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing is independent of the model:**     \n",
    "To ensure a consistent comparison, the preprocessing pipeline is independent of the model. Therefore, the insights are the same for all models.\n",
    "\n",
    "\n",
    "**Visualize outputs** of each preprocessing step:     \n",
    "We use this `DebugTransformer` to print the data shape and check the analysis of y-data-profiling for missing values etc. to ensure our pipeline works as expected. This facilitates the experimenting process massively and helps finding errors within the pipeline fast.     \n",
    "For the visualizations we use ydata-profiling instead of our own plots like in the EDA for a more concise output.\n",
    "\n",
    "**Findings**:     \n",
    "The outcome of each pipeline step that is visualized here is explained in the respective Subsections in Section 2 on Data Preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle verbosity of output by setting show_data or even y_data_profiling to True\n",
    "show_data = True\n",
    "y_data_profiling = False\n",
    "\n",
    "# Set output to pandas DataFrames for easier inspection while we use numpy arrays for efficient model training (default)\n",
    "enc_transf_scale.set_output(transform=\"pandas\")\n",
    "fs_pipe.set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.1) Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    ('debug_start', DebugTransformer('START', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "])\n",
    "\n",
    "print(\"Show outputs of each step in the preprocessing pipeline:\")\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.2) Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True, verbose=True)),\n",
    "    ('debug_after_clean', DebugTransformer('AFTER CLEANING', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.3) [Unused] Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for debugging and visualizing the outlier handling is included in the unused_experiments.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.4) Missing Values Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    ('debug_after_impute', DebugTransformer('AFTER IMPUTATION', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.5) Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020, verbose=True)),\n",
    "    ('debug_after_fe', DebugTransformer('AFTER FEATURE ENGINEERING', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.6) Transformation, Scaling, Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", (enc_transf_scale)),\n",
    "    ('debug_after_ct', DebugTransformer('AFTER COLUMN TRANSFORMER', show_data=show_data, y_data_profiling=y_data_profiling))\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.7) Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", (enc_transf_scale)),\n",
    "    (\"fs\", (fs_pipe)),\n",
    "    ('debug_after_fs', DebugTransformer('AFTER FEATURE SELECTION', show_data=show_data, y_data_profiling=y_data_profiling))\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.8) Visualize entire pipe before adding the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.9) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a detailed Inspection of the best model refer to the Feature Importance analysis in the open-end-section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the debug preprocessor pipeline to get final feature names by hierarchically accessing each step\n",
    "feature_names_after_fs = debug_preprocessor_pipe.named_steps['fs'].get_feature_names_out()\n",
    "feat_names = feature_names_after_fs\n",
    "importances = et_tuned_pipe.named_steps[\"model\"].regressor_.feature_importances_\n",
    "df_feat_importance_et = pd.DataFrame({\"feature\": feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    " \n",
    "print(\"Feature Importances:\")\n",
    "for _, row in df_feat_importance_et.iterrows():\n",
    "    print(f\"{row['feature']:30s}: {row['importance']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bfd1b4a-0e13-443a-aeb7-fa6324f801e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### IV. Open-Ended-Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 SHAP Interpretability for Our Final Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see `OES_SHAP.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Global vs Brand- and Model-Specific Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Objective and motivation**\n",
    "\n",
    "We investigated how far Cars4You should specialize its pricing models:\n",
    "\n",
    "1. **Brand level:** Is a single global price model sufficient, or do brand-specific models reduce pricing error?\n",
    "2. **Brand–model level:** For frequent (brand, model) segments (e.g. “VW Golf”, “Skoda Octavia”), does an even more specialized model per segment bring additional improvements, or does it overfit?\n",
    "\n",
    "Starting point is our final tuned production pipeline **`et_tuned_pipe`** (full preprocessing + tuned **ExtraTrees** regressor). We compare:\n",
    "\n",
    "- **Global model:** trained on all cars, evaluated only on a given segment.\n",
    "- **Brand-specific model:** same pipeline structure and hyperparameters, fitted only on cars of a given brand.\n",
    "- **Brand–model-specific model:** same pipeline structure and hyperparameters, fitted only on cars of a given (brand, model) pair.\n",
    "\n",
    "We measured **MAE** and **RMSE** per segment using **5-fold cross-validation**. This quantifies the gain/loss when moving from:\n",
    "\n",
    "> one global model → several brand models → many brand–model models.\n",
    "\n",
    "---\n",
    "\n",
    "**b) Difficulty of the tasks**\n",
    "\n",
    "This multi-level comparison is not easy because it requires leakage-free, segment-wise evaluation inside cross-validation:\n",
    "\n",
    "- **Per-segment metrics inside CV (not a single score):** each fold must report MAE/RMSE *for specific brands/pairs* on the fold’s validation set.\n",
    "- **Fair protocol for global vs specialized models (within each fold):**\n",
    "  - global model is trained on all training rows, but evaluated only on validation rows belonging to the segment;\n",
    "  - segment-specific model is trained and evaluated only on that segment’s rows.\n",
    "- **Imbalanced / small segments:** data is heavily skewed across brands/models, so we enforce minimum segment sizes:\n",
    "  - **brand level:** only brands with **≥ 500** samples overall;\n",
    "  - **brand–model level:** only pairs with **≥ 80** samples overall;\n",
    "  - plus per-fold minimum training-size checks to avoid unstable tiny fits.\n",
    "- **Cleaning labels before grouping:** inconsistent text labels (casing/spacing/typos) can split real segments; we normalize `(brand, model)` once using the same cleaning logic as in the pipeline and use cleaned labels for masks.\n",
    "- **Manual RMSE:** computed as `sqrt(MSE)` inside the CV loops for compatibility with our environment.\n",
    "\n",
    "---\n",
    "\n",
    "**c) Correctness and efficiency of implementation**\n",
    "\n",
    "We kept the evaluation correct and efficient:\n",
    "\n",
    "- **Leakage-free out-of-fold evaluation:** in each fold the pipeline is fitted only on that fold’s training rows; metrics are computed only on validation rows.\n",
    "- **Single CV design reused everywhere:** identical KFold splits (`n_splits=5`, `shuffle=True`, fixed `random_state`) are reused for all comparisons, making deltas directly comparable.\n",
    "- **Efficient global baseline:** the global model is fitted **once per fold**, then reused to compute metrics for many brands/pairs in that fold (instead of refitting per segment).\n",
    "- **Segmentation labels separated from training data:** model training always uses the original fold rows; segment membership uses cleaned labels for correct grouping.\n",
    "- **Guards for tiny segments:** segments/folds with insufficient training samples are skipped to avoid unstable conclusions.\n",
    "\n",
    "---\n",
    "\n",
    "**d) Discussion of results**\n",
    "\n",
    "- **Candidate brands:** all brands\n",
    "- **Candidate (brand, model) pairs:** ≥ 80 samples; 100 pairs\n",
    "\n",
    "\n",
    "**Brand-level comparison (global vs brand-specific, using `et_tuned_pipe`)**\n",
    "\n",
    "Results show that brand-specific training is **not consistently beneficial**. The pattern is **highly mixed**:\n",
    "\n",
    "**Brands with improvements (negative ΔMAE):**\n",
    "\n",
    "| Brand | n | MAE_global | MAE_brand | ΔMAE | RMSE_global | RMSE_brand | ΔRMSE |\n",
    "|---|---:|---:|---:|---:|---:|---:|---:|\n",
    "| BMW | 7542 | 1631.6 | 1618.1 | **-13.5** | 2806.9 | 2799.8 | **-7.2** |\n",
    "| Mercedes | 11904 | 1789.7 | 1788.5 | **-1.3** | 3233.7 | 3192.6 | **-41.2** |\n",
    "\n",
    "**Brands with degradations (positive ΔMAE):**\n",
    "\n",
    "| Brand | n | MAE_global | MAE_brand | ΔMAE | RMSE_global | RMSE_brand | ΔRMSE |\n",
    "|---|---:|---:|---:|---:|---:|---:|---:|\n",
    "| Opel | 9539 | 729.9 | 741.7 | **+11.8** | 1073.6 | 1092.3 | **+18.7** |\n",
    "| Skoda | 4382 | 1037.6 | 1051.2 | **+13.6** | 1718.6 | 1750.3 | **+30.9** |\n",
    "| Ford | 16386 | 870.1 | 880.2 | **+10.0** | 1328.6 | 1333.6 | **+5.0** |\n",
    "| Audi | 7460 | 1634.0 | 1644.3 | **+10.3** | 2525.2 | 2561.7 | **+36.6** |\n",
    "| Toyota | 4716 | 817.9 | 828.5 | **+10.7** | 1315.9 | 1349.5 | **+33.6** |\n",
    "\n",
    "**Notable outlier:**\n",
    "\n",
    "| Brand | n | MAE_global | MAE_brand | ΔMAE | RMSE_global | RMSE_brand | ΔRMSE |\n",
    "|---|---:|---:|---:|---:|---:|---:|---:|\n",
    "| VW | 10603 | 1172.6 | 1175.3 | **+2.7** | 1788.2 | 1807.6 | **+19.4** |\n",
    "\n",
    "**Interpretation:**\n",
    "- **Only 2 out of 8 brands show improvements**, and even these are marginal (BMW: -13.5 MAE, Mercedes: -1.3 MAE).\n",
    "- **6 out of 8 brands degrade** with brand-specific models, with degradations ranging from +2.7 to +13.6 MAE.\n",
    "- The degradations are **consistent across both MAE and RMSE**, indicating real generalization loss rather than metric artifacts.\n",
    "\n",
    "**Practical interpretation:** \n",
    "- Brand-level specialization is **not recommended** as a default strategy.\n",
    "- The global model already captures brand effects through features (including brand encodings and brand-specific feature engineering).\n",
    "- Restricting to single brands typically **reduces data diversity and hurts generalization** more than it helps capture brand-specific patterns.\n",
    "- Brand-specific models should only be deployed when showing **stable negative ΔMAE** in repeated CV and external validation.\n",
    "\n",
    "\n",
    "**Brand–model comparison (global vs brand–model-specific, `et_tuned_pipe`)**\n",
    "\n",
    "Results are clearly **mixed** even after filtering to pairs with ≥ 80 samples: some segments improve substantially, while others degrade dramatically.\n",
    "\n",
    "**Top 15 improvements (largest negative ΔMAE; specialized better than global):**\n",
    "\n",
    "| Rank | Brand | Model | n | MAE_global | MAE_pair | ΔMAE | RMSE_global | RMSE_pair | ΔRMSE |\n",
    "|---:|---|---|---:|---:|---:|---:|---:|---:|---:|\n",
    "| 10 | Audi | q7 | 268 | 3177.8 | 3047.6 | **-130.2** | 4273.6 | 4079.8 | **-193.8** |\n",
    "| 11 | Audi | tt | 222 | 1858.2 | 1783.1 | **-75.1** | 2633.9 | 2434.6 | **-199.4** |\n",
    "| 18 | BMW | m4 | 83 | 3417.4 | 3345.4 | **-72.0** | 5504.5 | 5416.3 | **-88.2** |\n",
    "| 27 | Ford | edge | 137 | 1105.6 | 1047.2 | **-58.3** | 1581.9 | 1452.8 | **-129.0** |\n",
    "| 94 | VW | t-cross | 233 | 1255.8 | 1201.8 | **-54.0** | 1577.2 | 1538.5 | **-38.7** |\n",
    "| 35 | Ford | s-max | 201 | 1281.1 | 1231.9 | **-49.2** | 1704.0 | 1638.2 | **-65.8** |\n",
    "| 40 | Hyundai | ioniq | 203 | 1484.0 | 1441.1 | **-42.9** | 1939.5 | 1887.9 | **-51.6** |\n",
    "| 60 | Opel | combo life | 99 | 1346.3 | 1315.7 | **-30.6** | 1690.2 | 1674.2 | **-16.0** |\n",
    "| 63 | Opel | grandland x | 389 | 1461.2 | 1432.0 | **-29.2** | 2099.9 | 2006.6 | **-93.3** |\n",
    "| 46 | Mercedes | b class | 418 | 1060.5 | 1034.8 | **-25.7** | 1513.4 | 1608.3 | **+94.9** |\n",
    "| 22 | BMW | x4 | 114 | 1588.2 | 1562.5 | **-25.7** | 2113.8 | 2090.5 | **-23.2** |\n",
    "| 71 | Skoda | kamiq | 109 | 1262.1 | 1237.8 | **-24.4** | 1698.0 | 1666.2 | **-131.8** |\n",
    "| 74 | Skoda | octavia | 1021 | 942.9 | 925.8 | **-17.2** | 1330.2 | 1304.3 | **-26.0** |\n",
    "| 67 | Opel | viva | 315 | 446.2 | 437.6 | **-8.5** | 573.3 | 571.2 | **-2.1** |\n",
    "| 16 | BMW | 5 series | 727 | 1652.5 | 1645.0 | **-7.5** | 2476.1 | 2529.2 | **+53.0** |\n",
    "\n",
    "**Notes:**\n",
    "- **14 out of 15 segments show consistent wins** (negative ΔMAE and negative ΔRMSE), demonstrating genuine improvements.\n",
    "- Two segments show **MAE improvement but RMSE degradation** (Mercedes b class: ΔMAE -25.7 but ΔRMSE +94.9; BMW 5 series: ΔMAE -7.5 but ΔRMSE +53.0), suggesting fewer average errors but worse tail behavior.\n",
    "- The **largest improvements** come from premium segments with distinctive pricing (Audi q7: -130.2 MAE, Audi tt: -75.1 MAE, BMW m4: -72.0 MAE).\n",
    "\n",
    "**Top 15 degradations (largest positive ΔMAE; specialized worse than global):**\n",
    "\n",
    "| Rank | Brand | Model | n | MAE_global | MAE_pair | ΔMAE | RMSE_global | RMSE_pair | ΔRMSE |\n",
    "|---:|---|---|---:|---:|---:|---:|---:|---:|---:|\n",
    "| 6 | Audi | a_unknown | 94 | 2059.3 | 2676.8 | **+617.6** | 2783.8 | 3491.2 | **+707.4** |\n",
    "| 55 | Mercedes | s class | 134 | 3969.8 | 4389.0 | **+419.2** | 5942.4 | 6318.6 | **+376.2** |\n",
    "| 49 | Mercedes | cls class | 160 | 1778.6 | 2161.6 | **+383.0** | 3224.0 | 3918.6 | **+694.6** |\n",
    "| 57 | Mercedes | v class | 139 | 2627.2 | 2844.9 | **+217.7** | 3767.1 | 4210.4 | **+443.3** |\n",
    "| 17 | BMW | 6 series | 80 | 2223.6 | 2491.8 | **+268.2** | 3372.4 | 3497.6 | **+125.2** |\n",
    "| 87 | VW | amarok | 83 | 2395.8 | 2652.4 | **+256.5** | 3101.5 | 3450.2 | **+348.7** |\n",
    "| 5 | Audi | a7 | 83 | 1919.0 | 2148.3 | **+229.4** | 2728.0 | 2923.2 | **+195.2** |\n",
    "| 85 | Toyota | verso | 83 | 789.8 | 998.2 | **+208.4** | 1140.1 | 1364.1 | **+224.0** |\n",
    "| 97 | VW | touareg | 244 | 2187.9 | 2382.3 | **+194.4** | 3372.4 | 3828.2 | **+455.8** |\n",
    "| 82 | Toyota | corolla | 168 | 1147.9 | 1337.4 | **+189.5** | 1439.4 | 1795.6 | **+356.3** |\n",
    "| 23 | BMW | x5 | 313 | 2277.4 | 2462.9 | **+185.5** | 3056.2 | 3502.8 | **+446.6** |\n",
    "| 39 | Hyundai | i40 | 81 | 772.4 | 942.4 | **+169.9** | 1033.7 | 1300.5 | **+266.9** |\n",
    "| 54 | Mercedes | gle class | 327 | 2072.3 | 2223.3 | **+151.0** | 3121.1 | 3342.7 | **+221.6** |\n",
    "| 9 | Audi | q5 | 594 | 1726.8 | 1851.8 | **+125.0** | 2424.0 | 2687.0 | **+263.0** |\n",
    "| 4 | Audi | a6 | 483 | 1617.8 | 1727.2 | **+109.4** | 2539.6 | 2940.6 | **+401.0** |\n",
    "\n",
    "**Interpretation:**\n",
    "- Even with ≥ 80 samples, **pair-level specialization frequently overfits** and loses beneficial cross-segment learning.\n",
    "- Degradations are often **severe in both MAE and RMSE** (e.g., Audi a_unknown: +617.6 MAE, +707.4 RMSE), indicating materially worse generalization.\n",
    "- **Premium/luxury segments are particularly prone to degradation** (Mercedes s class, cls class; Audi a6, a7, q5), likely due to high price variance and limited comparable samples.\n",
    "- The degradations **outnumber improvements** in magnitude: the worst degradation (+617.6) is ~4.7× larger than the best improvement (-130.2).\n",
    "\n",
    "**Overall conclusion:**\n",
    "- **Pair-level specialization is high-risk**: improvements are possible but inconsistent, and failures can be catastrophic.\n",
    "- **Default recommendation: use the global model** unless a specific (brand, model) pair shows:\n",
    "  1. Stable negative ΔMAE across multiple CV splits\n",
    "  2. Sufficient sample size (n ≥ 200 recommended)\n",
    "  3. Consistent improvements in both MAE and RMSE\n",
    "- For the **15 pairs showing strong improvements** (particularly Audi q7, tt; Ford edge; BMW m4), pair-specific models could be deployed with monitoring.\n",
    "- For all other pairs, especially those with degradations, **the global `et_tuned` model is superior**.\n",
    "\n",
    "---\n",
    "\n",
    "**e) Alignment with objectives**\n",
    "\n",
    "This study directly answers whether additional specialization layers are justified for deployment under real data constraints, using the final tuned pipeline `et_tuned_pipe` and a consistent leakage-free CV protocol.\n",
    "\n",
    "**Deployment recommendation (based on ExtraTrees results and the observed Δ patterns):**\n",
    "- Keep a **single global model** as the default (most robust across segments).\n",
    "- Consider **brand-level specialization only selectively**, and only where it demonstrates **repeatable negative ΔMAE and/or ΔRMSE** under the same CV protocol.\n",
    "- Consider **brand–model specialization only as a gated option**:\n",
    "  - only for pairs with sufficient data (≥ 80 samples + per-fold minimum training size),\n",
    "  - only if the pair shows **stable negative ΔMAE and not unacceptable ΔRMSE** (watch tail risk),\n",
    "  - and with monitoring, because many pairs **degrade** substantially (positive Δ).\n",
    "\n",
    "This demonstrates not only a tuned final model, but also an evidence-based assessment of whether “more specialized” models actually improve pricing accuracy versus increasing complexity and overfitting risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.1 Load final tuned pipeline, define key columns, brand frequency and metric helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the squared error here and compare it to the performance on the squared error because of disproportionate computational expensive training with the absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_template = et_tuned_pipe_squared_err\n",
    "\n",
    "# Required inputs\n",
    "brand_col = \"brand\"\n",
    "model_col = \"model\"\n",
    "\n",
    "assert brand_col in X_train.columns, f\"Missing column '{brand_col}' in X_train.\"\n",
    "assert model_col in X_train.columns, f\"Missing column '{model_col}' in X_train.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = next(v for v in pipe_template.get_params(deep=True).values()\n",
    "               if v.__class__.__name__ == \"CarDataCleaner\")\n",
    "\n",
    "X_seg = X_train.copy()\n",
    "tmp = cleaner.fit_transform(X_train.copy(), y_train)\n",
    "X_seg[[brand_col, model_col]] = tmp[[brand_col, model_col]]\n",
    "\n",
    "X_train = X_seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect brand frequencies\n",
    "brand_counts = X_train[brand_col].value_counts()\n",
    "display(brand_counts.head(15).to_frame(\"count\"))\n",
    "\n",
    "\n",
    "# Select candidate brands\n",
    "#    - TOP_K: max number of brands to compare.\n",
    "#    - MIN_SAMPLES: minimum number of rows per brand.\n",
    "TOP_K = 8\n",
    "MIN_SAMPLES = 500  # ensures enough data for stable per-brand estimates\n",
    "\n",
    "candidate_brands = (\n",
    "    brand_counts[brand_counts >= MIN_SAMPLES]\n",
    "    .head(TOP_K)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(\"Candidate brands:\", candidate_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation: same folds reused everywhere for fairness and reproducibility\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "splits = list(cv.split(X_train, y_train))\n",
    "\n",
    "def mae_rmse(y_true, y_pred):\n",
    "    \"\"\"Return MAE and RMSE (RMSE computed as sqrt(MSE) for sklearn-compatibility).\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    return mae, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2 Brand-level comparison (Global vs Brand-specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_global_by_brand(pipe_template, X, y, brand_col, brands, splits):\n",
    "    \"\"\"\n",
    "    Global model evaluation per brand (out-of-fold):\n",
    "    - Fit 1 global model per fold on ALL training rows.\n",
    "    - Compute MAE/RMSE only on validation rows belonging to each brand.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(splits, start=1):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        pipe = clone(pipe_template)\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        y_pred = pipe.predict(X_va)\n",
    "\n",
    "        for b in brands:\n",
    "            mask = (X_va[brand_col] == b)\n",
    "            n = int(mask.sum())\n",
    "            if n == 0:\n",
    "                continue\n",
    "\n",
    "            mae, rmse = mae_rmse(y_va[mask], y_pred[mask])\n",
    "            rows.append({\"fold\": fold, \"brand\": b, \"MAE\": mae, \"RMSE\": rmse, \"n\": n})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    summary = (\n",
    "        df.groupby(\"brand\")\n",
    "          .apply(lambda g: pd.Series({\n",
    "              \"MAE_mean\": g[\"MAE\"].mean(),\n",
    "              \"MAE_std\":  g[\"MAE\"].std(ddof=0),\n",
    "              \"RMSE_mean\": g[\"RMSE\"].mean(),\n",
    "              \"RMSE_std\":  g[\"RMSE\"].std(ddof=0),\n",
    "              \"n\": int(g[\"n\"].sum()),\n",
    "          }))\n",
    "          .reset_index()\n",
    "    )\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_brand_specific(pipe_template, X, y, brand_col, brands, splits, min_train_per_fold=50):\n",
    "    \"\"\"\n",
    "    Brand-specific evaluation:\n",
    "    - For each fold and brand: train the SAME pipeline structure only on that brand's training rows.\n",
    "    - Evaluate only on that brand's validation rows.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(splits, start=1):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        for b in brands:\n",
    "            tr_mask = (X_tr[brand_col] == b)\n",
    "            va_mask = (X_va[brand_col] == b)\n",
    "\n",
    "            n_tr = int(tr_mask.sum())\n",
    "            n_va = int(va_mask.sum())\n",
    "\n",
    "            # These checks are necessary: some folds can have very few samples for a segment.\n",
    "            if n_va == 0 or n_tr < min_train_per_fold:\n",
    "                continue\n",
    "\n",
    "            pipe = clone(pipe_template)\n",
    "            pipe.fit(X_tr[tr_mask], y_tr[tr_mask])\n",
    "            y_pred_b = pipe.predict(X_va[va_mask])\n",
    "\n",
    "            mae, rmse = mae_rmse(y_va[va_mask], y_pred_b)\n",
    "            rows.append({\"fold\": fold, \"brand\": b, \"MAE\": mae, \"RMSE\": rmse, \"n\": n_va})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    summary = (\n",
    "        df.groupby(\"brand\")\n",
    "          .apply(lambda g: pd.Series({\n",
    "              \"MAE_mean\": g[\"MAE\"].mean(),\n",
    "              \"MAE_std\":  g[\"MAE\"].std(ddof=0),\n",
    "              \"RMSE_mean\": g[\"RMSE\"].mean(),\n",
    "              \"RMSE_std\":  g[\"RMSE\"].std(ddof=0),\n",
    "              \"n\": int(g[\"n\"].sum()),\n",
    "          }))\n",
    "          .reset_index()\n",
    "    )\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_brand = eval_global_by_brand(\n",
    "    pipe_template, X_train, y_train, brand_col, candidate_brands, splits\n",
    ").rename(columns={\n",
    "    \"MAE_mean\": \"MAE_mean_global\", \"MAE_std\": \"MAE_std_global\",\n",
    "    \"RMSE_mean\": \"RMSE_mean_global\", \"RMSE_std\": \"RMSE_std_global\",\n",
    "    \"n\": \"n_global\"\n",
    "})\n",
    "\n",
    "df_brand_spec = eval_brand_specific(\n",
    "    pipe_template, X_train, y_train, brand_col, candidate_brands, splits, min_train_per_fold=50\n",
    ").rename(columns={\n",
    "    \"MAE_mean\": \"MAE_mean_brand\", \"MAE_std\": \"MAE_std_brand\",\n",
    "    \"RMSE_mean\": \"RMSE_mean_brand\", \"RMSE_std\": \"RMSE_std_brand\",\n",
    "    \"n\": \"n_brand\"\n",
    "})\n",
    "\n",
    "df_compare_brand = df_global_brand.merge(df_brand_spec, on=\"brand\", how=\"inner\")\n",
    "df_compare_brand[\"delta_MAE\"] = df_compare_brand[\"MAE_mean_brand\"] - df_compare_brand[\"MAE_mean_global\"]\n",
    "df_compare_brand[\"delta_RMSE\"] = df_compare_brand[\"RMSE_mean_brand\"] - df_compare_brand[\"RMSE_mean_global\"]\n",
    "\n",
    "df_compare_brand = df_compare_brand.sort_values(\"delta_MAE\")\n",
    "display(df_compare_brand)\n",
    "\n",
    "# Long Duration (~3min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "x = np.arange(len(df_compare_brand))\n",
    "w = 0.35\n",
    "\n",
    "plt.bar(x - w/2, df_compare_brand[\"MAE_mean_global\"], w, label=\"Global\")\n",
    "plt.bar(x + w/2, df_compare_brand[\"MAE_mean_brand\"],  w, label=\"Brand-specific\")\n",
    "plt.xticks(x, df_compare_brand[\"brand\"], rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"MAE (GBP)\")\n",
    "plt.title(\"Global vs Brand-specific (MAE)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.bar(df_compare_brand[\"brand\"], df_compare_brand[\"delta_MAE\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Δ MAE (brand - global)\")\n",
    "plt.title(\"Effect of brand specialization (negative = MAE improvement)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.3 Brand-Model comparison (Global vs Pair-specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent (brand, model) pairs only (avoid conclusions from tiny segments)\n",
    "pair_counts = (\n",
    "    X_train.groupby([brand_col, model_col])\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "MIN_PAIR_SAMPLES = 80  # no overfitting on low sample sizes\n",
    "\n",
    "candidate_pairs = pair_counts[pair_counts >= MIN_PAIR_SAMPLES]\n",
    "\n",
    "print(f\"Number of candidate pairs (n >= {MIN_PAIR_SAMPLES}): {len(candidate_pairs)}\")\n",
    "\n",
    "# Show the most frequent pairs for context (readable table with names)\n",
    "display(candidate_pairs.head(500).reset_index(name=\"count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_global_by_pair(pipe_template, X, y, brand_col, model_col, pairs, splits):\n",
    "    \"\"\"\n",
    "    Global model evaluation per (brand, model):\n",
    "    - Fit once per fold on ALL cars.\n",
    "    - Score only on validation rows for each selected pair.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(splits, start=1):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        pipe = clone(pipe_template)\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        y_pred = pipe.predict(X_va)\n",
    "\n",
    "        for (b, m) in pairs:\n",
    "            mask = (X_va[brand_col] == b) & (X_va[model_col] == m)\n",
    "            n = int(mask.sum())\n",
    "            if n == 0:\n",
    "                continue\n",
    "\n",
    "            mae, rmse = mae_rmse(y_va[mask], y_pred[mask])\n",
    "            rows.append({\"fold\": fold, \"brand\": b, \"model\": m, \"MAE\": mae, \"RMSE\": rmse, \"n\": n})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    summary = (\n",
    "        df.groupby([\"brand\", \"model\"])\n",
    "          .apply(lambda g: pd.Series({\n",
    "              \"MAE_mean\": g[\"MAE\"].mean(),\n",
    "              \"MAE_std\":  g[\"MAE\"].std(ddof=0),\n",
    "              \"RMSE_mean\": g[\"RMSE\"].mean(),\n",
    "              \"RMSE_std\":  g[\"RMSE\"].std(ddof=0),\n",
    "              \"n\": int(g[\"n\"].sum()),\n",
    "          }))\n",
    "          .reset_index()\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "\n",
    "def eval_pair_specific(pipe_template, X, y, brand_col, model_col, pairs, splits, min_train_per_fold=40):\n",
    "    \"\"\"\n",
    "    Pair-specific models:\n",
    "    - For each fold and (brand, model): fit the pipeline only on that segment's training rows.\n",
    "    - Evaluate only on that segment's validation rows.\n",
    "\n",
    "    The min_train_per_fold guard is necessary because some folds can have too few samples\n",
    "    even if the pair is frequent overall.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(splits, start=1):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        for (b, m) in pairs:\n",
    "            tr_mask = (X_tr[brand_col] == b) & (X_tr[model_col] == m)\n",
    "            va_mask = (X_va[brand_col] == b) & (X_va[model_col] == m)\n",
    "\n",
    "            n_tr = int(tr_mask.sum())\n",
    "            n_va = int(va_mask.sum())\n",
    "\n",
    "            if n_va == 0 or n_tr < min_train_per_fold:\n",
    "                continue\n",
    "\n",
    "            pipe = clone(pipe_template)\n",
    "            pipe.fit(X_tr[tr_mask], y_tr[tr_mask])\n",
    "            y_pred = pipe.predict(X_va[va_mask])\n",
    "\n",
    "            mae, rmse = mae_rmse(y_va[va_mask], y_pred)\n",
    "            rows.append({\"fold\": fold, \"brand\": b, \"model\": m, \"MAE\": mae, \"RMSE\": rmse, \"n\": n_va})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    summary = (\n",
    "        df.groupby([\"brand\", \"model\"])\n",
    "          .apply(lambda g: pd.Series({\n",
    "              \"MAE_mean\": g[\"MAE\"].mean(),\n",
    "              \"MAE_std\":  g[\"MAE\"].std(ddof=0),\n",
    "              \"RMSE_mean\": g[\"RMSE\"].mean(),\n",
    "              \"RMSE_std\":  g[\"RMSE\"].std(ddof=0),\n",
    "              \"n\": int(g[\"n\"].sum()),\n",
    "          }))\n",
    "          .reset_index()\n",
    "    )\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_list = list(candidate_pairs.index)  # list of (brand, model) tuples\n",
    "\n",
    "df_global_pair = eval_global_by_pair(\n",
    "    pipe_template, X_train, y_train, brand_col, model_col, pairs_list, splits\n",
    ").rename(columns={\n",
    "    \"MAE_mean\": \"MAE_mean_global\", \"MAE_std\": \"MAE_std_global\",\n",
    "    \"RMSE_mean\": \"RMSE_mean_global\", \"RMSE_std\": \"RMSE_std_global\",\n",
    "    \"n\": \"n_global\"\n",
    "})\n",
    "\n",
    "df_pair_spec = eval_pair_specific(\n",
    "    pipe_template, X_train, y_train, brand_col, model_col, pairs_list, splits, min_train_per_fold=40\n",
    ").rename(columns={\n",
    "    \"MAE_mean\": \"MAE_mean_pair\", \"MAE_std\": \"MAE_std_pair\",\n",
    "    \"RMSE_mean\": \"RMSE_mean_pair\", \"RMSE_std\": \"RMSE_std_pair\",\n",
    "    \"n\": \"n_pair\"\n",
    "})\n",
    "\n",
    "df_compare_pair = df_global_pair.merge(df_pair_spec, on=[\"brand\", \"model\"], how=\"inner\")\n",
    "df_compare_pair[\"delta_MAE\"] = df_compare_pair[\"MAE_mean_pair\"] - df_compare_pair[\"MAE_mean_global\"]\n",
    "df_compare_pair[\"delta_RMSE\"] = df_compare_pair[\"RMSE_mean_pair\"] - df_compare_pair[\"RMSE_mean_global\"]\n",
    "\n",
    "df_compare_pair = df_compare_pair.sort_values(\"delta_MAE\")\n",
    "\n",
    "# Full results (all frequent pairs) are here:\n",
    "display(df_compare_pair)\n",
    "\n",
    "# For the report: show the most improved + most harmed (readable subset)\n",
    "display_cols = [\"brand\", \"model\", \"n_global\", \"MAE_mean_global\", \"MAE_mean_pair\", \"delta_MAE\",\n",
    "                \"RMSE_mean_global\", \"RMSE_mean_pair\", \"delta_RMSE\"]\n",
    "\n",
    "print(\"Top 15 improvements (most negative ΔMAE):\")\n",
    "display(df_compare_pair[display_cols].head(15).round(1))\n",
    "\n",
    "print(\"Top 15 degradations (most positive ΔMAE):\")\n",
    "display(df_compare_pair[display_cols].tail(15).round(1))\n",
    "\n",
    "# Plots (same as before): ΔMAE bar plot for stable segments + scatter vs size\n",
    "MIN_PLOT_SAMPLES = 100\n",
    "df_plot = df_compare_pair[df_compare_pair[\"n_global\"] >= MIN_PLOT_SAMPLES].copy()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "x = np.arange(len(df_plot))\n",
    "plt.bar(x, df_plot[\"delta_MAE\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xticks(\n",
    "    x,\n",
    "    [f\"{b} {m}\" for b, m in zip(df_plot[\"brand\"], df_plot[\"model\"])],\n",
    "    rotation=90, ha=\"right\"\n",
    ")\n",
    "plt.ylabel(\"Δ MAE (pair - global)\")\n",
    "plt.title(\"Effect of (brand, model) specialization (negative = improvement)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(df_compare_pair[\"n_global\"], df_compare_pair[\"delta_MAE\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xlabel(\"Number of samples per (brand, model) (out-of-fold counted)\")\n",
    "plt.ylabel(\"Δ MAE (pair - global)\")\n",
    "plt.title(\"ΔMAE vs segment size\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Long Duration (~5min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Price Predictor Web Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ML Cars 4 You` Web Application is implemented in `app.py` and hosted live: \n",
    "\n",
    "👉 **[Click here to open the ML Cars 4 You App](https://ml-cars-4-you.streamlit.app)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.) Objective and Motivation\n",
    "\n",
    "**Project Goal**  \n",
    "\n",
    "The objective of this web application is to provide an accessible, user-friendly interface for predicting used car prices based on our best machine learning model. This deployment serves as the practical implementation component of the Open End Section, transforming our trained model into a production-ready tool.\n",
    "\n",
    "**Motivation**\n",
    "- **Accessibility**: Make the ML model accessible to non-technical users through an intuitive web interface\n",
    "- **User Experience**: Provide instant price predictions with intervals to help users make informed decisions\n",
    "\n",
    "**Target Users**\n",
    "- Used car buyers seeking fair price estimates\n",
    "- Car dealers looking for market-competitive pricing\n",
    "- People who want to sell their cars and want to get an idea about the potential price\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.) Difficulty of Tasks\n",
    "\n",
    "1. Model Size Management (High Difficulty)\n",
    "- **Challenge**: The trained model file (`rf_tuned_pipe.pkl`) was 900 mb, far exceeding GitHub's 100 MB limit\n",
    "- **Solution**: Implemented Git Large File Storage (LFS) to handle the massive model file\n",
    "- **Complexity**: Required understanding of Git LFS, bandwidth limitations, and deployment constraints\n",
    "\n",
    "2. Dynamic Feature Engineering (Medium Difficulty)\n",
    "- **Challenge**: The model uses custom preprocessing functions from `pipeline_functions.py`\n",
    "- **Solution**: Ensured all custom transformers and functions are properly included in the repository\n",
    "- **Complexity**: Maintaining consistency between training pipeline and deployment environment\n",
    "\n",
    "3. Brand-Model Dependency Mapping (Medium-Low Difficulty)\n",
    "- **Challenge**: Creating a user-friendly interface where model selection dynamically updates based on brand choice\n",
    "- **Solution**: Implemented a nested dictionary structure with 9 brands and 114 models\n",
    "- **Complexity**: Balancing comprehensive coverage with UI simplicity\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.) Correctness and Efficiency of Implementation\n",
    "\n",
    "**Accurate Prediction Pipeline**\n",
    "- The `load_model()` function correctly loads the sklearn pipeline\n",
    "- Input data is formatted as a pandas DataFrame with exact feature names and types expected by the model\n",
    "\n",
    "**Input Validation**\n",
    "- Realistic min/max values based on dataset ranges (e.g., year: 1970-2020)\n",
    "- Step increments appropriate for each input type\n",
    "- Default values represent typical vehicles\n",
    "\n",
    "**Performance Optimizations**\n",
    "- `@st.cache_resource` decorator caches the model in memory (loads only once)\n",
    "- Single prediction call per button press (no redundant computations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.) Discussion of Results\n",
    "\n",
    "This web application successfully bridges the gap between ML model development and real-world deployment. Despite technical challenges, the implementation achieves its core objectives:\n",
    "\n",
    "- **Accessible**: Anyone can use the tool without ML knowledge  \n",
    "- **Accurate**: MAE of £1,200 provides actionable price estimates  \n",
    "- **Efficient**: Cached model and optimized code ensure fast predictions  \n",
    "\n",
    "The deployment demonstrates practical ML engineering skills including version control (Git LFS), dependency management, cloud deployment (Streamlit Cloud), and user-centric design.  \n",
    "\n",
    "\n",
    "However, this tool has important limitations:\n",
    "\n",
    "- **Brand Bias**: Performance is strongest on well-represented brands while predictions for underrepresented manufacturers may be less reliable\n",
    "- **Temporal Relevance**: The model was trained on data up to 2020 and may not fully capture recent market dynamics such as semiconductor shortages or accelerated EV adoption\n",
    "- **Edge Cases**: Predictions for rare models, unusual configurations, or vehicles at price extremes should be treated with additional caution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see `OES_DLExperiment.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Deployment and Prediction on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the notebook includes the deployment to generate reliable predictions for new data. The pipeline is stored in the `et_tuned_pipe.pkl` file and the final output of the predicted test data is stored in `Group05_Version20.csv` (selected as best on Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(et_tuned_pipe, \"et_tuned_pipe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test(model_pipeline, model_name):\n",
    "    # Load best model from Joblib and predict on validation set to verify\n",
    "    pipe_best = joblib.load(model_pipeline)\n",
    "    \n",
    "    # Predict on test set\n",
    "    df_cars_test['price'] = pipe_best.predict(df_cars_test)\n",
    "    df_cars_test[['carID', 'price']].to_csv(f'Group05_Version20.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_test(\"et_tuned_pipe.pkl\", \"ET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c cars4you -f Group05_Version05.csv -m \"Message\" # Uncomment to submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submissions -c cars4you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**     \n",
    "The test score is close to validation score so our model should generalize well, assuming that the data in validation and test is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Discussion and Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree-based models performed best:**     \n",
    "Regarding the baseline models we used in Section 5, it becomes clear that the tree-based models outperform the other models. This is probably due to the capability of trees to split on most important characteristics of the cars. Especially, for target encoded features that contain rich information about the price of the car.\n",
    "\n",
    "**Constraints:**     \n",
    "However, the nature of tree-based models constraints the predictions to never be lower than the lowest or higher than the highest price in the train set. This is because tree-based models return the average price of the cars in the leaf node.\n",
    "\n",
    "**Outlook:**     \n",
    "A potential solution of this could be a **Stacking Regressor** that combines a tree-based model with another model that is better in extrapolation (e.g. RF + Linear Regression). A final Meta-Learner (also Linear) can combine their predictions. If the RF predicts \"Max Value\" but the Linear Model predicts \"Higher Value,\" the Meta-Learner could follow the Linear trend upward.    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "group05_main_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
