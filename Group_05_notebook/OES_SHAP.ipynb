{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6df97636-355b-48e4-9c26-f188a4eb8029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install shap\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f54ab11e-fcfc-4278-8997-89a92adf8eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import and load Data"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({\"figure.max_open_warning\": 0, \"figure.dpi\": 100})\n",
    "import shap\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "375bef87-944e-4bc0-bb70-727575f55635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "et_tuned_pipe = joblib.load(\"et_tuned_pipe.pkl\") # The main notebook has to be run to create this file. It is not included in the zip due to its file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7635b5e1-9e97-41d2-b598-1d9958fbc39e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preprocessor_pipe = et_tuned_pipe.named_steps['preprocess']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ec41562-43a9-4f5c-926b-87a030817982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### SHAP Interpretability for Our Final Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6ab8416-e284-420e-b706-49813911001d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**a) Objective and motivation**\n",
    "\n",
    "After our end-to-end pipeline is finished, we use **SHAP (SHapley Additive exPlanations)**.\n",
    "\n",
    "Goals:\n",
    "- Identify the **most influential features** for the final tuned model (`et_tuned_pipe`).\n",
    "- Validate whether feature effects are **plausible** (age, mileage, engine, etc.).\n",
    "- Check how much **target encodings** and engineered interactions contribute.\n",
    "\n",
    "---\n",
    "\n",
    "**b) Difficulty of the task**\n",
    "\n",
    "This is non-trivial because SHAP must explain the model input **after** our preprocessing and feature selection:\n",
    "\n",
    "- The model does not see raw columns.  \n",
    "  It sees: engineered numeric features (e.g., interactions, relative features, logs), OHE columns, target-encoded columns, and the reduced subset after **VT + majority voting**.\n",
    "- We therefore reconstruct:\n",
    "  - the exact **post-preprocess feature matrix**, and\n",
    "  - aligned **feature names** after applying both selection masks (VT support + majority selector mask).\n",
    "- Because the full pipeline includes engineered preprocessing + selection, we treat the tuned pipeline as a **black box** and use SHAP via a **PermutationExplainer** (robust but expensive).\n",
    "- Runtime: SHAP is costly, so we explain only a **subsample** (`sample_size=1000`) with a small background set.\n",
    "\n",
    "---\n",
    "\n",
    "**c) Correctness and efficiency**\n",
    "\n",
    "We kept the analysis correct and consistent with the production pipeline:\n",
    "\n",
    "- **No leakage / no optimization loop:** SHAP is computed on the already-fitted `et_tuned_pipe` and used only for interpretation.\n",
    "- **Exact alignment:** feature names come from the ColumnTransformer output and are then filtered by VT + majority voting masks.\n",
    "- **Global SHAP importance:** features are ranked by mean absolute contribution:\n",
    "\n",
    "  $$\n",
    "  Importance(feature_j) = \\frac{1}{N}\\sum_{i=1}^{N} |SHAP_{i,j}|\n",
    "  $$\n",
    "\n",
    "- **Efficient computation:** stable ranking via subsampling (PermutationExplainer on 1000 rows; runtime ≈ 21 minutes).\n",
    "\n",
    "---\n",
    "\n",
    "**d) Results and interpretation**\n",
    "\n",
    "**Model context**\n",
    "- Final tuned model: `et_tuned_pipe` (**ExtraTrees**)\n",
    "- Total features used after preprocessing + FS: **30**\n",
    "- SHAP explainer used: **PermutationExplainer**  \n",
    "  (1001 iterations; runtime ≈ 21 minutes)\n",
    "\n",
    "**Top drivers (mean |SHAP|), excerpt**\n",
    "\n",
    "| Feature | Importance | Interpretation |\n",
    "|---|---:|---|\n",
    "| `median_te__model` | 1425.87 | Model-level median target encoding (strong price anchor) |\n",
    "| `mean_te__model` | 1312.21 | Model-level mean target encoding (market value proxy) |\n",
    "| `num__mpg_x_age` | 861.84 | Interaction capturing efficiency–age trade-offs |\n",
    "| `num__age` | 741.45 | Direct age / depreciation signal |\n",
    "| `num__engineSize` | 617.75 | Engine size as segment & performance proxy |\n",
    "| `num__age_rel_brand` | 615.97 | Age relative to typical age within brand |\n",
    "| `log__mileage` | 595.59 | Non-linear mileage effect (diminishing impact) |\n",
    "| `cat__transmission_Manual` | 578.61 | Manual transmission effect |\n",
    "| `mean_te__brand_trans` | 496.39 | Brand × transmission mean target encoding |\n",
    "| `num__age_rel_model` | 445.87 | Age relative to typical age of the model |\n",
    "| `median_te__brand_trans` | 434.62 | Brand × transmission median target encoding |\n",
    "| `num__engine_per_mpg` | 321.67 | Performance vs. efficiency ratio |\n",
    "| `log__miles_per_year` | 216.26 | Usage intensity normalized by age |\n",
    "\n",
    "**Key takeaways**\n",
    "\n",
    "- **Target encodings dominate global importance**, especially at the *model* level (`median_te__model`, `mean_te__model`).\n",
    "- **Transmission is highly influential**, with `cat__transmission_Manual` ranking in the top 10.\n",
    "- **Age and mileage effects are main drivers**, appearing in raw, relative, and non-linear forms.\n",
    "- **Engine matters**, being in the top features right after TE models and age related features\n",
    "\n",
    "### **Beeswarm plot observations**\n",
    "\n",
    "- **`median_te__model` and `mean_te__model` show the widest SHAP dispersion**, confirming model identity as the strongest pricing signal.\n",
    "- **Manual transmission has a clear directional effect**:  \n",
    "  `cat__transmission_Manual = 1` tends to push predictions **down**, while non-manual pushes them **up**.\n",
    "- **Mileage is strongly non-linear** (`log__mileage`), with diminishing marginal impact at high values.\n",
    "- **Relative age features sharpen depreciation effects**, especially when vehicles are older than typical for their brand or model.\n",
    "\n",
    "---\n",
    "\n",
    "## **e) Alignment with objectives**\n",
    "\n",
    "This SHAP analysis improves transparency of the models decision making:\n",
    "\n",
    "- SHAP is used **only for post-hoc interpretation** of the final tuned pipeline.\n",
    "- The dominant drivers—**target encodings, age, mileage, transmission, and engineered interactions**—are domain-consistent and support trust in the final ExtraTrees model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14e95284-b5e6-4bc5-bed0-dcbd0bc08895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e482273-1672-43c3-85da-021bab9a6730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rs = 5\n",
    "# Get Feature names aligned with X_proc (after preprocess incl. VT + majority voting)\n",
    "def get_pipeline_feature_matrix(pipe, X, preprocessor_pipe):\n",
    "    \"\"\"\n",
    "    Given a fitted model pipeline with steps:\n",
    "      'preprocess' -> 'model'\n",
    "    where preprocess itself is a Pipeline:\n",
    "      clean -> group_imputer -> fe -> ct -> fs(vt + selector)\n",
    "    return:\n",
    "      X_proc: 2D numpy array of features just before the model step\n",
    "      feat_names: 1D np.array of feature names aligned with X_proc columns\n",
    "    \"\"\"\n",
    "    pre = pipe.named_steps[\"preprocess\"]\n",
    "\n",
    "    # Transform to model-ready matrix and get feature names\n",
    "    X_proc = pre.transform(X)\n",
    "    feat_names = preprocessor_pipe.named_steps['fs'].get_feature_names_out()\n",
    "\n",
    "    return X_proc, feat_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "376efbe1-3436-4902-bf58-e6d72d8ec213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compute SHAP Importance\n",
    "def compute_shap_importance(\n",
    "    pipe,\n",
    "    X,\n",
    "    sample_size=1000,\n",
    "    seed=rs,\n",
    "    model_name=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute global SHAP feature importances for a fitted pipeline (informative only).\n",
    "\n",
    "    Fix:\n",
    "      - TreeExplainer additivity check can fail for some sklearn tree implementations (incl. HGB).\n",
    "        We disable it via check_additivity=False.\n",
    "      - If TreeExplainer still fails, fall back to a model-agnostic SHAP explainer.\n",
    "    \"\"\"\n",
    "    # Extract processed feature matrix and names\n",
    "    X_proc, feat_names = get_pipeline_feature_matrix(pipe, X, preprocessor_pipe)\n",
    "\n",
    "    # Convert to numpy if DataFrame (for proper integer indexing)\n",
    "    if isinstance(X_proc, pd.DataFrame):\n",
    "        X_proc = X_proc.values\n",
    "    \n",
    "    # Subsample rows for SHAP (for speed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = min(sample_size, len(X_proc))\n",
    "    idx = rng.choice(len(X_proc), n, replace=False)\n",
    "    X_sample = X_proc[idx]\n",
    "\n",
    "    # Underlying model (last step in pipeline)\n",
    "    model = pipe.named_steps[\"model\"]\n",
    "    tag = model_name or model.__class__.__name__\n",
    "\n",
    "    # Background for SHAP (small subset)\n",
    "    bg_n = min(200, len(X_sample))\n",
    "    bg_idx = rng.choice(len(X_sample), bg_n, replace=False)\n",
    "    X_bg = X_sample[bg_idx]\n",
    "\n",
    "    # Try TreeExplainer first (fast for tree models)\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model, X_bg)\n",
    "        shap_vals = explainer.shap_values(X_sample, check_additivity=False)\n",
    "\n",
    "        # shap_vals can be list-like in some setups; regression should be 2D\n",
    "        if isinstance(shap_vals, list):\n",
    "            shap_vals = shap_vals[0]\n",
    "\n",
    "        base_vals = getattr(explainer, \"expected_value\", 0.0)\n",
    "        shap_values = shap.Explanation(\n",
    "            values=shap_vals,\n",
    "            base_values=np.full((len(X_sample),), base_vals) if np.isscalar(base_vals) else base_vals,\n",
    "            data=X_sample,\n",
    "            feature_names=feat_names,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback: model-agnostic explainer (slower but robust)\n",
    "        explainer = shap.Explainer(model.predict, X_bg, feature_names=feat_names)\n",
    "        shap_values = explainer(X_sample)\n",
    "\n",
    "    importance = np.abs(shap_values.values).mean(axis=0)\n",
    "\n",
    "    shap_df = (\n",
    "        pd.DataFrame({\"feature\": feat_names, \"importance\": importance})\n",
    "        .sort_values(\"importance\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(f\"Features by SHAP for {tag}:\")\n",
    "    print(shap_df.head(40).to_string(index=False))\n",
    "\n",
    "    return shap_df, feat_names, shap_values, X_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b60df542-e409-4e40-9e0d-88b89df8ee79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SHAP Plots\n",
    "def plot_top_shap_bar(shap_df, model_name, top_k):\n",
    "    \"\"\"\n",
    "    Horizontal bar plot of top_k features by mean |SHAP|.\n",
    "    \"\"\"\n",
    "    top_df = shap_df.head(top_k).iloc[::-1]  # reverse for nicer barh order\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.barh(top_df[\"feature\"], top_df[\"importance\"])\n",
    "    ax.set_xlabel(\"Average |SHAP| value\")\n",
    "    ax.set_title(f\"Top {top_k} features by SHAP – {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_shap_beeswarm(shap_values, X_sample, feat_names, model_name, max_display=20):\n",
    "    \"\"\"\n",
    "    SHAP summary (beeswarm) plot for top features.\n",
    "    \"\"\"\n",
    "    X_df = pd.DataFrame(X_sample, columns=feat_names)\n",
    "\n",
    "    # Create one figure and tell SHAP not to auto-show\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values.values, X_df, max_display=max_display, show=False)\n",
    "\n",
    "    plt.title(f\"SHAP Beeswarm – {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e9d6767-48ad-4d33-b40b-49861baeb6b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### SHAP of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aef535f9-31ab-455c-9fba-1ff8631f4de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ExtraTrees baseline report + SHAP\n",
    "et_pipe = et_tuned_pipe\n",
    "df_cars_train = pd.read_csv(\"train.csv\").rename(columns={\"Brand\": \"brand\",\n",
    "                                                        \"paintQuality%\": \"paintQuality\"})\n",
    "X_train = df_cars_train.drop(columns='price')\n",
    "\n",
    "# Feature matrix + names after preprocess (clean+impute+fe+ct+fs)\n",
    "X_proc_et, feat_names_et = get_pipeline_feature_matrix(et_pipe, X_train, preprocessor_pipe)\n",
    "n_features_total_et = X_proc_et.shape[1]\n",
    "\n",
    "print(\"ExtraTrees (tuned pipe) - feature space info:\")\n",
    "print(f\"Total features used: {n_features_total_et}\")\n",
    "\n",
    "shap_importance_et, feat_names_et, shap_vals_et, X_sample_et = compute_shap_importance(\n",
    "    et_pipe,\n",
    "    X_train,\n",
    "    sample_size=1000,\n",
    "    seed=rs,\n",
    "    model_name=\"ExtraTrees\",\n",
    ")\n",
    "\n",
    "plot_top_shap_bar(shap_importance_et, model_name=\"ExtraTrees\", top_k=n_features_total_et)\n",
    "plot_shap_beeswarm(shap_vals_et, X_sample_et, feat_names_et, model_name=\"ExtraTrees\", max_display=n_features_total_et)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "V2Open_End_Section_SHAP",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
