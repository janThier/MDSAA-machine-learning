{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156fc835-43ad-4e58-a2ea-e12bb559c844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Open Questions (maybe ask Ricardo):\n",
    "# - Allowed to use QuantileEncoder from category_encoders or do we have to use TargetEncoder from sklearn? Can we use both because it improves our MAE or is it generally bad practice to involve multiple encodings of one feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0d9d0c-5598-4a4e-b28b-a2921d27ce73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Your work will be evaluated according to the following criteria:**\n",
    "- Project Structure and Notebook(s) Quality (4/20)\n",
    "- Data Exploration & Initial Preprocessing (4/20)\n",
    "- Regression Benchmarking and Optimization (7/20)\n",
    "- Open-Ended Section (4/20)\n",
    "- Deployment (1/20)\n",
    "- Extra Point: Have Project Be Publicly Available on GitHub (1/20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4467ab3-7594-4e02-a19f-8c5e47fb14e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"\n",
    "    background: rgba(25, 25, 25, 0.55);\n",
    "    backdrop-filter: blur(16px) saturate(150%);\n",
    "    -webkit-backdrop-filter: blur(16px) saturate(150%);\n",
    "    border: 1px solid rgba(255, 255, 255, 0.12);\n",
    "    border-radius: 18px;\n",
    "    padding: 45px 30px;\n",
    "    text-align: center;\n",
    "    font-family: 'Inter', 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;\n",
    "    color: #e0e0e0;\n",
    "    box-shadow: 0 0 30px rgba(0, 0, 0, 0.35);\n",
    "    margin: 40px auto;\n",
    "    max-width: 800px;\n",
    "\">\n",
    "\n",
    "  <h1 style=\"\n",
    "      font-size: 2.8em;\n",
    "      font-weight: 700;\n",
    "      margin: 0 0 8px 0;\n",
    "      letter-spacing: -0.02em;\n",
    "      background: linear-gradient(90deg, #00e0ff, #9c7eff);\n",
    "      -webkit-background-clip: text;\n",
    "      -webkit-text-fill-color: transparent;\n",
    "  \">\n",
    "      Machine Learning Project\n",
    "  </h1>\n",
    "\n",
    "  <h2 style=\"\n",
    "      font-size: 1.6em;\n",
    "      font-weight: 500;\n",
    "      margin: 0 0 25px 0;\n",
    "      color: #b0b0b0;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Cars 4 You - Predicting Car Prices\n",
    "  </h2>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.25em;\n",
    "      font-weight: 500;\n",
    "      color: #c0c0c0;\n",
    "      margin-bottom: 6px;\n",
    "  \">\n",
    "      Group 5 - Lukas Belser, Samuel Braun, Elias Karle, Jan Thier\n",
    "  </p>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.05em;\n",
    "      font-weight: 400;\n",
    "      color: #8a8a8a;\n",
    "      font-style: italic;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Machine Learning End Results · 22.12.2025\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372c87aa-b7f3-4a34-a596-bb5e584db114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Table of Contents**\n",
    " \n",
    "- [1. Import Packages and Data](#1-import-packages-and-data)  \n",
    "  - [1.1 Import Required Packages](#11-import-required-packages)  \n",
    "  - [1.2 Load Datasets](#12-load-datasets)  \n",
    "  - [1.3 Kaggle Setup](#13-kaggle-setup)  \n",
    "- [2. Preprocessing](#2-data-cleaning-feature-engineering-split--preprocessing)  \n",
    "  - [2.1 Data Cleaning](#21-data-cleaning)  \n",
    "  - [2.2 Feature Engineering](#22-feature-engineering)  \n",
    "  - [2.3 (No) Data Split](#23-data-split)  \n",
    "  - [2.4 Encoding, Transforming and Scaling](#24-preprocessing)  \n",
    "  - [2.5. Feature Selection](#3-feature-selection)  \n",
    "- [4. Model Evaluation Metrics, Baselining, Setup](#4-model-evaluation-metrics-baselining-setup)  \n",
    "- [5. Hyperparameter Tuning and Model Evaluation](#5-hyperparameter-tuning-and-model-evaluation)  \n",
    "  - [5.1 ElasticNet](#51-elasticnet)  \n",
    "  - [5.2 HistGradientBoost](#52-histgradientboost)  \n",
    "  - [5.3 RandomForest](#53-randomforest)  \n",
    "  - [5.4 ExtraTrees](#54-extratrees)  \n",
    "- [6. Feature Importance of Tree Models (with SHAP)](#6-feature-importance-of-tree-models-with-shap)  \n",
    "  - [6.1 HGB](#61-hgb)  \n",
    "  - [6.2 RF](#62-rf)  \n",
    "- [7. Kaggle Competition](#7-kaggle-competition)  \n",
    "\n",
    "TODO finish + update toc > at the end of project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/process_ML.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group Member Contribution**    \n",
    "What part(s) of the work were done by each member and an estimated %\n",
    "contribution of each member towards the final work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Project Outline (Abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifying Business Needs**      \n",
    "Overview and main goals of the project:\n",
    "- ...\n",
    "\n",
    "Description of the overall process and identification of model assessment approach adopted in the work:\n",
    "- Cross-Validation approach for model assessment following the steps outlined in the image.\n",
    "- For more details on the respective steps in the pipeline, refer to the specific part in the notebook below.\n",
    "\n",
    "[image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add visualization of the pipe in a markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Exploration**     \n",
    "For Data Exploration of the original and the engineered features including the consequences for preprocessing, refer to notebook `group05_exploratory_data_analysis.ipynb`.\n",
    "\n",
    "Top 3 Key insights:\n",
    "- ...\n",
    "\n",
    "The findings from the EDA are used for the following steps taken to clean and prepare the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fff3e4-624f-4b1f-b258-fbbe7b69cb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Import Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e6aadb-727a-4399-8888-9d6d4e593898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aa35af5d-8257-4fb6-a380-86bbbfac3244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!pip install shap\n",
    "!pip install -U scikit-learn\n",
    "!pip install category_encoders\n",
    "!pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f54ab11e-fcfc-4278-8997-89a92adf8eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import and load Data"
    }
   },
   "outputs": [],
   "source": [
    "# TODO Imports klassifizieren, was für welchen Part genutzt wird\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({\"figure.max_open_warning\": 0, \"figure.dpi\": 100})\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE\n",
    "from scipy.stats import spearmanr, uniform, randint\n",
    "from sklearn.metrics import mean_absolute_error\n",
    " \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder, StandardScaler, FunctionTransformer, RobustScaler\n",
    "from sklearn.base import clone, BaseEstimator, TransformerMixin\n",
    " \n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, GridSearchCV, cross_validate, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from category_encoders import QuantileEncoder # used for median target encoding (sklearn only supports mean target encoding with their TargetEncoder class)\n",
    " \n",
    "from pipeline_functions import CarDataCleaner, OutlierHandler, GroupImputer, CarFeatureEngineer, NamedFunctionTransformer, model_hyperparameter_tuning, DebugTransformer, MajorityVoteSelectorTransformer, MutualInfoThresholdSelector, SpearmanRelevancyRedundancySelector, create_model_pipe, get_cv_results\n",
    "from visualization_functions import plot_selector_agreement, plot_val_mae_comparison, plot_train_val_comparison\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.inspection import permutation_importance\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9737aef-d2bd-4748-89ed-9b0e2cea8637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.2 Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3da1ad2b-8ee0-48b0-917d-545c70633ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cars_train = pd.read_csv(\"train.csv\").rename(columns={\"Brand\": \"brand\"})\n",
    "df_cars_test = pd.read_csv(\"test.csv\").rename(columns={\"Brand\": \"brand\"})\n",
    "\n",
    "# TODO rename columns here instead of in cleaning file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab6500b5-94a1-4a0e-b7f6-ea9d35c787eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Kaggle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "556ffc24-70f7-48c9-96a4-3a19380f91db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Connect"
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle API Connect\n",
    "\n",
    "# Folder containing kaggle.json\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/Workspace/Users/20250355@novaims.unl.pt\" #add your own kaggle.json api token\n",
    "\n",
    "# Test\n",
    "!echo $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75ee214-0996-406f-b77f-9e72fa7153e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7890fb35-e1ca-463b-b83b-262df6947367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c7ca8f-1b74-4848-89b9-2bac509cb4db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- Train and Val: We use `Cross-Validation` in the `sklearn pipeline` on the available training data to make use of all data while validating different approaches.    \n",
    "-> We fix the random states everywhere to ensure that all models use the same split to ensure a fair model comparison\n",
    "- Test: Use external hold-out set from kaggle as final test set (remains completely unseen to avoid leakage)\n",
    "-> An additional val set is therefore not necessary and would waste training data\n",
    "\n",
    "1. **Training Set (n-1 folds from CV)**: Used to fit models.\n",
    "2. **Validation Set (1 fold from CV)**: Used to evaluate performance of models and tune hyperparameters, detect overfitting. \n",
    "3. **Test Set (Kaggle)**: Used only once at the end of the entire process to evaluate final model performance. Not considered before to prevent leakage.\n",
    "\n",
    "\n",
    "\n",
    "<u>Place in the pipe:</u> The split is decided here because the data has to be split before the preprocessing steps to avoid data leakage. All of the following steps are part of the sklearn pipeline while the CV is not an explicit part of the pipeline but rather the technique that calls the pipeline with its separate folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c64124-1444-462c-a986-0d06cdb71d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create CV (shuffle to ensure randomness in splits, random_state to make it reproducible and comparable across models)\n",
    "rs = 5\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=rs) # TODO maybe increase split or use RepeatedKFold(n_splits=10, n_repeats=3) for final tuning\n",
    "# => This cv will be passed for hyperparameter tuning later when training the models\n",
    "\n",
    "# Split features and target\n",
    "X_train = df_cars_train.drop(columns='price')\n",
    "y_train = df_cars_train['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8287784a-0d6e-4a6b-87d1-ee92ec55485f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**\n",
    "- CV achieves better results than using a hold-out set\n",
    "\n",
    "**Consequences/Interpretation:**\n",
    "- Usage of all available data is better for the model than 'wasting' training data for a hold-out set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53a2a97-4662-40a3-8b24-b238dad5138e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de98bdb8-509e-4be3-b275-3925088ba791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- We `clean data inconsistencies` and data entry errors that we found in the EDA\n",
    "- These columns will be `set to NaN` for that specific entry to not lose rows in the data due to removing\n",
    "- Afterwards, this value will be imputed (see Section 2.3)\n",
    "\n",
    "| **Feature** | **Allowed thresholds** | **Reasoning** | **# filtered below threshold** | **# filtered above threshold** |\n",
    "| :--- | :--- | :--- | :---: | :---: |\n",
    "| `year` | 1886 to 2020 (inclusive) | The first car was built in 1886; the dataset is from 2020, so newer cars are logically impossible. | . | ... |\n",
    "| `mileage` | ≥ 0 | Negative mileage is not possible. | . | . |\n",
    "| `tax` | ≥ 0 | Negative tax is not possible. | . | . |\n",
    "| `mpg` | 5 to 60 (inclusive) | Realistic range for mass-market road cars. U.S. EPA list (2025): least efficient Bugatti Mistral (9 mpg), most efficient Toyota Prius (57 mpg). | . | . |\n",
    "| `engineSize` | 0.6 to 12.7 (inclusive) | Practical bounds: smallest mass-production cars are in the Japanese kei class (~0.66L); very large historical production engines up to Bugatti Type 41 Royale (12.7L). | . | . |\n",
    "| `paintQuality%` → `paintQuality` | 0 to 100 (inclusive) | Percentage values must be between 0 and 100. | . | . |\n",
    "| `previousOwners` | ≥ 0 | Negative owner counts are not possible. | . | . |\n",
    "| `hasDamage` | - | Only 0 and NaN values in the data -> no thresholding | . | . |\n",
    "| TODO Categoricals | | | | |\n",
    "\n",
    "**Legend**\n",
    "- `-` means “not applicable”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f5aae4cf-4df3-4f71-bff7-8a736d9a5b52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the data cleaning by running it on raw df and inspect uniques\n",
    "cleaner = CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)\n",
    "\n",
    "df_cars_train_clean = cleaner.fit_transform(df_cars_train)\n",
    "df_cars_test_clean  = cleaner.transform(df_cars_test)\n",
    "\n",
    "# Don't print values for test set because that would be data leakage (test set remains unseen until final prediction)\n",
    "print(\"CLEANED TRAIN uniques\")\n",
    "for col in df_cars_train_clean.columns:\n",
    "    print(col, df_cars_train_clean[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393a3fff-9a33-43c1-979f-e31b6d1c5574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**\n",
    "- The findings are already included in the table above for easier overview and direct comparison\n",
    "    - 'Number of filtered values below threshold'\n",
    "    - 'Number of filtered values above threshold'\n",
    "\n",
    "==> In total, [TODO] values are identified as data errors in the available training data and are set to NaN\n",
    "\n",
    "**Consequences/Interpretation:**\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e48e30f-ff20-46b7-94e1-d318088733da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.3 Outlier Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD by Jan     \n",
    "**Our approach:**\n",
    "- After the obvious logical inconsistencies in the data were already targetted in the data cleaning section we aim to identify other extreme values here\n",
    "- These extreme values have one of two natures: \n",
    "    - wrong values for that specific model but missed in the data cleaning \n",
    "    - extreme but valid values that distort the distribution\n",
    "- Outlier detection through multiple methods to increase the probability that it's actually an outlier\n",
    "- ...\n",
    "\n",
    "\n",
    "<u>Place in the pipe</u>: Before imputation to use original distribution for identifying the outliers (otherwise we would inflate the distributions with the imputed values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac78688e-006e-481a-9106-0292a5216301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Our approach:**\n",
    "- We treat outliers as a **data quality + robustness problem**, not as a reason to delete rare cars.\n",
    "- We keep the process **leakage-safe** by implementing outlier handling as an sklearn transformer inside the pipeline (thresholds learned on training folds only).\n",
    "- We explicitly separate:\n",
    "  - **logical inconsistencies** found in the EDA (handled deterministically in `CarDataCleaner`),\n",
    "  - vs. **statistical extremes** (handled in `OutlierHandler`).\n",
    "- **Identification** of outliers\n",
    "  - **Voting of robust univariate detectors:**\n",
    "    - **Tukey IQR fences (1.5×IQR)**\n",
    "      - Flags a value if it lies outside:\n",
    "        - `Q1 − 1.5·IQR` or `Q3 + 1.5·IQR`\n",
    "      - Strength: non-parametric, robust, widely used baseline (boxplot rule).\n",
    "    - **Modified Z-score using Median Absolute Deviation (MAD)**\n",
    "      - Robust alternative to z-scores:\n",
    "        - uses the **median** instead of mean\n",
    "        - uses **MAD** instead of standard deviation\n",
    "      - Typical threshold: `|modified_z| > 3.5`\n",
    "      - Strength: less sensitive to extreme tails than mean/std-based z-scores.\n",
    "    - **Voting rule** (for robustness):\n",
    "      - A value is treated as an outlier only if both methods agree (`min_votes=2`).\n",
    "      - This reduces false positives compared to using only IQR fences on skewed distributions.\n",
    "- **Treatment of Outliers:**\n",
    "  - **Winsorization** (clip extreme values):\n",
    "    - We keep every car in the dataset (no row deletion).\n",
    "    - We reduce the influence of extreme values while still preserving information and rank order in the feature.\n",
    "    - We **clip** flagged values to conservative bounds (`action=\"clip\"`):\n",
    "      - For each numeric feature we compute robust lower/upper bounds (from IQR and MAD-based thresholds).\n",
    "      - Values outside those bounds are replaced by the nearest bound (winsorization).\n",
    "- **Benefits** of this approach:\n",
    "  - Keeps rare cars (no row deletion).\n",
    "  - Avoids replacing informative extremes with typical medians (which can hurt tree models).\n",
    "  - Stabilizes downstream steps (imputation, feature engineering, scaling) without collapsing signal into missingness.\n",
    "\n",
    "<u>Place in the pipe</u>: \n",
    "- Before imputation to use original distribution for identifying the outliers (otherwise we would inflate the distributions with the imputed values)\n",
    "- Then in imputation, fill the original gaps based on a distribution that does not includes the massive outliers (skewing the mean/median)    \n",
    "  -> kill the outliers first (set to NaN) so the imputation for everyone becomes cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical considerations:\n",
    "- **EVs:** handled in Cleaning because there are only 4 cars and 2 entries are wrong. In addition, their price and characteristics structure differs strongly (cannot generalize) from combustion cars.\n",
    "- **Zeros in tax:** not treated as a global outlier by default; handled via robust pipeline + hierarchical imputation. If EDA confirms systematic tax=0 errors in specific segments, we can add a deterministic cleaner rule.\n",
    "- **Per-model outlier rules:** can be added later as a refinement (only when sample size per model is large enough).\n",
    "- **Model-family sensitivity:** winsorization is especially helpful for linear/SVR models (reduces leverage points) and remains safe for trees; we keep one unified default pipeline for comparability.\n",
    "\n",
    "Unused techniques:\n",
    "- **Drop outside 1.5*IQR:** We decided against the classical “drop rows outside 1.5×IQR” because of:\n",
    "    - The classical 1.5×IQR boxplot rule (Tukey fences) is a strong baseline, but real-world car variables (especially mileage) are often skewed / heavy-tailed, which can over-flag valid high values.\n",
    "    - Dropping rows removes rare but valid cars (e.g., very high mileage vehicles), which is undesirable for production.\n",
    "- **NaN into Imputation:** Set outliers found by Voting of robust univariate detectors to NaN and impute later with (`action=\"nan\"`), but this significantly hurt the best averaged CV MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40162612-0ddf-487a-9183-da3493e1fce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# e.g. how to handle Zeros in tax (use groupimputer?) -> features that are computed with tax are also affected and need to be handled then ~J\n",
    "# e.g. maybe outlier handling per model (if sample size big enough) ~J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8df3a4b-2983-424b-9306-1d2f64951576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings**:\n",
    "- ...\n",
    "\n",
    "**Consequences/Interpretation:**\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0bafff7-43a2-4c3c-81cf-1e54efd19ffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.4 Missing Values Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD by Jan     \n",
    "**Our approach:**\n",
    "- `Group Imputer`: We use a custom GroupImputer that imputes the missing values to be the median of entries within the same group\n",
    "    - For that we use a hierarchical structure to identify the most similar group to the one with the missing value:\n",
    "        - 1st level: ...\n",
    "        - 2nd level: ...\n",
    "        - ...\n",
    "        - 4th level: Model\n",
    "        - 5th level: Brand\n",
    "    - The group values are only computed on the respective train folds and transformed on the val set to prevent leakage.   \n",
    "        -> When refitting the entire model, the entire train set is used to fit and the kaggle test set is transformed using the fitted values\n",
    "\n",
    "<u>Place in the pipe:</u> The Imputation is decided here because the data has to be imputed on original values before engineering new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85109666-c312-47d7-b39c-b0e492ab64b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- We use a custom **hierarchical GroupImputer** to impute missing values in a way that matches the structure of the car market.\n",
    "- Instead of imputing from the full dataset only (global statistics), we first try to impute from **the most similar cars**:\n",
    "  - same `brand` and same `model` (closest peer group),\n",
    "  - otherwise same `brand`,\n",
    "  - otherwise the global dataset.\n",
    "- This is more realistic than a single global median because many variables (e.g., `engineSize`, `mpg`, `tax`) are strongly segment-dependent.\n",
    "\n",
    "**Leakage safety:**\n",
    "- The `GroupImputer` is implemented as an sklearn transformer in the pipeline.\n",
    "- Therefore, during cross-validation it learns all medians/modes **only on the training fold** in `fit()` and applies them to the validation fold in `transform()` (no leakage).\n",
    "\n",
    "---\n",
    "\n",
    "##### Place in the pipe\n",
    "\n",
    "> `CarDataCleaner` → `OutlierHandler` → `GroupImputer` → `CarFeatureEngineer` → encoding/scaling → FS → model\n",
    "\n",
    "**Justification:**\n",
    "- Imputation must happen on **original features** first, because feature engineering creates ratios/interactions (e.g., `miles_per_year`, `engine_per_mpg`) that would otherwise explode or become undefined when inputs are missing.\n",
    "- We impute **before** feature engineering to ensure engineered features are computed on complete, consistent base variables.\n",
    "\n",
    "---\n",
    "\n",
    "##### Why medians/modes:\n",
    "\n",
    "- **Median** is robust to skewed distributions (common in `mileage`, `tax`) and less sensitive to extreme values than the mean.\n",
    "- **Mode** is the natural robust default for categorical variables.\n",
    "\n",
    "---\n",
    "\n",
    "##### Implementation notes:\n",
    "\n",
    "- `group_cols` are used only to define groups; they themselves are **not imputed**.\n",
    "- The transformer is deterministic: ties in categorical mode are handled consistently (pandas `.mode()` → first entry).\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae0e83a-723b-4eff-9642-dac08cbe345d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO maybe a visual here that shows how the group imputer works in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c48d8a2-2615-4533-9a66-0375bc994fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**\n",
    "- GroupImputation improves performance over a simple strategy by ...\n",
    "\n",
    "**Consequences/Interpretation:**\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b198720e-b5a0-46ef-8055-4ac33b852410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.5 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d75d2e08-9cae-4186-b329-d389b4ca8a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- We implement feature engineering as an sklearn transformer (`CarFeatureEngineer`) **inside the pipeline**.\n",
    "  - This makes the process **CV-safe / leakage-free**: all fold-specific statistics (e.g., model frequency, mean ages) are learned only on the training fold in `fit()` and applied to the validation fold in `transform()`.\n",
    "- We engineer features with two goals:\n",
    "  1. **Inject domain structure** (age, usage intensity, efficiency, “big engine + old car” effects).\n",
    "  2. **Create stronger signals for models** by expressing ratios and interactions that are difficult to learn reliably from raw variables.\n",
    "\n",
    "**Input columns used (after cleaning + imputation):**\n",
    "- Numeric: `year`, `mileage`, `tax`, `mpg`, `engineSize`, `previousOwners`\n",
    "- Categorical: `brand`, `model`, `transmission`, `fuelType`\n",
    "\n",
    "**Important design notes:**\n",
    "- Interaction features use `(age + 1)` to avoid division by zero for cars in the reference year.\n",
    "\n",
    "---\n",
    "\n",
    "| **New Feature** | **Calculation** | **Nature** | **Reasoning** |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| `age` | `ref_year - year` | Base | Captures depreciation; turns a calendar value into a meaningful pricing variable. |\n",
    "| `miles_per_year` | `mileage / (age + 1)` | Interaction (ratio) | Normalizes mileage by lifetime: 60k miles on a 3-year car is very different from 60k on a 10-year car; reduces collinearity between `mileage` and `age`. |\n",
    "| `mpg_x_engine` | `mpg * engineSize` | Interaction (product) | Joint signal for “performance vs efficiency” patterns (high engine + low mpg vs small engine + high mpg). |\n",
    "| `engine_x_age` | `engineSize * (age + 1)` | Interaction (product) | Differentiates large engines in older cars vs newer cars; helps model capture age-dependent valuation of engine size. |\n",
    "| `mileage_x_age` | `mileage * (age + 1)` | Interaction (product) | Amplifies the “old + heavily used” signal which is typically strongly negative for price. |\n",
    "| `mpg_x_age` | `mpg * (age + 1)` | Interaction (product) | Captures age-dependent fuel-efficiency patterns (e.g., older fleets / technology differences) that can correlate with price. |\n",
    "| `tax_x_age` | `tax * (age + 1)` | Interaction (product) | Models that tax effects can differ by car age (policy/regime + car segment composition). |\n",
    "| `tax_per_mpg` | `tax / mpg` | Interaction (ratio) | “Cost pressure” proxy: high tax relative to efficiency can reflect segment / running cost patterns. |\n",
    "| `engine_per_mpg` | `engineSize / mpg` | Interaction (ratio) | Performance-style signal: high engine with low mpg tends to indicate sporty/luxury configurations. |\n",
    "| `brand_fuel` | `brand + \"_\" + fuelType` | Interaction (categorical) | Creates configuration groups for target encoding (e.g., Diesel BMW differs from Petrol BMW). |\n",
    "| `brand_trans` | `brand + \"_\" + transmission` | Interaction (categorical) | Creates configuration groups for target encoding (e.g., Automatic Mercedes vs Manual Mercedes). |\n",
    "| `model_freq` | `P(model)` from training fold | Popularity | Approximates market supply/demand stability: common models have more stable pricing; learned CV-safe in `fit()`. |\n",
    "| `age_rel_brand` | `age - mean_age(brand)` | Relative / group-stat | Measures whether a car is newer/older than typical within its brand (brand-relative positioning). |\n",
    "| `age_rel_model` | `age - mean_age(model)` | Relative / group-stat | Measures whether a car is newer/older than typical within its model (model-relative positioning). |\n",
    "| `engine_rel_model` | `engineSize / mean_engineSize(model)` | Relative / group-stat | Captures whether a car is under-/over-engined relative to its model’s typical configuration. |\n",
    "\n",
    "---\n",
    "\n",
    "Legend (feature “nature”)\n",
    "\n",
    "- **Base Features**: derived from a single original variable (e.g. `age` from `year`)\n",
    "- **Interaction Features**: combine multiple variables to capture non-additive effects\n",
    "  - products (“amplifiers”) and ratios (“normalizers”)\n",
    "- **Popularity Features**: learned from the training fold distribution (e.g. model frequency)\n",
    "- **Relative / Group-stat Features**: compare a car to typical peers within `brand` or `model`\n",
    "  - learned in `fit()` and applied in `transform()` to avoid leakage\n",
    "\n",
    "---\n",
    "\n",
    "Relation to encoding (Target Encoded Features)\n",
    "\n",
    "We also create categorical “group keys” (`brand_fuel`, `brand_trans`) specifically so that our later encoding step (median target encoding / QuantileEncoder inside the preprocessing pipeline) can learn stable, configuration-specific signals.  \n",
    "This encoding is handled **after** feature engineering and is **CV-safe** because it is part of the pipeline.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO model_freq is basically frequency_encoding of model -> maybe add it this way because its cleaner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee88274f-d05f-42a2-800a-45b7367b311f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO maybe add ('poly', PolynomialFeatures(degree=2)) to the pipeline for interaction terms ~J\n",
    "# TODO maybe add a plot of correlation with target of new features or show feature importance of new features ~J\n",
    "# Beware that this should only be for visualization processes because if including this in the decision making process it would be data leakage ~J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a40f2b62-88b2-4530-92e3-86423cb38ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**\n",
    "- The engineered features are main drivers for performance improvement\n",
    "\n",
    "(This part is already kind of an ablation study if we can determine the impact)\n",
    "\n",
    "| Feature | Impact |\n",
    "| :--- | :--- |\n",
    "| age | ... |\n",
    "\n",
    "\n",
    "**Consequences/Interpretation:**\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9425a7c0-ec96-4458-a0da-8ef65acb3f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.6 Encoding, Transforming and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d59f995-10c7-4391-a5ce-759858b69889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- We separate features in their `groups of variables` and combine their different treatments in the ColumnTransformer\n",
    "    - Numerics vs. Categoricals (e.g. transformation vs. encoding)\n",
    "    - Unused features:\n",
    "        - year: dropped because replaced by derived feature 'age'\n",
    "        - paintQuality: dropped because added by mechanic so not available for our predictions in production\n",
    "- We have one `baseline pipe` and one `optimized pipe` to compare basic preprocessing to optimized preprocessing\n",
    "    - The baseline pipe does the bare minimum for the algorithms to work cleanly\n",
    "    - The optimized pipe was adjusted iteratively through multiple experiments and trials during the process\n",
    "\n",
    "##### Summary of preprocessing: Baseline vs Optimized (including outliers)\n",
    "\n",
    "| | **Baseline** | **Optimized** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Data cleaning** | minimal manual cleaning | `CarDataCleaner` (Section 2.2) |\n",
    "| **Outlier handling** | none | `OutlierHandler` (Section 2.3) (IQR + MAD voting) → set to `NaN` |\n",
    "| **Imputation** | SimpleImputer median/mode <br>(simplicity; median more robust than mean)</br> | `GroupImputer` (Section 2.4) |\n",
    "| **Feature engineering** | none | `CarFeatureEngineer` (Section 2.5) |\n",
    "| **Transformation** | none | Transform selected skewed numerics (Section 2.6) |\n",
    "| **Scaling** | StandardScaler | Scaling (Section 2.6) |\n",
    "| **Encoding** | OneHotEncoder | OneHotEncoder + Target Encoding (Section 2.6) |\n",
    "| **Feature selection** | none | VT + Majority voting (Section 2.7) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5afcbfc2-4bef-4717-9554-a8b0a31c03b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Original features\n",
    "orig_numeric_features = [\"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\", \"previousOwners\", \"hasDamage\"]\n",
    "# TODO create origic_boolean_features for hasDamage ~J\n",
    "orig_categorical_features = [\"brand\", \"model\", \"transmission\", \"fuelType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fc30fe2f-afee-4751-9fda-dc8af979c0bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO maybe this step can also be automated within the pipeline ~J\n",
    "# -> Automatically categorize the features into numeric, boolean and categorical features for further processing in the pipeline\n",
    "# --> Then look at the numeric ones to see which one might benefit from (log) transformation\n",
    "numeric_features = [\n",
    "    \"hasDamage\",\n",
    "    \"age\", \"tax\", \"mpg\", \"engineSize\", \"previousOwners\",        # Original features (mileage is handled separately because of log transformation)\n",
    "    \"mpg_x_engine\",                                             # TODO this feature does not really make sense, however it improves MAE slightly (3) ~J\n",
    "    \"engine_x_age\", \"mpg_x_age\", \"tax_x_age\",                   # multiplication interaction features (multiplying for amplification)                                   \n",
    "    \"engine_per_mpg\", \"tax_per_mpg\",                            # division interaction features (division for normalization for ratios (efficiency))                 \n",
    "    \"model_freq\",\n",
    "    \"age_rel_brand\", \"age_rel_model\", \"engine_rel_model\"\n",
    "]\n",
    "numeric_features_for_log = [\"mileage\", \"miles_per_year\"] #, \"mileage_x_age\"] # mileage_x_age decreases performance slightly\n",
    "boolean_features = [\"hasDamage\"]                                # TODO create logic for boolean features in GroupImputer and ColumnTransformer\n",
    "categorical_features_ohe = [\"transmission\", \"fuelType\"]\n",
    "# categorical_features_te_mean = [\"brand\", \"model\"]             # TODO currently not used because median TE is used\n",
    "categorical_features_te_median = [\"brand\", \"model\",             # original features\n",
    "                                  \"brand_fuel\", \"brand_trans\"]  # engineered features for anchors\n",
    "unused_columns = [\"year\"]                                       # replaced by age\n",
    "\n",
    "all_feature_names_before_encoding = numeric_features + numeric_features_for_log + boolean_features + categorical_features_ohe + categorical_features_te_median\n",
    "print(len(all_feature_names_before_encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c03c6ba-c3f8-4865-bea6-3b5c2b372272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Baseline Pipe:**\n",
    "- Only the necessary steps for the original variables in the baseline pipe\n",
    "    - Scaling Numerics (Standard)\n",
    "    - Encoding Categoricals (OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dbdfc5f-b865-4c1a-8093-fe021c5ed311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2.6.2 Optimized Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4385f8f4-0a8b-4b53-8350-52ee453af34a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- `Transformation` for skewed Numerics:\n",
    "    - Log-transform for right-skewed variables\n",
    "    - box-cox not used in final pipe because ...\n",
    "- `Scaler` for Numerics:\n",
    "    - StandardScaler because ...\n",
    "    - MinMaxScaler performed worse because ...\n",
    "    - RobustScaler performed worse because ...     \n",
    "    -> Scaling only on training data to avoid data leakage and then scale val and later test set with the fitted scaler of the training set\n",
    "\n",
    "- `Encoding` for Categoricals:\n",
    "    - Low cardinality:\n",
    "        - OHE because best performance with tree-based models\n",
    "    - High Cardinality:\n",
    "        - Median TE on categorical features because performs better than Mean TE\n",
    "\n",
    "| **Feature** | Nature | Transformation | Encoding | Scaling |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| age | Numerical | - | - | Standard |\n",
    "| ... | ... | ... | ... | ... |\n",
    "| mileage | Numerical | Log | - | Standard |\n",
    "| ... | ... | ... | ... | ... |\n",
    "| hasDamage | Boolean | - | - | TODO |\n",
    "| transmission | Categorical | - | OHE | - |\n",
    "| ... | ... | ... | ... | ... |\n",
    "| Brand | Categorical | - | TE | Standard |\n",
    "\n",
    "==> All operations are combined in a `ColumnTransformer` which applies the different steps to different columns of the data in one unified pipeline (reproducible and prevents data leakage)    \n",
    "  -> outputs a combined feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fc19882e-d4db-4c22-a2d6-7bb290b42eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_transformer_and_scaler = Pipeline([\n",
    "    (\"log\",    NamedFunctionTransformer(np.log1p, feature_names=numeric_features_for_log, validate=False)),  # log1p handles zeros safely\n",
    "    (\"scaler\", RobustScaler()),\n",
    "])\n",
    "\n",
    "numeric_scaler = Pipeline([\n",
    "    (\"scaler\", RobustScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer_ohe = Pipeline([\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)), # TODO maybe drop='first', (like in prac03) # Use sparse_output=False to get dense array back (e.g. necessary for hgb)\n",
    "])\n",
    "\n",
    "# Keep mean target encoder in the code but dont use it for now because median TE seems more robust and we use only one method for consistency ~J\n",
    "# categorical_transformer_te_mean = Pipeline([ \n",
    "#     (\"encoder\", TargetEncoder(target_type='continuous', cv=cv, smooth='auto', random_state=rs)), # Prevents data leakage with CV (e.g. for the samples in Fold 1, it calculates the target mean using the data from Folds 2, 3, 4, and 5) # TODO If it overfits test data too much, increasing the smoothing parameter can help\n",
    "#     (\"scaler\", StandardScaler()),\n",
    "# ])\n",
    "\n",
    "# Names for median-TE features (one per input column, since QuantileEncoder outputs 1 column per feature)\n",
    "median_te_feature_names = [f\"{col}_median_te\" for col in categorical_features_te_median]\n",
    "categorical_transformer_te_median = Pipeline(steps=[\n",
    "    ('median_encoder', QuantileEncoder(quantile=0.5, m=10.0)), # not specifying the cols means it encodes all columns (m is the smoothing parameter -> smoothing mitigates but doesnt eliminate leakage) # TODO tune m?\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('name_wrapper', NamedFunctionTransformer(feature_names=median_te_feature_names, validate=False)),\n",
    "])\n",
    "\n",
    "# TODO put handling of feature groups directly in columntrasnformer instead of declaring them separately ~J\n",
    "enc_transf_scale = ColumnTransformer([\n",
    "    (\"log\", log_transformer_and_scaler, numeric_features_for_log),\n",
    "    (\"num\", numeric_scaler, numeric_features),\n",
    "    (\"cat\", categorical_transformer_ohe, categorical_features_ohe),\n",
    "    # (\"mean_te\", categorical_transformer_te_mean, categorical_features_te_mean), # Mean TE is currently not used but we keep it in the code for reference or later experimenting ~J\n",
    "    (\"median_te\", categorical_transformer_te_median, categorical_features_te_median)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fbfaa0a-e16e-4211-9be6-18192720905a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.7 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98263698-5b76-4d51-b2fb-11e10fdd81b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**     \n",
    "We apply an automatic feature selection approach in addition to the previously removed features (data cleaning, feature engineering)\n",
    "- year: dropped because replaced by derived feature 'age'\n",
    "- paintQuality: dropped because filled by mechanic so not available for our predictions in production  as the car prediction skips the mechanic\n",
    "\n",
    "The goal is to create a very robust feature selection approach that finds features that are most likely actually irrelevant/redundant and therefore generate noise in the model that might lead to overfitting.     \n",
    "To achieve that goal, we apply **two steps** inside the feature selection:\n",
    "1) `Variance Threshold` (Filter) to filter constant variables ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html))\n",
    "2) Majority voter:\n",
    "    - `Spearman` handles the clean, obvious trends and cleans up redundancy ([docu](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html))\n",
    "    - `MI` catches more complex relations that Spearman misses ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html))\n",
    "    - `RF feature importance` to account for importance of the features ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#selectfrommodel))\n",
    "\n",
    "\n",
    "==> The different voters capture different aspects:\n",
    "| **Voter** | **Nature** | **Role & Responsibility** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Spearman Voter** <br> *(SpearmanRedundancySelector)* | Filter | **Linear/Monotonic**<br>Captures obvious, strong relationships (e.g., \"Newer cars are expensive\"). Also handles Redundancy by filtering out features that are exact duplicates of better ones. |\n",
    "| **MI Voter** <br> *(MutualInfoThresholdSelector)* | Filter | **Non-Linear**<br>Captures complex \"physics\" and non-monotonic patterns that correlation misses. |\n",
    "| **RF Voter** <br> *(SelectFromModel)* | Embedded | **Interactions**<br>Captures features that are only important in combination with others. |\n",
    "\n",
    "==> The feature selection is performed inside the pipelines cross-validation and consistent across all models, ensuring no data leakage and consistent feature selection logic.\n",
    "\n",
    "<u>Place in the pipe:</u> The Feature Selection is placed after the scaling to have the features on one scale (just like in the lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!! OLD markdown !!!!! More for us to understand the techniques and be able to explain them in the project defense\n",
    "\n",
    "*Filter* methods to make an initial screening of the statistical properties of the data: \n",
    "- `Correlation Indices` to filter irrelevant and redundant features (Maximum Relevance, Minimum Redundancy (mRMR)-style pruning).     \n",
    "    - Metric: We use Spearman because we want a single, unified pipeline step after encoding even though it treats binary OHE columns as \"ranks,\" which is a fine but rough approximation. Spearman because not all features are normally distributed as it would be necessary for Pearson.\n",
    "    - Irrelevant: Little correlation with the target\n",
    "    - Redundant: Important because other methods like MI and RF will likely keep redundant features as both of them are important if they contain valuable information. However, one of them should be eliminated for cleaner model interpretation of trees and correct model building for models that work better without multicolinearity between features. Of the redundant features, we keep the one with a higher correlation with the target.\n",
    "\n",
    "*Wrapper* methods create multiple models and use their performance as a proxy for the relevance of the features instead of relying on statistical properties of the data by themselves\n",
    "    - `RFECV` with Random Forest as the base estimator (removes least important feature based on feature importancy by base estimator) ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html)).\n",
    "\n",
    "*Embedded* methods perform feature selection as part of the model training process itself -> FS is integrated into the model and is not a separate step (Train model on all features -> Get FI -> Select based on FI)\n",
    "    - `Random Forest` (Tree-based method): Reduce impurity of the tree using sklearn SelectFromModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bd45610f-7196-4ca8-a5e2-f43bb8bffe8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "######################## Pre-Filter ########################\n",
    "# Variance Threshold (If using a different value than threshold 0, VarianceThreshold has to be applied before scaling)\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "\n",
    "# TODO Use caching for the prefiltering step to avoid re-computation during hyperparameter tuning ~J\n",
    "\n",
    "######################## Voters ########################\n",
    "##### Filter: Statistical Feature Selectors use statistical properties of the data to select features\n",
    "\n",
    "# Voter: Combined Relevance and Redundancy Spearman Correlation (sort by relevance and remove redundant features based on correlation to target) # TODO maybe use this as a prefilter as well instead of a majority voter\n",
    "stat_voter_relevancy_redundancy_corr = SpearmanRelevancyRedundancySelector(relevance_threshold=0.05, redundancy_threshold=0.95) # If we set redundancy threshold to 1.01, this becomes similar to just relevance filtering\n",
    "\n",
    "# Voter: Mutual Information (Non-Linear Dependency)\n",
    "stat_voter_nonlinear_mi = MutualInfoThresholdSelector(threshold=0.01, n_neighbors=10) # Increasing n_neighbors makes the estimation more stable but computationally slower\n",
    "\n",
    "\n",
    "##### Wrapper:\n",
    "# [Unused] Voter: RFE (Recursive Feature Elimination) is excluded for now because it is very expensive\n",
    "# TODO maybe use RFECV in the end to find optimal number of features considering also the feature interactions after removing variables compared to the embedded model which does not do that (optimizes strictly against target metric MAE)\n",
    "# rf_for_fs = RandomForestRegressor(n_jobs=-1, max_depth=50)\n",
    "# rfecv_rf = RFECV(estimator = rf_for_fs, step=1, random_state=rs, cv=cv, scoring='neg_mean_absolute_error', min_features_to_select=5)\n",
    "# -> Unused because of high computational cost\n",
    "\n",
    "##### Embedded:\n",
    "# Voter: Tree Importance (SelectFromModel trains the model once and selects features based on importance scores above threshold)\n",
    "rf_for_fs = RandomForestRegressor(n_estimators=100, max_depth=8, n_jobs=-1, random_state=rs)  # max_depts not too low (miss interactions) and not too high (selecting noise -> overfitting)\n",
    "select_from_rf = SelectFromModel(rf_for_fs, threshold='0.001*mean')        # threshold relative because it sums to 1 and if we have many features, many features will have a low importance but are still important\n",
    "\n",
    "\n",
    "# TODO Maybe add printer/loggers in the majority voter to log which features were selected by which voters (see plots in lab1 FS) ~J\n",
    "\n",
    "# ==> Final FS pipeline\n",
    "fs_pipe = Pipeline([\n",
    "    (\"vt\", vt), # Apply VT first to remove constant features (it serves as a \"dictator\" and not a \"voter\" in our pipeline)\n",
    "    ('selector', MajorityVoteSelectorTransformer(\n",
    "        selectors=[\n",
    "            stat_voter_relevancy_redundancy_corr,\n",
    "            stat_voter_nonlinear_mi,\n",
    "            select_from_rf],\n",
    "        min_votes=2))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d86caa12-6ce1-4684-950d-42efcecdf4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**\n",
    "- While trees are comparatively robust to unnecessary features, applying the feature selection pipeline improves the performance slighty (TODO add MAE difference here when including fs pipe vs. not including fs pipe)\n",
    "\n",
    "\n",
    "**Consequences/Interpretation:**\n",
    "- ...\n",
    "\n",
    "________\n",
    "\n",
    "SKlearn elements we also considered but decided not to use:\n",
    "- Filter Methods: & SelectPercentile\n",
    "    - SelectFwe (Family-Wise Error Rate)   \n",
    "    -> too strict and we don't want to be too conservative in our feature selection (we prefer to keep weak but useful signals)\n",
    "    - SelectKBest     \n",
    "    -> we didn't want to fix k (number of selected features)\n",
    "- Wrapper Methods: \n",
    "    - RFECV   \n",
    "    -> too expensive\n",
    "    - SequentialFeatureSelector (forward, backward selection)   \n",
    "    -> too expensive\n",
    "- Embedded:\n",
    "    - Regularization Method (Lasso)     \n",
    "    -> considers only linear relationships so discarded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0f4406-27be-4204-bd85-653c8d5ac173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.8 Create Final Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a716cfa0-d1e3-4978-83f2-183c1a346b94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- The `Pipeline` combines feature engineering, group imputation and the column transformer into the final preprocessing pipe ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html))\n",
    "    - Data Cleaning (Section 2.2)\n",
    "    - Outlier Handling (Section 2.3)\n",
    "    - Group Imputer (Section 2.4)\n",
    "    - Feature Engineering (Section 2.5)\n",
    "    - Column Transformer (Section 2.6)\n",
    "    - Feature Selection (Section 2.7)\n",
    "- Through calling the pipeline for data preparation, we ensure that the data is preprocessed independently for each training fold (filling missing values, scaling, encoding, etc.)     \n",
    "    -> prevent leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_orig = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), orig_numeric_features),\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)) # TODO maybe drop='first', (like in prac03) # Use sparse_output=False to get dense array back (e.g. necessary for hgb)\n",
    "    ]), orig_categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6949511c-a9e8-44f6-b9d4-a4d2950fff69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "simple_imputation_ct = ColumnTransformer([\n",
    "    (\"num\", SimpleImputer(strategy=\"median\"), orig_numeric_features),\n",
    "    (\"cat\", SimpleImputer(strategy=\"most_frequent\"), orig_categorical_features)\n",
    "], verbose_feature_names_out=False)  # Don't add prefixes to column names\n",
    "\n",
    "# Set output to pandas so CarFeatureEngineer receives DataFrame\n",
    "simple_imputation_ct.set_output(transform=\"pandas\")\n",
    "\n",
    "preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # (\"outliers\", OutlierHandler(\n",
    "    #     cols=[c for c in orig_numeric_features if c != \"mileage\"],      # only original numeric features here, no mileage because of log transform later\n",
    "    #     methods=(\"iqr\", \"mod_z\"),                                       # robust voting\n",
    "    #     min_votes=2,                                                    # outlier if BOTH methods agree\n",
    "    #     iqr_k=1.5,\n",
    "    #     z_thresh=3.5,\n",
    "    #     action=\"clip\",                                                   \n",
    "    #     verbose=False,\n",
    "    # )),\n",
    "    # (\"group_imputer\", GroupImputer(\n",
    "    #     group_cols=(\"brand\", \"model\"),\n",
    "    #     num_cols=orig_numeric_features,                                 # We have to use the original features here because the others are engineered in the next step\n",
    "    #     cat_cols=orig_categorical_features,                             # We have to use the original features here because the others are engineered in the next step\n",
    "    #     fallback=\"__MISSING__\",\n",
    "    # )),\n",
    "    (\"simple_imputer\", simple_imputation_ct),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", enc_transf_scale),\n",
    "    (\"fs\", fs_pipe)\n",
    "])\n",
    "\n",
    "# Save preprocessor for reuse in DL experiments\n",
    "with open('preprocessor_pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor_pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b9f0e68-668b-4730-8cd6-800efce68e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize outputs of each step in the preprocessing pipeline\n",
    "# Set output to pandas DataFrames for easier inspection while we use numpy arrays for efficient model training (default)\n",
    "enc_transf_scale.set_output(transform=\"pandas\")\n",
    "fs_pipe.set_output(transform=\"pandas\")\n",
    "\n",
    "show_data = True\n",
    "y_data_profiling = True\n",
    "debug_preprocessor_pipe = Pipeline([\n",
    "    ('debug_start', DebugTransformer('START', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    ('debug_after_clean', DebugTransformer('AFTER CLEANING', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "\n",
    "    # (\"outliers\", OutlierHandler(\n",
    "    #     cols=[c for c in orig_numeric_features if c != \"mileage\"],\n",
    "    #     methods=(\"iqr\", \"mod_z\"),\n",
    "    #     min_votes=2,\n",
    "    #     iqr_k=1.5,\n",
    "    #     z_thresh=3.5,\n",
    "    #     action=\"clip\",\n",
    "    #     verbose=True,  # useful in debug pipe\n",
    "    # )),\n",
    "    # ('debug_after_outliers', DebugTransformer('AFTER OUTLIER HANDLING', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "\n",
    "    # (\"group_imputer\", GroupImputer(\n",
    "    #     group_cols=(\"brand\", \"model\"),\n",
    "    #     num_cols=orig_numeric_features, # numeric_features + numeric_features_for_log,                      # We have to use the original features here because the others are engineered in the next step\n",
    "    #     cat_cols=orig_categorical_features, # categorical_features_ohe + categorical_features_te_median,    # We have to use the original features here because the others are engineered in the next step\n",
    "    #     fallback=\"__MISSING__\",\n",
    "    # )),\n",
    "    (\"simple_imputer\", simple_imputation_ct),\n",
    "    ('debug_after_impute', DebugTransformer('AFTER IMPUTATION', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    ('debug_after_fe', DebugTransformer('AFTER FEATURE ENGINEERING', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "\n",
    "    # (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    # ('debug_after_poly', DebugTransformer('AFTER POLYNOMIAL FEATURES', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "\n",
    "    (\"ct\", (enc_transf_scale)),\n",
    "    ('debug_after_ct', DebugTransformer('AFTER COLUMN TRANSFORMER', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "    \n",
    "    (\"fs\", (fs_pipe)),\n",
    "    ('debug_after_fs', DebugTransformer('AFTER FEATURE SELECTION', show_data=show_data, y_data_profiling=y_data_profiling))\n",
    "])\n",
    "\n",
    "print(\"Show outputs of each step in the preprocessing pipeline:\") # Set show_data=True in DebugTransformer to see the data at each step\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)\n",
    "\n",
    "# Reset output to default (numpy arrays) for model training\n",
    "enc_transf_scale.set_output(transform=\"default\")\n",
    "fs_pipe.set_output(transform=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "32efebf2-72f6-448e-b5a1-3fa27bd272bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # TODO delete following cell later - this is for us to see if the group imputer works - but it is GPT slop\n",
    "\n",
    "# brand = \"VW\"\n",
    "# model = \"golf\"\n",
    "\n",
    "# # 1) Get the fitted steps from preprocessor_pipe\n",
    "# preprocessor_pipe.fit(X_train, y_train)\n",
    "# fe = preprocessor_pipe.named_steps[\"fe\"]              # CarFeatureEngineer\n",
    "# imp = preprocessor_pipe.named_steps[\"group_imputer\"]  # GroupImputer\n",
    "\n",
    "# # 2) Inspect GroupImputer internal numeric stats\n",
    "# pair_table = getattr(imp, \"num_pair_\", None)    # indexed by (_g0, _g1) = (Brand, model)\n",
    "# brand_table = getattr(imp, \"num_first_\", None)  # indexed by _g0 = Brand\n",
    "# global_med = getattr(imp, \"num_global_\", None)  # Series of global medians\n",
    "\n",
    "# print(\"Has pair-level medians table:\",\n",
    "#       pair_table is not None and not getattr(pair_table, \"empty\", True))\n",
    "# print(\"Has brand-level medians table:\",\n",
    "#       brand_table is not None and not getattr(brand_table, \"empty\", True))\n",
    "# print(\"Has global median:\",\n",
    "#       global_med is not None and not global_med.empty if global_med is not None else False)\n",
    "# print()\n",
    "\n",
    "# _g0 = brand\n",
    "# _g1 = model\n",
    "\n",
    "# # 2a) Pair-level\n",
    "# if pair_table is not None and (_g0, _g1) in pair_table.index:\n",
    "#     print(f\"Pair-level median FOUND for ({brand}, {model}):\")\n",
    "#     display(pair_table.loc[(_g0, _g1)])\n",
    "# else:\n",
    "#     print(f\"No pair-level median for ({brand}, {model}).\")\n",
    "#     if pair_table is not None and not pair_table.empty:\n",
    "#         print(\"Sample of pair-level medians (top 5):\")\n",
    "#         display(pair_table.head())\n",
    "\n",
    "# # 2b) Brand-level\n",
    "# if brand_table is not None and _g0 in brand_table.index:\n",
    "#     print(f\"\\nBrand-level median for {brand}:\")\n",
    "#     display(brand_table.loc[_g0])\n",
    "# else:\n",
    "#     print(\"\\nNo brand-level median for\", brand)\n",
    "#     if brand_table is not None and not brand_table.empty:\n",
    "#         print(\"Sample of brand-level medians (top 5):\")\n",
    "#         display(brand_table.head())\n",
    "\n",
    "# # 2c) Global medians\n",
    "# print(\"\\nGlobal median (fallback):\")\n",
    "# display(global_med)\n",
    "\n",
    "# # 3) Apply CarFeatureEngineer + GroupImputer to VW Golf rows and compare\n",
    "# #    (GroupImputer was fitted after CarFeatureEngineer, so we must mimic that order)\n",
    "\n",
    "# # 3a) Feature engineering on full X_train\n",
    "# X_train_fe = fe.transform(X_train)\n",
    "\n",
    "# # 3b) Filter for VW Golf in the feature-engineered space\n",
    "# vw_golf = X_train_fe[(X_train_fe[\"Brand\"] == brand) & (X_train_fe[\"model\"] == model)].copy()\n",
    "\n",
    "# if vw_golf.empty:\n",
    "#     print(\"\\nNo VW Golf rows found in X_train.\")\n",
    "# else:\n",
    "#     print(f\"\\nFound {len(vw_golf)} VW Golf rows in X_train.\")\n",
    "\n",
    "#     # 3c) GroupImputer expects the columns it saw at fit time\n",
    "#     cols_for_imp = imp.feature_names_in_\n",
    "#     vw_input = vw_golf.loc[:, cols_for_imp]\n",
    "\n",
    "#     vw_imp = imp.transform(vw_input)\n",
    "#     vw_imp_df = pd.DataFrame(vw_imp, columns=cols_for_imp, index=vw_golf.index)\n",
    "\n",
    "#     print(\"\\nImputed data (first 8 rows):\")\n",
    "#     display(vw_imp_df[[\"mpg\", \"mileage\", \"tax\"]].head(8))\n",
    "\n",
    "#     # 4) Build comparison table (original vs imputed, for selected columns)\n",
    "#     comp = pd.DataFrame(index=vw_golf.index)\n",
    "#     comp[\"orig_mpg\"] = vw_golf[\"mpg\"]\n",
    "#     comp[\"imp_mpg\"] = vw_imp_df[\"mpg\"]\n",
    "#     comp[\"orig_tax\"] = vw_golf[\"tax\"]\n",
    "#     comp[\"imp_tax\"] = vw_imp_df[\"tax\"]\n",
    "#     comp[\"orig_mileage\"] = vw_golf[\"mileage\"]\n",
    "#     comp[\"imp_mileage\"] = vw_imp_df[\"mileage\"]\n",
    "\n",
    "#     print(\"\\nOriginal vs imputed (first 12 rows):\")\n",
    "#     display(comp.head(12))\n",
    "\n",
    "#     # 5) Determine imputation source per row\n",
    "#     def source_of_imputation(col):\n",
    "#         srcs = []\n",
    "#         for idx, row in comp.iterrows():\n",
    "#             val = row[f\"imp_{col}\"]\n",
    "#             src = \"other\"\n",
    "\n",
    "#             # Pair-level\n",
    "#             if pair_table is not None and (_g0, _g1) in pair_table.index and col in pair_table.columns:\n",
    "#                 pair_val = pair_table.loc[(_g0, _g1), col]\n",
    "#                 if pd.notna(pair_val) and pd.notna(val) and val == pair_val:\n",
    "#                     src = \"pair\"\n",
    "\n",
    "#             # Brand-level\n",
    "#             if src == \"other\" and brand_table is not None and _g0 in brand_table.index and col in brand_table.columns:\n",
    "#                 brand_val = brand_table.loc[_g0, col]\n",
    "#                 if pd.notna(brand_val) and pd.notna(val) and val == brand_val:\n",
    "#                     src = \"brand\"\n",
    "\n",
    "#             # Global\n",
    "#             if src == \"other\" and global_med is not None and col in global_med.index:\n",
    "#                 glob_val = global_med[col]\n",
    "#                 if pd.notna(glob_val) and pd.notna(val) and val == glob_val:\n",
    "#                     src = \"global\"\n",
    "\n",
    "#             srcs.append(src)\n",
    "#         return srcs\n",
    "\n",
    "#     comp[\"src_mpg\"] = source_of_imputation(\"mpg\")\n",
    "#     comp[\"src_tax\"] = source_of_imputation(\"tax\")\n",
    "#     comp[\"src_mileage\"] = source_of_imputation(\"mileage\")\n",
    "\n",
    "#     print(\"\\nImputation sources for the shown rows:\")\n",
    "#     display(comp.head(12))\n",
    "\n",
    "#     # 6) Summary counts: NaN before vs after imputation\n",
    "#     print(\"\\nSummary counts: NaN before -> NaN after\")\n",
    "#     before = vw_golf[[\"mpg\", \"mileage\", \"tax\"]].isna().sum()\n",
    "#     after = pd.DataFrame({\n",
    "#         \"mpg\": comp[\"imp_mpg\"],\n",
    "#         \"mileage\": comp[\"imp_mileage\"],\n",
    "#         \"tax\": comp[\"imp_tax\"],\n",
    "#     }).isna().sum()\n",
    "#     display(pd.DataFrame({\"na_before\": before, \"na_after\": after}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "225688cc-6157-4613-bf28-9e430473a3a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Model Assessment Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6297f034-638d-4db6-a681-597f20036432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definition of Function for Model Assessment"
    }
   },
   "source": [
    "**Our approach:**\n",
    "- Several metrics are used to compare model performance (on train and val data to evaluate overfitting):\n",
    "    - MAE (Mean Absolute Error):\n",
    "        - average absolute deviation between predicted and true car prices\n",
    "        - easy to interpret in pounds, same metric used in Kaggle competition\n",
    "    - MAE std across folds:\n",
    "        - Because MAE is our primary metric we also look at the std across folds to see how much the performance varies on different folds\n",
    "        - For the other metrics this is not necessary because we get a good idea of the variance from this one std\n",
    "    - RMSE (Root Mean Squared Error):\n",
    "        - sensitive to outliers, helps identify large prediction errors\n",
    "    - R²:\n",
    "        - Coefficient of determination: proportion of variance explained by the model\n",
    "        - 1.0 = perfect predictions, 0.0 = same as predicting mean, < 0.0 = worse than mean\n",
    "- First we run multiple models with their default parameters to find the ones that will most likely be top candidates for the best model\n",
    "- Finally, we run hyperparameter tuning on the top candidates to find the best model for our use case (best performance on primary metric MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0dc6124-f736-46fb-9f73-895b930faae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Model Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-transforming the target (price) because EDA showed that it is heavily right-skewed.  The model predicts the log-price (handling outliers easily) and automatically convert it back to pounds at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc50f05c-4c02-4ff6-9a48-319af5baecfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.1 Compare Default Models: Original vs. Optimized Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **default parameters** to get a first result of models potential to decide on which ones to use for further optimizing (hyperparameter tuning). Only use the same random_state for reproducibility and n_jobs to speed up computations.    \n",
    "Baseline: DummyRegressor using the median price as prediction\n",
    "\n",
    "The **log-transform** of the target is performed here because it is the most straightforwared implementation using the TransformedTargetRegressor ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html)). It handles transformation and afterwards uses the inverse automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_median_pipe_orig = create_model_pipe(preprocessor_orig, DummyRegressor(strategy=\"median\"))\n",
    "baseline_median_pipe_adjusted = create_model_pipe(preprocessor_pipe, DummyRegressor(strategy=\"median\"))\n",
    "\n",
    "### Linear Models ###\n",
    "linear_reg_default = LinearRegression()\n",
    "linear_reg_default = TransformedTargetRegressor(\n",
    "    regressor=linear_reg_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "linear_reg_pipe_orig = create_model_pipe(preprocessor_orig, linear_reg_default)\n",
    "linear_reg_pipe_adjusted = create_model_pipe(preprocessor_pipe, linear_reg_default)\n",
    "\n",
    "\n",
    "elasticnet_default = ElasticNet(random_state=rs)\n",
    "elasticnet_default = TransformedTargetRegressor(\n",
    "    regressor=elasticnet_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "elastic_pipe_orig = create_model_pipe(preprocessor_orig, elasticnet_default)\n",
    "elastic_pipe_adjusted = create_model_pipe(preprocessor_pipe, elasticnet_default)\n",
    "# Long Duration (~30sec)\n",
    "\n",
    "\n",
    "### Instance-Based ###\n",
    "knn_default = KNeighborsRegressor(n_jobs=-3)\n",
    "knn_default = TransformedTargetRegressor(\n",
    "    regressor=knn_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "knn_pipe_orig = create_model_pipe(preprocessor_orig, knn_default)\n",
    "knn_pipe_adjusted = create_model_pipe(preprocessor_pipe, knn_default)\n",
    "# Long Duration (~4min)\n",
    "# => Better performance than linear models but still worse than tree-based models -> not further optimized\n",
    "\n",
    "\n",
    "### Neural Networks ###\n",
    "mlp_default = MLPRegressor(random_state=rs) # TODO argue some rule of thumb parameters\n",
    "mlp_default = TransformedTargetRegressor(\n",
    "    regressor=mlp_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "mlp_pipe_orig = create_model_pipe(preprocessor_orig, mlp_default)\n",
    "mlp_pipe_adjusted = create_model_pipe(preprocessor_pipe, mlp_default)\n",
    "# Long Duration (~4min)\n",
    "# => Worse performance than KNN and tree-based models (notably, orig better than preprocessed)\n",
    "\n",
    "\n",
    "### Tree-Based Models ###\n",
    "hgb_default = HistGradientBoostingRegressor(random_state=rs, loss='squared_error')\n",
    "hgb_default = TransformedTargetRegressor(\n",
    "    regressor=hgb_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "hgb_pipe_orig = create_model_pipe(preprocessor_orig, hgb_default)\n",
    "hgb_pipe_adjusted = create_model_pipe(preprocessor_pipe, hgb_default)\n",
    "# Long Duration (~1mins)\n",
    "\n",
    "\n",
    "rf_default = RandomForestRegressor(random_state=rs, n_jobs=-1, criterion='squared_error')\n",
    "rf_default = TransformedTargetRegressor(\n",
    "    regressor=rf_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "rf_pipe_orig = create_model_pipe(preprocessor_orig, rf_default)\n",
    "rf_pipe_adjusted = create_model_pipe(preprocessor_pipe, rf_default)\n",
    "# Long Duration (~5mins)\n",
    "# Good performance -> further hyperparameter tuning\n",
    "\n",
    "et_default = ExtraTreesRegressor(random_state=rs, n_jobs=-1, criterion='squared_error')\n",
    "et_default = TransformedTargetRegressor(\n",
    "    regressor=et_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "et_pipe_orig = create_model_pipe(preprocessor_orig, et_default)\n",
    "et_pipe_adjusted = create_model_pipe(preprocessor_pipe, et_default)\n",
    "# Long Duration (~7mins)\n",
    "# Good performance -> further hyperparameter tuning\n",
    "\n",
    "### Kernel-Based Models ###\n",
    "svr_default = SVR()\n",
    "svr_default = TransformedTargetRegressor(\n",
    "    regressor=svr_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "svr_pipe_orig = create_model_pipe(preprocessor_orig, svr_default)\n",
    "svr_pipe_adjusted = create_model_pipe(preprocessor_pipe, svr_default)\n",
    "# Long Duration (~12mins)\n",
    "# => Much worse performance than other models -> not further optimized\n",
    "\n",
    "\n",
    "### Ensemble Meta Model ###\n",
    "# The 'final_estimator' (Meta-Learner) looks at the predictions from the estimators and decides how to combine them.\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('rf_main', rf_pipe_adjusted),\n",
    "        ('linear_extrapolator', elastic_pipe_adjusted)\n",
    "    ],\n",
    "    final_estimator=LinearRegression(), # A linear final estimator allows the prediction to go beyond bounds (extrapolate)\n",
    "    n_jobs=-1,\n",
    "    passthrough=False # False = Meta-learner only sees the PREDICTIONS of the base models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a21d8b-3aeb-4155-b6db-17475e30c3b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.2 Run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "80a5dc82-4f35-4d66-9e97-e249166949e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "default_models = {\n",
    "    # \"Baseline_Median_orig\": baseline_median_pipe_orig,\n",
    "    # \"Baseline_Median\": baseline_median_pipe_adjusted,\n",
    "    # \"ElasticNet_orig\": elastic_pipe_orig,\n",
    "    # \"ElasticNet\": elastic_pipe_adjusted,\n",
    "    # \"KNN_orig\": knn_pipe_orig,\n",
    "    # \"KNN\": knn_pipe_adjusted,\n",
    "    \"HGB_orig\": hgb_pipe_orig,\n",
    "    \"HGB\": hgb_pipe_adjusted,\n",
    "    # \"RF_orig\": rf_pipe_orig,\n",
    "    \"RF\": rf_pipe_adjusted,\n",
    "    # \"ET_orig\": et_pipe_orig,\n",
    "    \"ET\": et_pipe_adjusted,\n",
    "    # \"SVR_orig\": svr_pipe_orig, # TODO remove comment for final run and submission, (currently too slow)\n",
    "    # \"SVR\": svr_pipe_adjusted,\n",
    "    # \"Stack_orig\": stack_pipe_orig,\n",
    "    # \"Stack\": stack_pipe_adjusted,\n",
    "}\n",
    "\n",
    "default_models_results_df = get_cv_results(default_models, X_train, y_train, cv=cv, rs=rs)\n",
    "display(default_models_results_df)\n",
    "\n",
    "# Long Duration (~15mins)\n",
    "\n",
    "### Results\n",
    "# After setting up everything\n",
    "\n",
    "# model\tpreprocessing\tval_MAE\tstd_MAE\tval_RMSE\tval_R2\ttrain_MAE\ttrain_std_MAE\ttrain_RMSE\ttrain_R2\n",
    "# 0\tRF\t    optimized\t1337.5711\t20.7784\t2394.1089\t0.9396\t511.1459\t2.0723\t921.3749\t0.9910\n",
    "# 1\tET\t    optimized\t1352.4122\t16.6569\t2344.0351\t0.9421\t15.5697\t0.3377\t168.3326\t0.9997\n",
    "# 2\tET_orig\toriginal\t1442.9137\t12.9364\t2552.8816\t0.9313\t4.0813\t0.3644\t88.9362\t0.9999\n",
    "# 3\tRF_orig\toriginal\t1474.5770\t22.9152\t2576.9752\t0.9300\t547.5251\t1.5878\t969.5442\t0.9901\n",
    "# 4\tHGB\t    optimized\t    1491.8818\t17.1434\t2422.1513\t0.9382\t1443.1274\t5.4173\t2239.4684\t0.9471\n",
    "# 5\tKNN\t    optimized\t    1563.6059\t23.1749\t2782.3605\t0.9185\t1263.3461\t1.4033\t2249.3184\t0.9466\n",
    "# 6\tHGB_orig\toriginal\t1721.0859\t20.6166\t2799.4723\t0.9174\t1666.1467\t4.7953\t2624.5468\t0.9273\n",
    "# 7\tKNN_orig\toriginal\t1767.8346\t12.5795\t3090.0236\t0.8994\t1424.8752\t3.1977\t2475.4926\t0.9354\n",
    "# 8\tElasticNet\t    optimized\t2738.8684\t17.0733\t4556.0807\t0.7812\t2753.6263\t5.3896\t4557.6904\t0.7809\n",
    "# 9\tElasticNet_orig\toriginal\t3674.8651\t10.9441\t5803.9831\t0.6448\t3674.0435\t4.3854\t5801.7026\t0.6450\n",
    "# 10\tBaseline_Median_orig\toriginal\t6801.3202\t15.0554\t9976.8558\t-0.0499\t6801.1833\t3.7168\t9976.8186\t-0.0499\n",
    "# 11\tBaseline_Median\t        optimized\t6801.3202\t15.0554\t9976.8558\t-0.0499\t6801.1833\t3.7168\t9976.8186\t-0.0499\n",
    "\n",
    "# Replace values with np.NaN instead of pd.NA and use simple-imputer\n",
    "# model\tpreprocessing\tval_MAE\tstd_MAE\tval_RMSE\tval_R2\ttrain_MAE\ttrain_std_MAE\ttrain_RMSE\ttrain_R2\n",
    "# 0\tRF\t    optimized\t1311.1342\t18.5152\t2306.1141\t0.9440\t496.8170\t1.6132\t880.5758\t0.9918\n",
    "# 1\tET\t    optimized\t1320.0298\t10.5134\t2243.3910\t0.9470\t23.8594\t0.6903\t223.5786\t0.9995\n",
    "# 2\tHGB\t    optimized\t1463.2612\t19.6563\t2344.3935\t0.9421\t1397.8480\t4.9193\t2155.0675\t0.9510\n",
    "# 3\tKNN\t    optimized\t1528.4661\t19.1572\t2706.9532\t0.9228\t1223.5450\t5.3128\t2167.5167\t0.9504\n",
    "# 4\tET_orig\toriginal\t1442.9137\t12.9364\t2552.8816\t0.9313\t4.0813\t0.3644\t88.9362\t0.9999\n",
    "# 5\tRF_orig\toriginal\t1474.5770\t22.9152\t2576.9752\t0.9300\t547.5251\t1.5878\t969.5442\t0.9901\n",
    "# 6\tHGB_orig\toriginal\t1721.0859\t20.6166\t2799.4723\t0.9174\t1666.1467\t4.7953\t2624.5468\t0.9273\n",
    "# 7\tKNN_orig\toriginal\t1767.8346\t12.5795\t3090.0236\t0.8994\t1424.8752\t3.1977\t2475.4926\t0.9354\n",
    "\n",
    "\n",
    "# Log-Transform Target\n",
    "# 0\tRF\toptimized\t1288.7907\t15.2675\t2242.0406\t0.9470\t493.7640\t1.6063\t954.6323\t0.9904\n",
    "# 1\tET\toptimized\t1315.4824\t11.2401\t2253.5793\t0.9465\t23.7744\t0.7060\t224.6614\t0.9995\n",
    "# 2\tHGB\toptimized\t1454.5998\t10.7600\t2443.8428\t0.9371\t1404.8214\t5.5232\t2326.7533\t0.9429\n",
    "# 3\tET_orig\toriginal\t1442.8195\t13.4039\t2568.2720\t0.9305\t4.0362\t0.3626\t89.2101\t0.9999\n",
    "# 4\tRF_orig\toriginal\t1464.0531\t18.1173\t2620.6917\t0.9276\t551.0627\t1.9496\t1092.9956\t0.9874\n",
    "# 5\tHGB_orig\toriginal\t1715.8092\t9.9868\t2969.5871\t0.9070\t1677.5670\t4.7293\t2870.0856\t0.9131\n",
    "\n",
    "# transmission unknown to nan\n",
    "# model\tpreprocessing\tval_MAE\tstd_MAE\tval_RMSE\tval_R2\ttrain_MAE\ttrain_std_MAE\ttrain_RMSE\ttrain_R2\n",
    "# 0\tRF\toptimized\t1288.1381\t15.0811\t2239.6206\t0.9471\t493.6988\t1.4552\t954.1244\t0.9904\n",
    "# 1\tET\toptimized\t1316.3009\t9.7471\t2256.6712\t0.9463\t23.8059\t0.7017\t224.7270\t0.9995\n",
    "# 2\tHGB\toptimized\t1453.9956\t13.2153\t2458.2873\t0.9363\t1401.2912\t4.6736\t2328.1083\t0.9428\n",
    "# 3\tHGB_orig\toriginal\t1715.8092\t9.9868\t2969.5871\t0.9070\t1677.5670\t4.7293\t2870.0856\t0.9131\n",
    "\n",
    "\n",
    "# Fixed GI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec9ab706-7a45-41c2-a768-f1e4238c258b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.5 Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c992be-2e7b-42ec-8fd1-5e2b0debc7fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**\n",
    "- All models perform better on the adjusted pipeline with the following added components\n",
    "    - Imputation\n",
    "    - Feature Engineering\n",
    "    - Encoding\n",
    "    - Transforming and Scaling\n",
    "    - Feature Selection\n",
    "\n",
    "\n",
    "| **Model** | **Performance** | **Reasoning**  | **Next steps** |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **ElasticNet** <br> *(Linear)* | ... | ... | Discard |\n",
    "| **KNN** <br> *(Instance-based)* | ... | ... | ... |\n",
    "| **Bagging** <br> *(Tree-based)* | ... | ... | ... |\n",
    "| **RF** <br> *(Tree-based)* | ... | ... | Optimize |\n",
    "| **ET** <br> *(Tree-based)* | ... | ... | ... |\n",
    "| **HGB** <br> *(Tree-based)* | ... | ... | ... |\n",
    "| **SVR** <br> *(Kernel-based)* | ... | ... | ... |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Hyperparameter Tuning of Preselected Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0eb17d8-4b9a-433f-98f6-26777d49fa3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- After the first runs we only keep the top candidates for further hyperparameter tuning to focus on most promising approaches and not waste computing power.\n",
    "- After first experiments we decided to skip hyperparameter-tuning for...\n",
    "- We tune using RandomizedSearchCV ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)) which calls the pipeline object for consistent preprocessing. An example by sklearn of calling the pipeline similar to this can be found [here](https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html#sphx-glr-auto-examples-compose-plot-compare-reduction-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5a0287-45b6-4022-8d47-cde8cc52469e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.2 [Tree-Based] HistGradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "998dab6d-7494-483c-ab9e-a075b4c75c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# hgb_param_dist = {\n",
    "#     \"preprocess__fs__vt__threshold\": [0.0, 0.005, 0.01],\n",
    "#     \"model__loss\": ['absolute_error'],\n",
    "#     # 'fs__selector__relevance_threshold': [0.01, 0.05],          # TODO \n",
    "#     # 'fs__selector__redundancy_threshold': [0.85, 0.95, 1.01],   # TODO If this is 1.01, redundancy filtering is disabled -> hp-tuning will tell whether redundancy selection improves model performance\n",
    "#     # \"preprocess__fs__selector__min_votes\": [1, 2, 3],         # TODO try different vote thresholds for MajorityVoteSelectorTransformer in FS pipeline\n",
    "#     \"model__learning_rate\": uniform(0.01, 0.15),                \n",
    "#     \"model__max_leaf_nodes\": randint(50, 170),         \n",
    "#     \"model__min_samples_leaf\": randint(2, 20),         \n",
    "#     \"model__max_iter\": randint(200, 900),              \n",
    "#     \"model__l2_regularization\": uniform(0.0, 1.0),      \n",
    "#     \"model__early_stopping\": [True],\n",
    "#     \"model__validation_fraction\": [0.1],\n",
    "#     \"model__n_iter_no_change\": [20],\n",
    "#     \"model__random_state\":[rs]\n",
    "# }\n",
    "\n",
    "# # optimized the parameter distributions based on previous runs to focus search space\n",
    "# hgb_param_dist = {\n",
    "#     \"preprocess__fs__vt__threshold\": [0.0],\n",
    "#     \"model__loss\": ['absolute_error'],\n",
    "#     # 'preprocess__fs__selector__selectors__0__relevance_threshold': [0.01, 0.05],          # TODO \n",
    "#     # 'preprocess__fs__selector__selectors__0__redundancy_threshold': [0.85, 0.95, 1.01],   # TODO If this is 1.01, redundancy filtering is disabled -> hp-tuning will tell whether redundancy selection improves model performance\n",
    "#     # \"preprocess__fs__selector__min_votes\": [2],                             # TODO try different vote thresholds for MajorityVoteSelectorTransformer in FS pipeline\n",
    "#     \"model__learning_rate\": [0.05889383578028271],\n",
    "#     \"model__max_leaf_nodes\": [139],\n",
    "#     \"model__min_samples_leaf\": [4],\n",
    "#     \"model__max_iter\": [602],\n",
    "#     \"model__l2_regularization\": [0.8583588048137198],\n",
    "#     \"model__early_stopping\": [True],\n",
    "#     \"model__validation_fraction\": [0.1],\n",
    "#     \"model__n_iter_no_change\": [20],\n",
    "#     \"model__random_state\":[rs]\n",
    "# }\n",
    "\n",
    "# hgb_tuned_pipe, hgb_random_search_object, hgb_scores_dict = model_hyperparameter_tuning(X_train, y_train, cv, hgb_pipe_adjusted, hgb_param_dist, n_iter=50)\n",
    "# joblib.dump(hgb_tuned_pipe, \"hgb_tuned_pipe.pkl\")\n",
    "\n",
    "# # Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70de2673-0443-4cc4-b1cd-f3813a969246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.3 [Tree-Based] RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "# Old parameter distribution\n",
    "rf_param_dist = {\n",
    "    \"preprocess__fs__vt__threshold\": [0.0],\n",
    "    # 'fs__selector__relevance_threshold': [0.01, 0.05],          # TODO \n",
    "    # 'fs__selector__redundancy_threshold': [0.85, 0.95, 1.01],   # TODO If this is 1.01, redundancy filtering is disabled -> hp-tuning will tell whether redundancy selection improves model performance\n",
    "    # \"preprocess__fs__selector__min_votes\": [1, 2, 3],         # TODO try different vote thresholds for MajorityVoteSelectorTransformer in FS pipeline\n",
    "    \"model__criterion\": [\"absolute_error\"],                 # use MAE as split criterion\n",
    "    \"model__n_estimators\": randint(200, 600),               # number of trees\n",
    "    \"model__max_depth\": randint(5, 40),                     # depth of each tree\n",
    "    \"model__min_samples_split\": randint(2, 10),             # min samples to split an internal node\n",
    "    \"model__min_samples_leaf\": randint(1, 12),               # min samples per leaf (increse to not overfit)\n",
    "    \"model__min_weight_fraction_leaf\": uniform(0.0, 0.1),   # min weighted fraction per leaf\n",
    "    \"model__max_features\": [\"sqrt\"],                        # feature sampling strategy (sqrt performed better than log2 and None in previous tests)\n",
    "    # \"model__max_leaf_nodes\": randint(20, 100),            # max number of leaf nodes\n",
    "    \"model__min_impurity_decrease\": uniform(0.0, 0.05),     # min impurity decrease to split\n",
    "    \"model__bootstrap\": [True, False],                      # use bootstrapping or not (False performed better than True in previous tests)\n",
    "    # \"model__oob_score\": [True, False],                      # whether to use out-of-bag samples to estimate the R² on unseen data\n",
    "}\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "rf_param_dist = {\n",
    "    \"preprocess__fs__vt__threshold\": [0.0],\n",
    "    \"model__regressor__criterion\": ['squared_error'], # Use “absolute_error” to optimize for MAE but its significantly slower than when using “squared_error” (~5x) (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "    \"model__regressor__n_estimators\": [328],\n",
    "    \"model__regressor__max_depth\": [20],\n",
    "    \"model__regressor__min_samples_split\": [5],\n",
    "    \"model__regressor__min_samples_leaf\": [1],\n",
    "    \"model__regressor__max_features\": [\"sqrt\"],\n",
    "    \"model__regressor__bootstrap\": [False],\n",
    "    \"model__regressor__oob_score\": [False], # TODO try True\n",
    "}\n",
    "\n",
    "rf_tuned_pipe, rf_random_search_object, rf_scores_dict = model_hyperparameter_tuning(X_train, y_train, cv, rf_pipe_adjusted, rf_param_dist, n_iter=1)\n",
    "joblib.dump(rf_tuned_pipe, \"rf_tuned_pipe.pkl\")\n",
    "\n",
    "# Long Duration (~6min)\n",
    "\n",
    "# Set transmission Unknown to NaN\n",
    "# MAE: 1233.3946\n",
    "# RMSE: 2170.1968\n",
    "# R²: 0.9504\n",
    "# Best Model params: {'preprocess__fs__vt__threshold': 0.0, 'model__regressor__oob_score': False, 'model__regressor__n_estimators': 328, 'model__regressor__min_samples_split': 5, 'model__regressor__min_samples_leaf': 1, 'model__regressor__max_features': 'sqrt', 'model__regressor__max_depth': 20, 'model__regressor__criterion': 'squared_error', 'model__regressor__bootstrap': False}\n",
    "\n",
    "# Use Median AND Mean TE to let the model decide which to use at what node (probably increases performance but decreases interpretation)\n",
    "\n",
    "\n",
    "# TODO change to “absolute_error” again for final best performance but for other tests use squared_error to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d0fce47e-39a6-480f-979f-7f91422734dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO Remove looking at FI here (not part of tuning) -> just for inspection of feature importances after tuning ~J\n",
    "\n",
    "# Use the debug preprocessor pipeline to get final feature names by hierarchically accessing each step\n",
    "feature_names_after_fs = debug_preprocessor_pipe.named_steps['fs'].get_feature_names_out()\n",
    "feat_names = feature_names_after_fs\n",
    "importances = rf_tuned_pipe.named_steps[\"model\"].regressor_.feature_importances_\n",
    "df_feat_importance_rf = pd.DataFrame({\"feature\": feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for _, row in df_feat_importance_rf.iterrows():\n",
    "    print(f\"{row['feature']:30s}: {row['importance']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "917f1472-aee1-41b0-bc59-12f436c1f17f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.4 [Tree-Based] Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c162067b-9019-40a6-9bb3-2977ffc615a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Old parameter distribution\n",
    "et_param_dist = {\n",
    "    \"preprocess__fs__vt__threshold\": [0.0],\n",
    "    \"model__criterion\": [\"absolute_error\"],\n",
    "    \"model__n_estimators\": randint(200, 600),        # number of trees\n",
    "    \"model__max_depth\": randint(5, 40),              # depth of each tree\n",
    "    \"model__min_samples_split\": randint(2, 10),      # min samples to split an internal node\n",
    "    \"model__min_samples_leaf\": randint(1, 8),        # min samples per leaf\n",
    "    \"model__min_weight_fraction_leaf\": [0.0, 0.1],    # min weighted fraction of total sum of weights required at a leaf node\n",
    "    \"model__max_features\": [\"sqrt\"],           # feature sampling strategy (sqrt performed better than log2 and None in previous tests)\n",
    "    \"model__bootstrap\": [False]                      # use bootstrapping or not (False performed better than True in previous tests)\n",
    "}\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "et_param_dist = {\n",
    "    \"preprocess__fs__vt__threshold\": [0.0],\n",
    "    \"model__criterion\": [\"squared_error\"],\n",
    "    \"model__n_estimators\": [328],\n",
    "    \"model__max_depth\": [20],\n",
    "    \"model__min_samples_split\": [5],\n",
    "    \"model__min_samples_leaf\": [1],\n",
    "    \"model__max_features\": [\"sqrt\"],\n",
    "    \"model__bootstrap\": [False],\n",
    "}\n",
    "\n",
    "et_tuned_pipe, et_random_search_object, et_scores_dict = model_hyperparameter_tuning(X_train, y_train, cv, et_pipe_adjusted, et_param_dist, n_iter=50)\n",
    "\n",
    "joblib.dump(et_tuned_pipe, \"et_tuned_pipe.pkl\")\n",
    "\n",
    "# Before HP-tuning\n",
    "# MAE: 1389.3234\n",
    "# RMSE: 2471.9891\n",
    "# R²: 0.9356\n",
    "# Best Model params: {'preprocess__fs__vt__threshold': 0.0, 'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__criterion': 'absolute_error', 'model__bootstrap': False}\n",
    "\n",
    "# TODO use “absolute_error” for final best performance\n",
    "\n",
    "# After HP-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32163c42-019f-487e-af27-893f1a602467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.6 [Combination] StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "651ede43-b75e-483f-9965-dd042c90d9e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Old parameter distribution\n",
    "stack_param_dist = {\n",
    "    \"final_estimator__learning_rate\": uniform(0.02, 0.1),\n",
    "    \"final_estimator__max_depth\": randint(3, 10),\n",
    "    \"final_estimator__min_samples_leaf\": randint(3, 20),\n",
    "    \"final_estimator__l2_regularization\": uniform(0.0, 1.0),\n",
    "}\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "stack_param_dist = {\n",
    "    \"final_estimator__learning_rate\": [0.061135390505667866],\n",
    "    \"final_estimator__max_depth\": [5],\n",
    "    \"final_estimator__min_samples_leaf\": [10],\n",
    "    \"final_estimator__l2_regularization\": [0.19438003399487302]\n",
    "}\n",
    "\n",
    "stack_tuned_pipe, stack_random_search_object, stack_scores_dict = model_hyperparameter_tuning(X_train, y_train, stack_pipe_fe, stack_param_dist, splits=3)\n",
    "# joblib.dump(stack_tuned_pipe, \"stack_best.pkl\")\n",
    "\n",
    "\n",
    "# Long Duration (~3mins)\n",
    "\n",
    "# MAE: 1351.8682\n",
    "# RMSE: 2498.2822\n",
    "# R²: 0.9342\n",
    "\n",
    "# After RandomizedSearchCV:\n",
    "# MAE: 1350.4717\n",
    "# RMSE: 2497.0474\n",
    "# R²: 0.9343\n",
    "# Best Model params: {'final_estimator__l2_regularization': np.float64(0.978892858275009), 'final_estimator__learning_rate': np.float64(0.06867421529594551), 'final_estimator__max_depth': 6, 'final_estimator__min_samples_leaf': 13}\n",
    "\n",
    "# Removed ElasticNet from stacking due to poor performance compared to RF and HGB alone\n",
    "# canceled but the cv scores didnt seem to show much improvement\n",
    "\n",
    "# Using transmission and fuelType as OHE instead of TE():\n",
    "# MAE: 1357.4291\n",
    "# RMSE: 2516.5470\n",
    "# R²: 0.9333\n",
    "# Best Model params: {'final_estimator__l2_regularization': np.float64(0.19438003399487302), 'final_estimator__learning_rate': np.float64(0.061135390505667866), 'final_estimator__max_depth': 5, 'final_estimator__min_samples_leaf': 10}\n",
    "\n",
    "\n",
    "# Removed fillna(0) in feature engineering for a_x_b and model_freq():\n",
    "# was worse for hgb and rf so not tested for stacking\n",
    "\n",
    "# ...\n",
    "\n",
    "# implemented GroupModeImputer\n",
    "# MAE: 1329.2379\n",
    "# RMSE: 2453.0239\n",
    "# R²: 0.9366\n",
    "# Best Model params: {'final_estimator__min_samples_leaf': 10, 'final_estimator__max_depth': 5, 'final_estimator__learning_rate': 0.061135390505667866, 'final_estimator__l2_regularization': 0.19438003399487302}\n",
    "\n",
    "# Fixed GroupImputer and added Feature Engineering to pipeline\n",
    "# MAE: 1369.6876\n",
    "# RMSE: 2516.2583\n",
    "# R²: 0.9333\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf7394c-9f64-4866-a068-52dcd03c77e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Comparison of Fine-Tuned Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7507adbc-4aa9-4871-98d2-ee8987dc109a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- The performance metrics are compared on the same data split to ensure a fair comparison (same CV seed)\n",
    "- We compare the performance of the preselected models based on 3 metrics with 1 primary metric\n",
    "    - MAE\n",
    "    - RMSE\n",
    "    - R2\n",
    "- We compare the mean results on the training and on the validation data to evaluate overfitting of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd683a0-76c8-469b-8530-d27d834b1c69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use object from randomizedsearch to retrieve the mean metrics of the best model (that was also refit on entire data for final predictions later)\n",
    "model_scores = {\n",
    "    \"hgb_tuned\": hgb_scores_dict,\n",
    "    \"rf_tuned\": rf_scores_dict,\n",
    "    # \"et_tuned\": et_scores_dict,\n",
    "    # \"stack_tuned\": stacked_scores_dict,\n",
    "}\n",
    "\n",
    "# Convert dictionary to DataFrame (transpose to have models as rows)\n",
    "df_scores = pd.DataFrame(model_scores).T \n",
    "df_scores = df_scores[['val_mae', 'val_rmse', 'val_r2','train_mae', 'train_rmse', 'train_r2']]\n",
    "\n",
    "# Sort by val_mae (primary metric)\n",
    "df_scores = df_scores.sort_values(by='val_mae')\n",
    "\n",
    "print(\"Model Comparison Table:\")\n",
    "display(df_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf74f27d-4b15-407e-a9a1-1e52b11f071b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_val_mae_comparison(df_scores)\n",
    "plot_train_val_comparison(df_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad64c4a-5c7f-4527-be22-c71b9e7f0e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**\n",
    "- Primary Metric val MAE:\n",
    "    - RF performs best...\n",
    "- Train vs. Val Score (Overfitting):\n",
    "    - RF overfits significantly more...\n",
    "- Secondary Metrics\n",
    "    - RMSE:\n",
    "    - R2:\n",
    "\n",
    "\n",
    "==> Final decision: Use RF because it has the lowest MAE and our final goal is to minimize the MAE. Therefore, we acccept the fact the RF is overfitting..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32e4b47f-ba46-4f29-ab5b-6c527cf7af86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Comparison of optimized model with previous models:**\n",
    "- Hyperparameter tuning massively improves the performance:\n",
    "\n",
    "| **Model** | **Performance** | **Biggest Change in HPs compared to default model** |\n",
    "| :--- | :--- | :--- |\n",
    "| **HGB** <br> *(Tree-based)* | ... | ... |\n",
    "| **RF** <br> *(Tree-based)* | ... | ... |\n",
    "| **ET** <br> *(Tree-based)* | ... | ... |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add code for deployment here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Prediction on Test setKaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test(model_pipeline, model_name):\n",
    "    # Load best model from Joblib and predict on validation set to verify\n",
    "    pipe_best = joblib.load(model_pipeline)\n",
    "    \n",
    "    # Predict on test set\n",
    "    df_cars_test['price'] = pipe_best.predict(df_cars_test)\n",
    "    df_cars_test[['carID', 'price']].to_csv(f'Group05_{model_name}_Version12.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_test(\"hgb_final_shap_pipe.pkl\", \"HGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_test(\"rf_tuned_pipe.pkl\", \"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_on_test(\"stack_pipe.pkl\", \"Stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c cars4you -f Group05_Version05.csv -m \"Message\" # Uncomment to submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c cars4you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Visualizations and Analysis of Best Model (Pipeline Processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.7 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The votes of each contributor are shown, resulting in the final decision whether to keep the feature or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the feed names after VT because VT is applied before the majority voting to remove constant features\n",
    "feature_names_after_vt = debug_preprocessor_pipe.named_steps['fs'].named_steps['vt'].get_feature_names_out()\n",
    "plot_selector_agreement(\n",
    "    majority_selector = debug_preprocessor_pipe.named_steps['fs'].named_steps['selector'], \n",
    "    feature_names = feature_names_after_vt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO we need good argument to justify the final features (notes from lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Findings and Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree-based models performed best:**     \n",
    "Regarding the baseline models we used in Section 5, it becomes clear that the tree-based models outperform the other models. This is probably due to ...\n",
    "\n",
    "**Constraints:**     \n",
    "However, the nature of tree-based models constraints the predictions to never be lower than the lowest or higher than the highest price in the train set. This is because tree-based models return the average price of the cars in the leaf node.\n",
    "\n",
    "**Outlook:**     \n",
    "A potential solution of this could be a Stacking Regressor that combines a tree-based model with another model that is better in extrapolation (e.g. RF + Ridge). A final Meta-Learner (also Linear) can combine their predictions. If the RF predicts \"Max Value\" but the Linear Model predicts \"Higher Value,\" the Meta-Learner can follow the Linear trend upward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bfd1b4a-0e13-443a-aeb7-fa6324f801e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. Open-Ended-Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110a34b9-7fb3-4b5b-b73d-b62f44ab3748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "#### 8.1 SHAP Interpretability for Our Final Tree Model (Informative Only)\n",
    "\n",
    "##### a) Objective and motivation (0.5v)\n",
    "\n",
    "After building a strong pipeline (data cleaning → imputation → feature engineering → encoding/scaling → VT + majority-vote FS → tuned tree model), we use **SHAP (SHapley Additive exPlanations)** purely for **interpretability**.\n",
    "\n",
    "Goals:\n",
    "- Identify the **most influential features** for our final tuned tree model (`hgb_tuned_pipe`).\n",
    "- Validate whether feature effects are **plausible** (age, mileage, engine, etc.).\n",
    "- Understand whether **target encodings** dominate and how engineered interactions contribute.\n",
    "\n",
    "Important: **SHAP does not change the model or feature set.** We do not build a new pipeline based on SHAP.\n",
    "\n",
    "---\n",
    "\n",
    "##### b) Difficulty of the task (1v)\n",
    "\n",
    "This was non-trivial because SHAP must explain the model input **after** our preprocessing:\n",
    "\n",
    "- The model does not see raw columns. It sees:\n",
    "  - engineered features,\n",
    "  - OHE columns,\n",
    "  - median target-encoded columns,\n",
    "  - and the reduced subset after **VT + majority voting**.\n",
    "- We therefore implemented a helper to reconstruct:\n",
    "  - the exact **post-preprocess feature matrix**, and\n",
    "  - aligned **feature names** after applying both selection masks (VT support + majority selector mask).\n",
    "- For `HistGradientBoostingRegressor`, SHAP’s **additivity check** can fail even with correct shapes. We handle this safely by disabling it (`check_additivity=False`) and keeping a robust fallback explainer if needed.\n",
    "- Runtime: SHAP is expensive, so we compute explanations on a **subsample** (`sample_size=1000`) plus a small background set.\n",
    "\n",
    "---\n",
    "\n",
    "##### c) Correctness and efficiency (1v)\n",
    "\n",
    "We kept the analysis correct and consistent with the production pipeline:\n",
    "\n",
    "- **No leakage / no optimization loop:** SHAP is computed on the already fitted tuned model and used only to interpret it.\n",
    "- **Exact alignment:** feature names are derived from the ColumnTransformer output and then filtered by VT + majority voting masks.\n",
    "- **Global SHAP importance:** we rank features by mean absolute contribution:\n",
    "  \n",
    "  $$\n",
    "  Importance(feature_j) = \\frac{1}{N}\\sum_{i=1}^{N} |SHAP_{i,j}|\n",
    "  $$\n",
    "\n",
    "- **Efficient computation:** stable ranking via subsampling.\n",
    "\n",
    "---\n",
    "\n",
    "##### d) Results and interpretation (1v)\n",
    "\n",
    "Model context:\n",
    "- Final tuned model: `hgb_tuned_pipe`\n",
    "- Total features after preprocessing + FS: **26**\n",
    "\n",
    "Top drivers (mean |SHAP|), excerpt:\n",
    "\n",
    "| Feature | Importance | Interpretation |\n",
    "|---|---:|---|\n",
    "| `median_te__model_median_te` | 2850.28 | Model-level median target encoding (strong market-value proxy) |\n",
    "| `num__mpg_x_age` | 1659.43 | Interaction: MPG × age |\n",
    "| `num__engineSize` | 1367.63 | Engine size (segment/performance proxy) |\n",
    "| `log__mileage` | 1151.75 | Log mileage (diminishing marginal effect) |\n",
    "| `num__age_rel_model` | 642.22 | Age relative to typical age within the model |\n",
    "| `median_te__brand_trans_median_te` | 500.44 | Brand × transmission median target encoding |\n",
    "| `num__engine_per_mpg` | 497.18 | Performance/efficiency ratio |\n",
    "| `cat__transmission_Manual` | 406.37 | Manual transmission effect |\n",
    "\n",
    "Key takeaways:\n",
    "- **Target encodings dominate** global importance, especially the model-level encoding. This is expected because model identity carries a large fraction of price signal.\n",
    "- **Engineered interactions matter** (`mpg_x_age`, `engine_per_mpg`, `mpg_x_engine`), confirming that our feature engineering adds useful non-additive structure.\n",
    "- **Mileage and age appear in strong, intuitive forms** (log mileage, relative age vs model/brand), supporting both predictive performance and interpretability.\n",
    "\n",
    "Beeswarm plot (distribution of effects):\n",
    "- **`median_te__model_median_te` dominates** and shows a wide SHAP spread → model identity (via median target encoding) is the strongest pricing signal.\n",
    "- **Mileage effect is non-linear** (`log__mileage`): low mileage produces strong positive contributions; high mileage pushes predictions down, but with diminishing marginal impact (consistent with log-transform).\n",
    "- **Engine/performance features matter across many cars** (`num__engineSize`, `num__engine_per_mpg`, `num__mpg_x_age`) and show heterogeneous spreads → effects differ by segment (e.g., sporty vs economy cars).\n",
    "- **Relative positioning features stabilize predictions** (`num__age_rel_model`, `num__engine_rel_model`): the model compares a car to what is “typical” within its model/segment, not only absolute values.\n",
    "- **Transmission signal is consistent** (`cat__transmission_Manual`): manual cars tend to shift predictions in one direction (dataset-dependent), but the spread indicates exceptions (model/brand interactions).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##### e) Alignment with objectives (0.5v)\n",
    "\n",
    "This section adds transparency without changing the modeling procedure:\n",
    "\n",
    "- Feature selection stays **VT + majority voting** (robust, leakage-safe, model-agnostic).\n",
    "- SHAP is used **only** to explain the final tuned model.\n",
    "- The resulting drivers (target encodings + age/mileage/engine + interactions) are consistent with domain logic and support trust in the final pipeline.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "884a9de8-1140-4441-ba63-de286e116908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get Feature names aligned with X_proc (after preprocess incl. VT + majority voting)\n",
    "def get_pipeline_feature_matrix(pipe, X):\n",
    "    \"\"\"\n",
    "    Given a fitted model pipeline with steps:\n",
    "      'preprocess' -> 'model'\n",
    "    where preprocess itself is a Pipeline:\n",
    "      clean -> group_imputer -> fe -> ct -> fs(vt + selector)\n",
    "    return:\n",
    "      X_proc: 2D numpy array of features just before the model step\n",
    "      feat_names: 1D np.array of feature names aligned with X_proc columns\n",
    "    \"\"\"\n",
    "    pre = pipe.named_steps[\"preprocess\"]\n",
    "\n",
    "    # 1) Transform to model-ready matrix\n",
    "    X_proc = pre.transform(X)\n",
    "\n",
    "    # 2) Reconstruct feature names: ct -> vt mask -> majority selector mask\n",
    "    ct = pre.named_steps[\"ct\"]\n",
    "    feat_names = np.asarray(ct.get_feature_names_out(), dtype=object)\n",
    "\n",
    "    fs = pre.named_steps.get(\"fs\", None)\n",
    "    if fs is not None:\n",
    "        # VT (dictator) first\n",
    "        vt = fs.named_steps.get(\"vt\", None)\n",
    "        if vt is not None and hasattr(vt, \"get_support\"):\n",
    "            feat_names = feat_names[vt.get_support()]\n",
    "\n",
    "        # Majority selector next\n",
    "        sel = fs.named_steps.get(\"selector\", None)\n",
    "        if sel is not None and hasattr(sel, \"support_mask_\") and sel.support_mask_ is not None:\n",
    "            feat_names = feat_names[sel.support_mask_]\n",
    "\n",
    "    return X_proc, feat_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "19ab4ce1-20fe-4aed-87d4-bac317455159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compute SHAP Importance\n",
    "def compute_shap_importance(\n",
    "    pipe,\n",
    "    X,\n",
    "    sample_size=1000,\n",
    "    seed=rs,\n",
    "    model_name=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute global SHAP feature importances for a fitted pipeline (informative only).\n",
    "\n",
    "    Fix:\n",
    "      - TreeExplainer additivity check can fail for some sklearn tree implementations (incl. HGB).\n",
    "        We disable it via check_additivity=False.\n",
    "      - If TreeExplainer still fails, fall back to a model-agnostic SHAP explainer.\n",
    "    \"\"\"\n",
    "    # Extract processed feature matrix and names\n",
    "    X_proc, feat_names = get_pipeline_feature_matrix(pipe, X)\n",
    "\n",
    "    # Subsample rows for SHAP (for speed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = min(sample_size, len(X_proc))\n",
    "    idx = rng.choice(len(X_proc), n, replace=False)\n",
    "    X_sample = X_proc[idx]\n",
    "\n",
    "    # Underlying model (last step in pipeline)\n",
    "    model = pipe.named_steps[\"model\"]\n",
    "    tag = model_name or model.__class__.__name__\n",
    "\n",
    "    # Background for SHAP (small subset)\n",
    "    bg_n = min(200, len(X_sample))\n",
    "    bg_idx = rng.choice(len(X_sample), bg_n, replace=False)\n",
    "    X_bg = X_sample[bg_idx]\n",
    "\n",
    "    # --- Try TreeExplainer first (fast for tree models) ---\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model, X_bg)\n",
    "        shap_vals = explainer.shap_values(X_sample, check_additivity=False)\n",
    "\n",
    "        # shap_vals can be list-like in some setups; regression should be 2D\n",
    "        if isinstance(shap_vals, list):\n",
    "            shap_vals = shap_vals[0]\n",
    "\n",
    "        base_vals = getattr(explainer, \"expected_value\", 0.0)\n",
    "        shap_values = shap.Explanation(\n",
    "            values=shap_vals,\n",
    "            base_values=np.full((len(X_sample),), base_vals) if np.isscalar(base_vals) else base_vals,\n",
    "            data=X_sample,\n",
    "            feature_names=feat_names,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        # --- Fallback: model-agnostic explainer (slower but robust) ---\n",
    "        explainer = shap.Explainer(model.predict, X_bg, feature_names=feat_names)\n",
    "        shap_values = explainer(X_sample)\n",
    "\n",
    "    importance = np.abs(shap_values.values).mean(axis=0)\n",
    "\n",
    "    shap_df = (\n",
    "        pd.DataFrame({\"feature\": feat_names, \"importance\": importance})\n",
    "        .sort_values(\"importance\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(f\"Top 20 features by SHAP for {tag}:\")\n",
    "    print(shap_df.head(20).to_string(index=False))\n",
    "\n",
    "    return shap_df, feat_names, shap_values, X_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c0d95247-5f01-4a13-8631-5652df7e902e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SHAP Plots\n",
    "def plot_top_shap_bar(shap_df, model_name, top_k):\n",
    "    \"\"\"\n",
    "    Horizontal bar plot of top_k features by mean |SHAP|.\n",
    "    \"\"\"\n",
    "    top_df = shap_df.head(top_k).iloc[::-1]  # reverse for nicer barh order\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.barh(top_df[\"feature\"], top_df[\"importance\"])\n",
    "    ax.set_xlabel(\"Average |SHAP| value\")\n",
    "    ax.set_title(f\"Top {top_k} features by SHAP – {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_shap_beeswarm(shap_values, X_sample, feat_names, model_name, max_display=20):\n",
    "    \"\"\"\n",
    "    SHAP summary (beeswarm) plot for top features.\n",
    "    \"\"\"\n",
    "    X_df = pd.DataFrame(X_sample, columns=feat_names)\n",
    "\n",
    "    # Create one figure and tell SHAP not to auto-show\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values.values, X_df, max_display=max_display, show=False)\n",
    "\n",
    "    plt.title(f\"SHAP Beeswarm – {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fc3f958a-23ad-46ce-933e-b2f0df060207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ARCHIVE: \"top-k from SHAP\" code commented out\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# The following would create a new model/pipeline based on SHAP-top-k features.\n",
    "# We keep it for reference but do not use it because:\n",
    "# - We already have a robust, leakage-safe FS pipeline (VT + majority voting).\n",
    "# - Here we want SHAP to be interpretability only.\n",
    "\n",
    "# def cv_mae_topk_from_shap(\n",
    "#     pipe,\n",
    "#     shap_importance,\n",
    "#     X,\n",
    "#     y,\n",
    "#     n_features_list,\n",
    "#     folds=5,\n",
    "#     seed=rs,\n",
    "#     model_name=None,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     For a fitted pipeline `pipe` and its SHAP importances:\n",
    "#       - Build X_proc, feat_names from the pipeline.\n",
    "#       - For each k in n_features_list:\n",
    "#           * Take top-k features by SHAP.\n",
    "#           * Run KFold CV on X_proc[:, idx] with the pipeline's final estimator.\n",
    "#       - Print MAE per k and return the best (k, model, feature list).\n",
    "#\n",
    "#     Returns:\n",
    "#       best_model: fitted estimator on full X_proc restricted to best-k features\n",
    "#       best_features: list of feature names used\n",
    "#     \"\"\"\n",
    "#     # 1) Get processed features and names\n",
    "#     X_proc, feat_names = get_pipeline_feature_matrix(pipe, X)\n",
    "#     feat_names = np.asarray(feat_names, dtype=object)\n",
    "#\n",
    "#     # 2) SHAP ranking\n",
    "#     shap_sorted = shap_importance.sort_values(\"importance\", ascending=False)\n",
    "#     shap_order = shap_sorted[\"feature\"].tolist()\n",
    "#\n",
    "#     # helper: indices of top-k by SHAP\n",
    "#     def indices_for_topk(k):\n",
    "#         top_feats = shap_order[:k]\n",
    "#         return [i for i, fname in enumerate(feat_names) if fname in top_feats]\n",
    "#\n",
    "#     kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "#     model_proto = pipe.named_steps[\"model\"]\n",
    "#     tag = model_name or model_proto.__class__.__name__\n",
    "#\n",
    "#     results = []\n",
    "#\n",
    "#     for k in n_features_list:\n",
    "#         idx = indices_for_topk(k)\n",
    "#         if len(idx) == 0:\n",
    "#             print(f\"Skipping k={k}: no matching feature indices.\")\n",
    "#             continue\n",
    "#\n",
    "#         mae_folds = []\n",
    "#\n",
    "#         for train_idx, val_idx in kf.split(X_proc):\n",
    "#             X_tr, X_val = X_proc[train_idx][:, idx], X_proc[val_idx][:, idx]\n",
    "#             y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "#\n",
    "#             est = clone(model_proto)\n",
    "#             est.fit(X_tr, y_tr)\n",
    "#             y_pred = est.predict(X_val)\n",
    "#             mae_folds.append(mean_absolute_error(y_val, y_pred))\n",
    "#\n",
    "#         mae_mean = float(np.mean(mae_folds))\n",
    "#         results.append({\"k\": k, \"mae\": mae_mean, \"idx\": idx})\n",
    "#\n",
    "#     # pick best k\n",
    "#     if not results:\n",
    "#         raise RuntimeError(\"No valid k in n_features_list produced results.\")\n",
    "#\n",
    "#     best = min(results, key=lambda r: r[\"mae\"])\n",
    "#     best_k = best[\"k\"]\n",
    "#     best_mae = best[\"mae\"]\n",
    "#     best_idx = best[\"idx\"]\n",
    "#     best_features = [feat_names[i] for i in best_idx]\n",
    "#\n",
    "#     print(f\"\\nTop-k SHAP feature CV – {tag}\")\n",
    "#     for r in results:\n",
    "#         print(f\"  k={r['k']:3d} | MAE={r['mae']:.2f}\")\n",
    "#     print(f\"Best: k={best_k} | MAE={best_mae:.2f}\")\n",
    "#\n",
    "#     # fit final estimator on full X_proc restricted to best_k features\n",
    "#     final_est = clone(model_proto)\n",
    "#     final_est.fit(X_proc[:, best_idx], y)\n",
    "#\n",
    "#     return final_est, best_features\n",
    "#\n",
    "#\n",
    "# class ShapTopKColumnSelector(BaseEstimator, TransformerMixin):\n",
    "#     \"\"\"\n",
    "#     Transformer that selects a fixed subset of columns by name.\n",
    "#\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     selected_features : list of str\n",
    "#         Feature names (after preprocessing) to keep.\n",
    "#\n",
    "#     all_feature_names : array-like of str\n",
    "#         Full list of feature names aligned with the columns of X after preprocessing.\n",
    "#         These are typically obtained from get_pipeline_feature_matrix(...).\n",
    "#     \"\"\"\n",
    "#     def __init__(self, selected_features, all_feature_names):\n",
    "#         self.selected_features = list(selected_features)\n",
    "#         self.all_feature_names = np.asarray(all_feature_names, dtype=object)\n",
    "#\n",
    "#     def fit(self, X, y=None):\n",
    "#         # Compute the column indices corresponding to selected_features\n",
    "#         name_to_idx = {name: i for i, name in enumerate(self.all_feature_names)}\n",
    "#         self.idx_ = [\n",
    "#             name_to_idx[name]\n",
    "#             for name in self.selected_features\n",
    "#             if name in name_to_idx\n",
    "#         ]\n",
    "#         if len(self.idx_) == 0:\n",
    "#             raise ValueError(\n",
    "#                 \"ShapTopKColumnSelector: none of the selected_features were found \"\n",
    "#                 \"in all_feature_names.\"\n",
    "#             )\n",
    "#         return self\n",
    "#\n",
    "#     def transform(self, X):\n",
    "#         # X is the matrix after preprocessing; select only the chosen columns\n",
    "#         return X[:, self.idx_]\n",
    "#\n",
    "#     def get_feature_names_out(self, input_features=None):\n",
    "#         # For consistency with sklearn's feature-name API\n",
    "#         return np.asarray(self.selected_features, dtype=object)\n",
    "#\n",
    "#\n",
    "# def build_shap_topk_pipeline(\n",
    "#     base_pipe,\n",
    "#     best_features,\n",
    "#     all_feature_names,\n",
    "#     step_model_name=\"model\",\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Build a final pipeline that:\n",
    "#       - reuses the preprocessing (and vt/fs if present) from `base_pipe`\n",
    "#       - inserts a ShapTopKColumnSelector to keep only `best_features`\n",
    "#       - uses a fresh clone of the base model as final estimator\n",
    "#\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     base_pipe : sklearn Pipeline\n",
    "#         Fitted pipeline with steps: 'preprocess' -> optional 'vt'/'fs' -> 'model'.\n",
    "#\n",
    "#     best_features : list of str\n",
    "#         Names of the features (after preprocessing) to keep.\n",
    "#\n",
    "#     all_feature_names : array-like of str\n",
    "#         Full list of feature names aligned with the output of preprocessing\n",
    "#         (and vt/fs if they were applied when computing SHAP).\n",
    "#\n",
    "#     step_model_name : str, default=\"model\"\n",
    "#         Name of the final estimator step in base_pipe.\n",
    "#\n",
    "#     Returns\n",
    "#     -------\n",
    "#     final_pipe : sklearn Pipeline\n",
    "#         Unfitted pipeline. Call final_pipe.fit(X, y) to train on full data.\n",
    "#     \"\"\"\n",
    "#     steps = []\n",
    "#\n",
    "#     # 1) Preprocess step (clone so we refit on full data)\n",
    "#     pre = base_pipe.named_steps[\"preprocess\"]\n",
    "#     steps.append((\"preprocess\", clone(pre)))\n",
    "#\n",
    "#     # 2) Optional VarianceThreshold\n",
    "#     if \"vt\" in base_pipe.named_steps and base_pipe.named_steps[\"vt\"] is not None:\n",
    "#         steps.append((\"vt\", clone(base_pipe.named_steps[\"vt\"])))\n",
    "#\n",
    "#     # 3) SHAP-based column selector\n",
    "#     shap_selector = ShapTopKColumnSelector(\n",
    "#         selected_features=best_features,\n",
    "#         all_feature_names=all_feature_names,\n",
    "#     )\n",
    "#     steps.append((\"shap_select\", shap_selector))\n",
    "#\n",
    "#     # 4) Final estimator – fresh clone of the base model\n",
    "#     base_model = base_pipe.named_steps[step_model_name]\n",
    "#     steps.append((\"model\", clone(base_model)))\n",
    "#\n",
    "#     final_pipe = Pipeline(steps)\n",
    "#     return final_pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c3fddf-e4ef-4541-9399-0ac1f65a9b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### HGB SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e799e384-74b9-48f1-b6bf-913eb9ea14dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# HGB baseline report + feature count\n",
    "hgb_pipe = hgb_tuned_pipe\n",
    "\n",
    "# Feature count after preprocess (clean+impute+fe+ct+fs)\n",
    "X_proc_hgb, feat_names_hgb = get_pipeline_feature_matrix(hgb_pipe, X_train)\n",
    "n_features_total_hgb = X_proc_hgb.shape[1]\n",
    "\n",
    "print(\"HGB (tuned pipe) – feature space info:\")\n",
    "print(f\"Total features used: {n_features_total_hgb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9f631d7f-f7c0-454d-96fb-adf14ee763b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# HGB SHAP\n",
    "shap_importance_hgb, feat_names_hgb, shap_vals_hgb, X_sample_hgb = compute_shap_importance(\n",
    "    hgb_pipe,\n",
    "    X_train,\n",
    "    sample_size=1000,\n",
    "    seed=rs,\n",
    "    model_name=\"HGB\",\n",
    ")\n",
    "\n",
    "plot_top_shap_bar(shap_importance_hgb, model_name=\"HGB\", top_k=n_features_total_hgb)\n",
    "plot_shap_beeswarm(shap_vals_hgb, X_sample_hgb, feat_names_hgb, model_name=\"HGB\", max_display=n_features_total_hgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d15d2eb8-908b-4172-8082-262720a51d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### RF SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "52b49bb4-16b5-4ab4-b247-66c831d285d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RandomForest baseline report + SHAP\n",
    "rf_pipe = rf_tuned_pipe\n",
    "\n",
    "# Feature matrix + names after preprocess (clean+impute+fe+ct+fs)\n",
    "X_proc_rf, feat_names_rf = get_pipeline_feature_matrix(rf_pipe, X_train)\n",
    "n_features_total_rf = X_proc_rf.shape[1]\n",
    "\n",
    "print(\"RandomForest (tuned pipe) – feature space info:\")\n",
    "print(f\"Total features used: {n_features_total_rf}\")\n",
    "\n",
    "shap_importance_rf, feat_names_rf, shap_vals_rf, X_sample_rf = compute_shap_importance(\n",
    "    rf_pipe,\n",
    "    X_train,\n",
    "    sample_size=1000,\n",
    "    seed=rs,\n",
    "    model_name=\"RandomForest\",\n",
    ")\n",
    "\n",
    "plot_top_shap_bar(shap_importance_rf, model_name=\"RandomForest\", top_k=20)\n",
    "plot_shap_beeswarm(shap_vals_rf, X_sample_rf, feat_names_rf, model_name=\"RandomForest\", max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c0b4f23d-b3c7-46e6-ac72-94d8a3fb279a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RF SHAP\n",
    "shap_importance_rf, feat_names_rf, shap_vals_rf, X_sample_rf = compute_shap_importance(\n",
    "    rf_pipe,\n",
    "    X_train,\n",
    "    sample_size=1000,\n",
    "    seed=rs,\n",
    "    model_name=\"RandomForest\",\n",
    ")\n",
    "\n",
    "plot_top_shap_bar(shap_importance_rf, model_name=\"RandomForest\", top_k=20)\n",
    "plot_shap_beeswarm(shap_vals_rf, X_sample_rf, feat_names_rf, model_name=\"RandomForest\", max_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a3ac74-bda2-40f7-97f1-ca1574ae7896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 8.2 Global vs Brand- and Model-Specific Models\n",
    "\n",
    "##### a) Objective and motivation (0.5v)\n",
    "\n",
    "We investigated how far Cars4You should specialize its pricing models:\n",
    "\n",
    "1. **Brand level:** Is a single global price model for all brands sufficient, or do separate brand-specific models reduce pricing error?\n",
    "2. **Brand–model level:** For frequent models (e.g. “Skoda Octavia”, “VW Golf”), does an even more specialized model per (brand, model) segment bring additional improvements, or does it overfit?\n",
    "\n",
    "Concretely, we started from our final production pipeline `hgb_final_shap_pipe` (full preprocessing + SHAP-based feature selection + HGB regressor) and compared:\n",
    "\n",
    "- **Global model:** trained on all cars, evaluated only on a given segment.\n",
    "- **Brand-specific model:** same preprocessing and SHAP selector, but the regressor re-fitted only on cars of a given brand.\n",
    "- **Brand–model-specific model:** same preprocessing and SHAP selector, but the regressor re-fitted only on cars of a given (brand, model) pair.\n",
    "\n",
    "We measured mean absolute error (MAE) and root mean squared error (RMSE) per segment. This answers how much performance we gain by moving from:\n",
    "\n",
    "> one global model → several brand models → many brand–model models.\n",
    "\n",
    "---\n",
    "\n",
    "##### b) Difficulty of tasks (1v)\n",
    "\n",
    "Extending the existing solution to this multi-level comparison was non-trivial:\n",
    "\n",
    "- **Complex pipeline with a custom SHAP selector**  \n",
    "  The final pipeline contains a `ShapTopKColumnSelector` that is not clone-compatible. Standard `cross_val_score` + `clone` would fail. We therefore implemented manual cross-validation:\n",
    "  - reuse the fitted preprocessing + SHAP selector from `hgb_final_shap_pipe`;\n",
    "  - only re-fit the final regressor for each fold and segment.\n",
    "\n",
    "- **Consistent and fair evaluation protocol**  \n",
    "  We reused the same 5-fold KFold strategy (`n_splits`, `shuffle`, `random_state`) and the same target (`price`) as in the main project. For each fold and segment:\n",
    "  - the global model is trained on all training rows but evaluated only on validation rows belonging to that segment;\n",
    "  - the segment-specific model is trained and evaluated only on that segment’s rows.\n",
    "\n",
    "- **Handling data imbalance**  \n",
    "  Data is unevenly distributed across brands and models. We therefore:\n",
    "  - restricted the analysis to brands with at least 500 training samples;\n",
    "  - for brand–model analysis, kept only frequent pairs (e.g. Skoda Octavia, VW Golf) with a minimum sample threshold per segment;\n",
    "  - enforced additional checks per fold (minimum training size) to avoid fits on a handful of cars.\n",
    "\n",
    "- **Manual metric computation**  \n",
    "  Due to an older `sklearn` version (no `squared=` parameter), RMSE had to be computed manually as `sqrt(MSE)` inside the CV loops instead of relying on built-in scorers.\n",
    "\n",
    "Overall, the task required custom CV logic, careful reuse of the production pipeline, and multiple levels of segment-wise filtering.\n",
    "\n",
    "---\n",
    "\n",
    "##### c) Correctness and efficiency of implementation (1v)\n",
    "\n",
    "To keep the analysis correct and reasonably efficient we:\n",
    "\n",
    "- **Reused the production pipeline as-is**  \n",
    "  All preprocessing (imputation, scaling, encoding, price anchors) and SHAP-based feature selection are exactly the same as in the final model used on the test set. Only the last regressor is re-fit for segment-specific models.\n",
    "\n",
    "- **Used a single CV design for all comparisons**  \n",
    "  The same KFold splits (`splits = list(KFold(...).split(X_train, y_train))`) are reused for:\n",
    "  - global per-brand evaluation;\n",
    "  - brand-specific evaluation;\n",
    "  - global per (brand, model) evaluation;\n",
    "  - brand–model-specific evaluation.  \n",
    "  This removes extra randomness and makes differences directly comparable.\n",
    "\n",
    "- **Implemented clear separation between global and segment-specific training**  \n",
    "  - For brands:  \n",
    "    - global: fit on all brands, compute metrics only on that brand’s validation rows;  \n",
    "    - brand-specific: use the fixed preprocessor, fit a fresh regressor only on that brand’s transformed data.\n",
    "  - For (brand, model) pairs:  \n",
    "    - global: fit on all cars, compute metrics only on that (brand, model) validation subset;  \n",
    "    - brand–model-specific: fixed preprocessor + fresh regressor only on that pair.\n",
    "\n",
    "- **Guarded against tiny segments**  \n",
    "  Only segments with enough rows at dataset level and per fold are evaluated. Otherwise, metrics are set to NaN and those segments are excluded via `dropna`.\n",
    "\n",
    "This design produces stable segment-wise estimates without changing the core production pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "##### d) Discussion of results (1v)\n",
    "\n",
    "#### Brand-level comparison\n",
    "\n",
    "For the main brands, the final summary table (MAE in GBP) is:\n",
    "\n",
    "| Brand    | MAE (global) | MAE (brand) | ΔMAE (brand – global) | n_samples |\n",
    "|----------|--------------|-------------|------------------------|-----------|\n",
    "| Ford     | 966.7        | 929.2       | -37.6                  | 16,371    |\n",
    "| BMW      | 1,828.0      | 1,792.8     | -35.2                  | 7,540     |\n",
    "| Mercedes | 1,968.7      | 1,934.6     | -34.1                  | 11,899    |\n",
    "| VW       | 1,299.7      | 1,287.8     | -11.9                  | 10,572    |\n",
    "| Audi     | 1,806.0      | 1,794.5     | -11.5                  | 7,456     |\n",
    "| Skoda    | 1,174.6      | 1,165.9     | -8.7                   | 4,380     |\n",
    "| Toyota   |   926.7      |   920.9     | -5.9                   | 4,714     |\n",
    "| Opel     |   777.1      |   774.5     | -2.6                   | 9,530     |\n",
    "\n",
    "Key observations:\n",
    "\n",
    "- **High-volume premium brands benefit the most from brand-specific models.**  \n",
    "  Ford, BMW and Mercedes gain about 35–38 GBP lower MAE per car (≈ 2–4% relative improvement). This is meaningful at scale and based on large sample sizes.\n",
    "\n",
    "- **Moderate gains for VW, Audi, Skoda, Toyota.**  \n",
    "  MAE improvements are smaller (5–12 GBP, typically <1% relative), but still consistent in sign.\n",
    "\n",
    "- **Minimal benefit for Opel.**  \n",
    "  The improvement for Opel (≈ 2.6 GBP) is negligible relative to its base MAE. The global model already captures Opel’s pricing patterns.\n",
    "\n",
    "- **RMSE sometimes increases slightly for brand-specific models.**  \n",
    "  For some brands, RMSE is marginally higher, indicating that brand-specific models reduce typical errors but can perform worse on rare/extreme cases, hinting at mild overfitting in the tails.\n",
    "\n",
    "Overall, moving from a global to a brand-specific layer consistently does not harm MAE and clearly helps for some large brands, but the absolute gains are moderate.\n",
    "\n",
    "#### Brand–model-level comparison\n",
    "\n",
    "For frequent (brand, model) pairs, the analysis shows a more mixed picture. A selection of results (MAE in GBP):\n",
    "\n",
    "| Brand   | Model        | MAE global | MAE seg | ΔMAE (seg – global) | n_samples |\n",
    "|---------|--------------|-----------:|--------:|---------------------:|----------:|\n",
    "| Skoda   | kamiq        | 1,418.6    | 1,107.1 | -311.5               | 109       |\n",
    "| VW      | amarok       | 2,988.7    | 2,801.3 | -187.4               | 83        |\n",
    "| Mercedes| x-class      | 3,592.8    | 3,448.9 | -144.0               | 59        |\n",
    "| Skoda   | scala        | 1,175.7    | 1,100.5 | -75.2                | 147       |\n",
    "| Ford    | b-max        |   640.2    |   578.1 | -62.1                | 248       |\n",
    "| Skoda   | octavia      | 1,089.4    | 1,031.9 | -57.5                | 1,021     |\n",
    "| Skoda   | fabia        |   845.8    |   795.1 | -50.6                | 1,069     |\n",
    "| VW      | up           |   645.2    |   608.1 | -37.1                | 608       |\n",
    "| BMW     | 1 series     | 1,158.2    | 1,130.0 | -28.1                | 1,358     |\n",
    "| VW      | golf         | 1,151.0    | 1,155.8 |  +4.8                | 3,515     |\n",
    "| Ford    | fiesta       |   753.3    |   762.7 |  +9.3                | 4,470     |\n",
    "| Toyota  | aygo         |   557.1    |   576.7 | +19.6                | 1,381     |\n",
    "| BMW     | 7 series     | 3,146.7    | 4,751.9 | +1,605.2             | 71        |\n",
    "| Mercedes| gls class    | 3,295.8    | 5,906.4 | +2,610.7             | 54        |\n",
    "\n",
    "Patterns:\n",
    "\n",
    "- **Some compact, relatively frequent models benefit from model-level specialization.**  \n",
    "  Examples: Skoda Kamiq, Scala, Octavia and Fabia; VW up; Ford B-MAX.  \n",
    "  These segments see large MAE reductions (50–300 GBP), and RMSE also tends to decrease. Here, the model-level regressor can exploit consistent, model-specific patterns.\n",
    "\n",
    "- **For many common volume models, gains are small or negative.**  \n",
    "  VW Golf, Ford Fiesta, Opel Corsa, Toyota Yaris, etc. often show small positive ΔMAE and/or higher RMSE. For these, splitting by model does not significantly improve typical error and can worsen extreme cases.\n",
    "\n",
    "- **For rare, high-priced models, model-specific fits severely overfit.**  \n",
    "  BMW 7 series, BMW X6, Mercedes GLS/S/SL/CLS class, VW Beetle, Toyota Avensis/Verso and others exhibit very large increases in MAE (hundreds to thousands of GBP) and often huge increases in RMSE.  \n",
    "  These models have small sample sizes (often <100 cars), so a separate model per (brand, model) is clearly not robust.\n",
    "\n",
    "In short:\n",
    "\n",
    "- Moving from **global → brand** is often beneficial and relatively safe for high-volume brands.\n",
    "- Moving further from **brand → brand–model** brings strong improvements only for a small subset of frequent models; for many others, especially rare premium models, it clearly overfits.\n",
    "\n",
    "---\n",
    "\n",
    "##### e) Alignment with objectives (0.5v)\n",
    "\n",
    "This extended open-ended study:\n",
    "\n",
    "- Directly addresses and expands a suggested topic (“global vs brand-specific models”), and pushes it one step further to **brand–model** specialization.\n",
    "- Uses fully the final production pipeline and a consistent CV protocol, so the conclusions are directly relevant for deployment.\n",
    "- Provides a **clear design recommendation**:\n",
    "  - Use a **single global model** as the base.\n",
    "  - Optionally introduce **brand-level specialization** for a small set of high-volume brands (e.g. Ford, BMW, Mercedes) where MAE improvements are meaningful.\n",
    "  - Avoid full **brand–model specialization** except potentially for a handful of very frequent models with demonstrated gains; for most models, especially rare and expensive ones, splitting further clearly overfits.\n",
    "\n",
    "This shows that we not only tuned a strong model, but also explored the trade-off between model complexity and robustness in a structured, data-driven way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "44783a34-5a57-4037-a7d8-54b387152db9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load final production pipeline (preprocessing + SHAP + HGB)\n",
    "# hgb_final_shap_pipe = load(\"hgb_final_shap_pipe.pkl\")\n",
    "pipe_global = hgb_final_shap_pipe \n",
    "\n",
    "assert \"X_train\" in globals() and \"y_train\" in globals(), \"Define X_train and y_train before proceeding.\"\n",
    "\n",
    "# Identify the brand column (name may be 'Brand' or 'brand')\n",
    "brand_col = \"Brand\" if \"Brand\" in X_train.columns else \"brand\"\n",
    "assert brand_col in X_train.columns, (\n",
    "    f\"Brand column not found in X_train. \"\n",
    "    f\"First columns: {X_train.columns.tolist()[:20]}\"\n",
    ")\n",
    "\n",
    "print(\"Using brand column:\", brand_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dc47c72d-b82a-4b94-aa47-638eda95115a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inspect brand frequencies\n",
    "brand_counts = X_train[brand_col].value_counts()\n",
    "print(\"Top brands by count:\")\n",
    "print(brand_counts.head(15))\n",
    "\n",
    "# Select candidate brands\n",
    "#    - TOP_K: max number of brands to compare.\n",
    "#    - MIN_SAMPLES: minimum number of rows per brand.\n",
    "\n",
    "TOP_K = 8\n",
    "MIN_SAMPLES = 500  # adjust if needed\n",
    "\n",
    "candidate_brands = [\n",
    "    b for b, cnt in brand_counts.items()\n",
    "    if cnt >= MIN_SAMPLES\n",
    "][:TOP_K]\n",
    "\n",
    "print(\"\\nCandidate brands used in the comparison:\")\n",
    "print(candidate_brands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f0f1d273-7c33-4818-94d6-52585c8d9196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cross-validation setup: We reuse the same KFold splits for all evaluations to keep comparisons fair and to reduce randomness.\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=rs)\n",
    "splits = list(cv.split(X_train, y_train)) # TODO no random_state necessary here?\n",
    "\n",
    "\n",
    "def eval_global_for_brand(model, X, y, brand_col, brand, splits):\n",
    "    \"\"\"\n",
    "    Evaluate the global pipeline for a single brand.\n",
    "\n",
    "    The model is trained on all brands in each fold, but the error\n",
    "    is computed only on validation rows belonging to the given brand.\n",
    "    \"\"\"\n",
    "    maes, rmses = [], []\n",
    "    n_obs = 0\n",
    "\n",
    "    for train_idx, val_idx in splits:\n",
    "        # Split data for this fold\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Train global model on ALL brands in this fold\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Restrict metrics to the target brand in validation\n",
    "        mask_b = (X_val[brand_col] == brand)\n",
    "        if mask_b.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        y_val_b = y_val[mask_b]\n",
    "        y_pred_b = y_pred[mask_b]\n",
    "\n",
    "        mae = mean_absolute_error(y_val_b, y_pred_b)\n",
    "        mse = mean_squared_error(y_val_b, y_pred_b)\n",
    "        rmse = float(np.sqrt(mse))\n",
    "\n",
    "        maes.append(mae)\n",
    "        rmses.append(rmse)\n",
    "        n_obs += mask_b.sum()\n",
    "\n",
    "    return {\n",
    "        \"MAE_mean\": float(np.mean(maes)),\n",
    "        \"MAE_std\":  float(np.std(maes)),\n",
    "        \"RMSE_mean\": float(np.mean(rmses)),\n",
    "        \"RMSE_std\":  float(np.std(rmses)),\n",
    "        \"n\": int(n_obs),\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_brand_specific(pipe_global, X, y, brand_col, brand, splits,\n",
    "                        min_train_per_fold=50):\n",
    "    \"\"\"\n",
    "    Evaluate a brand-specific model for a single brand.\n",
    "\n",
    "    Preprocessing + SHAP selection are kept fixed (from pipe_global).\n",
    "    In each fold:\n",
    "      - Transform the brand's data with the fixed preprocessor.\n",
    "      - Fit a fresh regressor (clone of the final step) only on that brand.\n",
    "      - Evaluate on validation rows of that brand.\n",
    "    \"\"\"\n",
    "    # Split the pipeline into:\n",
    "    # - preproc: all steps except the final regressor\n",
    "    # - base_reg: the final regressor template\n",
    "    preproc = pipe_global[:-1]\n",
    "    base_reg = pipe_global[-1]\n",
    "\n",
    "    maes, rmses = [], []\n",
    "    n_obs = 0\n",
    "\n",
    "    for train_idx, val_idx in splits:\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Keep only this brand in train/val\n",
    "        mask_tr = (X_tr[brand_col] == brand)\n",
    "        mask_val = (X_val[brand_col] == brand)\n",
    "\n",
    "        if mask_val.sum() == 0:\n",
    "            # No validation examples of this brand in this fold\n",
    "            continue\n",
    "        if mask_tr.sum() < min_train_per_fold:\n",
    "            # Too few training examples for a stable brand-specific fit\n",
    "            continue\n",
    "\n",
    "        X_tr_b, y_tr_b = X_tr[mask_tr], y_tr[mask_tr]\n",
    "        X_val_b, y_val_b = X_val[mask_val], y_val[mask_val]\n",
    "\n",
    "        # Do NOT refit the preprocessor; just transform with the fitted one\n",
    "        X_tr_b_proc = preproc.transform(X_tr_b)\n",
    "        X_val_b_proc = preproc.transform(X_val_b)\n",
    "\n",
    "        # Fresh regressor for this fold\n",
    "        reg = clone(base_reg)\n",
    "        reg.fit(X_tr_b_proc, y_tr_b)\n",
    "\n",
    "        y_pred_b = reg.predict(X_val_b_proc)\n",
    "\n",
    "        mae = mean_absolute_error(y_val_b, y_pred_b)\n",
    "        mse = mean_squared_error(y_val_b, y_pred_b)\n",
    "        rmse = float(np.sqrt(mse))\n",
    "\n",
    "        maes.append(mae)\n",
    "        rmses.append(rmse)\n",
    "        n_obs += len(y_val_b)\n",
    "\n",
    "    return {\n",
    "        \"MAE_mean\": float(np.mean(maes)) if maes else np.nan,\n",
    "        \"MAE_std\":  float(np.std(maes))  if maes else np.nan,\n",
    "        \"RMSE_mean\": float(np.mean(rmses)) if rmses else np.nan,\n",
    "        \"RMSE_std\":  float(np.std(rmses))  if rmses else np.nan,\n",
    "        \"n\": int(n_obs),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "68fac82c-6faf-4fcd-9a77-853202ce992e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate both models for each candidate brand\n",
    "\n",
    "pipe_global = hgb_final_shap_pipe \n",
    "\n",
    "global_results = []\n",
    "brand_specific_results = []\n",
    "\n",
    "for brand in candidate_brands:\n",
    "    print(\"Evaluating brand:\", brand)\n",
    "\n",
    "    # 1) Global: train on all brands, measure only this brand in validation\n",
    "    res_g = eval_global_for_brand(\n",
    "        pipe_global,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        brand_col=brand_col,\n",
    "        brand=brand,\n",
    "        splits=splits,\n",
    "    )\n",
    "    res_g.update({\n",
    "        \"brand\": brand,\n",
    "        \"model_type\": \"global\",\n",
    "    })\n",
    "    global_results.append(res_g)\n",
    "\n",
    "    # 2) Brand-specific: preproc fixed, regressor trained only on this brand\n",
    "    res_b = eval_brand_specific(\n",
    "        pipe_global,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        brand_col=brand_col,\n",
    "        brand=brand,\n",
    "        splits=splits,\n",
    "    )\n",
    "    res_b.update({\n",
    "        \"brand\": brand,\n",
    "        \"model_type\": \"brand_specific\",\n",
    "    })\n",
    "    brand_specific_results.append(res_b)\n",
    "\n",
    "# Collect results into DataFrames\n",
    "df_global = pd.DataFrame(global_results)\n",
    "df_brand = pd.DataFrame(brand_specific_results)\n",
    "\n",
    "print(\"\\nGlobal model results per brand:\")\n",
    "display(df_global)\n",
    "\n",
    "print(\"\\nBrand-specific model results per brand:\")\n",
    "display(df_brand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3ae57229-677c-4848-a3e2-55f45bf905ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean results and compute performance differences\n",
    "\n",
    "# Drop any brands where evaluation failed (NaNs)\n",
    "df_global = df_global.dropna(subset=[\"MAE_mean\", \"RMSE_mean\"])\n",
    "df_brand  = df_brand.dropna(subset=[\"MAE_mean\", \"RMSE_mean\"])\n",
    "\n",
    "# Merge global vs brand-specific results\n",
    "df_compare = df_global.merge(\n",
    "    df_brand,\n",
    "    on=\"brand\",\n",
    "    suffixes=(\"_global\", \"_brand\"),\n",
    ")\n",
    "\n",
    "# Compute deltas:\n",
    "#   delta_MAE  < 0  -> brand-specific has lower MAE (better)\n",
    "#   delta_RMSE < 0  -> brand-specific has lower RMSE (better)\n",
    "df_compare[\"delta_MAE\"]  = df_compare[\"MAE_mean_brand\"]  - df_compare[\"MAE_mean_global\"]\n",
    "df_compare[\"delta_RMSE\"] = df_compare[\"RMSE_mean_brand\"] - df_compare[\"RMSE_mean_global\"]\n",
    "\n",
    "# Sort by delta_MAE (most improvement first)\n",
    "df_compare_sorted = df_compare.sort_values(\"delta_MAE\")\n",
    "\n",
    "print(\"Per-brand comparison (head):\")\n",
    "display(df_compare_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1ffeb757-0371-4df0-81dd-981148360a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualizations: bar plots for MAE and ΔMAE\n",
    "\n",
    "# Global vs Brand-specific MAE per brand\n",
    "plt.figure(figsize=(8, 4))\n",
    "x = np.arange(len(df_compare_sorted))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(\n",
    "    x - width / 2,\n",
    "    df_compare_sorted[\"MAE_mean_global\"],\n",
    "    width,\n",
    "    label=\"Global model\",\n",
    ")\n",
    "plt.bar(\n",
    "    x + width / 2,\n",
    "    df_compare_sorted[\"MAE_mean_brand\"],\n",
    "    width,\n",
    "    label=\"Brand-specific model\",\n",
    ")\n",
    "\n",
    "plt.xticks(x, df_compare_sorted[\"brand\"], rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"MAE (GBP)\")\n",
    "plt.title(\"Global vs Brand-specific models (MAE per brand)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 1ΔMAE per brand (negative = improvement with specialization)\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.bar(df_compare_sorted[\"brand\"], df_compare_sorted[\"delta_MAE\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Δ MAE (brand - global)\")\n",
    "plt.title(\"Effect of model specialization per brand\\n(negative = brand-specific MAE is lower)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1614895-99f2-43b7-b656-c540e7eeda77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Brand-Model Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "403e36df-4193-4b30-be8f-d6eb93b1889a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation helpers for brand–model segments\n",
    "\n",
    "def eval_global_for_brand_model(model, X, y, brand_col, model_col,\n",
    "                                brand, model_name, splits):\n",
    "    \"\"\"\n",
    "    Evaluate the global pipeline for a specific (brand, model) pair.\n",
    "\n",
    "    In each fold:\n",
    "      - Train on all cars.\n",
    "      - Compute MAE / RMSE only on validation rows where\n",
    "        Brand == brand AND model == model_name.\n",
    "    \"\"\"\n",
    "    maes, rmses = [], []\n",
    "    n_obs = 0\n",
    "\n",
    "    for train_idx, val_idx in splits:\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Train global model on ALL brands and models\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Restrict to this brand–model in validation\n",
    "        mask_seg = (\n",
    "            (X_val[brand_col] == brand) &\n",
    "            (X_val[model_col] == model_name)\n",
    "        )\n",
    "        if mask_seg.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        y_val_seg = y_val[mask_seg]\n",
    "        y_pred_seg = y_pred[mask_seg]\n",
    "\n",
    "        mae = mean_absolute_error(y_val_seg, y_pred_seg)\n",
    "        mse = mean_squared_error(y_val_seg, y_pred_seg)\n",
    "        rmse = float(np.sqrt(mse))\n",
    "\n",
    "        maes.append(mae)\n",
    "        rmses.append(rmse)\n",
    "        n_obs += mask_seg.sum()\n",
    "\n",
    "    return {\n",
    "        \"MAE_mean\": float(np.mean(maes)),\n",
    "        \"MAE_std\":  float(np.std(maes)),\n",
    "        \"RMSE_mean\": float(np.mean(rmses)),\n",
    "        \"RMSE_std\":  float(np.std(rmses)),\n",
    "        \"n\": int(n_obs),\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_brand_model_specific(pipe_global, X, y, brand_col, model_col,\n",
    "                              brand, model_name, splits,\n",
    "                              min_train_per_fold=40):\n",
    "    \"\"\"\n",
    "    Evaluate a brand–model-specific regressor.\n",
    "\n",
    "    Preprocessing + SHAP selection stay fixed (from pipe_global).\n",
    "    In each fold:\n",
    "      - Keep only rows with this (brand, model).\n",
    "      - Transform them with the fixed preprocessor.\n",
    "      - Fit a fresh regressor only on this segment.\n",
    "      - Evaluate on validation rows of the same segment.\n",
    "    \"\"\"\n",
    "    preproc = pipe_global[:-1]\n",
    "    base_reg = pipe_global[-1]\n",
    "\n",
    "    maes, rmses = [], []\n",
    "    n_obs = 0\n",
    "\n",
    "    for train_idx, val_idx in splits:\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Restrict to this brand–model in train/val\n",
    "        mask_tr = (\n",
    "            (X_tr[brand_col] == brand) &\n",
    "            (X_tr[model_col] == model_name)\n",
    "        )\n",
    "        mask_val = (\n",
    "            (X_val[brand_col] == brand) &\n",
    "            (X_val[model_col] == model_name)\n",
    "        )\n",
    "\n",
    "        if mask_val.sum() == 0:\n",
    "            continue\n",
    "        if mask_tr.sum() < min_train_per_fold:\n",
    "            continue\n",
    "\n",
    "        X_tr_seg, y_tr_seg = X_tr[mask_tr], y_tr[mask_tr]\n",
    "        X_val_seg, y_val_seg = X_val[mask_val], y_val[mask_val]\n",
    "\n",
    "        # Transform with fixed preprocessor\n",
    "        X_tr_seg_proc = preproc.transform(X_tr_seg)\n",
    "        X_val_seg_proc = preproc.transform(X_val_seg)\n",
    "\n",
    "        # Fresh regressor for this fold\n",
    "        reg = clone(base_reg)\n",
    "        reg.fit(X_tr_seg_proc, y_tr_seg)\n",
    "        y_pred_seg = reg.predict(X_val_seg_proc)\n",
    "\n",
    "        mae = mean_absolute_error(y_val_seg, y_pred_seg)\n",
    "        mse = mean_squared_error(y_val_seg, y_pred_seg)\n",
    "        rmse = float(np.sqrt(mse))\n",
    "\n",
    "        maes.append(mae)\n",
    "        rmses.append(rmse)\n",
    "        n_obs += len(y_val_seg)\n",
    "\n",
    "    return {\n",
    "        \"MAE_mean\": float(np.mean(maes)) if maes else np.nan,\n",
    "        \"MAE_std\":  float(np.std(maes))  if maes else np.nan,\n",
    "        \"RMSE_mean\": float(np.mean(rmses)) if rmses else np.nan,\n",
    "        \"RMSE_std\":  float(np.std(rmses))  if rmses else np.nan,\n",
    "        \"n\": int(n_obs),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fb73408c-9253-4166-ba56-49eb7d898d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run evaluation for each (brand, model) pair\n",
    "\n",
    "bm_global_results = []\n",
    "bm_specific_results = []\n",
    "\n",
    "for (brand, model_name), cnt in candidate_pairs.items():\n",
    "    print(f\"Evaluating pair: {brand} / {model_name} (n={cnt})\")\n",
    "\n",
    "    # Global model on this brand–model segment\n",
    "    res_g = eval_global_for_brand_model(\n",
    "        pipe_global,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        brand_col=brand_col,\n",
    "        model_col=model_col,\n",
    "        brand=brand,\n",
    "        model_name=model_name,\n",
    "        splits=splits,\n",
    "    )\n",
    "    res_g.update({\n",
    "        \"brand\": brand,\n",
    "        \"model\": model_name,\n",
    "        \"segment_type\": \"global\",\n",
    "    })\n",
    "    bm_global_results.append(res_g)\n",
    "\n",
    "    # Brand–model-specific regressor\n",
    "    res_bm = eval_brand_model_specific(\n",
    "        pipe_global,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        brand_col=brand_col,\n",
    "        model_col=model_col,\n",
    "        brand=brand,\n",
    "        model_name=model_name,\n",
    "        splits=splits,\n",
    "    )\n",
    "    res_bm.update({\n",
    "        \"brand\": brand,\n",
    "        \"model\": model_name,\n",
    "        \"segment_type\": \"brand_model_specific\",\n",
    "    })\n",
    "    bm_specific_results.append(res_bm)\n",
    "\n",
    "df_bm_global = pd.DataFrame(bm_global_results)\n",
    "df_bm_spec   = pd.DataFrame(bm_specific_results)\n",
    "\n",
    "print(\"\\nGlobal results per (brand, model):\")\n",
    "display(df_bm_global)\n",
    "\n",
    "print(\"\\nBrand–model-specific results:\")\n",
    "display(df_bm_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b457ac57-92eb-4403-94c7-b8ef15bafc87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare global vs brand–model-specific performance\n",
    "\n",
    "# Drop failed / NaN segments\n",
    "df_bm_global = df_bm_global.dropna(subset=[\"MAE_mean\", \"RMSE_mean\"])\n",
    "df_bm_spec   = df_bm_spec.dropna(subset=[\"MAE_mean\", \"RMSE_mean\"])\n",
    "\n",
    "df_bm_compare = df_bm_global.merge(\n",
    "    df_bm_spec,\n",
    "    on=[\"brand\", \"model\"],\n",
    "    suffixes=(\"_global\", \"_bm\"),\n",
    ")\n",
    "\n",
    "df_bm_compare[\"delta_MAE\"]  = df_bm_compare[\"MAE_mean_bm\"]  - df_bm_compare[\"MAE_mean_global\"]\n",
    "df_bm_compare[\"delta_RMSE\"] = df_bm_compare[\"RMSE_mean_bm\"] - df_bm_compare[\"RMSE_mean_global\"]\n",
    "\n",
    "df_bm_sorted = df_bm_compare.sort_values(\"delta_MAE\")\n",
    "\n",
    "print(\"Brand–model comparison (most improvement first):\")\n",
    "display(df_bm_sorted)\n",
    "\n",
    "# Optional readable table\n",
    "bm_display_cols = [\n",
    "    \"brand\", \"model\",\n",
    "    \"MAE_mean_global\", \"MAE_mean_bm\", \"delta_MAE\",\n",
    "    \"RMSE_mean_global\", \"RMSE_mean_bm\", \"delta_RMSE\",\n",
    "    \"n_global\",\n",
    "]\n",
    "df_bm_display = (\n",
    "    df_bm_sorted[bm_display_cols]\n",
    "    .copy()\n",
    "    .rename(columns={\"n_global\": \"n_samples\"})\n",
    ")\n",
    "\n",
    "for c in df_bm_display.columns:\n",
    "    if \"MAE\" in c or \"RMSE\" in c or \"delta\" in c:\n",
    "        df_bm_display[c] = df_bm_display[c].round(1)\n",
    "\n",
    "print(\"\\nReadable brand–model summary:\")\n",
    "display(df_bm_display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "24c38ee2-baf5-4ee9-a269-cfc88390fccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extra plots for brand–model specialization\n",
    "\n",
    "# Focus on segments with at least 100 samples for more stable numbers\n",
    "df_bm_plot = df_bm_display[df_bm_display[\"n_samples\"] >= 100]\n",
    "\n",
    "# Sort by delta_MAE (most improvement first)\n",
    "df_bm_plot = df_bm_plot.sort_values(\"delta_MAE\")\n",
    "\n",
    "# 1) Bar plot of ΔMAE for brand–model segments (filtered)\n",
    "plt.figure(figsize=(10, 4))\n",
    "x = np.arange(len(df_bm_plot))\n",
    "plt.bar(x, df_bm_plot[\"delta_MAE\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xticks(x, [f\"{b} {m}\" for b, m in zip(df_bm_plot[\"brand\"], df_bm_plot[\"model\"])],\n",
    "           rotation=90, ha=\"right\")\n",
    "plt.ylabel(\"Δ MAE (brand–model - global)\")\n",
    "plt.title(\"Effect of brand–model specialization\\n(negative = lower MAE than global)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) Scatter plot: n_samples vs ΔMAE to visualise overfitting at low sample sizes\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(df_bm_display[\"n_samples\"], df_bm_display[\"delta_MAE\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xlabel(\"Number of samples per (brand, model)\")\n",
    "plt.ylabel(\"Δ MAE (brand–model - global)\")\n",
    "plt.title(\"ΔMAE vs segment size\\n(negative = brand–model-specific is better)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0cbdcf-fa28-4bf8-b1d6-cdc2ac295387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9. Ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04b1798-3f92-4262-a25c-7b5590f26526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO try different transformations (box-cox, yeo-johnson)\n",
    "# TODO use i.e. 3 parallel exact same pipelines with different scalers to see the difference (input from lab)\n",
    "# TODO use different encoding ohe, target, label, frequency (frequency encoder is ricardos favorite encoder)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "group05_main_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
