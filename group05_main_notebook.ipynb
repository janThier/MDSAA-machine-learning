{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4467ab3-7594-4e02-a19f-8c5e47fb14e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"\n",
    "    background: rgba(25, 25, 25, 0.55);\n",
    "    backdrop-filter: blur(16px) saturate(150%);\n",
    "    -webkit-backdrop-filter: blur(16px) saturate(150%);\n",
    "    border: 1px solid rgba(255, 255, 255, 0.12);\n",
    "    border-radius: 18px;\n",
    "    padding: 45px 30px;\n",
    "    text-align: center;\n",
    "    font-family: 'Inter', 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;\n",
    "    color: #e0e0e0;\n",
    "    box-shadow: 0 0 30px rgba(0, 0, 0, 0.35);\n",
    "    margin: 40px auto;\n",
    "    max-width: 800px;\n",
    "\">\n",
    "\n",
    "  <h1 style=\"\n",
    "      font-size: 2.8em;\n",
    "      font-weight: 700;\n",
    "      margin: 0 0 8px 0;\n",
    "      letter-spacing: -0.02em;\n",
    "      background: linear-gradient(90deg, #00e0ff, #9c7eff);\n",
    "      -webkit-background-clip: text;\n",
    "      -webkit-text-fill-color: transparent;\n",
    "  \">\n",
    "      Machine Learning Project\n",
    "  </h1>\n",
    "\n",
    "  <h2 style=\"\n",
    "      font-size: 1.6em;\n",
    "      font-weight: 500;\n",
    "      margin: 0 0 25px 0;\n",
    "      color: #b0b0b0;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Cars 4 You - Predicting Car Prices\n",
    "  </h2>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.25em;\n",
    "      font-weight: 500;\n",
    "      color: #c0c0c0;\n",
    "      margin-bottom: 6px;\n",
    "  \">\n",
    "      Group 5 - Lukas Belser, Samuel Braun, Elias Karle, Jan Thier\n",
    "  </p>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.05em;\n",
    "      font-weight: 400;\n",
    "      color: #8a8a8a;\n",
    "      font-style: italic;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Machine Learning Midterm Delivery · 03.11.2025\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372c87aa-b7f3-4a34-a596-bb5e584db114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Table of Contents**\n",
    " \n",
    "- [1. Import Packages and Data](#1-import-packages-and-data)  \n",
    "  - [1.1 Import Required Packages](#11-import-required-packages)  \n",
    "  - [1.2 Load Datasets](#12-load-datasets)  \n",
    "  - [1.3 Kaggle Setup](#13-kaggle-setup)  \n",
    "- [2. Data Cleaning, Feature Engineering, Split & Preprocessing](#2-data-cleaning-feature-engineering-split--preprocessing)  \n",
    "  - [2.1 Data Cleaning](#21-data-cleaning)  \n",
    "  - [2.2 Feature Engineering](#22-feature-engineering)  \n",
    "  - [2.3 Data Split](#23-data-split)  \n",
    "  - [2.4 Preprocessing](#24-preprocessing)  \n",
    "- [3. Feature Selection](#3-feature-selection)  \n",
    "- [4. Model Evaluation Metrics, Baselining, Setup](#4-model-evaluation-metrics-baselining-setup)  \n",
    "- [5. Hyperparameter Tuning and Model Evaluation](#5-hyperparameter-tuning-and-model-evaluation)  \n",
    "  - [5.1 ElasticNet](#51-elasticnet)  \n",
    "  - [5.2 HistGradientBoost](#52-histgradientboost)  \n",
    "  - [5.3 RandomForest](#53-randomforest)  \n",
    "  - [5.4 ExtraTrees](#54-extratrees)  \n",
    "- [6. Feature Importance of Tree Models (with SHAP)](#6-feature-importance-of-tree-models-with-shap)  \n",
    "  - [6.1 HGB](#61-hgb)  \n",
    "  - [6.2 RF](#62-rf)  \n",
    "- [7. Kaggle Competition](#7-kaggle-competition)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fff3e4-624f-4b1f-b258-fbbe7b69cb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Import Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e6aadb-727a-4399-8888-9d6d4e593898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aa35af5d-8257-4fb6-a380-86bbbfac3244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f02b1610-84c8-46a7-8c37-980904a0cef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "008f906c-e207-4658-8e03-59bd25b8e2d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f54ab11e-fcfc-4278-8997-89a92adf8eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import and load Data"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, chi2, RFE\n",
    "from scipy.stats import spearmanr, uniform, loguniform, randint\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    " \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    " \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    " \n",
    "from data_cleaning import clean_car_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9737aef-d2bd-4748-89ed-9b0e2cea8637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.2 Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3da1ad2b-8ee0-48b0-917d-545c70633ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cars_train = pd.read_csv(\"train.csv\")\n",
    "df_cars_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab6500b5-94a1-4a0e-b7f6-ea9d35c787eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Kaggle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "556ffc24-70f7-48c9-96a4-3a19380f91db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Connect"
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle API Connect => everyone has to do this himself, with his own kaggle.json api token\n",
    "\n",
    "# Folder containing kaggle.json\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/Workspace/Users/20250355@novaims.unl.pt\"\n",
    "\n",
    "# Test\n",
    "!echo $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75ee214-0996-406f-b77f-9e72fa7153e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data Cleaning, Feature Engineering, Split & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78e7ca8-ba25-4ea3-873d-4f3515281440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task II (5 Points): Clean and preprocess the dataset \n",
    "- Missing Value handling, Outlier preprocessing + justify decisions -> in data_cleaning.py\n",
    "- Review current features and create extra features if needed + explain -> in Feature Engineering\n",
    "- Deal with categorical variables -> in One-Hot-Encoding \n",
    "- Perform data scaling, explain reasoning -> in Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53a2a97-4662-40a3-8b24-b238dad5138e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6fa0cfe9-be7e-4e90-ac1c-cd600bb7d13f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Numerical / Categorical Data Cleaning: with Pandas"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier Preprocessing, Missing Value Handling and Decision justifying happens here\n",
    "df_cars_train = clean_car_dataframe(df_cars_train)\n",
    "df_cars_test = clean_car_dataframe(df_cars_test)\n",
    "\n",
    "\n",
    "# Safety Check: print unique values of all columns of df_cars_train // df_cars_test to see if data cleaning worked and if there are still odd values\n",
    "for col in df_cars_train.columns:\n",
    "    print(col, df_cars_train[col].unique())\n",
    "print(\"X\"*150)\n",
    "print(\"-\"*150)\n",
    "print(\"X\"*150)\n",
    "for col in df_cars_test.columns:\n",
    "    print(col, df_cars_test[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b198720e-b5a0-46ef-8055-4ac33b852410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Feature Creation**\n",
    "\n",
    "These are foundational features derived directly from the original data, often to create linear relationships or capture interactions.\n",
    "- `age`: Calculated as `2020 - year`. Creates a simple linear feature representing the car's age. Newer cars (lower age) generally have higher prices.\n",
    "- `miles_per_year`: Calculated as `mileage / age`. This normalizes the car's usage, preventing high correlation (multicollinearity) between `mileage` and `age`. A 3-year-old car with 60,000 miles is different from a 6-year-old car with 60,000 miles.\n",
    "- `age_x_engine`: An interaction term `age * engineSize`. This helps the model capture non-linear relationships, such as the possibility that the value of cars with large engines might depreciate faster (or slower) than cars with small engines.\n",
    "- `mpg_x_engine`: An interaction term `mpg * engineSize`. This captures the combined effect of fuel efficiency and engine power.\n",
    "- `tax_per_engine`: Calculated as `tax / engineSize`. This feature represents the tax cost relative to the engine's power, which could be an indicator of overall running costs or vehicle class.\n",
    "- `mpg_per_engine`: Calculated as `mpg / engineSize`. This creates an \"efficiency\" metric, representing how many miles per gallon the car achieves for each unit of engine size.\n",
    "\n",
    "\n",
    "**Popularity & Demand Features**\n",
    "\n",
    "These features attempt to quantify a car's popularity or market demand, which directly influences price.\n",
    "- `model_freq`: Calculates the frequency (percentage) of each `model` in the training dataset. Popular, common models often have more stable and predictable pricing and demand.\n",
    "\n",
    "\n",
    "**Price Anchor Features**\n",
    "\n",
    "These features \"anchor\" a car's price relative to its group. They provide a strong baseline price signal based on brand, model, and configuration.\n",
    "- `brand_med_price`: The median price for the car's `Brand` (e.g., the typical price for a BMW vs. a Skoda). This captures overall brand positioning.\n",
    "- `model_med_price`: The median price for the car's `model` (e.g., the typical price for a 3-Series vs. a 1-Series). This captures the model's positioning within the brand.\n",
    "- `brand_fuel_med_price`: The median price for the car's specific `Brand` and `fuelType` combination (e.g., a Diesel BMW vs. a Petrol BMW).\n",
    "- `brand_trans_med_price`: The median price for the `Brand` and `transmission` combination (e.g., an Automatic BMW vs. a Manual BMW).\n",
    "\n",
    "\n",
    "**Normalized & Relative Features**\n",
    "\n",
    "These features compare a car to its peers rather than using absolute values.\n",
    "- `*_anchor` (e.g., `brand_med_price_anchor`): Created by dividing the median price features (from section 3) by the `overall_mean_price`. This makes the feature dimensionless and represents the group's price *relative* to the entire market (e.g., \"this brand is 1.5x the market average\").\n",
    "- `age_rel_brand`: Calculated as `age - brand_median_age`. This shows if a car is newer or older than the *typical* car for that specific brand, capturing relative age within its own group.\n",
    "\n",
    "\n",
    "**CV-Safe Target Encodings**\n",
    "\n",
    "This is an advanced technique to encode categorical variables (like `model` or `Brand`) using information from the target variable (`price`) without causing data leakage.\n",
    "- `*_te` (e.g., `model_te`): Represents the *average price* for that category (e.g., the average price for a \"Fiesta\").\n",
    "- **Why is it \"CV-Safe\"?** Instead of just calculating the global average price for \"Fiesta\" and applying it to all rows (which leaks target information), this method uses K-Fold cross-validation. For each fold of the data, the target encoding is calculated *only* from the *other* folds. This ensures the encoding for any given row never includes its own price, preventing leakage and leading to a more robust model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "83812e71-e13e-43d4-883b-41ffd3b3466b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Base Feature Creation\n",
    "\n",
    "# Car Age: Newer cars usually have higher prices, models prefer linear features\n",
    "df_cars_train['age'] = 2020 - df_cars_train['year']\n",
    "df_cars_test['age']  = 2020 - df_cars_test['year']\n",
    "\n",
    "# Miles per Year: Normalizes mileage by age (solves multicollinearity between year and mileage)\n",
    "df_cars_train['miles_per_year'] = df_cars_train['mileage'] / df_cars_train['age'].replace({0: np.nan})\n",
    "df_cars_train['miles_per_year'] = df_cars_train['miles_per_year'].fillna(df_cars_train['mileage'])\n",
    "\n",
    "df_cars_test['miles_per_year'] = df_cars_test['mileage'] / df_cars_test['age'].replace({0: np.nan})\n",
    "df_cars_test['miles_per_year'] = df_cars_test['miles_per_year'].fillna(df_cars_test['mileage'])\n",
    "\n",
    "# Interaction Terms: Capture non-linear effects between engine and other numeric features\n",
    "df_cars_train['age_x_engine'] = df_cars_train['age'] * df_cars_train['engineSize'].fillna(0)\n",
    "df_cars_test['age_x_engine']  = df_cars_test['age']  * df_cars_test['engineSize'].fillna(0)\n",
    "\n",
    "df_cars_train['mpg_x_engine'] = df_cars_train['mpg'].fillna(0) * df_cars_train['engineSize'].fillna(0)\n",
    "df_cars_test['mpg_x_engine']  = df_cars_test['mpg'].fillna(0)  * df_cars_test['engineSize'].fillna(0)\n",
    "\n",
    "# tax per engine\n",
    "df_cars_train['tax_per_engine'] = df_cars_train['tax'] / df_cars_train['engineSize'].replace({0: np.nan})\n",
    "df_cars_train['tax_per_engine'] = df_cars_train['tax_per_engine'].fillna(df_cars_train['tax'])\n",
    "\n",
    "df_cars_test['tax_per_engine'] = df_cars_test['tax'] / df_cars_test['engineSize'].replace({0: np.nan})\n",
    "df_cars_test['tax_per_engine'] = df_cars_test['tax_per_engine'].fillna(df_cars_test['tax'])\n",
    "\n",
    "# MPG per engineSize to represent the efficiency\n",
    "df_cars_train['mpg_per_engine'] = df_cars_train['mpg'] / df_cars_train['engineSize'].replace({0: np.nan})\n",
    "df_cars_train['mpg_per_engine'] = df_cars_train['mpg_per_engine'].fillna(df_cars_train['mpg'])\n",
    "\n",
    "df_cars_test['mpg_per_engine'] = df_cars_test['mpg'] / df_cars_test['engineSize'].replace({0: np.nan})\n",
    "df_cars_test['mpg_per_engine'] = df_cars_test['mpg_per_engine'].fillna(df_cars_test['mpg'])\n",
    "\n",
    "\n",
    "# 2. Model Frequency: Popular models tend to have stable demand and prices\n",
    "model_freq = df_cars_train['model'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "df_cars_train['model_freq'] = df_cars_train['model'].map(model_freq).fillna(0.0)\n",
    "df_cars_test['model_freq']  = df_cars_test['model'].map(model_freq).fillna(0.0)\n",
    "\n",
    "\n",
    "# 3. Brand and Model Anchors: Represent typical price levels (positioning)\n",
    "overall_mean_price = df_cars_train['price'].mean()\n",
    "\n",
    "# Brand median price: captures brand positioning (e.g., BMW > Skoda)\n",
    "brand_median_price = df_cars_train.groupby('Brand')['price'].median().to_dict()\n",
    "df_cars_train['brand_med_price'] = df_cars_train['Brand'].map(brand_median_price)\n",
    "df_cars_test['brand_med_price']  = df_cars_test['Brand'].map(brand_median_price)\n",
    "\n",
    "# Model median price: captures model hierarchy within brand (e.g., 3er > 1er)\n",
    "model_median_price = df_cars_train.groupby('model')['price'].median().to_dict()\n",
    "df_cars_train['model_med_price'] = df_cars_train['model'].map(model_median_price)\n",
    "df_cars_test['model_med_price']  = df_cars_test['model'].map(model_median_price)\n",
    "\n",
    "# Brand × Fuel median price: different fuels have different price segments\n",
    "brand_fuel_median_price = df_cars_train.groupby(['Brand','fuelType'])['price'].median().to_dict()\n",
    "df_cars_train['brand_fuel_med_price'] = list(zip(df_cars_train['Brand'], df_cars_train['fuelType']))\n",
    "df_cars_train['brand_fuel_med_price'] = df_cars_train['brand_fuel_med_price'].map(brand_fuel_median_price)\n",
    "df_cars_test['brand_fuel_med_price']  = list(zip(df_cars_test['Brand'], df_cars_test['fuelType']))\n",
    "df_cars_test['brand_fuel_med_price']  = df_cars_test['brand_fuel_med_price'].map(brand_fuel_median_price)\n",
    "\n",
    "# Brand × Transmission median price: automatic or manual may influence resale\n",
    "brand_trans_median_price = df_cars_train.groupby(['Brand','transmission'])['price'].median().to_dict()\n",
    "df_cars_train['brand_trans_med_price'] = list(zip(df_cars_train['Brand'], df_cars_train['transmission']))\n",
    "df_cars_train['brand_trans_med_price'] = df_cars_train['brand_trans_med_price'].map(brand_trans_median_price)\n",
    "df_cars_test['brand_trans_med_price']  = list(zip(df_cars_test['Brand'], df_cars_test['transmission']))\n",
    "df_cars_test['brand_trans_med_price']  = df_cars_test['brand_trans_med_price'].map(brand_trans_median_price)\n",
    "\n",
    "\n",
    "# 4. Normalized Anchors (dimensionless): relative position vs overall mean\n",
    "for col in ['brand_med_price','model_med_price','brand_fuel_med_price','brand_trans_med_price']:\n",
    "    df_cars_train[f'{col}_anchor'] = df_cars_train[col] / overall_mean_price\n",
    "    df_cars_test[f'{col}_anchor']  = df_cars_test[col]  / overall_mean_price\n",
    "\n",
    "\n",
    "# 5. Relative Age (within brand): newer/older than brand median year\n",
    "brand_median_age = df_cars_train.groupby('Brand')['age'].median().to_dict()\n",
    "\n",
    "df_cars_train['age_rel_brand'] = df_cars_train['age'] - df_cars_train['Brand'].map(brand_median_age)\n",
    "df_cars_test['age_rel_brand']  = df_cars_test['age']  - df_cars_test['Brand'].map(brand_median_age)\n",
    "\n",
    "\n",
    "# 6. CV-Safe Target Encodings (no leakage): smooth category means # TODO: check whether this mean-target-encoding is necessary after already using features such as brand_med_price_anchor which uses the median instead of the mean\n",
    "def m_estimate_mean(sum_, prior, count, m=50):\n",
    "    \"\"\"Posterior mean with M-estimate smoothing.\"\"\"\n",
    "    return (sum_ + m * prior) / (count + m)\n",
    "\n",
    "def cv_target_encode(train_df, test_df, col, ycol='price', m=50, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Leakage-safe KFold target encoding:\n",
    "    Each fold gets encodings computed only from the remaining training folds.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    global_mean = train_df[ycol].mean()\n",
    "    tr_encoded = pd.Series(index=train_df.index, dtype=float)\n",
    "\n",
    "    # Encode training data using CV folds\n",
    "    for tr_idx, val_idx in kf.split(train_df):\n",
    "        tr, val = train_df.iloc[tr_idx], train_df.iloc[val_idx]\n",
    "        stats = tr.groupby(col)[ycol].agg(['sum','count'])\n",
    "        stats['enc'] = m_estimate_mean(stats['sum'], global_mean * stats['count'], stats['count'], m=m)\n",
    "        tr_encoded.iloc[val_idx] = val[col].map(stats['enc']).fillna(global_mean)\n",
    "\n",
    "    # Encode test data using full training statistics\n",
    "    full_stats = train_df.groupby(col)[ycol].agg(['sum','count'])\n",
    "    full_stats['enc'] = m_estimate_mean(full_stats['sum'], global_mean * full_stats['count'], full_stats['count'], m=m)\n",
    "    te_map = full_stats['enc'].to_dict()\n",
    "\n",
    "    te_train = tr_encoded.fillna(global_mean).astype('float32')\n",
    "    te_test  = test_df[col].map(te_map).fillna(global_mean).astype('float32')\n",
    "\n",
    "    return te_train, te_test\n",
    "\n",
    "# Apply target encoding to categorical columns\n",
    "for col, m in [('model', 100), ('Brand', 30), ('fuelType', 20), ('transmission', 20)]:\n",
    "    tr_enc, te_enc = cv_target_encode(df_cars_train, df_cars_test, col, ycol='price', m=m)\n",
    "    df_cars_train[f'{col}_te'] = tr_enc\n",
    "    df_cars_test[f'{col}_te']  = te_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e72b5602-fc3d-4397-bb66-10e7b501438a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.3 Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dda482e9-4a68-438d-86c0-e0524a9b8692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split Data"
    }
   },
   "outputs": [],
   "source": [
    "# Split Data, Stratify not necessary due to regression problem and Cross Validation later\n",
    "X = df_cars_train.drop(columns='price')\n",
    "y = df_cars_train['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.2, \n",
    "                                                  random_state = 42, \n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9425a7c0-ec96-4458-a0da-8ef65acb3f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.4 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "39142374-d485-42e7-a6cb-83ecffae0091",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocessing: with sklearn Pipeline & Column Transformer"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing: with sklearn Pipeline & Column Transformer\n",
    "\n",
    "numeric_features = [\n",
    "    \"age\", \"age_rel_brand\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"model_freq\",\n",
    "    \"brand_med_price_anchor\", \"model_med_price_anchor\", \"brand_fuel_med_price_anchor\", \"brand_trans_med_price_anchor\",\n",
    "    \"age_x_engine\", \"mpg_x_engine\",\n",
    "    \"model_te\", \"Brand_te\", \"fuelType_te\", \"transmission_te\",\n",
    "    \"tax_per_engine\", \"mpg_per_engine\"\n",
    "]\n",
    "log_features = [\"mileage\", \"miles_per_year\"] # we will test other num columns here while going on, mileage is right-skewed\n",
    "categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "# left out columns: year (age is better), hasDamage (unsure what the two values 0 and NaN mean)\n",
    "\n",
    "def to_float_array(x):\n",
    "    \"\"\"Convert input to float array.\"\"\"\n",
    "    return np.array(x, dtype=float)\n",
    "\n",
    "log_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),  # Handling of missing numerical values with SimpleImputer (soon to be replaced by a custom GroupMedianImputer)\n",
    "    (\"to_float\", FunctionTransformer(to_float_array)),\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=False)), # log1p handles zeros safely\n",
    "    (\"scaler\", StandardScaler()) # Data Scaling with sklearn StandardScaler (we will test RobustScaler and also MinMaxScaler combined with neural networks going on)\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"to_float\", FunctionTransformer(to_float_array)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), # fill by mode instead of Unknown (a diesel 3er BMW is probably a diesel)\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)) # Deal with Categorical Variables with sklearn OneHotEncoder\n",
    "])\n",
    "\n",
    "# Apply the preprocessing steps to the data with ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"log\", log_transformer, log_features),\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "\n",
    "# Fit preprocessor on training data - avoid data leakage\n",
    "preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8f763f-298d-444a-80a9-0270ba842dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e9a293-7457-4767-8a26-7c930f7dbad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task III (3 Points): Define and Implement a clear and unambiguous strategy for feature selection. Use the methods **discussed in the course**. Present and justify your final selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d989a4b-077d-47bd-b4bb-c4528628888f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Model independent Filter Methods:\n",
    "- Remove constant numerical variables with VarianceThreshold\n",
    "- Check highly correlated numerical variables and keep one with Spearman\n",
    "- Remove unindependent categorical variables with Chi2\n",
    "\n",
    "Model dependent Wrapper Methods:\n",
    "- RFE LR / RFE SVR for linear Models: ElasticNet, SVM\n",
    "- Feature Importance for tree Models: RandomForest, HistGradientBoosting (see at 6 Feature Importance (with SHAP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "130e5929-dcd7-48db-8afa-8f9f560968fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Selection with Filter Methods"
    }
   },
   "outputs": [],
   "source": [
    "X_train_proc = preprocessor.transform(X_train)\n",
    "\n",
    "feature_names_all = []\n",
    "for name, trans, cols in preprocessor.transformers_:\n",
    "    if name != 'remainder':\n",
    "        if hasattr(trans, 'get_feature_names_out'):\n",
    "            # for categorical OHE\n",
    "            try:\n",
    "                feature_names_all.extend(trans.get_feature_names_out(cols))\n",
    "            except:\n",
    "                feature_names_all.extend(cols)\n",
    "        else:\n",
    "            feature_names_all.extend(cols)\n",
    "\n",
    "X_df = pd.DataFrame(X_train_proc, columns=feature_names_all)\n",
    "\n",
    "\n",
    "# Variance Threshold\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "vt.fit(X_df)\n",
    "vt_deselect = [f for f, keep in zip(feature_names_all, vt.get_support()) if not keep]\n",
    "print(\"Features to deselect according to VarianceThreshold:\", vt_deselect)\n",
    "\n",
    "\n",
    "# Spearman correlation (numeric + log only)\n",
    "numeric_log = numeric_features + log_features\n",
    "spearman_deselect = []\n",
    "for f in numeric_log:\n",
    "    if f in X_df.columns:\n",
    "        corr, _ = spearmanr(X_df[f], y_train)\n",
    "        if abs(corr) <= 0.05:\n",
    "            spearman_deselect.append(f)\n",
    "print(\"Features to deselect according to Spearman correlation:\", spearman_deselect)\n",
    "\n",
    "\n",
    "# Chi2 (categorical only, must be non-negative)\n",
    "cat_cols = [c for c in X_df.columns if c not in numeric_log]\n",
    "X_cat = X_df[cat_cols].astype(float)\n",
    "chi2_vals, _ = chi2(X_cat, y_train)\n",
    "chi2_deselect = [f for f, val in zip(cat_cols, chi2_vals) if val <= 0]\n",
    "print(\"Features to deselect according to Chi²:\", chi2_deselect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d62d3f6a-baa5-40e9-913a-b99f443b5080",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Deselection for linear Models"
    }
   },
   "outputs": [],
   "source": [
    "# Numeric/log features for linear models\n",
    "linear_numeric_features = [f for f in numeric_features + log_features if f not in spearman_deselect]\n",
    "\n",
    "preprocessor_linear = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, linear_numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# => use preprocessor_linear for linear model setup; since tree models are indifferent to irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0dc6124-f736-46fb-9f73-895b930faae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Model Evaluation Metrics, Baselining, Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06207ec2-af0d-489f-a23d-467019791499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TASK IV (4 Points): Build a simple model and assess the performance\n",
    "- Identify the type of problem and select the relevant algorithms\n",
    "- Select one Model Assessment Strategy to use throughout your work. Which metrics are you using to evaluate your model and why?\n",
    "\n",
    "\n",
    "=> Tip from lecturer: Use RandomSearch instead of GridSearchCV, set a wider Range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6297f034-638d-4db6-a681-597f20036432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definition of Function for Model Assessment"
    }
   },
   "outputs": [],
   "source": [
    "# Model evaluation metrics used throughout this analysis:\n",
    "#\n",
    "#   > MAE: Mean Absolute Error - average absolute deviation between predicted and true car prices\n",
    "#          Easy to interpret in pounds, same metric used by Kaggle competition\n",
    "#   > RMSE: Root Mean Squared Error - sensitive to outliers, helps identify large prediction errors  \n",
    "#   > R²: Coefficient of determination - proportion of variance explained by the model\n",
    "#         1.0 = perfect predictions, 0.0 = same as predicting mean, < 0.0 = worse than mean\n",
    "#\n",
    "# These metrics are appropriate for regression problems predicting continuous variables (car prices)\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred)) \n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"MAE: {mae:.4f} | RMSE: {rmse:.4f} | R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b26348e-a5ff-4dc9-9d61-7f59d0ea335d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Baselining mean/median"
    }
   },
   "outputs": [],
   "source": [
    "# Absolute basic baselining with the mean and median\n",
    "\n",
    "mean_pred = y_train.mean()\n",
    "median_pred = y_train.median()\n",
    "\n",
    "print(\"baseline mean predictor: \")\n",
    "print_metrics(y_val, [mean_pred]*len(y_val))\n",
    "# MAE: 6976.3626 | RMSE: 92839550.2849 | R2: -0.0000\n",
    "\n",
    "print(\"-\"*150)\n",
    "\n",
    "print(\"baseline median predictor: \") \n",
    "print_metrics(y_val, [median_pred]*len(y_val))\n",
    "# MAE: 6751.1604 | RMSE: 97557866.6363 | R2: -0.0508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "04d3aa5b-36ca-4dba-9592-24edd345897d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Models Setup (inkl. Prepro in Pipeline)"
    }
   },
   "outputs": [],
   "source": [
    "# Models Setup (inkl. Prepro in Pipeline)\n",
    "\n",
    "### LINEAR MODEL\n",
    "\n",
    "# ElasticNet\n",
    "elastic_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_linear),\n",
    "    (\"model\", ElasticNet(\n",
    "        alpha=0.01,            # mild regularization to stabilize if many features\n",
    "        l1_ratio=0.5,          # balanced L1/L2, can grid-search\n",
    "        max_iter=30000,        # allow more convergence iterations\n",
    "        tol=1e-4,              # stricter tolerance often improves accuracy\n",
    "        selection=\"cyclic\",    # usually converges faster than random\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### TREE MODELS\n",
    "\n",
    "# HistGradientBoostingRegressor: modern and very fast, handles missing values natively (no imputation needed!). often matches or beats XGBoost/LightGBM \n",
    "hgb_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5, # regularize slightly to prevent overfit, > 0.5 does not seem to work\n",
    "        random_state=42  \n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# RandomForestRegressor: excellent general baseline ensemble, handles non-linearities well, doesn’t overfit easily but can be slow for large data\n",
    "rf_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ExtraTreesRegressor: similar to RandomForest but with more randomization => often better generalization\n",
    "et_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=400,          \n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Hyperparameter Tuning and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffadcd30-bf06-4504-a3f5-a6c0e43dbd8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After a first runs for all of these models, we decided to focus on RandomForest and in particular on HistGradientBoost for the best prediction performance for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c880d2d-c7aa-478f-8d34-90aa6e33a172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.1 ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "18042056-7ef9-489f-afc3-958fe02fbfc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ElasticNet"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: ElasticNet\n",
    "\n",
    "elastic_param_grid = {\n",
    "    \"model__alpha\": [0.001],    #   tried 0.0001 to 0.5\n",
    "    \"model__l1_ratio\": [0.9]    #   tried 0.1 to 0.9  \n",
    "}\n",
    "\n",
    "elastic_grid = RandomizedSearchCV(\n",
    "    elastic_pipe,\n",
    "    param_distributions=elastic_param_grid,\n",
    "    n_iter=1,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "elastic_grid.fit(X_train, y_train)\n",
    "elastic_best = elastic_grid.best_estimator_\n",
    "elastic_val_pred = elastic_best.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"ElasticNet Results: \")\n",
    "print_metrics(y_val, elastic_val_pred)\n",
    "print(\"Best ElasticNet params:\", elastic_grid.best_params_)\n",
    "\n",
    "# ElasticNet Results: \n",
    "# MAE: 2375.8878 | RMSE: 3667.4444 | R2: 0.8523\n",
    "# Best ElasticNet params: {'model__l1_ratio': 0.9, 'model__alpha': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231a027a-dfc8-4e1e-834a-d77bf86eee91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ElasticNet + RFE (no CV)\n",
    "# reuse the tuned ElasticNet from the best pipeline\n",
    "en_base = clone(elastic_best.named_steps[\"model\"])\n",
    " \n",
    "# pick how many features to keep\n",
    "n_keep = 100\n",
    "\n",
    "rfe_pipe_linear = Pipeline([\n",
    "    (\"preprocess\", preprocessor_linear),\n",
    "    (\"rfe\", RFE(\n",
    "        estimator=en_base,\n",
    "        n_features_to_select=n_keep,\n",
    "        step=0.1,              # remove ~10% of features per iteration\n",
    "        importance_getter=\"auto\"\n",
    "    )),\n",
    "    (\"model\", clone(en_base))\n",
    "])\n",
    " \n",
    "# train / predict / evaluate\n",
    "rfe_pipe_linear.fit(X_train, y_train)\n",
    "val_pred_linear_rfe = rfe_pipe_linear.predict(X_val)\n",
    " \n",
    "print(\"ElasticNet with RFE (no CV):\")\n",
    "print_metrics(y_val, val_pred_linear_rfe)\n",
    " \n",
    "# show a quick look at which features survived\n",
    "rfe = rfe_pipe_linear.named_steps[\"rfe\"]\n",
    "try:\n",
    "    feat_names = rfe_pipe_linear.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "except Exception:\n",
    "    feat_names = (\n",
    "        linear_numeric_features +\n",
    "        list(rfe_pipe_linear.named_steps[\"preprocess\"]\n",
    "             .named_transformers_[\"cat\"].named_steps[\"encoder\"]\n",
    "             .get_feature_names_out(categorical_features))\n",
    "    )\n",
    " \n",
    "selected = [n for n, keep in zip(feat_names, rfe.support_) if keep]\n",
    "print(f\"Selected {len(selected)} features\")\n",
    "print(\"First 20:\", selected[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21989bd5-8ab1-4d43-a099-3b2f412c94e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reasoning**: We used 100 features as an initial, arbitrary cutoff for feature selection in the ElasticNet model. Preliminary experiments and insights from the EDA (see separate notebook) indicated that tree-based methods are likely to perform better. Therefore, we prioritized feature selection for the tree-based models based on SHAP values.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5a0287-45b6-4022-8d47-cde8cc52469e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.2 HistGradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a2d5effc-c1e9-4606-aa57-f0b5041d2a35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: HistGradientBoost"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to use it here and potentially use it later for a final hyperparameter tuning after feature selection again\n",
    "def hgb_hyperparameter_tuning(hgb_estimator, n_iter):\n",
    "    # hgb_param_dist_old = {\n",
    "    #     \"model__learning_rate\": uniform(0.01, 0.09),       # samples values\n",
    "    #     \"model__max_leaf_nodes\": randint(20, 120),         # tries between 20–120 leaves\n",
    "    #     \"model__min_samples_leaf\": randint(2, 20),         # samples leaf sizes between 2–20\n",
    "    #     \"model__max_iter\": randint(400, 1000),             # tries 400–1000 iterations\n",
    "    #     \"model__l2_regularization\": uniform(0.0, 1.0)      # samples small regularization values\n",
    "    # }\n",
    "\n",
    "    # optimized the parameter distributions based on previous runs to focus search space\n",
    "    hgb_param_dist = {\n",
    "        \"model__learning_rate\": [0.06389789198396824],\n",
    "        \"model__max_leaf_nodes\": [105],\n",
    "        \"model__min_samples_leaf\": [3],\n",
    "        \"model__max_iter\": [642],\n",
    "        \"model__l2_regularization\": [0.942853570557981],\n",
    "        \"model__early_stopping\": [True],\n",
    "        \"model__validation_fraction\": [0.1],\n",
    "        \"model__n_iter_no_change\": [20],\n",
    "        \"model__random_state\":[42]\n",
    "    }\n",
    "    \n",
    "\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=42)  # 3-fold for faster runtime\n",
    "\n",
    "    # Randomized search setup\n",
    "    hgb_random = RandomizedSearchCV(\n",
    "        estimator=hgb_estimator,\n",
    "        param_distributions=hgb_param_dist,\n",
    "        n_iter=n_iter,                         # number of random combinations to try\n",
    "        scoring=\"neg_mean_absolute_error\",     # optimize for MAE (primary metric)\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=3\n",
    "    )\n",
    "\n",
    "    # Fit the search\n",
    "    hgb_random.fit(X_train, y_train)\n",
    "    return hgb_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "998dab6d-7494-483c-ab9e-a075b4c75c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hgb_random = hgb_hyperparameter_tuning(hgb_pipe, n_iter=1) \n",
    "# Get best model\n",
    "hgb_best = hgb_random.best_estimator_\n",
    "print(\"Best Params:\", hgb_random.best_params_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "hgb_val_pred = hgb_random.best_estimator_.predict(X_val)\n",
    "print_metrics(y_val, hgb_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70de2673-0443-4cc4-b1cd-f3813a969246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.3 RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: RandomForest\n",
    "\n",
    "# Old parameter distribution\n",
    "# rf_param_dist_old = {\n",
    "#     \"model__n_estimators\": randint(200, 600),        # number of trees\n",
    "#     \"model__max_depth\": randint(5, 40),              # depth of each tree\n",
    "#     \"model__min_samples_split\": randint(2, 10),      # min samples to split an internal node\n",
    "#     \"model__min_samples_leaf\": randint(1, 8),        # min samples per leaf\n",
    "#     \"model__max_features\": [\"sqrt\", \"log2\"],         # feature sampling strategy\n",
    "#     \"model__bootstrap\": [True, False]                # use bootstrapping or not\n",
    "# }\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "rf_param_dist = {\n",
    "    \"model__n_estimators\": [467],        \n",
    "    \"model__max_depth\": [32],              \n",
    "    \"model__min_samples_split\": [9],      \n",
    "    \"model__min_samples_leaf\": [1],        \n",
    "    \"model__max_features\": [\"sqrt\"],         \n",
    "    \"model__bootstrap\": [False]                \n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Randomized search setup\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf_pipe,\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=1,                         # reduced number of random combinations to 1 because of fixed set after previous runs\n",
    "    scoring=\"neg_mean_absolute_error\", \n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the search\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "rf_best_rand = rf_random.best_estimator_\n",
    "print(\"Best Params:\", rf_random.best_params_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "rf_val_pred = rf_best_rand.predict(X_val)\n",
    "print_metrics(y_val, rf_val_pred)\n",
    "\n",
    "# Save best model\n",
    "# joblib.dump(rf_best_rand, \"rf_best.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6d3b92-efc6-4d8a-b3e1-cb6aff36b84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.4 ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6f4ff12e-07a4-47f5-8c08-43b5d8d86942",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ExtraTrees"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: ExtraTrees\n",
    "\n",
    "# Old parameter distribution\n",
    "# et_param_dist_old = {\n",
    "#     \"model__n_estimators\": randint(200, 600),\n",
    "#     \"model__max_depth\": randint(5, 40),\n",
    "#     \"model__min_samples_split\": randint(2, 10),\n",
    "#     \"model__min_samples_leaf\": randint(1, 8),\n",
    "#     \"model__max_features\": [\"sqrt\", \"log2\"],\n",
    "#     \"model__bootstrap\": [True, False]\n",
    "# }\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "et_param_dist = {\n",
    "    \"model__n_estimators\": [467],\n",
    "    \"model__max_depth\": [32],\n",
    "    \"model__min_samples_split\": [9],\n",
    "    \"model__min_samples_leaf\": [1],\n",
    "    \"model__max_features\": [\"sqrt\"],\n",
    "    \"model__bootstrap\": [False]\n",
    "}\n",
    "\n",
    "et_random = RandomizedSearchCV(\n",
    "    estimator=et_pipe,\n",
    "    param_distributions=et_param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "et_random.fit(X_train, y_train)\n",
    "\n",
    "et_best = et_random.best_estimator_\n",
    "print(\"ExtraTrees Best Params:\", et_random.best_params_)\n",
    "\n",
    "et_val_pred = et_best.predict(X_val)\n",
    "print_metrics(y_val, et_val_pred)\n",
    "\n",
    "# joblib.dump(et_best, \"et_best.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852bd7bc-33d9-4ae4-a552-0850b200be11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Feature Importance of Tree Models (with SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7c02463-def6-4c5c-93ce-5ca710b6eb7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  **Problem:** Current feature selection targets linear models\n",
    "  (ElasticNet), but we primarily use tree-based models (HGB,\n",
    "  RandomForest).\n",
    "\n",
    "  **Solution:** Use SHAP (SHapley Additive exPlanations) to\n",
    "  identify feature importance specifically for tree models\n",
    "\n",
    "  **Why SHAP for Trees:**\n",
    "  - Provides exact feature importance values for tree-based\n",
    "  models\n",
    "  - Tree models handle irrelevant features, but noise features\n",
    "  still impact performance\n",
    "  - Enables data-driven selection rather than statistical filter\n",
    "  methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO outsource the code in step 2 for HGB and RF into this function (same procedure as below with \"train_model_on_best_features\")\n",
    "def calculate_shap_values():\n",
    "    '''\n",
    "    We use SHAP's TreeExplainer to calculate feature importance values. TreeExplainer is specifically optimized for tree-based models and provides exact Shapley values efficiently.\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General function which can be called by the models to avoid redundant code and enable easy maintenance\n",
    "def train_model_on_best_features(baseline_mae, shap_importance, model, X_train_processed, X_val_processed, range_number_of_features, feature_names_all):\n",
    "    '''\n",
    "    We systematically test different numbers of top features to find the optimal subset:\n",
    "    We train the model with the same optimized hyperparameters but using only the most important features identified by SHAP\n",
    "    '''\n",
    "    # Track best model\n",
    "    results = []\n",
    "    best_model = None\n",
    "    best_mae = float(\"inf\")\n",
    "    best_n = None\n",
    "    best_features = None\n",
    "\n",
    "    # Find best feature counts\n",
    "    for n_features in range_number_of_features:\n",
    "        # Select top N features\n",
    "        top_features = shap_importance.head(n_features)[\"feature\"].tolist()\n",
    "        feature_indices = [i for i, fname in enumerate(feature_names_all) if fname in top_features]\n",
    "\n",
    "        X_train_subset = X_train_processed[:, feature_indices]\n",
    "        X_val_subset   = X_val_processed[:, feature_indices]\n",
    "\n",
    "        # Train and predict using selected amount of features (model uses tuned hyperparams)\n",
    "        model.fit(X_train_subset, y_train)\n",
    "        pred_subset = model.predict(X_val_subset)\n",
    "        mae_subset = mean_absolute_error(y_val, pred_subset)\n",
    "        results.append({\"n_features\": n_features, \"mae\": mae_subset})\n",
    "\n",
    "        # Check whether current mae is best so far\n",
    "        if mae_subset < best_mae:\n",
    "            best_mae = mae_subset\n",
    "            best_n = n_features\n",
    "            best_model = model\n",
    "            best_features = top_features\n",
    "\n",
    "        # Print MAE for each amount of features\n",
    "        if n_features in range_number_of_features:\n",
    "            improvement_rf = baseline_mae - mae_subset\n",
    "            print(f\"Top {n_features:3d} features: MAE: {mae_subset:.1f} (Δ: {improvement_rf:+.1f})\")\n",
    "\n",
    "\n",
    "    print(f\"\\nOptimal feature selection results:\")\n",
    "    print(f\"Best performance with {best_n} features: MAE: {best_mae:.2f}\")\n",
    "    print(f\"Improvement over baseline: {baseline_mae - best_mae:+.2f} MAE\\n\")\n",
    "\n",
    "    print(f\"Optimal {best_n} features for production model:\")\n",
    "    for i, feat in enumerate(best_features, start=1):\n",
    "        imp = shap_importance.loc[shap_importance['feature'] == feat, 'importance'].values[0]\n",
    "        print(f\"{i:2d}. {feat:25s} ({imp:.1f})\")\n",
    "    \n",
    "    # Retrain a fresh final estimator on the full training set restricted to best_features (guarantees correct input dimension)\n",
    "    selected_idx = [i for i, fname in enumerate(feature_names_all) if fname in best_features]\n",
    "    final_est = clone(model)\n",
    "    final_est.fit(X_train_processed[:, selected_idx], y_train)\n",
    "\n",
    "    return final_est, best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7de8112-ad4c-4761-a868-9ed084f86174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.1 HGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cadbb5c-a5f6-4fb1-963c-70ff79b146f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 1: Baseline Performance with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "51a1844a-48ae-4a25-8f9a-5759a7d13a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_val_processed_hgb = hgb_best.named_steps[\"preprocess\"].transform(X_val)\n",
    "hgb_val_pred = hgb_best.named_steps[\"model\"].predict(X_val_processed_hgb)\n",
    "n_features_total = X_val_processed_hgb.shape[1]\n",
    "baseline_mae_hgb = mean_absolute_error(y_val, hgb_val_pred)\n",
    "\n",
    "print(\"Baseline Performance of HGB model after Hyperparameter Tuning:\\n\")\n",
    "print_metrics(y_val, hgb_val_pred)\n",
    "print(f\"\\nTotal features used: {n_features_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a381d64-0406-4daa-a147-95081efa4eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 2: SHAP Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d9d985-7fa1-4326-9524-57af6c84e7a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility, otherwise we get different results each time we run the notebook\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract trained model and preprocessed data\n",
    "hgb_model = hgb_best.named_steps[\"model\"]\n",
    "X_train_processed_hgb = hgb_best.named_steps[\"preprocess\"].transform(X_train)\n",
    "\n",
    "# Build feature names manually (avoid GroupMedianImputer issue)\n",
    "feature_names_all = (\n",
    "    log_features\n",
    "    + numeric_features\n",
    "    + list(\n",
    "        hgb_best.named_steps[\"preprocess\"]\n",
    "        .named_transformers_[\"cat\"]\n",
    "        .named_steps[\"encoder\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "    )\n",
    ")\n",
    "\n",
    "# TreeExplainer for HGB model\n",
    "explainer = shap.TreeExplainer(hgb_model)\n",
    "\n",
    "# Sample for speed\n",
    "sample_size = min(1000, len(X_train_processed_hgb))\n",
    "sample_indices = np.random.choice(len(X_train_processed_hgb), sample_size, replace=False)\n",
    "X_sample = X_train_processed_hgb[sample_indices]\n",
    "\n",
    "print(f\"Computing SHAP values for HGB on {sample_size} samples...\")\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# Mean absolute SHAP = global importance\n",
    "feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Create DataFrame with proper columns\n",
    "shap_importance_df_hgb = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names_all,\n",
    "        \"importance\": feature_importance\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 20 most important features:\")\n",
    "print(shap_importance_df_hgb.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "355aaca1-92eb-42d9-82d5-e76c3a8b99bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 3: Automated Feature Selection Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model with the same hyperparams\n",
    "hgb_model = hgb_best.named_steps[\"model\"]\n",
    "hgb_selected = HistGradientBoostingRegressor(**hgb_model.get_params())\n",
    "range_number_of_features_hgb = range(17,18)\n",
    "\n",
    "best_model_hgb, best_features_hgb = train_model_on_best_features(baseline_mae_hgb, shap_importance_df_hgb, hgb_selected, X_train_processed_hgb, X_val_processed_hgb, range_number_of_features_hgb, feature_names_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "94cd8db0-31da-4f79-a4e1-00181450e3f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "top_df = shap_importance_df_hgb.head(top_k).iloc[::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.barh(top_df[\"feature\"], top_df[\"importance\"],\n",
    "               color=plt.cm.Blues(np.linspace(0.4, 0.9, len(top_df))))\n",
    "\n",
    "ax.set_xlabel(\"Average |SHAP| value\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "ax.set_title(f\"Top {top_k} HGB features by SHAP importance\")\n",
    "ax.bar_label(bars, fmt=\"%.0f\", padding=3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1e893a1a-3c61-4f13-9fd9-35715c65d492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build the final pipeline with feature selection included\n",
    "def select_best_features(X):\n",
    "    idx = [i for i, fname in enumerate(feature_names_all) if fname in best_features_hgb]\n",
    "    return X[:, idx]\n",
    "\n",
    "hgb_final_pipe = Pipeline([\n",
    "    (\"preprocess\", hgb_best.named_steps[\"preprocess\"]),\n",
    "    (\"feature_selector\", FunctionTransformer(select_best_features, validate=False)),\n",
    "    (\"model\", best_model_hgb)\n",
    "])\n",
    "\n",
    "# Save the best model for later use\n",
    "joblib.dump(hgb_final_pipe, \"hgb_best_feature.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16b675a-660c-46c8-8430-5b28d91b8c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.2 RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65c9142-f110-4d6f-a31c-5764d2492fc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 1: Baseline Performance with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "95adb47f-fb45-47f0-b24d-e75d5ef1c68e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the tuned RF pipeline (rf_best_rand) and compute baseline on the validation set\n",
    "X_val_processed_rf = rf_best_rand.named_steps[\"preprocess\"].transform(X_val)\n",
    "rf_val_pred = rf_best_rand.named_steps[\"model\"].predict(X_val_processed_rf)\n",
    "n_features_total_rf = X_val_processed_rf.shape[1] # TODO cant we just use one val_processed and one n_features_total or why did we split that? ~J\n",
    "baseline_mae_rf = mean_absolute_error(y_val, rf_val_pred)\n",
    "\n",
    "print(\"Baseline Performance of RF model after Hyperparameter Tuning:\\n\")\n",
    "print_metrics(y_val, rf_val_pred)\n",
    "print(f\"\\nTotal features used: {n_features_total_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef64537-b8f4-4f70-b060-78606e96731b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 2: SHAP Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09d26704-08d7-4e72-b236-d44d0c2433ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility, otherwise we get different results each time we run the notebook\n",
    "np.random.seed(42)\n",
    "\n",
    "# Preprocess training data (same preprocessor as in rf_best_rand)\n",
    "X_train_processed_rf = rf_best_rand.named_steps[\"preprocess\"].transform(X_train)\n",
    "\n",
    "# Build feature names (log + numeric + OHE categories), same logic as for HGB\n",
    "feature_names_all_rf = (\n",
    "    log_features\n",
    "    + numeric_features\n",
    "    + list(\n",
    "        rf_best_rand.named_steps[\"preprocess\"]\n",
    "        .named_transformers_[\"cat\"]\n",
    "        .named_steps[\"encoder\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "    )\n",
    ")\n",
    "\n",
    "# SHAP for RandomForest\n",
    "rf_model = rf_best_rand.named_steps[\"model\"]\n",
    "explainer_rf = shap.TreeExplainer(rf_model)\n",
    "\n",
    "sample_size_rf = min(50, len(X_train_processed_rf))\n",
    "sample_idx_rf = np.random.choice(len(X_train_processed_rf), sample_size_rf, replace=False)\n",
    "X_sample_rf = X_train_processed_rf[sample_idx_rf]\n",
    "\n",
    "print(f\"Computing SHAP values for RF on {sample_size_rf} samples...\")\n",
    "shap_values_rf = explainer_rf.shap_values(X_sample_rf)\n",
    "\n",
    "# Mean absolute SHAP = global importance\n",
    "feature_importance_rf = np.abs(shap_values_rf).mean(axis=0)\n",
    "\n",
    "# Create DataFrame with proper columns\n",
    "shap_importance_df_rf = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names_all_rf,\n",
    "        \"importance\": feature_importance_rf\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 20 most important RF features:\")\n",
    "print(shap_importance_df_rf.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0af8a6-1394-464d-8e8c-d452d0138cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 3: Automated Feature Selection Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same processed validation data and reuse tuned RF hyperparameters\n",
    "rf_params = {k.replace(\"model__\", \"\"): v for k, v in rf_random.best_params_.items()}\n",
    "rf_selected = RandomForestRegressor(random_state=42, n_jobs=-1, **rf_params)\n",
    "range_number_of_features_rf = range(26, 28) # We set this to a smaller range for more efficiency after first iterations\n",
    "\n",
    "best_model_rf, best_features_rf = train_model_on_best_features(baseline_mae_rf, shap_importance_df_rf, rf_selected, X_train_processed_rf, X_val_processed_rf, range_number_of_features_rf, feature_names_all_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5bdfcad-da5d-4543-8fce-7ab42593c972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RF SHAP bar plot\n",
    "top_k = 20\n",
    "top_df = shap_importance_df_rf.head(top_k).iloc[::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.barh(\n",
    "    top_df[\"feature\"],\n",
    "    top_df[\"importance\"],\n",
    "    color=plt.cm.Greens(np.linspace(0.4, 0.9, len(top_df)))  # andere Farbe für RF\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Average |SHAP| value\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "ax.set_title(f\"Top {top_k} RF features by SHAP importance\")\n",
    "ax.bar_label(bars, fmt=\"%.0f\", padding=3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8398f0e-737d-4ae1-8ba1-5f8f21aa211d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the best RF model for later use\n",
    "\n",
    "# Build the final RF pipeline with feature selection included\n",
    "def select_best_features_rf(X):\n",
    "    idx = [i for i, fname in enumerate(feature_names_all_rf) if fname in best_features_rf]\n",
    "    return X[:, idx]\n",
    "\n",
    "final_rf_pipe = Pipeline([\n",
    "    (\"preprocess\", rf_best_rand.named_steps[\"preprocess\"]),\n",
    "    (\"feature_selector\", FunctionTransformer(select_best_features_rf, validate=False)),\n",
    "    (\"model\", best_model_rf)\n",
    "])\n",
    "\n",
    "# joblib.dump(final_rf_pipe, \"rf_best_feature.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135acfad-1ad6-49f9-97fc-f1a81d0c3187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb68e758-d7b1-4f80-9dc4-ed985f721c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Extra Task (1 Point): Be in the Top 5 Groups on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a25f58c-4c24-4460-93d4-830176016ee8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pick best Model, predict on Test"
    }
   },
   "outputs": [],
   "source": [
    "# Load best model from Joblib\n",
    "hgb_best_99 = joblib.load(\"hgb_best_feature.pkl\")\n",
    "\n",
    "# Visualize preprocessor\n",
    "preprocessor = hgb_best_99.named_steps[\"preprocess\"]\n",
    "display(preprocessor)\n",
    "\n",
    "# Use the loaded model to predict on val set\n",
    "pred_loaded = hgb_best_99.predict(X_val)\n",
    "mae_loaded = mean_absolute_error(y_val, pred_loaded)\n",
    "print(f\"Loaded model MAE on validation set: {mae_loaded:.2f}\")\n",
    "\n",
    "# Predict on test set\n",
    "df_cars_test['price'] = hgb_best_99.predict(df_cars_test)\n",
    "df_cars_test['price'].to_csv('Group05_Version06.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b18805-e662-4807-bdb8-758df6027d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Submission with API push"
    }
   },
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c cars4you -f Group05_Version05.csv -m \"Message\" # Uncomment to submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350057a3-c771-4bf0-8fac-19b49be17e35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Score Check"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c cars4you"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "group05_midterm_main_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
