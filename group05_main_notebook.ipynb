{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0d9d0c-5598-4a4e-b28b-a2921d27ce73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Your work will be evaluated according to the following criteria:**\n",
    "- Project Structure and Notebook(s) Quality (4/20)\n",
    "- Data Exploration & Initial Preprocessing (4/20)\n",
    "- Regression Benchmarking and Optimization (7/20)\n",
    "- Open-Ended Section (4/20)\n",
    "- Deployment (1/20)\n",
    "- Extra Point: Have Project Be Publicly Available on GitHub (1/20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a937a442-783b-4e36-a847-80976966adeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Project Timeline**\n",
    "- 22.11.: Preprocessing and Model Preparation\n",
    "    - Finish clean preprocessing all included in pipeline\n",
    "    - Finish clean Hyperparameter Tuning\n",
    "- 29.11.: Feature Selection\n",
    "    - Clean and structured approach for feature selection for all models (best case: consistent approach imo)\n",
    "- 29.11.: Regression Benchmarking and Optimization\n",
    "    - Automize Optimization (add something like mlflow)\n",
    "- 06.12.: Open-End Section and Deployment\n",
    "    - Added 4 open-end-experiments\n",
    "    - Deployment\n",
    "- 13.12.: Notebook Feinschliff\n",
    "    - Super clean notebook structure similar to lab-notebooks by Ricardo\n",
    "    - Show and explain results of different models clearly in markdown tables etc. (see the lab-notebooks)\n",
    "- 14.12.: Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d89fe94-6eed-4612-ad0a-40032c4dd67a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO Open End Section:\n",
    "# Interface for new Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4467ab3-7594-4e02-a19f-8c5e47fb14e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"\n",
    "    background: rgba(25, 25, 25, 0.55);\n",
    "    backdrop-filter: blur(16px) saturate(150%);\n",
    "    -webkit-backdrop-filter: blur(16px) saturate(150%);\n",
    "    border: 1px solid rgba(255, 255, 255, 0.12);\n",
    "    border-radius: 18px;\n",
    "    padding: 45px 30px;\n",
    "    text-align: center;\n",
    "    font-family: 'Inter', 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;\n",
    "    color: #e0e0e0;\n",
    "    box-shadow: 0 0 30px rgba(0, 0, 0, 0.35);\n",
    "    margin: 40px auto;\n",
    "    max-width: 800px;\n",
    "\">\n",
    "\n",
    "  <h1 style=\"\n",
    "      font-size: 2.8em;\n",
    "      font-weight: 700;\n",
    "      margin: 0 0 8px 0;\n",
    "      letter-spacing: -0.02em;\n",
    "      background: linear-gradient(90deg, #00e0ff, #9c7eff);\n",
    "      -webkit-background-clip: text;\n",
    "      -webkit-text-fill-color: transparent;\n",
    "  \">\n",
    "      Machine Learning Project\n",
    "  </h1>\n",
    "\n",
    "  <h2 style=\"\n",
    "      font-size: 1.6em;\n",
    "      font-weight: 500;\n",
    "      margin: 0 0 25px 0;\n",
    "      color: #b0b0b0;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Cars 4 You - Predicting Car Prices\n",
    "  </h2>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.25em;\n",
    "      font-weight: 500;\n",
    "      color: #c0c0c0;\n",
    "      margin-bottom: 6px;\n",
    "  \">\n",
    "      Group 5 - Lukas Belser, Samuel Braun, Elias Karle, Jan Thier\n",
    "  </p>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.05em;\n",
    "      font-weight: 400;\n",
    "      color: #8a8a8a;\n",
    "      font-style: italic;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Machine Learning End Results · 22.12.2025\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372c87aa-b7f3-4a34-a596-bb5e584db114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Table of Contents**\n",
    " \n",
    "- [1. Import Packages and Data](#1-import-packages-and-data)  \n",
    "  - [1.1 Import Required Packages](#11-import-required-packages)  \n",
    "  - [1.2 Load Datasets](#12-load-datasets)  \n",
    "  - [1.3 Kaggle Setup](#13-kaggle-setup)  \n",
    "- [2. Data Cleaning, Feature Engineering, Split & Preprocessing](#2-data-cleaning-feature-engineering-split--preprocessing)  \n",
    "  - [2.1 Data Cleaning](#21-data-cleaning)  \n",
    "  - [2.2 Feature Engineering](#22-feature-engineering)  \n",
    "  - [2.3 Data Split](#23-data-split)  \n",
    "  - [2.4 Preprocessing](#24-preprocessing)  \n",
    "- [3. Feature Selection](#3-feature-selection)  \n",
    "- [4. Model Evaluation Metrics, Baselining, Setup](#4-model-evaluation-metrics-baselining-setup)  \n",
    "- [5. Hyperparameter Tuning and Model Evaluation](#5-hyperparameter-tuning-and-model-evaluation)  \n",
    "  - [5.1 ElasticNet](#51-elasticnet)  \n",
    "  - [5.2 HistGradientBoost](#52-histgradientboost)  \n",
    "  - [5.3 RandomForest](#53-randomforest)  \n",
    "  - [5.4 ExtraTrees](#54-extratrees)  \n",
    "- [6. Feature Importance of Tree Models (with SHAP)](#6-feature-importance-of-tree-models-with-shap)  \n",
    "  - [6.1 HGB](#61-hgb)  \n",
    "  - [6.2 RF](#62-rf)  \n",
    "- [7. Kaggle Competition](#7-kaggle-competition)  \n",
    "\n",
    "TODO finish + update toc > at the end of project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fff3e4-624f-4b1f-b258-fbbe7b69cb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Import Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e6aadb-727a-4399-8888-9d6d4e593898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aa35af5d-8257-4fb6-a380-86bbbfac3244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!pip install shap\n",
    "!pip install -U scikit-learn\n",
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f54ab11e-fcfc-4278-8997-89a92adf8eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import and load Data"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({\"figure.max_open_warning\": 0, \"figure.dpi\": 100})\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import VarianceThreshold, chi2, RFE\n",
    "from scipy.stats import spearmanr, uniform, randint\n",
    "from sklearn.metrics import mean_absolute_error\n",
    " \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.base import clone\n",
    " \n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, GridSearchCV, cross_validate\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from category_encoders import QuantileEncoder # used for median target encoding (sklearn only supports mean target encoding with their TargetEncoder class)\n",
    " \n",
    "from car_functions import clean_car_dataframe\n",
    "from pipeline_functions import GroupImputer, m_estimate_mean\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.inspection import permutation_importance\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9737aef-d2bd-4748-89ed-9b0e2cea8637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.2 Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3da1ad2b-8ee0-48b0-917d-545c70633ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cars_train = pd.read_csv(\"train.csv\")\n",
    "df_cars_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab6500b5-94a1-4a0e-b7f6-ea9d35c787eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Kaggle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "556ffc24-70f7-48c9-96a4-3a19380f91db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Connect"
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle API Connect\n",
    "\n",
    "# Folder containing kaggle.json\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/Workspace/Users/20250355@novaims.unl.pt\" #add your own kaggle.json api token\n",
    "\n",
    "# Test\n",
    "!echo $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75ee214-0996-406f-b77f-9e72fa7153e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data Cleaning, Feature Engineering, Split & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53a2a97-4662-40a3-8b24-b238dad5138e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6fa0cfe9-be7e-4e90-ac1c-cd600bb7d13f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Numerical / Categorical Data Cleaning: with Pandas"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier Preprocessing, Missing Value Handling and Decision justifying happens here\n",
    "df_cars_train = clean_car_dataframe(df_cars_train)\n",
    "df_cars_test = clean_car_dataframe(df_cars_test)\n",
    "\n",
    "# Safety Check: print unique values of all columns of df_cars_train // df_cars_test to see if data cleaning worked and if there are still odd values\n",
    "for col in df_cars_train.columns:\n",
    "    print(col, df_cars_train[col].unique())\n",
    "print(\"X\"*150)\n",
    "for col in df_cars_test.columns:\n",
    "    print(col, df_cars_test[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b198720e-b5a0-46ef-8055-4ac33b852410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd8845f0-241e-40a4-8c24-3e876d89b02b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Base Feature Creation**\n",
    "\n",
    "These are foundational features derived directly from the original data, often to create linear relationships or capture interactions.\n",
    "- `age`: Calculated as `2020 - year`. Creates a simple linear feature representing the car's age. Newer cars (lower age) generally have higher prices.\n",
    "- `miles_per_year`: Calculated as `mileage / age`. This normalizes the car's usage, preventing high correlation (multicollinearity) between `mileage` and `age`. A 3-year-old car with 60,000 miles is different from a 6-year-old car with 60,000 miles.\n",
    "- `age_x_engine`: An interaction term `age * engineSize`. This helps the model capture non-linear relationships, such as the possibility that the value of cars with large engines might depreciate faster (or slower) than cars with small engines.\n",
    "- `mpg_x_engine`: An interaction term `mpg * engineSize`. This captures the combined effect of fuel efficiency and engine power.\n",
    "- `tax_per_engine`: Calculated as `tax / engineSize`. This feature represents the tax cost relative to the engine's power, which could be an indicator of overall running costs or vehicle class.\n",
    "- `mpg_per_engine`: Calculated as `mpg / engineSize`. This creates an \"efficiency\" metric, representing how many miles per gallon the car achieves for each unit of engine size.\n",
    "\n",
    "\n",
    "**Popularity & Demand Features**\n",
    "\n",
    "These features attempt to quantify a car's popularity or market demand, which directly influences price.\n",
    "- `model_freq`: Calculates the frequency (percentage) of each `model` in the training dataset. Popular, common models often have more stable and predictable pricing and demand.\n",
    "\n",
    "\n",
    "**Price Anchor Features**\n",
    "\n",
    "These features \"anchor\" a car's price relative to its group. They provide a strong baseline price signal based on brand, model, and configuration.\n",
    "- `brand_med_price`: The median price for the car's `Brand` (e.g., the typical price for a BMW vs. a Skoda). This captures overall brand positioning.\n",
    "- `model_med_price`: The median price for the car's `model` (e.g., the typical price for a 3-Series vs. a 1-Series). This captures the model's positioning within the brand.\n",
    "- `brand_fuel_med_price`: The median price for the car's specific `Brand` and `fuelType` combination (e.g., a Diesel BMW vs. a Petrol BMW).\n",
    "- `brand_trans_med_price`: The median price for the `Brand` and `transmission` combination (e.g., an Automatic BMW vs. a Manual BMW).\n",
    "\n",
    "\n",
    "**Normalized & Relative Features**\n",
    "\n",
    "These features compare a car to its peers rather than using absolute values.\n",
    "- `*_anchor` (e.g., `brand_med_price_anchor`): Created by dividing the median price features (from section 3) by the `overall_mean_price`. This makes the feature dimensionless and represents the group's price *relative* to the entire market (e.g., \"this brand is 1.5x the market average\").\n",
    "- `age_rel_brand`: Calculated as `age - brand_median_age`. This shows if a car is newer or older than the *typical* car for that specific brand, capturing relative age within its own group.\n",
    "\n",
    "\n",
    "**CV-Safe Target Encodings**\n",
    "\n",
    "This is an advanced technique to encode categorical variables (like `model` or `Brand`) using information from the target variable (`price`) without causing data leakage.\n",
    "- `*_te` (e.g., `model_te`): Represents the *average price* for that category (e.g., the average price for a \"Fiesta\").\n",
    "- **Why is it \"CV-Safe\"?** Instead of just calculating the global average price for \"Fiesta\" and applying it to all rows (which leaks target information), this method uses K-Fold cross-validation. For each fold of the data, the target encoding is calculated *only* from the *other* folds. This ensures the encoding for any given row never includes its own price, preventing leakage and leading to a more robust model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "83812e71-e13e-43d4-883b-41ffd3b3466b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# docstring adden TODO\n",
    "class CarFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ref_year=None):\n",
    "        self.ref_year = ref_year\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        if self.ref_year is None:\n",
    "            self.ref_year_ = X_['year'].max()\n",
    "        else:\n",
    "            self.ref_year_ = self.ref_year\n",
    "        self.brand_median_age_ = (\n",
    "            (self.ref_year_ - X_['year'])\n",
    "            .groupby(X_['Brand'])\n",
    "            .median()\n",
    "            .to_dict()\n",
    "        )\n",
    "        self.model_freq_ = X_['model'].value_counts(normalize=True).to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        # # 1. Base Feature Creation: Car Age - Newer cars usually have higher prices, models prefer linear features\n",
    "        age = self.ref_year_ - X['year']\n",
    "        X['age'] = age\n",
    "\n",
    "        # Miles per Year: Normalizes mileage by age (solves multicollinearity between year and mileage)\n",
    "        X['miles_per_year'] = X['mileage'] / age.replace({0: np.nan})\n",
    "        X['miles_per_year'] = X['miles_per_year'].fillna(X['mileage']) # if age is 0, just use mileage because that's the mileage it has driven so far in that year\n",
    "\n",
    "        # Interaction Terms: Capture non-linear effects between engine and other numeric features\n",
    "        X['age_x_engine'] = X['age'] * X['engineSize']\n",
    "        X['mpg_x_engine']  = X['mpg'] * X['engineSize']\n",
    "\n",
    "        # tax per engine\n",
    "        X['tax_per_engine'] = X['tax'] / X['engineSize'].replace({0: np.nan})\n",
    "\n",
    "        # MPG per engineSize to represent the efficiency\n",
    "        X['mpg_per_engine'] = X['mpg'] / X['engineSize'].replace({0: np.nan})\n",
    "\n",
    "        # 2. Model Frequency: Popular models tend to have stable demand and prices\n",
    "        X['model_freq'] = X['model'].map(self.model_freq_).fillna(0.0)\n",
    "\n",
    "        # 3. Create Interaction Features for anchor (relative positioning within brand/model)\n",
    "        X['brand_fuel'] = X['Brand'].astype(str) + \"_\" + X['fuelType'].astype(str)\n",
    "        X['brand_trans'] = X['Brand'].astype(str) + \"_\" + X['transmission'].astype(str)\n",
    "        \n",
    "        # 4. Relative Age (within brand): newer/older than brand median year\n",
    "        X['age_rel_brand'] = X['age'] - X['Brand'].map(self.brand_median_age_)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "17ada114-a73d-417d-8d74-8aef78ba9ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cars_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0de09e53-b3c6-4669-942a-5c957f41e1f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cars_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e72b5602-fc3d-4397-bb66-10e7b501438a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.3 (No) Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda482e9-4a68-438d-86c0-e0524a9b8692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split Data"
    }
   },
   "outputs": [],
   "source": [
    "# Since we have an external hold-out set (kaggle) an additional val set is not necessary and wastes training data)\n",
    "X_train = df_cars_train.drop(columns='price')\n",
    "y_train = df_cars_train['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9425a7c0-ec96-4458-a0da-8ef65acb3f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.4 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ae68289b-ab02-40fb-b9d9-b530339f16f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO warum benutzen wir das => auch bei pipeline adden\n",
    "def to_float_array(x):\n",
    "    \"\"\"Convert input to float array.\"\"\"\n",
    "    return np.array(x, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6de5c82c-7a4c-4745-8070-43b8e78b94d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Patch FunctionTransformer to expose feature names and create function to call it\n",
    "class NamedFunctionTransformer(FunctionTransformer):\n",
    "    def __init__(self, func=None, feature_names=None, **kwargs):\n",
    "        # store as attribute so sklearn.get_params can access it\n",
    "        self.feature_names = feature_names\n",
    "        super().__init__(func=func, **kwargs)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        # if custom names specified, use them\n",
    "        if self.feature_names is not None:\n",
    "            return np.asarray(self.feature_names, dtype=object)\n",
    "        # otherwise just pass through the input feature names\n",
    "        return np.asarray(input_features, dtype=object)\n",
    "\n",
    "\n",
    "def get_feature_names_from_preprocessor(pre):\n",
    "    feature_names = []\n",
    "    for name, trans, cols in pre.transformers_:\n",
    "        if name != 'remainder':\n",
    "            if hasattr(trans, 'get_feature_names_out'):\n",
    "                # for categorical OHE\n",
    "                try:\n",
    "                    feature_names.extend(trans.get_feature_names_out(cols))\n",
    "                except:\n",
    "                    feature_names.extend(cols)\n",
    "            else:\n",
    "                feature_names.extend(cols)\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d59f995-10c7-4391-a5ce-759858b69889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Split into original and engineered features pipelines for initial model baselining and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "362f52e2-4378-4266-97b7-a4038b588e6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PIPELINE WITH preprocessor_orig CONTAINING ONLY ORIGINAL FEATURES\n",
    "\n",
    "orig_numeric_features = [\n",
    "    \"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\", \"previousOwners\"\n",
    "]\n",
    "orig_categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "# left out columns: hasDamage (unsure what the two values 0 and NaN mean), paintQuality (only added by mechanic so not available for our predictions in production)\n",
    "\n",
    "numeric_transformer_orig = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),    # simple global median imputation\n",
    "    (\"to_float\", FunctionTransformer()),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer_orig = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # fill by mode instead of Unknown\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))  # One-hot encoding\n",
    "])\n",
    "\n",
    "preprocessor_orig = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer_orig, orig_numeric_features),\n",
    "    (\"cat\", categorical_transformer_orig, orig_categorical_features)\n",
    "])\n",
    "\n",
    "preprocessor_orig.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "39142374-d485-42e7-a6cb-83ecffae0091",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocessing: with sklearn Pipeline & Column Transformer"
    }
   },
   "outputs": [],
   "source": [
    "# PIPELINE WITH preprocessor_fe CONTAINING ENGINEERED FEATURES\n",
    "\n",
    "numeric_features = [\n",
    "    \"age\", \"age_rel_brand\", \"tax\", \"mpg\", \"engineSize\", \"previousOwners\", \"model_freq\",\n",
    "    \"age_x_engine\", \"mpg_x_engine\",\n",
    "    \"tax_per_engine\", \"mpg_per_engine\"\n",
    "]\n",
    "log_features = [\"mileage\", \"miles_per_year\"]                                              \n",
    "categorical_features = [\"transmission\", \"fuelType\"]\n",
    "categorical_features_for_te = [\"Brand\", \"model\"]\n",
    "categorical_median_te = [\"Brand\", \"model\", \"brand_fuel\", \"brand_trans\"]\n",
    "\n",
    "# left out columns: year (age is better), hasDamage (unsure what the two values 0 and NaN mean), paintQuality (only added by mechanic so not available for our predictions in production)\n",
    "\n",
    "numeric_transformer_fe = Pipeline([\n",
    "    (\"to_float\", NamedFunctionTransformer(to_float_array, validate=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "log_transformer_fe = Pipeline([\n",
    "    # Hierarchical imputation on Brand_te/model_te, then log-transform\n",
    "    (\"to_float\", NamedFunctionTransformer(to_float_array, validate=False)),\n",
    "    (\"log\",    NamedFunctionTransformer(np.log1p, validate=False)),  # log1p handles zeros safely\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer_fe_ohe = Pipeline([\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "# Keep mean target encoder in code but dont use it for now because median TE seems more robust and we use only one method for consistency\n",
    "# categorical_transformer_fe_te = Pipeline([ \n",
    "#     (\"encoder\", TargetEncoder(target_type='continuous', cv=5, smooth='auto', random_state=42)), # Prevents data leakage with CV (e.g. for the samples in Fold 1, it calculates the target mean using the data from Folds 2, 3, 4, and 5) # TODO If it overfits test data too much, increasing the smoothing parameter can help\n",
    "#     (\"scaler\", StandardScaler()),\n",
    "# ])\n",
    "\n",
    "\n",
    "# names for median-TE features (one per input column, since QuantileEncoder outputs 1 column per feature)\n",
    "median_te_feature_names = [f\"{col}__median_te\" for col in categorical_median_te]\n",
    "\n",
    "categorical_transformer_fe_median_te = Pipeline(steps=[\n",
    "    ('median_encoder', QuantileEncoder(quantile=0.5, m=10.0)), # not specifying the cols mean it encodes all columns # quantile=0.5 = Median. m is the smoothing parameter (smoothing mitigates but doesnt eliminate leakage) # TODO tune m?\n",
    "    ('scaler', StandardScaler()) ,\n",
    "    ('name_wrapper', NamedFunctionTransformer(feature_names=median_te_feature_names,\n",
    "                                              validate=False)),\n",
    "])\n",
    "\n",
    "# TODO: Currently the transformed features from the categorical_transformer_fe_median_te dont have names but just numbers (see names in Section 5.3 Feature Importance for RF) -> fix that later if possible ~J\n",
    "\n",
    "# ColumnTransformer that uses all engineered features\n",
    "transformer_fe = ColumnTransformer([\n",
    "    (\"log\", log_transformer_fe, log_features),\n",
    "    (\"num\", numeric_transformer_fe, numeric_features),\n",
    "    (\"cat\", categorical_transformer_fe_ohe, categorical_features),\n",
    "    # (\"mean_te\", categorical_transformer_fe_te, categorical_features_for_te),\n",
    "    (\"median_te\", categorical_transformer_fe_median_te, categorical_median_te)\n",
    "])\n",
    "\n",
    "# preprocessor_fe with group imputing and scaler_fe\n",
    "preprocessor_fe = Pipeline([\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"group_imputer\", GroupImputer(\n",
    "        group_cols=(\"Brand\", \"model\"),\n",
    "        num_cols=numeric_features + log_features,\n",
    "        cat_cols=categorical_features + categorical_median_te,\n",
    "        fallback=\"__MISSING__\",\n",
    "    )),\n",
    "    (\"ct\", transformer_fe),   # ColumnTransformer\n",
    "])\n",
    "\n",
    "preprocessor_fe.fit(X_train, y_train) # Fit here already to have scaled data for feature selection later (y_train is necessary for target encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8449a5e1-33f6-4f26-954b-011ef4cebde9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ct = preprocessor_fe.named_steps[\"ct\"]\n",
    "fe_names = get_feature_names_from_preprocessor(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d555015-5346-4de3-8bd7-2bced64c258b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TODO delete following cell later - this is for us to see if the group imputer works - but it is GPT slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "32efebf2-72f6-448e-b5a1-3fa27bd272bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "brand = \"VW\"\n",
    "model = \"golf\"\n",
    "\n",
    "# 1) Get the fitted steps from preprocessor_fe\n",
    "fe = preprocessor_fe.named_steps[\"fe\"]              # CarFeatureEngineer\n",
    "imp = preprocessor_fe.named_steps[\"group_imputer\"]  # GroupImputer\n",
    "\n",
    "# 2) Inspect GroupImputer internal numeric stats\n",
    "pair_table = getattr(imp, \"num_pair_\", None)    # indexed by (_g0, _g1) = (Brand, model)\n",
    "brand_table = getattr(imp, \"num_first_\", None)  # indexed by _g0 = Brand\n",
    "global_med = getattr(imp, \"num_global_\", None)  # Series of global medians\n",
    "\n",
    "print(\"Has pair-level medians table:\",\n",
    "      pair_table is not None and not getattr(pair_table, \"empty\", True))\n",
    "print(\"Has brand-level medians table:\",\n",
    "      brand_table is not None and not getattr(brand_table, \"empty\", True))\n",
    "print(\"Has global median:\",\n",
    "      global_med is not None and not global_med.empty if global_med is not None else False)\n",
    "print()\n",
    "\n",
    "_g0 = brand\n",
    "_g1 = model\n",
    "\n",
    "# 2a) Pair-level\n",
    "if pair_table is not None and (_g0, _g1) in pair_table.index:\n",
    "    print(f\"Pair-level median FOUND for ({brand}, {model}):\")\n",
    "    display(pair_table.loc[(_g0, _g1)])\n",
    "else:\n",
    "    print(f\"No pair-level median for ({brand}, {model}).\")\n",
    "    if pair_table is not None and not pair_table.empty:\n",
    "        print(\"Sample of pair-level medians (top 5):\")\n",
    "        display(pair_table.head())\n",
    "\n",
    "# 2b) Brand-level\n",
    "if brand_table is not None and _g0 in brand_table.index:\n",
    "    print(f\"\\nBrand-level median for {brand}:\")\n",
    "    display(brand_table.loc[_g0])\n",
    "else:\n",
    "    print(\"\\nNo brand-level median for\", brand)\n",
    "    if brand_table is not None and not brand_table.empty:\n",
    "        print(\"Sample of brand-level medians (top 5):\")\n",
    "        display(brand_table.head())\n",
    "\n",
    "# 2c) Global medians\n",
    "print(\"\\nGlobal median (fallback):\")\n",
    "display(global_med)\n",
    "\n",
    "# 3) Apply CarFeatureEngineer + GroupImputer to VW Golf rows and compare\n",
    "#    (GroupImputer was fitted after CarFeatureEngineer, so we must mimic that order)\n",
    "\n",
    "# 3a) Feature engineering on full X_train\n",
    "X_train_fe = fe.transform(X_train)\n",
    "\n",
    "# 3b) Filter for VW Golf in the feature-engineered space\n",
    "vw_golf = X_train_fe[(X_train_fe[\"Brand\"] == brand) & (X_train_fe[\"model\"] == model)].copy()\n",
    "\n",
    "if vw_golf.empty:\n",
    "    print(\"\\nNo VW Golf rows found in X_train.\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(vw_golf)} VW Golf rows in X_train.\")\n",
    "\n",
    "    # 3c) GroupImputer expects the columns it saw at fit time\n",
    "    cols_for_imp = imp.feature_names_in_\n",
    "    vw_input = vw_golf.loc[:, cols_for_imp]\n",
    "\n",
    "    vw_imp = imp.transform(vw_input)\n",
    "    vw_imp_df = pd.DataFrame(vw_imp, columns=cols_for_imp, index=vw_golf.index)\n",
    "\n",
    "    print(\"\\nImputed data (first 8 rows):\")\n",
    "    display(vw_imp_df[[\"mpg\", \"mileage\", \"tax\"]].head(8))\n",
    "\n",
    "    # 4) Build comparison table (original vs imputed, for selected columns)\n",
    "    comp = pd.DataFrame(index=vw_golf.index)\n",
    "    comp[\"orig_mpg\"] = vw_golf[\"mpg\"]\n",
    "    comp[\"imp_mpg\"] = vw_imp_df[\"mpg\"]\n",
    "    comp[\"orig_tax\"] = vw_golf[\"tax\"]\n",
    "    comp[\"imp_tax\"] = vw_imp_df[\"tax\"]\n",
    "    comp[\"orig_mileage\"] = vw_golf[\"mileage\"]\n",
    "    comp[\"imp_mileage\"] = vw_imp_df[\"mileage\"]\n",
    "\n",
    "    print(\"\\nOriginal vs imputed (first 12 rows):\")\n",
    "    display(comp.head(12))\n",
    "\n",
    "    # 5) Determine imputation source per row\n",
    "    def source_of_imputation(col):\n",
    "        srcs = []\n",
    "        for idx, row in comp.iterrows():\n",
    "            val = row[f\"imp_{col}\"]\n",
    "            src = \"other\"\n",
    "\n",
    "            # Pair-level\n",
    "            if pair_table is not None and (_g0, _g1) in pair_table.index and col in pair_table.columns:\n",
    "                pair_val = pair_table.loc[(_g0, _g1), col]\n",
    "                if pd.notna(pair_val) and pd.notna(val) and val == pair_val:\n",
    "                    src = \"pair\"\n",
    "\n",
    "            # Brand-level\n",
    "            if src == \"other\" and brand_table is not None and _g0 in brand_table.index and col in brand_table.columns:\n",
    "                brand_val = brand_table.loc[_g0, col]\n",
    "                if pd.notna(brand_val) and pd.notna(val) and val == brand_val:\n",
    "                    src = \"brand\"\n",
    "\n",
    "            # Global\n",
    "            if src == \"other\" and global_med is not None and col in global_med.index:\n",
    "                glob_val = global_med[col]\n",
    "                if pd.notna(glob_val) and pd.notna(val) and val == glob_val:\n",
    "                    src = \"global\"\n",
    "\n",
    "            srcs.append(src)\n",
    "        return srcs\n",
    "\n",
    "    comp[\"src_mpg\"] = source_of_imputation(\"mpg\")\n",
    "    comp[\"src_tax\"] = source_of_imputation(\"tax\")\n",
    "    comp[\"src_mileage\"] = source_of_imputation(\"mileage\")\n",
    "\n",
    "    print(\"\\nImputation sources for the shown rows:\")\n",
    "    display(comp.head(12))\n",
    "\n",
    "    # 6) Summary counts: NaN before vs after imputation\n",
    "    print(\"\\nSummary counts: NaN before -> NaN after\")\n",
    "    before = vw_golf[[\"mpg\", \"mileage\", \"tax\"]].isna().sum()\n",
    "    after = pd.DataFrame({\n",
    "        \"mpg\": comp[\"imp_mpg\"],\n",
    "        \"mileage\": comp[\"imp_mileage\"],\n",
    "        \"tax\": comp[\"imp_tax\"],\n",
    "    }).isna().sum()\n",
    "    display(pd.DataFrame({\"na_before\": before, \"na_after\": after}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8f763f-298d-444a-80a9-0270ba842dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d8ae44b-0c43-4ff0-9934-ce06c4c8ad42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For all models, we adopted a consistent, embedded feature selection strategy. \n",
    "\n",
    "We first designed a domain-specific preprocessing pipeline (feature engineering + group-wise imputation + encoding).\n",
    "\n",
    "Then we use:\n",
    "1. VarianceThreshold() to remove constant features.\n",
    "2. SelectFromModel(RandomForest) inside each model pipeline as a supervised feature selector.\n",
    "    - This selector is trained within cross-validation and shared across all models, ensuring:\n",
    "        - No data leakage,\n",
    "        - Consistent feature selection logic,\n",
    "        - Model-agnostic, non-linear evaluation of feature relevance.\n",
    "\n",
    "We use a supervised, RF-based feature selector (SelectFromModel) for models that are more sensitive to high dimensionality and collinearity (ElasticNet, SVR). \n",
    "\n",
    "For tree-based ensemble models (RandomForest, ExtraTrees, HistGradientBoosting, and the stacking ensemble), our experiments showed that explicit feature pruning did not improve performance, as these models already handle redundant features well through their own regularization mechanisms. (l2 reg)\n",
    "\n",
    "Therefore, for these models we use only a light VarianceThreshold filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f67147d-0f6e-47d3-b6df-5f73bf38b33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO Samuel further review SKlearn pipeline FS techniques and see how to improve MAE again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b833bae-0b0d-4f4b-a655-7ad1a56c5491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Base estimator used purely for feature selection\n",
    "fs_estimator = RandomForestRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Global feature selector, used in *all* pipelines\n",
    "feature_selector = SelectFromModel(\n",
    "    estimator=fs_estimator,\n",
    "    threshold=\"median\",   # keep features with importance above median - tuning with mean or numeric values\n",
    "    prefit=False,         # fit inside the pipeline\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0dc6124-f736-46fb-9f73-895b930faae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Model Evaluation Metrics, Baselining, Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "225688cc-6157-4613-bf28-9e430473a3a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.1 Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6297f034-638d-4db6-a681-597f20036432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definition of Function for Model Assessment"
    }
   },
   "source": [
    "**MAE (Mean Absolute Error):**\n",
    "- average absolute deviation between predicted and true car prices\n",
    "- easy to interpret in pounds, same metric used by Kaggle competition\n",
    "\n",
    "**RMSE (Root Mean Squared Error):**\n",
    "- sensitive to outliers, helps identify large prediction errors\n",
    "\n",
    "**R²:**\n",
    "- Coefficient of determination: proportion of variance explained by the model\n",
    "- 1.0 = perfect predictions, 0.0 = same as predicting mean, < 0.0 = worse than mean\n",
    "\n",
    "=> We define the metrics in the method `print_metrics` in file `car_functions.py` TODO anpassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "653096f8-6a05-419c-a430-a43d15401295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.2 Baseline (median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b26348e-a5ff-4dc9-9d61-7f59d0ea335d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Baselining median"
    }
   },
   "outputs": [],
   "source": [
    "# Baseline: DummyRegressor using the median price as prediction\n",
    "baseline_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe_clean),  \n",
    "    (\"model\", DummyRegressor(strategy=\"median\")),\n",
    "])\n",
    "\n",
    "baseline_cv = cross_validate(\n",
    "    baseline_pipe,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring={\n",
    "        \"neg_mae\": \"neg_mean_absolute_error\",\n",
    "        \"neg_mse\": \"neg_mean_squared_error\",\n",
    "        \"r2\": \"r2\",\n",
    "    },\n",
    "    n_jobs=-2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "baseline_mae = -baseline_cv[\"test_neg_mae\"].mean()\n",
    "baseline_rmse = np.sqrt(-baseline_cv[\"test_neg_mse\"].mean())\n",
    "baseline_r2 = baseline_cv[\"test_r2\"].mean()\n",
    "\n",
    "print(\"Baseline (median) with engineered features & CV:\")\n",
    "print(f\"MAE:  {baseline_mae:,.4f}\")\n",
    "print(f\"RMSE: {baseline_rmse:,.4f}\")\n",
    "print(f\"R2:   {baseline_r2:,.4f}\")\n",
    "\n",
    "# Baseline (median) with engineered features & CV:\n",
    "# MAE:  6,801.8131\n",
    "# RMSE: 9,981.0186\n",
    "# R2:   -0.0508"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c31fd5-30e4-4f93-a6e4-8e67e0fdd0c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.3 Pipeline Definitions (preprocessor + model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d3aa5b-36ca-4dba-9592-24edd345897d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Models Setup (inkl. Prepro in Pipeline)"
    }
   },
   "outputs": [],
   "source": [
    "### LINEAR MODEL (ElasticNet)\n",
    "\n",
    "elastic_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"vt\", VarianceThreshold(threshold=0.0)),  # remove constant features\n",
    "    (\"model\", ElasticNet(\n",
    "        alpha=0.01,\n",
    "        l1_ratio=0.5,\n",
    "        max_iter=30000,\n",
    "        tol=1e-4,\n",
    "        selection=\"cyclic\",\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "elastic_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe_clean),\n",
    "    (\"vt\", VarianceThreshold(threshold=0.0)),\n",
    "    (\"fs\", SelectFromModel(fs_estimator, threshold=\"median\")),  # shared selector\n",
    "    (\"model\", ElasticNet(\n",
    "        alpha=0.01,\n",
    "        l1_ratio=0.5,\n",
    "        max_iter=30000,\n",
    "        tol=1e-4,\n",
    "        selection=\"cyclic\",\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "\n",
    "### TREE MODELS\n",
    "\n",
    "# HistGradientBoostingRegressor\n",
    "\n",
    "hgb_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"vt\", VarianceThreshold(threshold=0.0)),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "hgb_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe_clean),\n",
    "    (\"vt\", VarianceThreshold(threshold=0.0)),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "\n",
    "# RandomForestRegressor\n",
    "\n",
    "rf_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"vt\", VarianceThreshold(threshold=0.0)),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "rf_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe_clean),\n",
    "    (\"vt\", VarianceThreshold(threshold=0.0)),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "\n",
    "# ExtraTreesRegressor\n",
    "\n",
    "et_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"vt\", VarianceThreshold(threshold=0.0)),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "et_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe_clean),\n",
    "    (\"vt\", VarianceThreshold(threshold=0.0)),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "\n",
    "### KERNEL-BASED MODEL (SVR)\n",
    "\n",
    "svr_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"vt\", VarianceThreshold(threshold=0.0)),\n",
    "    (\"model\", SVR(\n",
    "        kernel=\"rbf\",\n",
    "        C=10,\n",
    "        epsilon=0.1,\n",
    "        gamma=\"scale\",\n",
    "    )),\n",
    "])\n",
    "\n",
    "svr_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe_clean),\n",
    "    (\"vt\", VarianceThreshold(threshold=0.0)),\n",
    "    (\"fs\", SelectFromModel(fs_estimator, threshold=\"median\")),\n",
    "    (\"model\", SVR(\n",
    "        kernel=\"rbf\",\n",
    "        C=10,\n",
    "        epsilon=0.1,\n",
    "        gamma=\"scale\",\n",
    "    )),\n",
    "])\n",
    "\n",
    "\n",
    "### ENSEMBLE META MODEL (Stacking)\n",
    "\n",
    "stack_pipe_orig = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"elastic_orig\", elastic_pipe_orig),\n",
    "        (\"hgb_orig\", hgb_pipe_orig),\n",
    "        (\"rf_orig\", rf_pipe_orig),\n",
    "    ],\n",
    "    final_estimator=HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    passthrough=False,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "stack_pipe_fe = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"hgb_fe\", hgb_pipe_fe),\n",
    "        (\"rf_fe\", rf_pipe_fe),\n",
    "        (\"et_fe\", et_pipe_fe),\n",
    "    ],\n",
    "    final_estimator=HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    passthrough=False,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a21d8b-3aeb-4155-b6db-17475e30c3b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.4 First run of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "80a5dc82-4f35-4d66-9e97-e249166949e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # TODO uncomment (currently its commented to save time during experimentation)\n",
    "\n",
    "# # First evaluation of metrics based on original and engineered feature pipeline to decide how to proceed\n",
    "\n",
    "\n",
    "# models_orig = {\n",
    "#     # \"ElasticNet_orig\": elastic_pipe_orig,\n",
    "#     \"HGB_orig\": hgb_pipe_orig,\n",
    "#     \"RF_orig\": rf_pipe_orig,\n",
    "#     \"ET_orig\": et_pipe_orig,\n",
    "#     \"SVR_orig\": svr_pipe_orig,\n",
    "#     \"Stack_orig\": stack_pipe_orig,\n",
    "# }\n",
    "\n",
    "# models_fe = {\n",
    "#     # \"ElasticNet_fe\": elastic_pipe_fe,\n",
    "#     \"HGB_fe\": hgb_pipe_fe,\n",
    "#     \"RF_fe\": rf_pipe_fe,\n",
    "#     \"ET_fe\": et_pipe_fe,\n",
    "#     \"SVR_fe\": svr_pipe_fe,\n",
    "#     \"Stack_fe\": stack_pipe_fe,\n",
    "# }\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # for name, model in {**models_orig, **models_fe}.items():\n",
    "#     print(f\"Fitting {name} with cross-validation...\")\n",
    "    \n",
    "#     # Perform cross-validation on the entire training set\n",
    "#     cv_results = cross_validate(\n",
    "#         model, \n",
    "#         X_train, \n",
    "#         y_train,\n",
    "#         cv=3,\n",
    "#         scoring={\n",
    "#             'neg_mae': 'neg_mean_absolute_error',\n",
    "#             'neg_mse': 'neg_mean_squared_error',\n",
    "#             'r2': 'r2'\n",
    "#         },\n",
    "#         return_train_score=False,\n",
    "#         verbose=3,\n",
    "#         n_jobs=-2\n",
    "#     )\n",
    "    \n",
    "#     # Calculate mean metrics across folds\n",
    "#     mae = -cv_results['test_neg_mae'].mean()\n",
    "#     rmse = np.sqrt(-cv_results['test_neg_mse'].mean())\n",
    "#     r2 = cv_results['test_r2'].mean()\n",
    "    \n",
    "#     results.append({\n",
    "#         \"model\": name,\n",
    "#         \"feature_set\": \"original\" if name.endswith(\"_orig\") else \"engineered\",\n",
    "#         \"MAE\": mae,\n",
    "#         \"RMSE\": rmse,\n",
    "#         \"R2\": r2,\n",
    "#     })\n",
    "\n",
    "# results_df = (\n",
    "#     pd.DataFrame(results)\n",
    "#       .sort_values([\"feature_set\", \"MAE\"])\n",
    "#       .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# print(results_df)\n",
    "\n",
    "# # Long Duration (with orig ca 25mins VS without orig ca 6mins VS with CV ca 16mins VS with njobs=-1 ca )\n",
    "\n",
    "# # Predicted on hold-out val set (20%):\n",
    "# #       model feature_set          MAE          RMSE        R2\n",
    "# # 0     RF_fe  engineered  1299.728938  4.509435e+06  0.950490\n",
    "# # 1  Stack_fe  engineered  1321.130612  4.831609e+06  0.946953\n",
    "# # 2     ET_fe  engineered  1328.051439  4.707534e+06  0.948315\n",
    "# # 3    HGB_fe  engineered  1534.496164  5.609255e+06  0.938415\n",
    "# # 4    SVR_fe  engineered  2955.064750  3.242891e+07  0.643956\n",
    "\n",
    "# # Predicted using 3-fold CV on entire data:\n",
    "# #       model feature_set          MAE         RMSE        R2\n",
    "# # 0     RF_fe  engineered  1336.806163  2375.850617  0.940424\n",
    "# # 1  Stack_fe  engineered  1357.266391  2505.029128  0.933786\n",
    "# # 2     ET_fe  engineered  1364.212656  2399.654669  0.939223\n",
    "# # 3    HGB_fe  engineered  1551.419964  2503.445871  0.933858\n",
    "# # 4    SVR_fe  engineered  3068.524237  6130.420383  0.603579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "38c85903-3f12-47d1-b9f5-dcd21a7c031d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO the following markdown and reasoning for hyperparameter tuning has to be adjusted regarding new insights (e.g. ET is not underperforming anymore) ~J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffadcd30-bf06-4504-a3f5-a6c0e43dbd8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After a first run comparing the original feature pipeline and the engineered feature pipeline for all models, we decided to focus on RandomForest and HistGradientBoost. \n",
    "\n",
    "They seem to have the best prediction performance for now. StackingRegressor currently performs best, but since it is blending existing models, we will focus on that and reevaluate in the end.\n",
    "\n",
    "With ExtraTrees and SVR really underperforming, we decide not to do Hyperparameter Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Hyperparameter Tuning and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "72e4dab9-59b7-427c-99a2-9e2ca3e57083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to use it here and potentially use it later for a final hyperparameter tuning after feature selection again\n",
    "def model_hyperparameter_tuning(pipeline, param_dist, n_iter=100, splits=5):\n",
    "    \n",
    "    cv = KFold(n_splits=splits, shuffle=True, random_state=42) # 5 folds for more robust estimation\n",
    "\n",
    "    # Randomized search setup\n",
    "    model_random = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,                      # number of different hyperparameter combinations that will be randomly sampled and evaluated (more iterations = more thorough search but longer runtime)\n",
    "        scoring={\n",
    "            'mae': 'neg_mean_absolute_error',\n",
    "            'mse': 'neg_mean_squared_error',\n",
    "            'r2': 'r2'\n",
    "        },\n",
    "        refit='mae', # Refit the best model based on MAE on the whole training set\n",
    "        cv=cv,\n",
    "        n_jobs=-2,\n",
    "        random_state=42,\n",
    "        verbose=3,\n",
    "    )\n",
    "\n",
    "    # Fit the search\n",
    "    model_random.fit(X_train, y_train)\n",
    "\n",
    "    mae = -model_random.cv_results_['mean_test_mae'][model_random.best_index_]\n",
    "    mse = -model_random.cv_results_['mean_test_mse'][model_random.best_index_]\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = model_random.cv_results_['mean_test_r2'][model_random.best_index_]\n",
    "\n",
    "    print(\"Model Results (CV metrics):\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(\"Best Model params:\", model_random.best_params_)\n",
    "\n",
    "    return model_random.best_estimator_, model_random # return the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c880d2d-c7aa-478f-8d34-90aa6e33a172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.1 ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "18042056-7ef9-489f-afc3-958fe02fbfc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ElasticNet"
    }
   },
   "outputs": [],
   "source": [
    "# TODO this cell is commented because we dont evaluate elasticnet for final performance (save time)\n",
    "# # Hyperparameter Tuning: ElasticNet\n",
    "\n",
    "# elastic_param_grid = {\n",
    "#     \"model__alpha\": [0.001],    # also tried 0.01, 0.05, 0.1, 0.5\n",
    "#     \"model__l1_ratio\": [0.9]    # also tried 0.1, 0.3, 0.5, 0.7  \n",
    "# }\n",
    "\n",
    "# # CV: Calculate all metrics but use MAE for selecting best model\n",
    "# elastic_grid = GridSearchCV(\n",
    "#     elastic_pipe_fe, \n",
    "#     param_grid=elastic_param_grid,\n",
    "#     cv=5,\n",
    "#     scoring={\n",
    "#         'mae': 'neg_mean_absolute_error',\n",
    "#         'mse': 'neg_mean_squared_error',\n",
    "#         'r2': 'r2'\n",
    "#     },\n",
    "#     refit='mae', # Refit the best model based on MAE on the whole training set\n",
    "#     n_jobs=-2,\n",
    "#     verbose=3,\n",
    "#     return_train_score=False\n",
    "# )\n",
    "# elastic_grid.fit(X_train, y_train)\n",
    "\n",
    "# # Get mean metrics across folds\n",
    "# mae = -elastic_grid.cv_results_['mean_test_mae'][elastic_grid.best_index_]\n",
    "# mse = -elastic_grid.cv_results_['mean_test_mse'][elastic_grid.best_index_]\n",
    "# rmse = np.sqrt(mse)\n",
    "# r2 = elastic_grid.cv_results_['mean_test_r2'][elastic_grid.best_index_]\n",
    "# print(\"ElasticNet Results (CV on entire train set):\")\n",
    "# print(f\"MAE: {mae:.4f}\")\n",
    "# print(f\"RMSE: {rmse:.4f}\")\n",
    "# print(f\"R²: {r2:.4f}\")\n",
    "# print(\"Best ElasticNet params:\", elastic_grid.best_params_)\n",
    "\n",
    "# elastic_best = elastic_grid.best_estimator_ # Final model trained on entire training set with best hyperparameters minimizing MAE\n",
    "\n",
    "# # Long Duration (Before removal of OHE-categoricals interrupted kernel after 64mins VS after removal ca 1min -> now 15secs with njobs=-2)\n",
    "\n",
    "# # ElasticNet Results: \n",
    "# # MAE: 2353.9112 | RMSE: 13356867.7860 | R2: 0.8534\n",
    "# # Best ElasticNet params: {'model__alpha': 0.001, 'model__l1_ratio': 0.9}\n",
    "\n",
    "# # MAE: 2589.6100\n",
    "# # RMSE: 4104.4515\n",
    "# # R²: 0.8222\n",
    "# # Best ElasticNet params: {'model__alpha': 0.001, 'model__l1_ratio': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8affc096-36be-4561-8aaf-e0c642a48684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # TODO this cell is commented because of time constraints\n",
    "# # Use GridSearchCV for features_to_select\n",
    "# # Base model: tuned ElasticNet from above\n",
    "# en_base = clone(elastic_best.named_steps[\"model\"])\n",
    "\n",
    "# # Pipeline: clean preprocessing -> RFE -> model\n",
    "# rfe_pipe_linear = Pipeline([\n",
    "#     (\"preprocess\", preprocessor_fe_clean),\n",
    "#     (\"rfe\", RFE(\n",
    "#         estimator=en_base,\n",
    "#         step=0.5,               # drop ~20% per iteration\n",
    "#         importance_getter=\"auto\"\n",
    "#     )),\n",
    "#     (\"model\", clone(en_base))\n",
    "# ])\n",
    "\n",
    "# # Try a few target feature counts (adjust as needed)\n",
    "# number_of_all_features = preprocessor_fe_clean.transform(X_train).shape[1]\n",
    "# rfe_param_grid = {\n",
    "#     \"rfe__n_features_to_select\": [int(number_of_all_features*0.5)]# , int(number_of_all_features*1)] # use only these two extremes to save time ~J\n",
    "# }\n",
    "\n",
    "# rfe_grid = GridSearchCV(\n",
    "#     rfe_pipe_linear,\n",
    "#     param_grid=rfe_param_grid,\n",
    "#     cv=2,\n",
    "#     scoring=\"neg_mean_absolute_error\",\n",
    "#     n_jobs=-1,\n",
    "#     verbose=3,\n",
    "#     return_train_score=False,\n",
    "# )\n",
    "\n",
    "# rfe_grid.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best n_features_to_select:\", rfe_grid.best_params_[\"rfe__n_features_to_select\"])\n",
    "# print(\"MAE (CV):\", -rfe_grid.best_score_)\n",
    "# rfe_best = rfe_grid.best_estimator_\n",
    "\n",
    "# # list kept features\n",
    "# best_rfe = rfe_best.named_steps[\"rfe\"]\n",
    "# all_feats = rfe_best.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "# kept = [f for f, keep in zip(all_feats, best_rfe.support_) if keep]\n",
    "# print(\"Kept features:\", kept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21989bd5-8ab1-4d43-a099-3b2f412c94e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reasoning**: We used 100 features as an initial, arbitrary cutoff for feature selection in the ElasticNet model. Preliminary experiments and insights from the EDA (see separate notebook) indicated that tree-based methods are likely to perform better. Therefore, we prioritized feature selection for the tree-based models based on SHAP values.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5a0287-45b6-4022-8d47-cde8cc52469e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.2 HistGradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998dab6d-7494-483c-ab9e-a075b4c75c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hgb_param_dist = {\n",
    "    \"vt__threshold\": [0.0, 0.005, 0.01],\n",
    "    \"model__learning_rate\": uniform(0.01, 0.15),       # samples values\n",
    "    \"model__max_leaf_nodes\": randint(50, 150),         \n",
    "    \"model__min_samples_leaf\": randint(2, 20),         # samples leaf sizes between 2–20\n",
    "    \"model__max_iter\": randint(200, 900),              # tries 200–900 iterations\n",
    "    \"model__l2_regularization\": uniform(0.0, 1.0),      # samples small regularization values\n",
    "    \"model__early_stopping\": [True],\n",
    "    \"model__validation_fraction\": [0.1],\n",
    "    \"model__n_iter_no_change\": [20],\n",
    "    \"model__random_state\":[42]\n",
    "}\n",
    "\n",
    "# optimized the parameter distributions based on previous runs to focus search space\n",
    "hgb_param_dist = {\n",
    "    \"vt__threshold\": [0.005],\n",
    "    \"model__learning_rate\": [0.06923222772633546],\n",
    "    \"model__max_leaf_nodes\": [137],\n",
    "    \"model__min_samples_leaf\": [12],\n",
    "    \"model__max_iter\": [847],\n",
    "    \"model__l2_regularization\": [0.4234014807063696],\n",
    "    \"model__early_stopping\": [True],\n",
    "    \"model__validation_fraction\": [0.1],\n",
    "    \"model__n_iter_no_change\": [20],\n",
    "    \"model__random_state\":[42]\n",
    "}\n",
    "\n",
    "hgb_best = model_hyperparameter_tuning(hgb_pipe_fe, hgb_param_dist\n",
    "    # , n_iter=3, \n",
    "    # splits=5\n",
    ") \n",
    "\n",
    "# Old preset hps (1min):\n",
    "# MAE: 1289.7294\n",
    "# RMSE: 2185.5006\n",
    "# R²: 0.9498\n",
    "\n",
    "# Reapplying RandomizedSearchCV (40mins):\n",
    "# MAE: 1289.6713\n",
    "# RMSE: 2181.5766\n",
    "# R²: 0.9500\n",
    "# Best Model params: {'model__early_stopping': True, 'model__l2_regularization': np.float64(0.14092422497476265), 'model__learning_rate': np.float64(0.08219772826786356), 'model__max_iter': 464, 'model__max_leaf_nodes': 108, 'model__min_samples_leaf': 8, 'model__n_iter_no_change': 20, 'model__random_state': 42, 'model__validation_fraction': 0.1}\n",
    "\n",
    "# Using transmission and fuelType as OHE instead of TE (10mins):\n",
    "# MAE: 1283.2876\n",
    "# RMSE: 2184.9071\n",
    "# R²: 0.9499\n",
    "# Best Model params: {'model__early_stopping': True, 'model__l2_regularization': np.float64(0.4234014807063696), 'model__learning_rate': np.float64(0.06923222772633546), 'model__max_iter': 847, 'model__max_leaf_nodes': 137, 'model__min_samples_leaf': 12, 'model__n_iter_no_change': 20, 'model__random_state': 42, 'model__validation_fraction': 0.1}\n",
    "\n",
    "# Removed manual TE to prevent data leakage (30sek):\n",
    "# MAE: 1264.2084\n",
    "# RMSE: 2152.0902\n",
    "# R²: 0.9513\n",
    "# [fixed] Best Model params: {'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 12, 'model__max_leaf_nodes': 137, 'model__max_iter': 847, 'model__learning_rate': 0.06923222772633546, 'model__l2_regularization': 0.4234014807063696, 'model__early_stopping': True}\n",
    "\n",
    "# Fix (remove certain) fillnas in feature_engineering:\n",
    "# MAE: 1259.7342\n",
    "# RMSE: 2153.3145\n",
    "# R²: 0.9513\n",
    "# [fixed] Best Model params: {'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 12, 'model__max_leaf_nodes': 137, 'model__max_iter': 847, 'model__learning_rate': 0.06923222772633546, 'model__l2_regularization': 0.4234014807063696, 'model__early_stopping': True}\n",
    "\n",
    "# Added sklearn targetencoder in pipeline:\n",
    "# MAE: 1275.7318\n",
    "# RMSE: 2171.0317\n",
    "# R²: 0.9505\n",
    "# Best Model params: {'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 12, 'model__max_leaf_nodes': 137, 'model__max_iter': 847, 'model__learning_rate': 0.06923222772633546, 'model__l2_regularization': 0.4234014807063696, 'model__early_stopping': True}\n",
    "\n",
    "# Scale target encoded features:\n",
    "# MAE: 1275.7318\n",
    "# RMSE: 2171.0317\n",
    "# R²: 0.9505\n",
    "# Best Model params: {'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 12, 'model__max_leaf_nodes': 137, 'model__max_iter': 847, 'model__learning_rate': 0.06923222772633546, 'model__l2_regularization': 0.4234014807063696, 'model__early_stopping': True}\n",
    "\n",
    "\n",
    "# Removed anchor in anchor features (no division by overall mean):\n",
    "# MAE: 1275.7318\n",
    "# RMSE: 2171.0317\n",
    "# R²: 0.9505\n",
    "# Best Model params: {'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 12, 'model__max_leaf_nodes': 137, 'model__max_iter': 847, 'model__learning_rate': 0.06923222772633546, 'model__l2_regularization': 0.4234014807063696, 'model__early_stopping': True}\n",
    "# ==> Tree-based models do not need normalized features, so this is expected\n",
    "\n",
    "# Fixed leakage in med_price_anchor (smoothing):\n",
    "# MAE: 1268.8764\n",
    "# RMSE: 2160.6397\n",
    "# R²: 0.9510\n",
    "# Best Model params: {'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 12, 'model__max_leaf_nodes': 137, 'model__max_iter': 847, 'model__learning_rate': 0.06923222772633546, 'model__l2_regularization': 0.4234014807063696, 'model__early_stopping': True}\n",
    "\n",
    "# Use only mean te for 'Brand' and 'model' instead of mean and median te:\n",
    "# MAE: 1273.0038\n",
    "# RMSE: 2182.3061\n",
    "# R²: 0.9500\n",
    "# Best Model params: {'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 12, 'model__max_leaf_nodes': 137, 'model__max_iter': 847, 'model__learning_rate': 0.06923222772633546, 'model__l2_regularization': 0.4234014807063696, 'model__early_stopping': True}\n",
    "\n",
    "# Use only median te for 'Brand' and 'model' instead of mean te:\n",
    "# MAE: 1256.4969\n",
    "# RMSE: 2134.2390\n",
    "# R²: 0.9521\n",
    "# Best Model params: {'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 12, 'model__max_leaf_nodes': 137, 'model__max_iter': 847, 'model__learning_rate': 0.06923222772633546, 'model__l2_regularization': 0.4234014807063696, 'model__early_stopping': True}\n",
    "\n",
    "\n",
    "# Use cv=10 in TargetEncoder instead of default 5:\n",
    "\n",
    "# Use GroupMedianImputer for categorical_transformer_fe_ohe and categorical_transformer_fe_te instead of SimpleImputer ():\n",
    " \n",
    "# --> \n",
    "\n",
    "# categorical_features = [\"transmission\", \"fuelType\",\"Brand\", \"model\"]\n",
    "# Model Results (CV metrics):\n",
    "# MAE: 1257.7883\n",
    "# RMSE: 2154.7975\n",
    "# R²: 0.9513\n",
    "\n",
    "# categorical_features = [\"transmission\", \"fuelType\"]\n",
    "# MAE: 1262.7020\n",
    "# RMSE: 2148.1356\n",
    "# R²: 0.9515\n",
    "# Best Model params: {'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 12, 'model__max_leaf_nodes': 137, 'model__max_iter': 847, 'model__learning_rate': 0.06923222772633546, 'model__l2_regularization': 0.4234014807063696, 'model__early_stopping': True}\n",
    "\n",
    "#Drop Features with correlation inbetween features (>0.9): \n",
    "# MAE: 1293.6773\n",
    "# RMSE: 2206.5226\n",
    "# R²: 0.9488\n",
    "\n",
    "# vt__threshold in pipeline\n",
    "# MAE: 1260.7132\n",
    "# RMSE: 2151.5396\n",
    "# R²: 0.9514\n",
    "# Best Model params: {'vt__threshold': 0.005, 'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 12, 'model__max_leaf_nodes': 137, 'model__max_iter': 847, 'model__learning_rate': 0.06923222772633546, 'model__l2_regularization': 0.4234014807063696, 'model__early_stopping': True}\n",
    "\n",
    "# added imputer to QuantileEncoder pipeline step (performance was like this before so the minimal change comes from differences in Jans setup compared to Elias):\n",
    "# MAE: 1259.5270\n",
    "# RMSE: 2148.1023\n",
    "# R²: 0.9515\n",
    "# Best Model params: {'vt__threshold': 0.005, 'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 12, 'model__max_leaf_nodes': 137, 'model__max_iter': 847, 'model__learning_rate': 0.06923222772633546, 'model__l2_regularization': 0.4234014807063696, 'model__early_stopping': True}\n",
    "\n",
    "# Use only median imputer\n",
    "# MAE: 1256.7489\n",
    "# RMSE: 2132.2187\n",
    "# R²: 0.9522\n",
    "# Best Model params: {'vt__threshold': 0.005, 'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 12, 'model__max_leaf_nodes': 137, 'model__max_iter': 847, 'model__learning_rate': 0.06923222772633546, 'model__l2_regularization': 0.4234014807063696, 'model__early_stopping': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70de2673-0443-4cc4-b1cd-f3813a969246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.3 RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "# Old parameter distribution\n",
    "rf_param_dist = {\n",
    "    \"vt__threshold\": [0.0, 0.005, 0.01],\n",
    "    \"model__n_estimators\": randint(200, 600),        # number of trees\n",
    "    \"model__max_depth\": randint(5, 40),              # depth of each tree\n",
    "    \"model__min_samples_split\": randint(2, 10),      # min samples to split an internal node\n",
    "    \"model__min_samples_leaf\": randint(1, 8),        # min samples per leaf\n",
    "    \"model__max_features\": [\"sqrt\"],           # feature sampling strategy (sqrt performed better than log2 and None in previous tests)\n",
    "    \"model__bootstrap\": [False]                      # use bootstrapping or not (False performed better than True in previous tests)\n",
    "}\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "# RF param grid incl. VarianceThreshold\n",
    "rf_param_dist = {\n",
    "    \"vt__threshold\": [0.005],\n",
    "    \"model__n_estimators\": [328],\n",
    "    \"model__max_depth\": [20],\n",
    "    \"model__min_samples_split\": [5],\n",
    "    \"model__min_samples_leaf\": [1],\n",
    "    \"model__max_features\": [\"sqrt\"],\n",
    "    \"model__bootstrap\": [False],\n",
    "}\n",
    "\n",
    "rf_best_rand = model_hyperparameter_tuning(rf_pipe_fe, rf_param_dist)\n",
    "\n",
    "# joblib.dump(rf_best_rand, \"rf_best_rand.pkl\")\n",
    "\n",
    "\n",
    "# Long Duration (~2min)\n",
    "\n",
    "# MAE: 1275.1518\n",
    "# RMSE: 2232.9070\n",
    "# R²: 0.9477\n",
    "\n",
    "# Reapplying RandomizedSearchCV (~120mins):\n",
    "# MAE: 1272.4144\n",
    "# RMSE: 2214.9228\n",
    "# R²: 0.9486\n",
    "# Best Model params: {'model__bootstrap': False, 'model__max_depth': 27, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 5, 'model__n_estimators': 328}\n",
    "\n",
    "# Using transmission and fuelType as OHE instead of TE (140mins):\n",
    "# Model Results (CV metrics):\n",
    "# MAE: 1271.8784\n",
    "# RMSE: 2223.6243\n",
    "# R²: 0.9482\n",
    "# Best Model params: {'model__bootstrap': False, 'model__max_depth': 20, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 5, 'model__n_estimators': 386}\n",
    "\n",
    "# Removed manual TE to prevent data leakage (1min):\n",
    "# MAE: 1270.0122\n",
    "# RMSE: 2203.1101\n",
    "# R²: 0.9491\n",
    "# [fixed] Best Model params: {'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "\n",
    "# # Fix (remove certain) fillnas in feature_engineering (1min):\n",
    "# MAE: 1267.4191\n",
    "# RMSE: 2199.6899\n",
    "# R²: 0.9492\n",
    "# [fixed] Best Model params: {'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "\n",
    "# Added sklearn targetencoder in pipeline:\n",
    "# MAE: 1254.2265\n",
    "# RMSE: 2185.9107\n",
    "# R²: 0.9499\n",
    "# Best Model params: {'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "\n",
    "# Scale target encoded features:\n",
    "# MAE: 1254.2265\n",
    "# RMSE: 2185.9107\n",
    "# R²: 0.9499\n",
    "# Best Model params: {'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "\n",
    "# Removed anchor in anchor features (no division by overall mean):\n",
    "# MAE: 1254.2265\n",
    "# RMSE: 2185.9107\n",
    "# R²: 0.9499\n",
    "# Best Model params: {'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "# ==> Tree-based models do not need normalized features, so this is expected\n",
    "\n",
    "# Fixed leakage in med_price_anchor (smoothing):\n",
    "# MAE: 1249.7952\n",
    "# RMSE: 2185.0019\n",
    "# R²: 0.9500\n",
    "# Best Model params: {'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "\n",
    "# Use only mean te for 'Brand' and 'model' instead of mean and median te:\n",
    "# MAE: 1262.8323\n",
    "# RMSE: 2215.5205\n",
    "# R²: 0.9486\n",
    "# Best Model params: {'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "\n",
    "# Use only median te for 'Brand' and 'model' instead of mean te:\n",
    "# MAE: 1262.6739\n",
    "# RMSE: 2196.6384\n",
    "# R²: 0.9494\n",
    "# Best Model params: {'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "\n",
    "# Use GroupMedianImputer for categorical_transformer_fe_ohe and categorical_transformer_fe_te instead of SimpleImputer ():\n",
    "# --> \n",
    "\n",
    "# categorical_features = [\"transmission\", \"fuelType\",\"Brand\", \"model\"]\n",
    "# Model Results (CV metrics):\n",
    "# MAE: 1301.7759\n",
    "# RMSE: 2273.5359\n",
    "# R²: 0.9458\n",
    "\n",
    "# categorical_features = [\"transmission\", \"fuelType\"]\n",
    "# MAE: 1249.6696\n",
    "# RMSE: 2184.4328\n",
    "# R²: 0.9500\n",
    "# Best Model params: {'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "\n",
    "#Drop Features with correlation inbetween features (>0.9): \n",
    "# MAE: 1376.7839\n",
    "# RMSE: 2399.4670\n",
    "# R²: 0.9396\n",
    "\n",
    "# vt__threshold in pipeline\n",
    "# MAE: 1248.3166\n",
    "# RMSE: 2179.9166\n",
    "# R²: 0.9502\n",
    "# Best Model params: {'vt__threshold': 0.005, 'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "\n",
    "# added imputer to QuantileEncoder pipeline step\n",
    "# MAE: 1248.3166\n",
    "# RMSE: 2179.9166\n",
    "# R²: 0.9502\n",
    "# Best Model params: {'vt__threshold': 0.005, 'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "\n",
    "# Use only median imputer\n",
    "# Model Results (CV metrics):\n",
    "# MAE: 1265.6359\n",
    "# RMSE: 2206.0571\n",
    "# R²: 0.9490\n",
    "# Best Model params: {'vt__threshold': 0.005, 'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d0fce47e-39a6-480f-979f-7f91422734dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipe = rf_best_rand if hasattr(rf_best_rand, \"named_steps\") else rf_best_rand[0]\n",
    "pre = pipe.named_steps[\"preprocess\"]\n",
    "vt = pipe.named_steps.get(\"vt\")\n",
    "\n",
    "# names after preprocessing\n",
    "feat_names = get_feature_names_from_preprocessor(ct)\n",
    "\n",
    "# apply VT mask if present\n",
    "if vt is not None:\n",
    "    mask = vt.get_support()\n",
    "    feat_names = np.array(feat_names)[mask]\n",
    "\n",
    "importances = pipe.named_steps[\"model\"].feature_importances_\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {\"feature\": feat_names, \"importance\": importances}\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for _, row in feature_importance_df.iterrows():\n",
    "    print(f\"{row['feature']:30s}: {row['importance']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e367455e-0022-4cc0-96bd-e5dd21edb104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#stop here # Fail the code here because we dont use SHAP values and dont need stackingregressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6d3b92-efc6-4d8a-b3e1-cb6aff36b84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.4 StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "37c25558-2e97-4565-9a57-8a46800b59ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Old parameter distribution\n",
    "stack_param_dist = {\n",
    "    \"final_estimator__learning_rate\": uniform(0.02, 0.1),\n",
    "    \"final_estimator__max_depth\": randint(3, 10),\n",
    "    \"final_estimator__min_samples_leaf\": randint(3, 20),\n",
    "    \"final_estimator__l2_regularization\": uniform(0.0, 1.0),\n",
    "}\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "stack_param_dist = {\n",
    "    \"final_estimator__learning_rate\": [0.061135390505667866],\n",
    "    \"final_estimator__max_depth\": [5],\n",
    "    \"final_estimator__min_samples_leaf\": [10],\n",
    "    \"final_estimator__l2_regularization\": [0.19438003399487302]\n",
    "}\n",
    "\n",
    "stack_best = model_hyperparameter_tuning(stack_pipe_fe, stack_param_dist, splits=3)\n",
    "# joblib.dump(stack_best, \"stack_best.pkl\")\n",
    "\n",
    "\n",
    "# Long Duration (~3mins)\n",
    "\n",
    "# MAE: 1351.8682\n",
    "# RMSE: 2498.2822\n",
    "# R²: 0.9342\n",
    "\n",
    "# After RandomizedSearchCV:\n",
    "# MAE: 1350.4717\n",
    "# RMSE: 2497.0474\n",
    "# R²: 0.9343\n",
    "# Best Model params: {'final_estimator__l2_regularization': np.float64(0.978892858275009), 'final_estimator__learning_rate': np.float64(0.06867421529594551), 'final_estimator__max_depth': 6, 'final_estimator__min_samples_leaf': 13}\n",
    "\n",
    "# Removed ElasticNet from stacking due to poor performance compared to RF and HGB alone\n",
    "# canceled but the cv scores didnt seem to show much improvement\n",
    "\n",
    "# Using transmission and fuelType as OHE instead of TE():\n",
    "# MAE: 1357.4291\n",
    "# RMSE: 2516.5470\n",
    "# R²: 0.9333\n",
    "# Best Model params: {'final_estimator__l2_regularization': np.float64(0.19438003399487302), 'final_estimator__learning_rate': np.float64(0.061135390505667866), 'final_estimator__max_depth': 5, 'final_estimator__min_samples_leaf': 10}\n",
    "\n",
    "\n",
    "# Removed fillna(0) in feature engineering for a_x_b and model_freq():\n",
    "# was worse for hgb and rf so not tested for stacking\n",
    "\n",
    "# ...\n",
    "\n",
    "# implemented GroupModeImputer\n",
    "# MAE: 1329.2379\n",
    "# RMSE: 2453.0239\n",
    "# R²: 0.9366\n",
    "# Best Model params: {'final_estimator__min_samples_leaf': 10, 'final_estimator__max_depth': 5, 'final_estimator__learning_rate': 0.061135390505667866, 'final_estimator__l2_regularization': 0.19438003399487302}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852bd7bc-33d9-4ae4-a552-0850b200be11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Feature Importance of Tree Models (with SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7c02463-def6-4c5c-93ce-5ca710b6eb7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  **Problem:** Current feature selection targets linear models\n",
    "  (ElasticNet), but we primarily use tree-based models (HGB,\n",
    "  RandomForest).\n",
    "\n",
    "  **Solution:** Use SHAP (SHapley Additive exPlanations) to\n",
    "  identify feature importance specifically for tree models\n",
    "\n",
    "  **Why SHAP for Trees:**\n",
    "  - Provides exact feature importance values for tree-based\n",
    "  models\n",
    "  - Tree models handle irrelevant features, but noise features\n",
    "  still impact performance\n",
    "  - Enables data-driven selection rather than statistical filter\n",
    "  methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "098e6e11-41b8-4f89-8fdd-ea92b7c5d10b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO probably use CV for SHAP\n",
    "def calculate_shap_values(pipeline, X, sample_size=1000, seed=42, label=None):\n",
    "    # accept either pipeline or (pipeline, search_obj) # TODO check what the parameter is and dont use if isinstance...\n",
    "    pipe = pipeline[0] if isinstance(pipeline, tuple) else pipeline\n",
    "\n",
    "    pre = pipe.named_steps[\"preprocess\"]\n",
    "    vt = pipe.named_steps.get(\"vt\")\n",
    "    model = pipe.named_steps[\"model\"]\n",
    "\n",
    "    X_proc = pre.transform(X)\n",
    "    feat_names = pre.get_feature_names_out()\n",
    "\n",
    "    # apply VT mask if present\n",
    "    if vt is not None:\n",
    "        mask = vt.get_support()\n",
    "        X_proc = vt.transform(X_proc)\n",
    "        feat_names = np.array(feat_names)[mask]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = min(sample_size, len(X_proc))\n",
    "    idx = rng.choice(len(X_proc), n, replace=False)\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_vals = explainer.shap_values(X_proc[idx])\n",
    "    importance = np.abs(shap_vals).mean(axis=0)\n",
    "\n",
    "    shap_df = (pd.DataFrame({\"feature\": feat_names, \"importance\": importance})\n",
    "               .sort_values(\"importance\", ascending=False)\n",
    "               .reset_index(drop=True))\n",
    "\n",
    "    tag = label or model.__class__.__name__\n",
    "    print(f\"Most important features ({tag}):\")\n",
    "    print(shap_df.head(20).to_string(index=False))\n",
    "    return shap_df, feat_names, X_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2fb5e74c-d256-463f-92b2-c702790160ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_model_on_best_features(shap_importance, pipeline, model, X, y,\n",
    "                                    range_number_of_features, folds=5, seed=42):\n",
    "    \"\"\"\n",
    "    Select top-N features by SHAP, evaluate MAE via CV, and return best estimator and feature list.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    pre = pipeline.named_steps[\"preprocess\"]\n",
    "    vt = pipeline.named_steps.get(\"vt\")\n",
    "\n",
    "    # preprocess once on full data\n",
    "    X_proc = pre.transform(X)\n",
    "    feat_names = pre.get_feature_names_out()\n",
    "    if vt is not None:\n",
    "        mask = vt.get_support()\n",
    "        X_proc = vt.transform(X_proc)\n",
    "        feat_names = np.array(feat_names)[mask]\n",
    "\n",
    "    # helper to extract column indices for top-N features\n",
    "    def select_cols(n):\n",
    "        top_feats = shap_importance.head(n)[\"feature\"].tolist()\n",
    "        return [i for i, fname in enumerate(feat_names) if fname in top_feats]\n",
    "\n",
    "    results = []\n",
    "    for n in range_number_of_features:\n",
    "        idx = select_cols(n)\n",
    "        mae_folds = []\n",
    "        for train_idx, val_idx in kf.split(X_proc):\n",
    "            X_tr, X_val = X_proc[train_idx][:, idx], X_proc[val_idx][:, idx]\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            est = clone(model)\n",
    "            est.fit(X_tr, y_tr)\n",
    "            mae_folds.append(mean_absolute_error(y_val, est.predict(X_val)))\n",
    "        results.append({\"n\": n, \"mae\": np.mean(mae_folds), \"idx\": idx})\n",
    "\n",
    "    best = min(results, key=lambda r: r[\"mae\"])\n",
    "    best_features = [feat_names[i] for i in best[\"idx\"]]\n",
    "\n",
    "    # fit final estimator on full data restricted to best features\n",
    "    final_est = clone(model)\n",
    "    final_est.fit(X_proc[:, best[\"idx\"]], y)\n",
    "\n",
    "    print(\"CV MAE by feature count:\")\n",
    "    for r in results:\n",
    "        print(f\"  n={r['n']:3d} | MAE={r['mae']:.2f}\")\n",
    "    print(f\"Best: n={best['n']} | MAE={best['mae']:.2f}\")\n",
    "\n",
    "    return final_est, best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b090388-f49d-4c7e-9a79-e9881c577c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Old Version\n",
    "# # General function which can be called by the models to avoid redundant code and enable easy maintenance\n",
    "# def train_model_on_best_features(baseline_mae, shap_importance, model, X_train_processed, X_val_processed, range_number_of_features, feature_names_all):\n",
    "#     '''\n",
    "#     We systematically test different numbers of top features to find the optimal subset:\n",
    "#     We train the model with the same optimized hyperparameters but using only the most important features identified by SHAP\n",
    "#     '''\n",
    "#     # Track best model\n",
    "#     results = []\n",
    "#     best_model = None\n",
    "#     best_mae = float(\"inf\")\n",
    "#     best_n = None\n",
    "#     best_features = None\n",
    "\n",
    "#     # Find best feature counts\n",
    "#     for n_features in range_number_of_features:\n",
    "#         # Select top N features\n",
    "#         top_features = shap_importance.head(n_features)[\"feature\"].tolist()\n",
    "#         feature_indices = [i for i, fname in enumerate(feature_names_all) if fname in top_features]\n",
    "\n",
    "#         X_train_subset = X_train_processed[:, feature_indices]\n",
    "#         X_val_subset   = X_val_processed[:, feature_indices]\n",
    "\n",
    "#         # Train and predict using selected amount of features (model uses tuned hyperparams)\n",
    "#         model.fit(X_train_subset, y_train)\n",
    "#         pred_subset = model.predict(X_val_subset)\n",
    "#         mae_subset = mean_absolute_error(y_val, pred_subset)\n",
    "#         results.append({\"n_features\": n_features, \"mae\": mae_subset})\n",
    "\n",
    "#         # Check whether current mae is best so far\n",
    "#         if mae_subset < best_mae:\n",
    "#             best_mae = mae_subset\n",
    "#             best_n = n_features\n",
    "#             best_model = model\n",
    "#             best_features = top_features\n",
    "\n",
    "#         # Print MAE for each amount of features\n",
    "#         if n_features in range_number_of_features:\n",
    "#             improvement_rf = baseline_mae - mae_subset\n",
    "#             print(f\"Top {n_features:3d} features: MAE: {mae_subset:.1f} (Δ: {improvement_rf:+.1f})\")\n",
    "\n",
    "\n",
    "#     print(f\"\\nOptimal feature selection results:\")\n",
    "#     print(f\"Best performance with {best_n} features: MAE: {best_mae:.2f}\")\n",
    "#     print(f\"Improvement over baseline: {baseline_mae - best_mae:+.2f} MAE\\n\")\n",
    "\n",
    "#     print(f\"Optimal {best_n} features for production model:\")\n",
    "#     for i, feat in enumerate(best_features, start=1):\n",
    "#         imp = shap_importance.loc[shap_importance['feature'] == feat, 'importance'].values[0]\n",
    "#         print(f\"{i:2d}. {feat:25s} ({imp:.1f})\")\n",
    "    \n",
    "#     # Retrain a fresh final estimator on the full training set restricted to best_features (guarantees correct input dimension)\n",
    "#     selected_idx = [i for i, fname in enumerate(feature_names_all) if fname in best_features]\n",
    "#     final_est = clone(model)\n",
    "#     final_est.fit(X_train_processed[:, selected_idx], y_train)\n",
    "\n",
    "#     return final_est, best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dac80f70-0890-45e0-b87f-33274924d2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_top_shap(shap_df, model_name, top_k=20):\n",
    "    top_df = shap_df.head(top_k).iloc[::-1]\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.barh(top_df[\"feature\"], top_df[\"importance\"], color=\"#4C72B0\")\n",
    "    ax.set_xlabel(\"Average |SHAP| value\")\n",
    "    ax.set_title(f\"Top {top_k} {model_name} features by SHAP\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7de8112-ad4c-4761-a868-9ed084f86174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.1 HGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cadbb5c-a5f6-4fb1-963c-70ff79b146f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 1: Baseline Performance with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c7eba00-aa0e-4b66-9163-e03f1ec2b234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# unpack tuple from model_hyperparameter_tuning\n",
    "hgb_pipe, hgb_search = hgb_best  # hgb_pipe is the fitted pipeline\n",
    "\n",
    "# CV metrics at best params\n",
    "idx = hgb_search.best_index_\n",
    "mae_cv  = -hgb_search.cv_results_['mean_test_mae'][idx]\n",
    "rmse_cv = np.sqrt(-hgb_search.cv_results_['mean_test_mse'][idx])\n",
    "r2_cv   = hgb_search.cv_results_['mean_test_r2'][idx]\n",
    "\n",
    "# feature count after preprocess (+ vt)\n",
    "X_proc = hgb_pipe.named_steps[\"preprocess\"].transform(X_train)\n",
    "vt = hgb_pipe.named_steps.get(\"vt\")\n",
    "if vt is not None:\n",
    "    X_proc = vt.transform(X_proc)\n",
    "n_features_total = X_proc.shape[1]\n",
    "\n",
    "print(\"Baseline Performance of HGB (CV on train):\")\n",
    "print(f\"MAE:  {mae_cv:.4f}\")\n",
    "print(f\"RMSE: {rmse_cv:.4f}\")\n",
    "print(f\"R²:   {r2_cv:.4f}\")\n",
    "print(f\"Total features used: {n_features_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a1844a-48ae-4a25-8f9a-5759a7d13a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Old Version\n",
    "# X_val_processed_hgb = hgb_best.named_steps[\"preprocess\"].transform(X_val)\n",
    "# hgb_val_pred = hgb_best.named_steps[\"model\"].predict(X_val_processed_hgb)\n",
    "# n_features_total = X_val_processed_hgb.shape[1]\n",
    "# baseline_mae_hgb = mean_absolute_error(y_val, hgb_val_pred)\n",
    "\n",
    "# print(\"Baseline Performance of HGB model after Hyperparameter Tuning:\\n\")\n",
    "# print_metrics(y_val, hgb_val_pred)\n",
    "# print(f\"\\nTotal features used: {n_features_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a381d64-0406-4daa-a147-95081efa4eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 2: SHAP Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd7fcf3-c415-45d6-8fe5-09a53f8621da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap_importance_df_hgb, feature_names_all_hgb, X_train_processed_hgb = calculate_shap_values(\n",
    "    hgb_best[0],  # the fitted pipeline\n",
    "    X_train,\n",
    "    sample_size=1000,\n",
    "    seed=42,\n",
    "    label=\"HGB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa44b8cb-0c89-4054-bbaf-38250ce56ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Old Version\n",
    "# #function\n",
    "# shap_importance_df_hgb, feature_names_all_hgb, X_train_processed_hgb = calculate_shap_values(\n",
    "#     hgb_best, X_train, log_features, numeric_features, categorical_features,\n",
    "#     sample_size=1000, seed=42, label=\"HGB\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df23f64e-fde8-4c8f-bb85-709553f68965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# HGB SHAP bar plot\n",
    "plot_top_shap(shap_importance_df_hgb, \"HGB\", top_k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "355aaca1-92eb-42d9-82d5-e76c3a8b99bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 3: Automated Feature Selection Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d331dd8-d71f-40be-9efc-1e65f51dd98d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# unpack tuned pipeline and base model\n",
    "hgb_pipe = hgb_best[0]  # best_estimator_\n",
    "hgb_model = hgb_pipe.named_steps[\"model\"]\n",
    "\n",
    "# feature count after preprocess (+ vt)\n",
    "X_proc = hgb_pipe.named_steps[\"preprocess\"].transform(X_train)\n",
    "vt = hgb_pipe.named_steps.get(\"vt\")\n",
    "if vt is not None:\n",
    "    X_proc = vt.transform(X_proc)\n",
    "n_features_total = X_proc.shape[1]\n",
    "\n",
    "# denser grid of feature counts (adjust step to your runtime budget)\n",
    "range_number_of_features_hgb = list(range(20, 31))\n",
    "# optionally include full set if not already in the list\n",
    "if n_features_total not in range_number_of_features_hgb:\n",
    "    range_number_of_features_hgb.append(n_features_total)\n",
    "\n",
    "best_model_hgb, best_features_hgb = train_model_on_best_features(\n",
    "    shap_importance_df_hgb,  # from your SHAP importance\n",
    "    hgb_pipe,\n",
    "    hgb_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    range_number_of_features_hgb,\n",
    "    folds=5,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02c55af4-eea0-4be4-bae7-0e103184254c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Old Version\n",
    "# # Define model with the same hyperparams\n",
    "# hgb_model = hgb_best.named_steps[\"model\"]\n",
    "# hgb_selected = HistGradientBoostingRegressor(**hgb_model.get_params())\n",
    "\n",
    "# # Number of top SHAP features to try\n",
    "# range_number_of_features_hgb = range(15, n_features_total + 1, 1) # After previous runs with higher step size, the range is now narrowed down\n",
    "\n",
    "# # Train/evaluate on subsets of top features\n",
    "# best_model_hgb, best_features_hgb = train_model_on_best_features(\n",
    "#     baseline_mae_hgb, shap_importance_df_hgb,\n",
    "#     hgb_selected,\n",
    "#     X_train_processed_hgb, X_val_processed_hgb,\n",
    "#     range_number_of_features_hgb,\n",
    "#     feature_names_all_hgb\n",
    "# )\n",
    "\n",
    "# # Long Duration (ca 2mins)\n",
    "\n",
    "# # start: Best performance with 17 features: MAE: 1288.12\n",
    "# # removed ohe categoricals: Best performance with 19 features: MAE: 1282.81\n",
    "# # After FS removal: Best performance with 19 features (all features bc deselection filtered the same features as SHAP): MAE: 1282.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e893a1a-3c61-4f13-9fd9-35715c65d492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build the final pipeline with feature selection included\n",
    "def select_best_features_hgb(X):\n",
    "    # X is the output of the preprocessing step: Matrix with all features after they have been scaled, encoded, and combined by the preprocessor\n",
    "    idx = [i for i, fname in enumerate(feature_names_all) if fname in best_features_hgb]\n",
    "    return X[:, idx]\n",
    "\n",
    "hgb_final_pipe = Pipeline([\n",
    "    (\"preprocess\", hgb_best.named_steps[\"preprocess\"]),\n",
    "    (\"feature_selector\", FunctionTransformer(select_best_features_hgb, validate=False)), # a flexible wrapper that applies a custom function to the data flow in a pipeline\n",
    "    (\"model\", best_model_hgb)\n",
    "])\n",
    "\n",
    "# Save the best model for later use\n",
    "joblib.dump(hgb_final_pipe, \"hgb_best_feature.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16b675a-660c-46c8-8430-5b28d91b8c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.2 RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65c9142-f110-4d6f-a31c-5764d2492fc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 1: Baseline Performance with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "32681cee-7e6a-45dd-9fc0-3f83e836dc5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# unpack tuned RF pipeline and search object\n",
    "rf_pipe, rf_search = rf_best_rand  # adjust if your variable is named differently, e.g. rf_best_rand\n",
    "\n",
    "# CV metrics at best params\n",
    "idx = rf_search.best_index_\n",
    "mae_cv  = -rf_search.cv_results_['mean_test_mae'][idx]\n",
    "rmse_cv = np.sqrt(-rf_search.cv_results_['mean_test_mse'][idx])\n",
    "r2_cv   = rf_search.cv_results_['mean_test_r2'][idx]\n",
    "\n",
    "# feature count after preprocess (+ vt)\n",
    "X_proc_rf = rf_pipe.named_steps[\"preprocess\"].transform(X_train)\n",
    "vt_rf = rf_pipe.named_steps.get(\"vt\")\n",
    "if vt_rf is not None:\n",
    "    X_proc_rf = vt_rf.transform(X_proc_rf)\n",
    "n_features_total_rf = X_proc_rf.shape[1]\n",
    "\n",
    "print(\"Baseline Performance of RF (CV on train):\")\n",
    "print(f\"MAE:  {mae_cv:.4f}\")\n",
    "print(f\"RMSE: {rmse_cv:.4f}\")\n",
    "print(f\"R²:   {r2_cv:.4f}\")\n",
    "print(f\"Total features used: {n_features_total_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "95adb47f-fb45-47f0-b24d-e75d5ef1c68e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Old Version\n",
    "# # Use the tuned RF pipeline (rf_best_rand) and compute baseline on the validation set\n",
    "# X_val_processed_rf = rf_best_rand.named_steps[\"preprocess\"].transform(X_val)\n",
    "# rf_val_pred = rf_best_rand.named_steps[\"model\"].predict(X_val_processed_rf)\n",
    "# n_features_total_rf = X_val_processed_rf.shape[1] # TODO cant we just use one val_processed and one n_features_total or why did we split that? ~J\n",
    "# baseline_mae_rf = mean_absolute_error(y_val, rf_val_pred)\n",
    "\n",
    "# print(\"Baseline Performance of RF model after Hyperparameter Tuning:\\n\")\n",
    "# print_metrics(y_val, rf_val_pred)\n",
    "# print(f\"\\nTotal features used: {n_features_total_rf}\")\n",
    "\n",
    "# # start: MAE: 1322.4418 | RMSE: 4630733.9922 | R2: 0.9492 (Total features used: 155)\n",
    "# # after removing ohe categoricals: MAE: 1282.0634 | RMSE: 4317505.8622 | R2: 0.9526 (Total features used: 22)\n",
    "# # After FS removal: MAE: 1275.2603 | RMSE: 4274727.2255 | R2: 0.9531 (Total features used: 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef64537-b8f4-4f70-b060-78606e96731b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 2: SHAP Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3d7e7fe7-7711-4088-8c0f-e2fdfa2ca494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap_importance_df_rf, feature_names_all_rf, X_train_processed_rf = calculate_shap_values(\n",
    "    rf_best_rand[0],        # unpacked best RF pipeline\n",
    "    X_train,\n",
    "    sample_size=100,   # adjust if you need more precision\n",
    "    seed=42,\n",
    "    label=\"RF\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121ae3dc-913e-48d4-99ae-a0e471852dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RF SHAP bar plot\n",
    "plot_top_shap(shap_importance_df_rf,  \"RF\",  top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8dfcd356-3197-4fa0-b6bd-1eb3a0e1697a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Old Version\n",
    "# shap_importance_df_rf, feature_names_all_rf, X_train_processed_rf = calculate_shap_values(\n",
    "#     rf_best_rand, X_train, log_features, numeric_features, categorical_features,\n",
    "#     sample_size=100, seed=42, label=\"RF\"\n",
    "# )\n",
    "\n",
    "# # Long Duration (ca 4mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0af8a6-1394-464d-8e8c-d452d0138cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 3: Automated Feature Selection Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c785ae58-8e0a-4a91-8a5b-9a0931066ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# unpack tuned RF pipeline and base model\n",
    "rf_pipe = rf_best_rand[0]              # best_estimator_ from tuning\n",
    "rf_model = rf_pipe.named_steps[\"model\"]\n",
    "\n",
    "# feature count after preprocess (+ vt)\n",
    "X_proc_rf = rf_pipe.named_steps[\"preprocess\"].transform(X_train)\n",
    "vt_rf = rf_pipe.named_steps.get(\"vt\")\n",
    "if vt_rf is not None:\n",
    "    X_proc_rf = vt_rf.transform(X_proc_rf)\n",
    "n_features_total_rf = X_proc_rf.shape[1]\n",
    "\n",
    "# choose candidate feature counts (dense; adjust for runtime)\n",
    "range_number_of_features_rf = list(range(20, 31))\n",
    "if n_features_total_rf not in range_number_of_features_rf:\n",
    "    range_number_of_features_rf.append(n_features_total_rf)\n",
    "\n",
    "# CV-based subset search\n",
    "best_model_rf, best_features_rf = train_model_on_best_features(\n",
    "    shap_importance_df_rf,   # from shap_importance_cv / calculate_shap_values\n",
    "    rf_pipe,\n",
    "    rf_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    range_number_of_features_rf,\n",
    "    folds=5,                 # or 3 if you need it faster\n",
    "    seed=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "da117a79-190d-4a3e-ba7f-86f6e166170d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Old\n",
    "# # Use the same processed validation data and reuse tuned RF hyperparameters\n",
    "# rf_params = {k.replace(\"model__\", \"\"): v for k, v in rf_random.best_params_.items()}\n",
    "# rf_selected = RandomForestRegressor(random_state=42, n_jobs=-1, **rf_params)\n",
    "# range_number_of_features_rf = range(16, n_features_total_rf + 1, 1) # After previous runs with higher step size, the range is now narrowed down\n",
    "\n",
    "# best_model_rf, best_features_rf = train_model_on_best_features(baseline_mae_rf, shap_importance_df_rf, rf_selected, X_train_processed_rf, X_val_processed_rf, range_number_of_features_rf, feature_names_all_rf)\n",
    "\n",
    "# # Long Duration (ca 1min)\n",
    "\n",
    "# # start: Best performance with 26 features: MAE: 1277.16\n",
    "# # removed ohe categoricals: Best performance with 20 features: MAE: 1273.89\n",
    "# # After FS removal: Best performance with 19 features (all features bc deselection filtered the same features as SHAP): MAE: 1275.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e8398f0e-737d-4ae1-8ba1-5f8f21aa211d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the best RF model for later use\n",
    "\n",
    "# Build the final RF pipeline with feature selection included\n",
    "def select_best_features_rf(X):\n",
    "    idx = [i for i, fname in enumerate(feature_names_all_rf) if fname in best_features_rf]\n",
    "    return X[:, idx]\n",
    "\n",
    "final_rf_pipe = Pipeline([\n",
    "    (\"preprocess\", rf_best_rand.named_steps[\"preprocess\"]),\n",
    "    (\"feature_selector\", FunctionTransformer(select_best_features_rf, validate=False)),\n",
    "    (\"model\", best_model_rf)\n",
    "])\n",
    "\n",
    "joblib.dump(final_rf_pipe, \"rf_best_feature.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f248b6e7-a945-45ee-aa5c-5ffd34c00cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.3 Build Final Stacking Regressor to mix tuned and feature selected HGB and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2e4ef23a-6313-43cc-91fc-34e3c7842684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stack_pipe_final = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"hgb_final\", hgb_final_pipe),   # tuned HGB pipeline (preprocessor + model)\n",
    "        (\"rf_final\",  final_rf_pipe),    # tuned RF pipeline (preprocessor + model)\n",
    "    ],\n",
    "    final_estimator=LinearRegression(),  # simple, perfect for 2 base preds\n",
    "    passthrough=False,                   # meta model sees only base predictions\n",
    "    cv=5,                                # proper OOF stacking\n",
    "    n_jobs=1                             # no BrokenProcessPool on Databricks\n",
    ")\n",
    "\n",
    "stack_pipe_final.fit(X_train, y_train)\n",
    "stack_val_pred = stack_pipe_final.predict(X_val)\n",
    "print_metrics(y_val, stack_val_pred)\n",
    "\n",
    "joblib.dump(stack_pipe_final, \"stack_pipe.pkl\")\n",
    "\n",
    "# MAE: 1255.3112 | RMSE: 4157099.9081 | R2: 0.9544\n",
    "\n",
    "# Kaggle Score submit 1274 !! OVERFITTED\n",
    "\n",
    "# Long Duration (ca 3mins)\n",
    "\n",
    "# start: MAE: 1256.5922 | RMSE: 4154147.3742 | R2: 0.9544\n",
    "# removed ohe categoricals: MAE: 1252.2718 | RMSE: 4146270.8422 | R2: 0.9545\n",
    "# After FS removal: MAE: 1252.9558 | RMSE: 4145097.0520 | R2: 0.9545"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f82fb46-4c09-4bd8-b405-7c99d4850eca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Final SR of the tuned HGB and RF models, did improve over the best single HGB and RF models on the validation set (MAE 1258 vs 1281/1289). \n",
    "\n",
    "However, it seems to be overfitted, Kaggle Score is only 1274\n",
    "\n",
    "Therefore we will keep the RF/HGB model => with such small difference in MAE, we further need to evaluate them both + the Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135acfad-1ad6-49f9-97fc-f1a81d0c3187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb68e758-d7b1-4f80-9dc4-ed985f721c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Extra Task (1 Point): Be in the Top 5 Groups on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0cb5aa-be61-4a2e-af5d-0f7e63f604f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict_on_test(model_pipeline, model_name):\n",
    "    # Load best model from Joblib and predict on validation set to verify\n",
    "    pipe_best = joblib.load(model_pipeline)\n",
    "    pred_loaded = pipe_best.predict(X_val)\n",
    "    print(f\"Loaded {model_name}-model MAE on validation set: {mean_absolute_error(y_val, pred_loaded):.2f}\")\n",
    "\n",
    "    # Predict on test set\n",
    "    df_cars_test['price'] = pipe_best.predict(df_cars_test)\n",
    "    df_cars_test['price'].to_csv(f'Group05_{model_name}_Version10.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9d9d9f8a-642e-405d-9de2-b8c802a90919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predict_on_test(\"hgb_best_feature.pkl\", \"HGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3745830-e49d-43f8-8910-09e6bad342d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predict_on_test(\"rf_best_feature.pkl\", \"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f119b3-d572-4c5b-830e-6f396cd167a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predict_on_test(\"stack_pipe.pkl\", \"Stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b18805-e662-4807-bdb8-758df6027d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Submission with API push"
    }
   },
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c cars4you -f Group05_Version05.csv -m \"Message\" # Uncomment to submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350057a3-c771-4bf0-8fac-19b49be17e35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Score Check"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c cars4you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bfd1b4a-0e13-443a-aeb7-fa6324f801e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. Open-Ended-Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a3ac74-bda2-40f7-97f1-ca1574ae7896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 8.1 Counterfactual Sensitivity Maps\n",
    "\n",
    "**a) Objective and motivation (0.5v)**\n",
    "\n",
    "We have a trained model that predicts a car's price from features (age, mileage, engine size, paint quality, mpg, etc.). For a single car, we want to answer:\n",
    "\n",
    "_“What small changes to the car's features would be required to make the model predict a x% higher or lower price?”_ (we tried +-10)\n",
    "\n",
    "That is a counterfactual question: “If this car were different in X ways, would the model value it differently?”\n",
    "\n",
    "Instead of only knowing which features are important overall, this tells  for this exact car what the model would need to see to change its valuation.\n",
    "\n",
    "Examples of use:\n",
    "- If the counterfactual says “increase paintQuality by 5 will increase the price by +10%”, a business could decide whether doing that repair is worth it.\n",
    "- If the model requires impossible changes to reach +10%, the car should be flagged for manual inspection or considered non-upgradeable.\n",
    "\n",
    "------------\n",
    "**Limitations:**\n",
    "- The greedy search is heuristic — it is fast and interpretable but not guaranteed to find the global minimal change. It is designed for practical interpretability rather than mathematical optimality.\n",
    "- Categorical changes (e.g., changing `Brand` or `model`) are **not** attempted by default to avoid generating invalid combinations; can be added later if you provide a realistic whitelist of allowed swaps.\n",
    "- Some input features are physically immutable or economically infeasible to change; results must be interpreted by a domain expert.\n",
    "-----\n",
    "\n",
    "\n",
    "**b) Difficulty of tasks (1v)**\n",
    "\n",
    "Implementing counterfactual sensitivity analysis in this project is non-trivial because:\n",
    "\n",
    "1. **Complex Preprocessing Pipeline:** The model uses hierarchical imputers, log transforms, target encodings, scaling, and many engineered features.\n",
    "All counterfactuals must operate in the raw input space, while still being compatible with the full sklearn pipeline.\n",
    "\n",
    "2. **Non-Differentiable Model:** Our final model is tree-based and does not provide gradients. Therefore, we implemented a greedy local search algorithm: Perturb the most influential numeric features (ranked via permutation importance).\n",
    "Accept only changes that monotonically move the prediction closer to the target.\n",
    "Shrink step sizes adaptively when progress stalls.\n",
    "Enforce realistic value ranges based on training quantiles.\n",
    "\n",
    "3. **Handling Mixed Pandas Dtypes:** The real dataset uses Int64 nullable types, which break when assigning floats. All numeric fields were therefore converted to consistent float64 inputs before search.\n",
    "\n",
    "-----\n",
    "\n",
    "**c) Correctness and efficiency of implementation (1v)**\n",
    "\n",
    "We used a greedy, local search that operates in the input (raw) feature space. Our idea:\n",
    "\n",
    "1. Pick one car (one row of features).\n",
    "2. Compute the current predicted price using the full pipeline.\n",
    "3. Decide a target: e.g., +10% (increase) or -10% (decrease).\n",
    "4. Pick a short list of numeric features that the model tends to rely on (we rank features by permutation importance on a training sample).\n",
    "5. Iteratively nudge each of those features a little bit in the direction that would move the prediction toward the target: \n",
    "    - Step sizes are based on realistic feature variation from the training set (q10→q90 range).\n",
    "    - After each proposed change we run the entire pipeline and get a new prediction.\n",
    "    - We keep the change only if it moved the prediction closer to the target.\n",
    "    - If nothing improves, we reduce step sizes and try again, for up to a maximum number of iterations.\n",
    "6. Stop when the target is reached or we run out of iterations / step size becomes tiny.\n",
    "\n",
    "Important implementation details to make this robust:\n",
    "- Coerce all numeric inputs to float (to avoid pandas nullable-int assignment errors).\n",
    "- Fill missing numeric inputs with training medians before searching so the pipeline will not break.\n",
    "- Bound proposed values within training min/max to avoid unrealistic values.\n",
    "- Use a deterministic search (no random elements) so results are reproducible.\n",
    "\n",
    "------\n",
    "\n",
    "**d) Discussion of results (1v)**\n",
    "\n",
    "For each sampled test car (we sampled 30):\n",
    "- orig_pred: the original model prediction for that car.\n",
    "- target (implicit): the target price we tried to reach (orig_pred x 1.10 for +10%).\n",
    "- final_pred: the model's prediction after applying the accepted changes.\n",
    "- success: True if the greedy search reached (or exceeded) the target, False otherwise.\n",
    "- iters: how many iterations the search ran.\n",
    "- changes: dictionary of features the search changed and the new values it set for them.\n",
    "\n",
    "We also saved open_ended_counterfactual_summary.csv with these results.\n",
    "\n",
    "1. **Success Rates:** +10% target: 6.67%  // -10% target: 0.00%\n",
    "    - For the sampled cars, only ~1 in 15 (6.7%) could reach a +10% increase by changing realistic features; none could be reduced by 10% using realistic changes.\n",
    "    - That means most cars are already at the top (or bottom) of what the model considers plausible given their brand, age, engine size, etc. Small **realistic** tweaks cannot move the valuation.\n",
    "\n",
    "2. **Most used Features** by the Algorithm: miles_per_year: 37 times; mpg: 31 times; tax: 28 times\n",
    "    - When the algorithm tries to change the prediction, it almost always does so by adjusting these three features. These are the model's “go-to” levers to move predictions locally.\n",
    "\n",
    "3. **Average Size of Suggested Changes**: mpg: +59.4; miles_per_year: +7,698; tax: +119.6\n",
    "    - The model requested large changes: +59 mpg or +7.7k miles/year or +120 tax units. These are not small, realistic fixes—large or impossible in practice.\n",
    "    - Increasing miles per year is not only impossible, it is also unrealistic that more miles would increase the car price. This shows us that the model learnt an unrealistic relationship.\n",
    "    - This tells the business: even where changes are suggested, they are generally unrealistic, so the car's price is effectively locked by stronger features (brand, age, engine size).\n",
    "\n",
    "4. **Successful Example** (index 100785): orig_pred = 10,783; final_pred = 12,014 (success True); changes: {'mpg': 54.7, 'tax': 164.5}  (1 iteration)\n",
    "    - The model was able to push the price up by 10% in one step by dramatically increasing mpg and tax. But those required changes are huge and not realistic — this was a “success” only in the model's internal logic, not a practical recommendation.\n",
    "\n",
    "5. **Failed example** (index 83321): orig_pred = 14,470; final_pred = 14,746  (success False); changes: {'mpg': 62.35, 'miles_per_year': 6,347.74}  (26 iterations)\n",
    "    - Even after many iterations and very large changes, the model could not reach a +10% increase. This shows the model is constrained by other features (age/brand/engine) that we did not change and cannot realistically change.\n",
    "\n",
    "----    \n",
    "\n",
    "**e) Alignment with objectives (0.5v)**\n",
    "\n",
    "1. **Clear objective**: Our goal was simple: check what small, realistic changes to a car's features would be needed to move its predicted price by ±10%. This gives practical “what-if” insights instead of only global feature importance.\n",
    "2. **Non-trivial task**: Although the question is simple, solving it was technically difficult because our model uses a complex preprocessing pipeline and non-linear algorithms. We had to design a custom search method that safely modifies input features and runs the entire pipeline each time.\n",
    "3. **Correct and efficient implementation**: The solution only changes features that make sense, keeps values within realistic ranges, and automatically handles pipeline steps like scaling and imputing. It is deterministic, fast, and works directly with the real production pipeline.\n",
    "4. **Meaningful findings**: The results showed: Most cars cannot be pushed ±10% with realistic feature changes. The model often requested unrealistic adjustments (e.g., huge mpg changes), revealing rigid pricing behaviour.\n",
    "Some features the model tried to use (like increasing miles_per_year) were unrealistic, helping us spot places where the model relies on odd patterns.\n",
    "5. **Objective achieved**: We obtained exactly what we wanted: clear, car-specific explanations and simple recommendations (“small repairs” vs. “manual review”).\n",
    "The insights directly support the project's aim of understanding and improving price evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "055b6be9-de70-4bd1-bfe2-3f03d0378ef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic asserts so failures are explicit \n",
    "assert \"pipe\" in globals(), \"pipeline object `pipe` not found - set `pipe` to your final Pipeline\"\n",
    "assert \"X_train\" in globals(), \"X_train not found\"\n",
    "assert \"y_train\" in globals(), \"y_train not found\"\n",
    "assert \"df_cars_test\" in globals(), \"df_cars_test not found\"\n",
    "\n",
    "# Build the list of numeric features we will consider\n",
    "num_candidates = []\n",
    "if \"numeric_features_clean\" in globals():\n",
    "    num_candidates += list(numeric_features_clean)\n",
    "if \"log_features_clean\" in globals():\n",
    "    num_candidates += list(log_features_clean)\n",
    "if not num_candidates:\n",
    "    if \"numeric_features\" in globals():\n",
    "        num_candidates += list(numeric_features)\n",
    "    if \"log_features\" in globals():\n",
    "        num_candidates += list(log_features)\n",
    "\n",
    "# keep only columns that actually exist in X_train (avoid KeyErrors)\n",
    "num_features = [f for f in dict.fromkeys(num_candidates) if f in X_train.columns]\n",
    "print(f\"Numeric features detected ({len(num_features)}): {num_features}\")\n",
    "\n",
    "# Build robust summary statistics (numpy floats only) used for bounding and step sizes.\n",
    "train_stats = {}\n",
    "for c in num_features:\n",
    "    arr = pd.to_numeric(X_train[c], errors=\"coerce\").dropna().to_numpy(dtype=\"float64\")\n",
    "    if arr.size > 0:\n",
    "        train_stats[c] = {\n",
    "            \"min\": float(np.nanmin(arr)),\n",
    "            \"max\": float(np.nanmax(arr)),\n",
    "            \"q10\": float(np.nanquantile(arr, 0.10)),\n",
    "            \"q90\": float(np.nanquantile(arr, 0.90)),\n",
    "            \"median\": float(np.nanmedian(arr)),\n",
    "            \"mean\": float(np.nanmean(arr)),\n",
    "            \"std\": float(np.nanstd(arr, ddof=1) if arr.size > 1 else 0.0)\n",
    "        }\n",
    "    else:\n",
    "        train_stats[c] = {\"min\": 0.0, \"max\": 0.0, \"q10\": 0.0, \"q90\": 0.0, \"median\": 0.0, \"mean\": 0.0, \"std\": 0.0}\n",
    "\n",
    "# quick sanity output for first features\n",
    "print(\"Example train stats (first 5 features):\")\n",
    "for i, (k, v) in enumerate(train_stats.items()):\n",
    "    print(f\"  {k}: q10={v['q10']}, median={v['median']}, q90={v['q90']}\")\n",
    "    if i >= 4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5baa20dc-b8f1-4583-8d23-e8acd6212ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Permutation importance gives a robust ranking even with encodings and OHE.\n",
    "\n",
    "COMPUTE_PERM = True  # flip to False if you want to skip this step and just use the numeric_features ordering\n",
    "\n",
    "perm_imp = None\n",
    "if COMPUTE_PERM:\n",
    "    sample_size = min(2000, len(X_train))\n",
    "    X_sample = X_train.sample(sample_size, random_state=0)\n",
    "    y_sample = y_train.loc[X_sample.index]\n",
    "    print(\"Computing permutation importance and sampling training data\")\n",
    "    res = permutation_importance(pipe, X_sample, y_sample, n_repeats=6, random_state=42, n_jobs=1)\n",
    "    # res.importances_mean corresponds to the columns in X_sample\n",
    "    perm_imp = pd.Series(res.importances_mean, index=X_sample.columns).sort_values(ascending=False)\n",
    "    # keep only numeric features for ranking\n",
    "    perm_imp = perm_imp.loc[[c for c in perm_imp.index if c in num_features]]\n",
    "    print(\"Top numeric features by permutation importance:\\n\", perm_imp.head(10))\n",
    "else:\n",
    "    print(\"Permutation importance disabled. Ranked features will default to numeric feature order.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c2324603-9c1a-447b-ad6b-287039a1c15e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Robust greedy counterfactual search\n",
    "def find_counterfactual_greedy(\n",
    "    instance: pd.DataFrame,\n",
    "    pipe,\n",
    "    X_train: pd.DataFrame,\n",
    "    target_rel: float = 0.10,\n",
    "    mode: str = \"realistic\",\n",
    "    step_frac: float = 0.15,\n",
    "    max_iters: int = 200,\n",
    "    verbose: bool = False,\n",
    "    perm_importance_series: pd.Series = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Greedy, deterministic counterfactual search for a single-row `instance`.\n",
    "    Returns a dict with original/final predictions, success flag, changed features, and trajectory.\n",
    "\n",
    "    Key design decisions:\n",
    "    - Work in input-space only (do not modify internals).\n",
    "    - Coerce numeric inputs to float64, fill missing with train median.\n",
    "    - Use training q10-q90 ranges to compute sensible step sizes.\n",
    "    - 'realistic' mode excludes non-actionable features (e.g. miles_per_year).\n",
    "    \"\"\"\n",
    "    assert instance.shape[0] == 1, \"instance must be single-row DataFrame\"\n",
    "\n",
    "    # copy and coerce numeric columns to float64 (fallback to training median)\n",
    "    inst = instance.copy()\n",
    "    numeric_present = [f for f in num_features if f in inst.columns]\n",
    "    for f in numeric_present:\n",
    "        v = pd.to_numeric(inst.at[inst.index[0], f], errors=\"coerce\")\n",
    "        if pd.isna(v):\n",
    "            inst.at[inst.index[0], f] = float(train_stats.get(f, {}).get(\"median\", 0.0))\n",
    "        else:\n",
    "            inst.at[inst.index[0], f] = float(v)\n",
    "    if numeric_present:\n",
    "        inst[numeric_present] = inst[numeric_present].astype(\"float64\")\n",
    "\n",
    "    cur = inst.copy()\n",
    "    original_pred = float(pipe.predict(cur)[0])\n",
    "    target = original_pred * (1.0 + target_rel)\n",
    "    direction = 1 if target_rel > 0 else -1\n",
    "\n",
    "    # realistic allowed features: keep only features that can be reasonably changed\n",
    "    # NOTE: miles_per_year is excluded because it's not actionable in reality.\n",
    "    if mode == \"realistic\":\n",
    "        allowed = [f for f in [\"paintQuality\", \"tax\", \"mpg\"] if f in num_features]\n",
    "        # allowed = [f for f in [\"paintQuality\"] if f in num_features]\n",
    "    else:\n",
    "        allowed = [f for f in num_features if f in cur.columns]\n",
    "\n",
    "    # rank allowed features by permutation importance if available\n",
    "    if perm_importance_series is not None:\n",
    "        ranked = [f for f in perm_importance_series.index if f in allowed]\n",
    "    else:\n",
    "        ranked = allowed.copy()\n",
    "\n",
    "    # fallback\n",
    "    if not ranked:\n",
    "        ranked = [f for f in num_features if f in cur.columns]\n",
    "\n",
    "    # compute step sizes from q10->q90\n",
    "    steps = {}\n",
    "    for f in ranked:\n",
    "        s = train_stats.get(f)\n",
    "        range_q = max(1e-8, (s[\"q90\"] - s[\"q10\"]) if s else step_frac)\n",
    "        steps[f] = range_q * step_frac\n",
    "\n",
    "    cur_pred = original_pred\n",
    "    trajectory = [cur_pred]\n",
    "    changes = {}\n",
    "    feat_steps = {f: 0.0 for f in ranked}\n",
    "\n",
    "    # quick exit if already at target\n",
    "    if direction * (cur_pred - target) >= 0:\n",
    "        return {'index': cur.index[0], 'original_pred': original_pred, 'target': target,\n",
    "                'final_pred': cur_pred, 'success': True, 'iters': 0, 'changes': {}, 'trajectory': trajectory}\n",
    "\n",
    "    it = 0\n",
    "    while it < max_iters:\n",
    "        progressed = False\n",
    "        for f in ranked:\n",
    "            old_raw = pd.to_numeric(cur.at[cur.index[0], f], errors=\"coerce\")\n",
    "            old_val = float(old_raw) if not pd.isna(old_raw) else float(train_stats.get(f, {}).get(\"median\", 0.0))\n",
    "            step = steps.get(f, 0.0)\n",
    "            if abs(step) < 1e-12:\n",
    "                continue\n",
    "\n",
    "            proposed = old_val + direction * step\n",
    "            bounds = train_stats.get(f, {\"min\": -1e12, \"max\": 1e12})\n",
    "            proposed = max(float(bounds[\"min\"]), min(float(bounds[\"max\"]), float(proposed)))\n",
    "            cur.at[cur.index[0], f] = float(proposed)\n",
    "\n",
    "            try:\n",
    "                new_pred = float(pipe.predict(cur)[0])\n",
    "            except Exception:\n",
    "                # revert and reduce step on failure\n",
    "                cur.at[cur.index[0], f] = float(old_val)\n",
    "                steps[f] *= 0.5\n",
    "                continue\n",
    "\n",
    "            # accept only if it moves towards the target\n",
    "            if direction * (new_pred - cur_pred) > 1e-8:\n",
    "                changes[f] = float(proposed)\n",
    "                feat_steps[f] += direction * float(step)\n",
    "                cur_pred = new_pred\n",
    "                trajectory.append(cur_pred)\n",
    "                progressed = True\n",
    "                if verbose:\n",
    "                    print(f\"it {it} | {f}: {old_val:.4f} -> {proposed:.4f} | pred {cur_pred:.2f}\")\n",
    "                if direction * (cur_pred - target) >= 0:\n",
    "                    break\n",
    "            else:\n",
    "                # revert and shrink this feature's step\n",
    "                cur.at[cur.index[0], f] = float(old_val)\n",
    "                steps[f] *= 0.5\n",
    "\n",
    "        if not progressed:\n",
    "            # shrink all steps if no progress this iteration\n",
    "            for k in steps:\n",
    "                steps[k] *= 0.5\n",
    "\n",
    "        if direction * (cur_pred - target) >= 0:\n",
    "            break\n",
    "        if all(abs(s) < 1e-12 for s in steps.values()):\n",
    "            break\n",
    "        it += 1\n",
    "\n",
    "    success = (direction * (cur_pred - target) >= 0)\n",
    "    return {\n",
    "        'index': cur.index[0],\n",
    "        'original_pred': original_pred,\n",
    "        'target': target,\n",
    "        'final_pred': cur_pred,\n",
    "        'success': bool(success),\n",
    "        'iters': it,\n",
    "        'changes': changes,\n",
    "        'trajectory': trajectory,\n",
    "        'feature_steps': feat_steps\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "42077ef1-f4c2-44b0-9cfa-753ae4ce3ba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run counterfactuals on a small test sample and save results\n",
    "\n",
    "# Parameters adjustable for other samples:\n",
    "SAMPLE_N = 30                 # how many test rows to run counterfactuals for\n",
    "TARGET_RELS = [0.10, -0.10]   # compute +10% and -10% counterfactuals\n",
    "MODE = \"realistic\"            # \"realistic\" or \"full\" features defined above in greedy counterfactual\n",
    "STEP_FRAC = 0.15\n",
    "MAX_ITERS = 200\n",
    "VERBOSE = False\n",
    "\n",
    "# sample deterministic indices from the provided test set\n",
    "sample_idx = df_cars_test.sample(min(SAMPLE_N, len(df_cars_test)), random_state=0).index.tolist()\n",
    "\n",
    "perm_imp_small = perm_imp.copy() if ('perm_imp' in globals() and perm_imp is not None) else None\n",
    "\n",
    "cf_records = []\n",
    "for idx in tqdm(sample_idx, desc=\"counterfactuals\"):\n",
    "    inst = df_cars_test.loc[[idx]].copy()\n",
    "    for tr in TARGET_RELS:\n",
    "        cf = find_counterfactual_greedy(\n",
    "            inst, pipe, X_train,\n",
    "            target_rel=tr,\n",
    "            mode=MODE,\n",
    "            step_frac=STEP_FRAC,\n",
    "            max_iters=MAX_ITERS,\n",
    "            verbose=VERBOSE,\n",
    "            perm_importance_series=perm_imp_small\n",
    "        )\n",
    "        cf['target_rel'] = tr\n",
    "        cf_records.append(cf)\n",
    "\n",
    "# collect into DataFrame and save\n",
    "rows = []\n",
    "for r in cf_records:\n",
    "    rows.append({\n",
    "        'index': r['index'],\n",
    "        'target_rel': r['target_rel'],\n",
    "        'orig_pred': r['original_pred'],\n",
    "        'final_pred': r['final_pred'],\n",
    "        'success': r['success'],\n",
    "        'iters': r['iters'],\n",
    "        'changes': r['changes']\n",
    "    })\n",
    "df_cf_summary = pd.DataFrame(rows).set_index('index')\n",
    "df_cf_summary.to_csv(\"open_ended_counterfactual_summary.csv\")\n",
    "print(\"Saved open_ended_counterfactual_summary.csv — sample of counterfactual attempts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fe613728-7834-48c4-b751-293b02ee9d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compact numeric summary results\n",
    "\n",
    "# Success rates\n",
    "success_rates = df_cf_summary.groupby(\"target_rel\")[\"success\"].mean().rename(\"success_rate\")\n",
    "print(\"Success rates (fraction of tested cars where the algorithm reached the target):\")\n",
    "for idx, v in success_rates.items():\n",
    "    print(f\"  {int(idx*100)}% target: {v:.3f} ({v*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Which features were changed most often\n",
    "feature_counter = Counter()\n",
    "for changes in df_cf_summary[\"changes\"]:\n",
    "    for f in changes.keys():\n",
    "        feature_counter[f] += 1\n",
    "print(\"Top features changed (count):\", feature_counter.most_common())\n",
    "\n",
    "# Average magnitude of changes\n",
    "avg_change = {}\n",
    "for f in feature_counter.keys():\n",
    "    deltas = [changes[f] for changes in df_cf_summary[\"changes\"] if f in changes]\n",
    "    if deltas:\n",
    "        avg_change[f] = float(np.mean(deltas))\n",
    "print(\"Average suggested change (approx):\", avg_change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a6112964-0523-442f-8156-06dc9c1ea793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example plots and recommendation table\n",
    "\n",
    "# helper: basic visualization for a single summary row\n",
    "def visualize_counterfactual_summary(index, target_rel, df_summary=df_cf_summary, df_test=df_cars_test, pipe=pipe):\n",
    "    rec = df_summary.loc[index]\n",
    "    if isinstance(rec, pd.DataFrame):\n",
    "        rec = rec.loc[rec['target_rel'] == target_rel].iloc[0]\n",
    "    inst = df_test.loc[[index]]\n",
    "    orig_pred = float(pipe.predict(inst)[0])\n",
    "    final_pred = float(rec['final_pred'])\n",
    "    changes = rec['changes']\n",
    "    plt.figure(figsize=(6,3.5))\n",
    "    plt.bar(['Original', f\"After changes ({int(target_rel*100)}%)\"], [orig_pred, final_pred], color=['#4C72B0', '#55A868'])\n",
    "    plt.ylabel(\"Predicted price (GBP)\")\n",
    "    plt.title(f\"Row {index} | Success: {rec['success']}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Original: £{orig_pred:,.0f} | After: £{final_pred:,.0f} | Steps: {rec['iters']} | Success: {rec['success']}\")\n",
    "    if changes:\n",
    "        print(\"Changes applied (feature -> new value):\")\n",
    "        for k, v in changes.items():\n",
    "            print(f\"  - {k}: {v:,.2f}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# show two illustrative examples if present (two indexes we use in markdown)\n",
    "examples_to_show = [100785, 83321]\n",
    "for idx in examples_to_show:\n",
    "    if idx in df_cf_summary.index:\n",
    "        try:\n",
    "            visualize_counterfactual_summary(idx, 0.10)\n",
    "        except Exception as e:\n",
    "            print(\"visualization error for\", idx, \":\", e)\n",
    "\n",
    "# simple textual recommendation column\n",
    "def simple_recommendation(row):\n",
    "    if row['success']:\n",
    "        changes = row['changes']\n",
    "        if not changes:\n",
    "            return \"No change needed\"\n",
    "        large = any((k == \"mpg\" and abs(v) > 15) or (k == \"miles_per_year\" and abs(v) > 2000) or (k == \"tax\" and abs(v) > 50)\n",
    "                    for k, v in changes.items())\n",
    "        return \"Consider small repairs\" if not large else \"Likely unrealistic — manual review\"\n",
    "    else:\n",
    "        return \"Cannot reach target — manual review\"\n",
    "\n",
    "df_readable = df_cf_summary.reset_index().copy()\n",
    "df_readable['recommendation'] = df_readable.apply(simple_recommendation, axis=1)\n",
    "\n",
    "# display a clean table\n",
    "display_cols = [\"index\", \"orig_pred\", \"final_pred\", \"target_rel\", \"success\", \"iters\", \"changes\", \"recommendation\"]\n",
    "display_df = df_readable.loc[:, display_cols].rename(columns={\n",
    "    \"index\": \"row_id\",\n",
    "    \"orig_pred\": \"Original price (GBP)\",\n",
    "    \"final_pred\": \"Final price after changes (GBP)\",\n",
    "    \"target_rel\": \"Target change\",\n",
    "    \"success\": \"Reached target?\",\n",
    "    \"iters\": \"Search steps\",\n",
    "    \"changes\": \"What changed (feature -> new value)\"\n",
    "})\n",
    "\n",
    "# format for readability\n",
    "display_df[\"Original price (GBP)\"] = display_df[\"Original price (GBP)\"].round(0)\n",
    "display_df[\"Final price after changes (GBP)\"] = display_df[\"Final price after changes (GBP)\"].round(0)\n",
    "display_df[\"Target change\"] = display_df[\"Target change\"].apply(lambda x: f\"{int(x*100)}%\")\n",
    "display_df[\"Reached target?\"] = display_df[\"Reached target?\"].apply(lambda v: \"Yes\" if v else \"No\")\n",
    "\n",
    "display(display_df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "group05_main_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
