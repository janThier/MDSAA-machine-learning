{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc0d9d0c-5598-4a4e-b28b-a2921d27ce73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Your work will be evaluated according to the following criteria:**\n",
    "- Project Structure and Notebook(s) Quality (4/20)\n",
    "- Data Exploration & Initial Preprocessing (4/20)\n",
    "- Regression Benchmarking and Optimization (7/20)\n",
    "- Open-Ended Section (4/20)\n",
    "- Deployment (1/20)\n",
    "- Extra Point: Have Project Be Publicly Available on GitHub (1/20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Timeline**\n",
    "- 22.11.: Preprocessing and Model Preparation\n",
    "    - Finish clean preprocessing all included in pipeline\n",
    "    - Finish clean Hyperparameter Tuning\n",
    "- 29.11.: Feature Selection\n",
    "    - Clean and structured approach for feature selection for all models (best case: consistent approach imo)\n",
    "- 29.11.: Regression Benchmarking and Optimization\n",
    "    - Automize Optimization (add something like mlflow)\n",
    "    - Show and explain results of different models clearly just as in the lab lectures\n",
    "- 06.12.: Open-End Section and Deployment\n",
    "    - Added 4 open-end-experiments\n",
    "    - Deployment\n",
    "- 13.12.: Notebook Feinschliff\n",
    "    - Super clean notebook structure similar to lab-notebooks by Ricardo\n",
    "- 14.12.: Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d89fe94-6eed-4612-ad0a-40032c4dd67a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO Brainstorm + Implementation of Ideas for open ended Section (Several can get explored):\n",
    "#   1. Create a classification Model, that predicts if a dataset is gonna be a price outlier (outlier flag) (SAMUEL)\n",
    "#   2. Jan:\n",
    "#       - Use multiple approaches for encoding to see how the models behave (e.g. target encoding AND OHE for the same categorical variable)\n",
    "#   3. Elias: \n",
    "#   4. Lukas: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4467ab3-7594-4e02-a19f-8c5e47fb14e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"\n",
    "    background: rgba(25, 25, 25, 0.55);\n",
    "    backdrop-filter: blur(16px) saturate(150%);\n",
    "    -webkit-backdrop-filter: blur(16px) saturate(150%);\n",
    "    border: 1px solid rgba(255, 255, 255, 0.12);\n",
    "    border-radius: 18px;\n",
    "    padding: 45px 30px;\n",
    "    text-align: center;\n",
    "    font-family: 'Inter', 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;\n",
    "    color: #e0e0e0;\n",
    "    box-shadow: 0 0 30px rgba(0, 0, 0, 0.35);\n",
    "    margin: 40px auto;\n",
    "    max-width: 800px;\n",
    "\">\n",
    "\n",
    "  <h1 style=\"\n",
    "      font-size: 2.8em;\n",
    "      font-weight: 700;\n",
    "      margin: 0 0 8px 0;\n",
    "      letter-spacing: -0.02em;\n",
    "      background: linear-gradient(90deg, #00e0ff, #9c7eff);\n",
    "      -webkit-background-clip: text;\n",
    "      -webkit-text-fill-color: transparent;\n",
    "  \">\n",
    "      Machine Learning Project\n",
    "  </h1>\n",
    "\n",
    "  <h2 style=\"\n",
    "      font-size: 1.6em;\n",
    "      font-weight: 500;\n",
    "      margin: 0 0 25px 0;\n",
    "      color: #b0b0b0;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Cars 4 You - Predicting Car Prices\n",
    "  </h2>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.25em;\n",
    "      font-weight: 500;\n",
    "      color: #c0c0c0;\n",
    "      margin-bottom: 6px;\n",
    "  \">\n",
    "      Group 5 - Lukas Belser, Samuel Braun, Elias Karle, Jan Thier\n",
    "  </p>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.05em;\n",
    "      font-weight: 400;\n",
    "      color: #8a8a8a;\n",
    "      font-style: italic;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Machine Learning End Results · 22.12.2025\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "372c87aa-b7f3-4a34-a596-bb5e584db114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Table of Contents**\n",
    " \n",
    "- [1. Import Packages and Data](#1-import-packages-and-data)  \n",
    "  - [1.1 Import Required Packages](#11-import-required-packages)  \n",
    "  - [1.2 Load Datasets](#12-load-datasets)  \n",
    "  - [1.3 Kaggle Setup](#13-kaggle-setup)  \n",
    "- [2. Data Cleaning, Feature Engineering, Split & Preprocessing](#2-data-cleaning-feature-engineering-split--preprocessing)  \n",
    "  - [2.1 Data Cleaning](#21-data-cleaning)  \n",
    "  - [2.2 Feature Engineering](#22-feature-engineering)  \n",
    "  - [2.3 Data Split](#23-data-split)  \n",
    "  - [2.4 Preprocessing](#24-preprocessing)  \n",
    "- [3. Feature Selection](#3-feature-selection)  \n",
    "- [4. Model Evaluation Metrics, Baselining, Setup](#4-model-evaluation-metrics-baselining-setup)  \n",
    "- [5. Hyperparameter Tuning and Model Evaluation](#5-hyperparameter-tuning-and-model-evaluation)  \n",
    "  - [5.1 ElasticNet](#51-elasticnet)  \n",
    "  - [5.2 HistGradientBoost](#52-histgradientboost)  \n",
    "  - [5.3 RandomForest](#53-randomforest)  \n",
    "  - [5.4 ExtraTrees](#54-extratrees)  \n",
    "- [6. Feature Importance of Tree Models (with SHAP)](#6-feature-importance-of-tree-models-with-shap)  \n",
    "  - [6.1 HGB](#61-hgb)  \n",
    "  - [6.2 RF](#62-rf)  \n",
    "- [7. Kaggle Competition](#7-kaggle-competition)  \n",
    "\n",
    "TODO finish + update toc > at the end of project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27fff3e4-624f-4b1f-b258-fbbe7b69cb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Import Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e6aadb-727a-4399-8888-9d6d4e593898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aa35af5d-8257-4fb6-a380-86bbbfac3244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f02b1610-84c8-46a7-8c37-980904a0cef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "008f906c-e207-4658-8e03-59bd25b8e2d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f54ab11e-fcfc-4278-8997-89a92adf8eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import and load Data"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, chi2, RFE\n",
    "from scipy.stats import spearmanr, uniform, randint\n",
    "from sklearn.metrics import mean_absolute_error\n",
    " \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.base import clone\n",
    " \n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, GridSearchCV, cross_validate\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    " \n",
    "from car_functions import GroupMedianImputer, clean_car_dataframe, cv_target_encode, print_metrics # add_price_anchor_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9737aef-d2bd-4748-89ed-9b0e2cea8637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.2 Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3da1ad2b-8ee0-48b0-917d-545c70633ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cars_train = pd.read_csv(\"train.csv\")\n",
    "df_cars_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab6500b5-94a1-4a0e-b7f6-ea9d35c787eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Kaggle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "556ffc24-70f7-48c9-96a4-3a19380f91db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Connect"
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle API Connect => everyone has to do this himself, with his own kaggle.json api token\n",
    "\n",
    "# Folder containing kaggle.json\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/Workspace/Users/20250355@novaims.unl.pt\"\n",
    "\n",
    "# Test\n",
    "!echo $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b75ee214-0996-406f-b77f-9e72fa7153e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data Cleaning, Feature Engineering, Split & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b78e7ca8-ba25-4ea3-873d-4f3515281440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task II (5 Points): Clean and preprocess the dataset \n",
    "- Missing Value handling, Outlier preprocessing + justify decisions -> in data_cleaning.py\n",
    "- Review current features and create extra features if needed + explain -> in Feature Engineering\n",
    "- Deal with categorical variables -> in One-Hot-Encoding \n",
    "- Perform data scaling, explain reasoning -> in Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f53a2a97-4662-40a3-8b24-b238dad5138e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6fa0cfe9-be7e-4e90-ac1c-cd600bb7d13f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Numerical / Categorical Data Cleaning: with Pandas"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier Preprocessing, Missing Value Handling and Decision justifying happens here\n",
    "df_cars_train = clean_car_dataframe(df_cars_train)\n",
    "df_cars_test = clean_car_dataframe(df_cars_test)\n",
    "\n",
    "\n",
    "# Safety Check: print unique values of all columns of df_cars_train // df_cars_test to see if data cleaning worked and if there are still odd values\n",
    "for col in df_cars_train.columns:\n",
    "    print(col, df_cars_train[col].unique())\n",
    "print(\"X\"*150)\n",
    "print(\"-\"*150)\n",
    "print(\"X\"*150)\n",
    "for col in df_cars_test.columns:\n",
    "    print(col, df_cars_test[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b198720e-b5a0-46ef-8055-4ac33b852410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd8845f0-241e-40a4-8c24-3e876d89b02b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Base Feature Creation**\n",
    "\n",
    "These are foundational features derived directly from the original data, often to create linear relationships or capture interactions.\n",
    "- `age`: Calculated as `2020 - year`. Creates a simple linear feature representing the car's age. Newer cars (lower age) generally have higher prices.\n",
    "- `miles_per_year`: Calculated as `mileage / age`. This normalizes the car's usage, preventing high correlation (multicollinearity) between `mileage` and `age`. A 3-year-old car with 60,000 miles is different from a 6-year-old car with 60,000 miles.\n",
    "- `age_x_engine`: An interaction term `age * engineSize`. This helps the model capture non-linear relationships, such as the possibility that the value of cars with large engines might depreciate faster (or slower) than cars with small engines.\n",
    "- `mpg_x_engine`: An interaction term `mpg * engineSize`. This captures the combined effect of fuel efficiency and engine power.\n",
    "- `tax_per_engine`: Calculated as `tax / engineSize`. This feature represents the tax cost relative to the engine's power, which could be an indicator of overall running costs or vehicle class.\n",
    "- `mpg_per_engine`: Calculated as `mpg / engineSize`. This creates an \"efficiency\" metric, representing how many miles per gallon the car achieves for each unit of engine size.\n",
    "\n",
    "\n",
    "**Popularity & Demand Features**\n",
    "\n",
    "These features attempt to quantify a car's popularity or market demand, which directly influences price.\n",
    "- `model_freq`: Calculates the frequency (percentage) of each `model` in the training dataset. Popular, common models often have more stable and predictable pricing and demand.\n",
    "\n",
    "\n",
    "**Price Anchor Features**\n",
    "\n",
    "These features \"anchor\" a car's price relative to its group. They provide a strong baseline price signal based on brand, model, and configuration.\n",
    "- `brand_med_price`: The median price for the car's `Brand` (e.g., the typical price for a BMW vs. a Skoda). This captures overall brand positioning.\n",
    "- `model_med_price`: The median price for the car's `model` (e.g., the typical price for a 3-Series vs. a 1-Series). This captures the model's positioning within the brand.\n",
    "- `brand_fuel_med_price`: The median price for the car's specific `Brand` and `fuelType` combination (e.g., a Diesel BMW vs. a Petrol BMW).\n",
    "- `brand_trans_med_price`: The median price for the `Brand` and `transmission` combination (e.g., an Automatic BMW vs. a Manual BMW).\n",
    "\n",
    "\n",
    "**Normalized & Relative Features**\n",
    "\n",
    "These features compare a car to its peers rather than using absolute values.\n",
    "- `*_anchor` (e.g., `brand_med_price_anchor`): Created by dividing the median price features (from section 3) by the `overall_mean_price`. This makes the feature dimensionless and represents the group's price *relative* to the entire market (e.g., \"this brand is 1.5x the market average\").\n",
    "- `age_rel_brand`: Calculated as `age - brand_median_age`. This shows if a car is newer or older than the *typical* car for that specific brand, capturing relative age within its own group.\n",
    "\n",
    "\n",
    "**CV-Safe Target Encodings**\n",
    "\n",
    "This is an advanced technique to encode categorical variables (like `model` or `Brand`) using information from the target variable (`price`) without causing data leakage.\n",
    "- `*_te` (e.g., `model_te`): Represents the *average price* for that category (e.g., the average price for a \"Fiesta\").\n",
    "- **Why is it \"CV-Safe\"?** Instead of just calculating the global average price for \"Fiesta\" and applying it to all rows (which leaks target information), this method uses K-Fold cross-validation. For each fold of the data, the target encoding is calculated *only* from the *other* folds. This ensures the encoding for any given row never includes its own price, preventing leakage and leading to a more robust model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83812e71-e13e-43d4-883b-41ffd3b3466b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO split this into real \"feature engineering\" and an additional \"encoding\" section ~J\n",
    "\n",
    "# 1. Base Feature Creation\n",
    "\n",
    "# Car Age: Newer cars usually have higher prices, models prefer linear features\n",
    "df_cars_train['age'] = 2020 - df_cars_train['year']\n",
    "df_cars_test['age']  = 2020 - df_cars_test['year']\n",
    "\n",
    "# Miles per Year: Normalizes mileage by age (solves multicollinearity between year and mileage)\n",
    "df_cars_train['miles_per_year'] = df_cars_train['mileage'] / df_cars_train['age'].replace({0: np.nan})\n",
    "df_cars_train['miles_per_year'] = df_cars_train['miles_per_year'].fillna(df_cars_train['mileage'])\n",
    "\n",
    "df_cars_test['miles_per_year'] = df_cars_test['mileage'] / df_cars_test['age'].replace({0: np.nan})\n",
    "df_cars_test['miles_per_year'] = df_cars_test['miles_per_year'].fillna(df_cars_test['mileage'])\n",
    "\n",
    "# Interaction Terms: Capture non-linear effects between engine and other numeric features\n",
    "df_cars_train['age_x_engine'] = df_cars_train['age'] * df_cars_train['engineSize'].fillna(0)\n",
    "df_cars_test['age_x_engine']  = df_cars_test['age']  * df_cars_test['engineSize'].fillna(0)\n",
    "\n",
    "df_cars_train['mpg_x_engine'] = df_cars_train['mpg'].fillna(0) * df_cars_train['engineSize'].fillna(0)\n",
    "df_cars_test['mpg_x_engine']  = df_cars_test['mpg'].fillna(0)  * df_cars_test['engineSize'].fillna(0)\n",
    "\n",
    "# tax per engine\n",
    "df_cars_train['tax_per_engine'] = df_cars_train['tax'] / df_cars_train['engineSize'].replace({0: np.nan})\n",
    "df_cars_train['tax_per_engine'] = df_cars_train['tax_per_engine'].fillna(df_cars_train['tax'])\n",
    "\n",
    "df_cars_test['tax_per_engine'] = df_cars_test['tax'] / df_cars_test['engineSize'].replace({0: np.nan})\n",
    "df_cars_test['tax_per_engine'] = df_cars_test['tax_per_engine'].fillna(df_cars_test['tax'])\n",
    "\n",
    "# MPG per engineSize to represent the efficiency\n",
    "df_cars_train['mpg_per_engine'] = df_cars_train['mpg'] / df_cars_train['engineSize'].replace({0: np.nan})\n",
    "df_cars_train['mpg_per_engine'] = df_cars_train['mpg_per_engine'].fillna(df_cars_train['mpg'])\n",
    "\n",
    "df_cars_test['mpg_per_engine'] = df_cars_test['mpg'] / df_cars_test['engineSize'].replace({0: np.nan})\n",
    "df_cars_test['mpg_per_engine'] = df_cars_test['mpg_per_engine'].fillna(df_cars_test['mpg'])\n",
    "\n",
    "\n",
    "# 2. Model Frequency: Popular models tend to have stable demand and prices\n",
    "model_freq = df_cars_train['model'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "df_cars_train['model_freq'] = df_cars_train['model'].map(model_freq).fillna(0.0)\n",
    "df_cars_test['model_freq']  = df_cars_test['model'].map(model_freq).fillna(0.0)\n",
    "\n",
    "\n",
    "# 3. Brand and Model Anchors: Represent typical price levels (positioning)\n",
    "overall_mean_price = df_cars_train['price'].mean()\n",
    "\n",
    "# Brand median price: captures brand positioning (e.g., BMW > Skoda)\n",
    "brand_median_price = df_cars_train.groupby('Brand')['price'].median().to_dict()\n",
    "df_cars_train['brand_med_price'] = df_cars_train['Brand'].map(brand_median_price)\n",
    "df_cars_test['brand_med_price']  = df_cars_test['Brand'].map(brand_median_price)\n",
    "\n",
    "# Model median price: captures model hierarchy within brand (e.g., 3er > 1er)\n",
    "model_median_price = df_cars_train.groupby('model')['price'].median().to_dict()\n",
    "df_cars_train['model_med_price'] = df_cars_train['model'].map(model_median_price)\n",
    "df_cars_test['model_med_price']  = df_cars_test['model'].map(model_median_price)\n",
    "\n",
    "# Brand × Fuel median price: different fuels have different price segments\n",
    "brand_fuel_median_price = df_cars_train.groupby(['Brand','fuelType'])['price'].median().to_dict()\n",
    "df_cars_train['brand_fuel_med_price'] = list(zip(df_cars_train['Brand'], df_cars_train['fuelType']))\n",
    "df_cars_train['brand_fuel_med_price'] = df_cars_train['brand_fuel_med_price'].map(brand_fuel_median_price)\n",
    "df_cars_test['brand_fuel_med_price']  = list(zip(df_cars_test['Brand'], df_cars_test['fuelType']))\n",
    "df_cars_test['brand_fuel_med_price']  = df_cars_test['brand_fuel_med_price'].map(brand_fuel_median_price)\n",
    "\n",
    "# Brand × Transmission median price: automatic or manual may influence resale\n",
    "brand_trans_median_price = df_cars_train.groupby(['Brand','transmission'])['price'].median().to_dict()\n",
    "df_cars_train['brand_trans_med_price'] = list(zip(df_cars_train['Brand'], df_cars_train['transmission']))\n",
    "df_cars_train['brand_trans_med_price'] = df_cars_train['brand_trans_med_price'].map(brand_trans_median_price)\n",
    "df_cars_test['brand_trans_med_price']  = list(zip(df_cars_test['Brand'], df_cars_test['transmission']))\n",
    "df_cars_test['brand_trans_med_price']  = df_cars_test['brand_trans_med_price'].map(brand_trans_median_price)\n",
    "\n",
    "\n",
    "# 4. Normalized Anchors (dimensionless): relative position vs overall mean\n",
    "for col in ['brand_med_price','model_med_price','brand_fuel_med_price','brand_trans_med_price']:\n",
    "    df_cars_train[f'{col}_anchor'] = df_cars_train[col] / overall_mean_price\n",
    "    df_cars_test[f'{col}_anchor']  = df_cars_test[col]  / overall_mean_price\n",
    "\n",
    "\n",
    "# 5. Relative Age (within brand): newer/older than brand median year\n",
    "brand_median_age = df_cars_train.groupby('Brand')['age'].median().to_dict()\n",
    "\n",
    "df_cars_train['age_rel_brand'] = df_cars_train['age'] - df_cars_train['Brand'].map(brand_median_age)\n",
    "df_cars_test['age_rel_brand']  = df_cars_test['age']  - df_cars_test['Brand'].map(brand_median_age)\n",
    "\n",
    "\n",
    "# 6. CV-Safe Target Encoding on categorical columns # TODO: check whether this mean-target-encoding is necessary after already using features such as brand_med_price_anchor which uses the median instead of the mean\n",
    "for col, m in [('model', 100), ('Brand', 30), ('fuelType', 20), ('transmission', 20)]:\n",
    "    tr_enc, te_enc = cv_target_encode(df_cars_train, df_cars_test, col, ycol='price', m=m)\n",
    "    df_cars_train[f'{col}_te'] = tr_enc\n",
    "    df_cars_test[f'{col}_te']  = te_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0de09e53-b3c6-4669-942a-5c957f41e1f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cars_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e72b5602-fc3d-4397-bb66-10e7b501438a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.3 Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dda482e9-4a68-438d-86c0-e0524a9b8692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split Data"
    }
   },
   "outputs": [],
   "source": [
    "# Split Data, Stratify not necessary due to regression problem and Cross Validation later\n",
    "X_train = df_cars_train.drop(columns='price')\n",
    "y_train = df_cars_train['price']\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 42, shuffle = True)\n",
    "# ==> Since we have an external hold-out set (kaggle) an additional val set is not necessary and wastes training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9425a7c0-ec96-4458-a0da-8ef65acb3f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.4 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae68289b-ab02-40fb-b9d9-b530339f16f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_float_array(x):\n",
    "    \"\"\"Convert input to float array.\"\"\"\n",
    "    return np.array(x, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explain why we split into original and engineered features here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "362f52e2-4378-4266-97b7-a4038b588e6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PIPELINE WITH preprocessor_orig CONTAINING ONLY ORIGINAL FEATURES\n",
    "\n",
    "\n",
    "orig_numeric_features = [\n",
    "    \"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\"\n",
    "]\n",
    "orig_categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "numeric_transformer_orig = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),    # simple global median imputation\n",
    "    (\"to_float\", FunctionTransformer(to_float_array)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer_orig = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # fill by mode instead of Unknown\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # One-hot encoding\n",
    "    # TODO use target-encoder here and remove manual target encoding from feature engineering ~J\n",
    "])\n",
    "\n",
    "preprocessor_orig = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer_orig, orig_numeric_features),\n",
    "    (\"cat\", categorical_transformer_orig, orig_categorical_features)\n",
    "])\n",
    "\n",
    "preprocessor_orig.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "39142374-d485-42e7-a6cb-83ecffae0091",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocessing: with sklearn Pipeline & Column Transformer"
    }
   },
   "outputs": [],
   "source": [
    "# PIPELINE WITH preprocessor_fe CONTAINING ENGINEERED FEATURES\n",
    "\n",
    "# Custom Written GroupMedianImputer to get Brand and Model specific Medians\n",
    "# Uses target-encoded Brand_te/model_te as grouping cols (present in numeric_features)\n",
    "group_imputer = GroupMedianImputer(group_cols=[\"Brand_te\", \"model_te\"])\n",
    "\n",
    "numeric_features = [\n",
    "    \"age\", \"age_rel_brand\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"model_freq\",\n",
    "    \"brand_med_price_anchor\", \"model_med_price_anchor\", \"brand_fuel_med_price_anchor\", \"brand_trans_med_price_anchor\",\n",
    "    \"age_x_engine\", \"mpg_x_engine\",\n",
    "    \"model_te\", \"Brand_te\", # \"fuelType_te\", \"transmission_te\",\n",
    "    \"tax_per_engine\", \"mpg_per_engine\"\n",
    "]\n",
    "log_features = [\"mileage\", \"miles_per_year\"]  # TODO other num columns here better?!\n",
    "categorical_features = [\"transmission\", \"fuelType\"] # [\"Brand\", \"model\", \"transmission\", \"fuelType\"] # TODO use target-encoder here and remove manual target encoding from feature engineering ~J # TODO play around with some OHE (e.g. fuelType or transmission)\n",
    "\n",
    "# left out columns: year (age is better), hasDamage (unsure what the two values 0 and NaN mean)\n",
    "\n",
    "numeric_transformer_fe = Pipeline([\n",
    "    (\"group_impute\", GroupMedianImputer(group_cols=[\"Brand_te\", \"model_te\"])),\n",
    "    (\"to_float\", FunctionTransformer(to_float_array)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "log_transformer_fe = Pipeline([\n",
    "    # Hierarchical imputation on Brand_te/model_te, then log-transform\n",
    "    (\"group_impute\", GroupMedianImputer(group_cols=[\"Brand_te\", \"model_te\"])),\n",
    "    (\"to_float\", FunctionTransformer(to_float_array)),\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=False)),  # log1p handles zeros safely\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer_fe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # fill by mode instead of Unknown\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # One-hot encoding\n",
    "])\n",
    "\n",
    "# ColumnTransformer that uses all engineered features\n",
    "preprocessor_fe = ColumnTransformer([\n",
    "    (\"log\", log_transformer_fe, log_features),\n",
    "    (\"num\", numeric_transformer_fe, numeric_features),\n",
    "    (\"cat\", categorical_transformer_fe, categorical_features)\n",
    "])\n",
    "\n",
    "preprocessor_fe.fit(X_train) # Fit here already to have scaled data for feature selection later\n",
    "\n",
    "# EXPLANATIONS:\n",
    "# 1) Pipeline bundles preprocessing + model training:\n",
    "#       > Ensures all preprocessing happens inside cross-validation folds (no data leakage)\n",
    "#       > Keeps the entire workflow reproducible — scaling, encoding, and modeling are learned together\n",
    "#       > After .fit(), the final model automatically knows how to preprocess new unseen data\n",
    "#       > When saving with joblib, the entire preprocessing (imputers, scalers, encoders) and model are stored together\n",
    "\n",
    "# 2) The ColumnTransformer applies different transformations to subsets of features:\n",
    "#       > Numeric Features are handled by our custom GroupMedianImputer (domain-aware filling)\n",
    "#           - Missing numeric values are imputed hierarchically:\n",
    "#           1. By (Brand, model)\n",
    "#           2. If missing model by Brand\n",
    "#           3. If missing Brand by global median\n",
    "#       > This approach captures brand/model-level patterns (e.g. BMWs have similar engine sizes)\n",
    "#       > After imputation, StandardScaler standardizes all numeric features\n",
    "#\n",
    "#       > Log Features use the same group-median imputation, followed by log1p() transformation\n",
    "#           - log1p() compresses large, skewed values (like mileage or price-related features), stabilizing variance and helping linear models perform better\n",
    "#           - StandardScaler then scales them to zero mean and unit variance\n",
    "#\n",
    "#       > Categorical Features are handled by SimpleImputer + OneHotEncoder\n",
    "#           - SimpleImputer fills missing categorical values with the most frequent (mode) value.\n",
    "#             (Alternative would be “Unknown”, but mode keeps categories realistic, e.g. most cars in a model share the same transmission) # TODO this is not per model yet so the explanation is not fully correct, or am I missing something here? ~J\n",
    "#           - OneHotEncoder converts each categorical label (Brand, model, etc.) into binary dummy variables\n",
    "#             This lets the model use category information numerically without implying order\n",
    "#\n",
    "# 3) Overall:\n",
    "#       > The pipeline ensures consistent preprocessing across training, validation, and test data.\n",
    "#       > It combines domain knowledge (brand/model-aware imputation) with numerical scaling.\n",
    "#       > Linear models (ElasticNet, Ridge, Lasso) and tree models (HistGradientBoosting, RandomForest)\n",
    "#           can now learn from the same standardized, clean, and information-rich feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba8f763f-298d-444a-80a9-0270ba842dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71e9a293-7457-4767-8a26-7c930f7dbad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task III (3 Points): Define and Implement a clear and unambiguous strategy for feature selection. Use the methods **discussed in the course**. Present and justify your final selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d989a4b-077d-47bd-b4bb-c4528628888f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Model independent Filter Methods:\n",
    "- Remove constant numerical variables identified by VarianceThreshold\n",
    "- Check spearman correlation of numerical variables with the target and deselect features with low correlation\n",
    "- Remove unindependent categorical variables identified by Chi2\n",
    "\n",
    "Model dependent Wrapper Methods:\n",
    "- RFE LR / RFE SVR for linear Models: ElasticNet, SVM\n",
    "- Feature Importance for tree Models: RandomForest, HistGradientBoosting (see at 6 Feature Importance (with SHAP))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Relevancy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "130e5929-dcd7-48db-8afa-8f9f560968fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Selection with Filter Methods"
    }
   },
   "outputs": [],
   "source": [
    "X_train_proc_before_fs = preprocessor_fe.transform(X_train)\n",
    "\n",
    "feature_names_all = [] # TODO better and more readable way to get feature names? ~J\n",
    "for name, trans, cols in preprocessor_fe.transformers_:\n",
    "    if name != 'remainder':\n",
    "        if hasattr(trans, 'get_feature_names_out'):\n",
    "            # for categorical OHE\n",
    "            try:\n",
    "                feature_names_all.extend(trans.get_feature_names_out(cols))\n",
    "            except:\n",
    "                feature_names_all.extend(cols)\n",
    "        else:\n",
    "            feature_names_all.extend(cols)\n",
    "print(f\"All feature names after preprocessing: {feature_names_all}\")\n",
    "\n",
    "X_df = pd.DataFrame(X_train_proc_before_fs, columns=feature_names_all)\n",
    "\n",
    "\n",
    "# Variance Threshold (simple preliminary filter before applying more complex methods)\n",
    "vt = VarianceThreshold(threshold=0.0) # TODO try different thresholds (e.g. 0.01) to capture quasi-constant features and evaluate impact on performance ~J\n",
    "vt.fit(X_df)\n",
    "vt_deselect = [f for f, keep in zip(feature_names_all, vt.get_support()) if not keep]\n",
    "print(\"Features to deselect according to VarianceThreshold:\", vt_deselect)\n",
    "\n",
    "\n",
    "# Spearman correlation (numeric + log only) to identify features with low correlation to target\n",
    "numeric_log = numeric_features + log_features\n",
    "spearman_deselect = []\n",
    "for f in numeric_log:\n",
    "    if f in X_df.columns: # should always be the case: just to be safe to prevent the code from crashing\n",
    "        corr, _ = spearmanr(X_df[f], y_train)\n",
    "        if abs(corr) <= 0.05:\n",
    "            spearman_deselect.append(f)\n",
    "print(\"Features to deselect according to Spearman correlation:\", spearman_deselect)\n",
    "\n",
    "\n",
    "# # Chi2 (categorical only, must be non-negative) # TODO remove because Chi2 does not make sense to find relationships to continuous target ~J\n",
    "# cat_cols = [c for c in X_df.columns if c not in numeric_log]\n",
    "# X_cat = X_df[cat_cols].astype(float)\n",
    "# chi2_vals, _ = chi2(X_cat, y_train)\n",
    "# chi2_deselect = [f for f, val in zip(cat_cols, chi2_vals) if val <= 0]\n",
    "# print(\"Features to deselect according to Chi²:\", chi2_deselect)\n",
    "\n",
    "irrelevant_features = list(set(vt_deselect + spearman_deselect)) # TODO deselect all the identified features for upcoming models instead of only removing them for ElasticNet ~J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Redundancy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_variables = []\n",
    "# TODO Maybe add filter methods for redundant features (e.g., high inter-feature correlation) ~J\n",
    "deselect_features = list(set(irrelevant_features + redundant_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Removal of identified features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the identified features for all upcoming models for consistency and to ensure more stable models even if they natively handle irrelevant features well (tree-based models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new, clean feature lists by removing the identified features\n",
    "log_features_clean = [f for f in log_features if f not in deselect_features]\n",
    "numeric_features_clean = [f for f in numeric_features if f not in deselect_features]\n",
    "categorical_features_clean = [f for f in categorical_features if f not in deselect_features]\n",
    "\n",
    "feature_names_all = log_features_clean + numeric_features_clean + categorical_features_clean # TODO refactor to feature_names_all_cleaned\n",
    "\n",
    "# Rebuild the preprocessor with the cleaned feature lists to be used for upcoming models\n",
    "preprocessor_fe_clean = ColumnTransformer([\n",
    "    (\"log\", log_transformer_fe, log_features_clean),\n",
    "    (\"num\", numeric_transformer_fe, numeric_features_clean),\n",
    "    (\"cat\", categorical_transformer_fe, categorical_features_clean)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d62d3f6a-baa5-40e9-913a-b99f443b5080",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Deselection for linear Models"
    }
   },
   "outputs": [],
   "source": [
    "# TODO @Samu 2 questions regarding the following commented-out code:\n",
    "# 1) can this be removed when we are using the preprocessor_fe_clean for the elasticnet model as well?\n",
    "# 2) Why was log_transformer not included in the linear model preprocessing before? ~J\n",
    "\n",
    "\n",
    "\n",
    "# # Numeric/log features for linear models\n",
    "# linear_numeric_features = [f for f in numeric_features + log_features if f not in spearman_deselect]\n",
    "\n",
    "# preprocessor_linear = ColumnTransformer([\n",
    "#     (\"num\", numeric_transformer_fe, linear_numeric_features),\n",
    "#     (\"cat\", categorical_transformer_fe, categorical_features)\n",
    "# ], remainder=\"drop\")\n",
    "\n",
    "# # => use preprocessor_linear for linear model setup; since tree models are indifferent to irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0dc6124-f736-46fb-9f73-895b930faae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Model Evaluation Metrics, Baselining, Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06207ec2-af0d-489f-a23d-467019791499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TASK IV (4 Points): Build a simple model and assess the performance\n",
    "- Identify the type of problem and select the relevant algorithms\n",
    "- Select one Model Assessment Strategy to use throughout your work. Which metrics are you using to evaluate your model and why?\n",
    "\n",
    "\n",
    "=> Tip from lecturer: Use RandomSearch instead of GridSearchCV, set a wider Range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6297f034-638d-4db6-a681-597f20036432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definition of Function for Model Assessment"
    }
   },
   "source": [
    "**MAE (Mean Absolute Error):**\n",
    "- average absolute deviation between predicted and true car prices\n",
    "- easy to interpret in pounds, same metric used by Kaggle competition\n",
    "\n",
    "**RMSE (Root Mean Squared Error):**\n",
    "- sensitive to outliers, helps identify large prediction errors\n",
    "\n",
    "**R²:**\n",
    "- Coefficient of determination: proportion of variance explained by the model\n",
    "- 1.0 = perfect predictions, 0.0 = same as predicting mean, < 0.0 = worse than mean\n",
    "\n",
    "=> We define the metrics in the method `print_metrics` in file `car_functions.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Baseline (mean and median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b26348e-a5ff-4dc9-9d61-7f59d0ea335d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Baselining mean/median"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: use CV here for evaluation too or remove the cell\n",
    "# mean_pred = y_train.mean()\n",
    "# median_pred = y_train.median()\n",
    "\n",
    "# print(\"baseline mean predictor: \")\n",
    "# print_metrics(y_val, [mean_pred]*len(y_val))\n",
    "# # MAE: 6949.2397 | RMSE: 9544.0803 | R2: -0.0001\n",
    "\n",
    "# print(\"-\"*150)\n",
    "\n",
    "# print(\"baseline median predictor: \") \n",
    "# print_metrics(y_val, [median_pred]*len(y_val))\n",
    "# # MAE: 6714.2387 | RMSE: 9774.3098 | R2: -0.0489"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Pipeline Definitions (preprocessor + model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "04d3aa5b-36ca-4dba-9592-24edd345897d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Models Setup (inkl. Prepro in Pipeline)"
    }
   },
   "outputs": [],
   "source": [
    "##### Split definitions into original features and engineered features pipelines\n",
    "\n",
    "### LINEAR MODEL (ElasticNet)\n",
    "\n",
    "elastic_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", ElasticNet(\n",
    "        alpha=0.01,\n",
    "        l1_ratio=0.5,\n",
    "        max_iter=30000,\n",
    "        tol=1e-4,\n",
    "        selection=\"cyclic\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "elastic_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe_clean), \n",
    "    (\"model\", ElasticNet(\n",
    "        alpha=0.01,\n",
    "        l1_ratio=0.5,\n",
    "        max_iter=30000,\n",
    "        tol=1e-4,\n",
    "        selection=\"cyclic\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### TREE MODELS\n",
    "\n",
    "# HistGradientBoostingRegressor\n",
    "\n",
    "hgb_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "hgb_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe_clean),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# RandomForestRegressor\n",
    "\n",
    "rf_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe_clean),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# ExtraTreesRegressor\n",
    "\n",
    "et_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "et_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe_clean),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### KERNEL-BASED MODEL (SVR)\n",
    "\n",
    "svr_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", SVR(\n",
    "        kernel=\"rbf\",\n",
    "        C=10,\n",
    "        epsilon=0.1,\n",
    "        gamma=\"scale\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "svr_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe_clean),\n",
    "    (\"model\", SVR(\n",
    "        kernel=\"rbf\",\n",
    "        C=10,\n",
    "        epsilon=0.1,\n",
    "        gamma=\"scale\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# ENSEMBLE META MODEL (Stacking)\n",
    "\n",
    "stack_pipe_orig = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"elastic_orig\", elastic_pipe_orig),\n",
    "        (\"hgb_orig\", hgb_pipe_orig),\n",
    "        (\"rf_orig\", rf_pipe_orig),\n",
    "    ],\n",
    "    final_estimator=HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42\n",
    "    ),\n",
    "    passthrough=False,   # <- was True: disable raw-X passthrough to avoid string->float error\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "stack_pipe_fe = StackingRegressor(\n",
    "    estimators=[\n",
    "        # (\"elastic_fe\", elastic_pipe_fe),\n",
    "        (\"hgb_fe\", hgb_pipe_fe),\n",
    "        (\"rf_fe\", rf_pipe_fe),\n",
    "    ],\n",
    "    final_estimator=HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42\n",
    "    ),\n",
    "    passthrough=False,   # <- same here\n",
    "    n_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 First run of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "80a5dc82-4f35-4d66-9e97-e249166949e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First evaluation of metrics based on original and engineered feature pipeline to decide how to proceed\n",
    "\n",
    "\n",
    "models_orig = {\n",
    "    # \"ElasticNet_orig\": elastic_pipe_orig,\n",
    "    \"HGB_orig\": hgb_pipe_orig,\n",
    "    \"RF_orig\": rf_pipe_orig,\n",
    "    \"ET_orig\": et_pipe_orig,\n",
    "    \"SVR_orig\": svr_pipe_orig,\n",
    "    \"Stack_orig\": stack_pipe_orig,\n",
    "}\n",
    "\n",
    "models_fe = {\n",
    "    # \"ElasticNet_fe\": elastic_pipe_fe,\n",
    "    \"HGB_fe\": hgb_pipe_fe,\n",
    "    \"RF_fe\": rf_pipe_fe,\n",
    "    \"ET_fe\": et_pipe_fe,\n",
    "    \"SVR_fe\": svr_pipe_fe,\n",
    "    \"Stack_fe\": stack_pipe_fe,\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# for name, model in {**models_orig, **models_fe}.items():\n",
    "# TODO remove this again and fit on original models as well (did this only to save time during testing) ~J\n",
    "for name, model in {**models_fe}.items():\n",
    "    print(f\"Fitting {name} with cross-validation...\")\n",
    "    \n",
    "    # Perform cross-validation on the entire training set\n",
    "    cv_results = cross_validate(\n",
    "        model, \n",
    "        X_train, \n",
    "        y_train,\n",
    "        cv=3,\n",
    "        scoring={\n",
    "            'neg_mae': 'neg_mean_absolute_error',\n",
    "            'neg_mse': 'neg_mean_squared_error',\n",
    "            'r2': 'r2'\n",
    "        },\n",
    "        return_train_score=False,\n",
    "        verbose=3,\n",
    "        n_jobs=-2\n",
    "    )\n",
    "    \n",
    "    # Calculate mean metrics across folds\n",
    "    mae = -cv_results['test_neg_mae'].mean()\n",
    "    rmse = np.sqrt(-cv_results['test_neg_mse'].mean())\n",
    "    r2 = cv_results['test_r2'].mean()\n",
    "    \n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"feature_set\": \"original\" if name.endswith(\"_orig\") else \"engineered\",\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "    })\n",
    "\n",
    "results_df = (\n",
    "    pd.DataFrame(results)\n",
    "      .sort_values([\"feature_set\", \"MAE\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# Long Duration (with orig ca 25mins VS without orig ca 6mins VS with CV ca 16mins VS with njobs=-1 ca )\n",
    "\n",
    "# Predicted on hold-out val set (20%):\n",
    "#       model feature_set          MAE          RMSE        R2\n",
    "# 0     RF_fe  engineered  1299.728938  4.509435e+06  0.950490\n",
    "# 1  Stack_fe  engineered  1321.130612  4.831609e+06  0.946953\n",
    "# 2     ET_fe  engineered  1328.051439  4.707534e+06  0.948315\n",
    "# 3    HGB_fe  engineered  1534.496164  5.609255e+06  0.938415\n",
    "# 4    SVR_fe  engineered  2955.064750  3.242891e+07  0.643956\n",
    "\n",
    "# Predicted using 3-fold CV on entire data:\n",
    "#       model feature_set          MAE         RMSE        R2\n",
    "# 0     RF_fe  engineered  1336.806163  2375.850617  0.940424\n",
    "# 1  Stack_fe  engineered  1357.266391  2505.029128  0.933786\n",
    "# 2     ET_fe  engineered  1364.212656  2399.654669  0.939223\n",
    "# 3    HGB_fe  engineered  1551.419964  2503.445871  0.933858\n",
    "# 4    SVR_fe  engineered  3068.524237  6130.420383  0.603579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO the following markdown and reasoning for hyperparameter tuning has to be adjusted regarding new insights (e.g. ET is not underperforming anymore) ~J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffadcd30-bf06-4504-a3f5-a6c0e43dbd8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After a first run comparing the original feature pipeline and the engineered feature pipeline for all models, we decided to focus on RandomForest and HistGradientBoost. \n",
    "\n",
    "They seem to have the best prediction performance for now. StackingRegressor currently performs best, but since it is blending existing models, we will focus on that and reevaluate in the end.\n",
    "\n",
    "With ExtraTrees and SVR really underperforming, we decide not to do Hyperparameter Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Hyperparameter Tuning and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to use it here and potentially use it later for a final hyperparameter tuning after feature selection again\n",
    "def model_hyperparameter_tuning(model_estimator, param_dist, n_iter=100, splits=5):\n",
    "    \n",
    "    cv = KFold(n_splits=splits, shuffle=True, random_state=42) # 5 folds for more robust estimation\n",
    "\n",
    "    # Randomized search setup\n",
    "    model_random = RandomizedSearchCV(\n",
    "        estimator=model_estimator,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,                      # nnumber of different hyperparameter combinations that will be randomly sampled and evaluated (more iterations = more thorough search but longer runtime)\n",
    "        scoring={\n",
    "            'mae': 'neg_mean_absolute_error',\n",
    "            'mse': 'neg_mean_squared_error',\n",
    "            'r2': 'r2'\n",
    "        },\n",
    "        refit='mae', # Refit the best model based on MAE on the whole training set\n",
    "        cv=cv,\n",
    "        n_jobs=-2,\n",
    "        random_state=42,\n",
    "        verbose=3,\n",
    "    )\n",
    "\n",
    "    # Fit the search\n",
    "    model_random.fit(X_train, y_train)\n",
    "\n",
    "    mae = -model_random.cv_results_['mean_test_mae'][model_random.best_index_]\n",
    "    mse = -model_random.cv_results_['mean_test_mse'][model_random.best_index_]\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = model_random.cv_results_['mean_test_r2'][model_random.best_index_]\n",
    "\n",
    "    print(\"Model Results (CV metrics):\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(\"Best Model params:\", model_random.best_params_)\n",
    "\n",
    "    return model_random.best_estimator_ # return the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c880d2d-c7aa-478f-8d34-90aa6e33a172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.1 ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "18042056-7ef9-489f-afc3-958fe02fbfc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ElasticNet"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: ElasticNet\n",
    "\n",
    "elastic_param_grid = {\n",
    "    \"model__alpha\": [0.001],    # also tried 0.01, 0.05, 0.1, 0.5\n",
    "    \"model__l1_ratio\": [0.9]    # also tried 0.1, 0.3, 0.5, 0.7  \n",
    "}\n",
    "\n",
    "# CV: Calculate all metrics but use MAE for selecting best model\n",
    "elastic_grid = GridSearchCV(\n",
    "    elastic_pipe_fe, \n",
    "    param_grid=elastic_param_grid,\n",
    "    cv=5,\n",
    "    scoring={\n",
    "        'mae': 'neg_mean_absolute_error',\n",
    "        'mse': 'neg_mean_squared_error',\n",
    "        'r2': 'r2'\n",
    "    },\n",
    "    refit='mae', # Refit the best model based on MAE on the whole training set\n",
    "    n_jobs=-2,\n",
    "    verbose=3,\n",
    "    return_train_score=False\n",
    ")\n",
    "elastic_grid.fit(X_train, y_train)\n",
    "\n",
    "# Get mean metrics across folds\n",
    "mae = -elastic_grid.cv_results_['mean_test_mae'][elastic_grid.best_index_]\n",
    "mse = -elastic_grid.cv_results_['mean_test_mse'][elastic_grid.best_index_]\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = elastic_grid.cv_results_['mean_test_r2'][elastic_grid.best_index_]\n",
    "print(\"ElasticNet Results (CV on entire train set):\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(\"Best ElasticNet params:\", elastic_grid.best_params_)\n",
    "\n",
    "elastic_best = elastic_grid.best_estimator_ # Final model trained on entire training set with best hyperparameters minimizing MAE\n",
    "\n",
    "# Long Duration (Before removal of OHE-categoricals interrupted kernel after 64mins VS after removal ca 1min -> now 15secs with njobs=-2)\n",
    "\n",
    "# ElasticNet Results: \n",
    "# MAE: 2353.9112 | RMSE: 13356867.7860 | R2: 0.8534\n",
    "# Best ElasticNet params: {'model__alpha': 0.001, 'model__l1_ratio': 0.9}\n",
    "\n",
    "# MAE: 2589.6100\n",
    "# RMSE: 4104.4515\n",
    "# R²: 0.8222\n",
    "# Best ElasticNet params: {'model__alpha': 0.001, 'model__l1_ratio': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "231a027a-dfc8-4e1e-834a-d77bf86eee91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO adjust this approach after removing ohe categoricals and adding feature selection in Section 3 -> We only have 19 features now and RFE is not filtering any because there are already preselected ~J\n",
    "\n",
    "# # ElasticNet + RFE (no CV)\n",
    "# # reuse the tuned ElasticNet from the best pipeline\n",
    "# en_base = clone(elastic_best.named_steps[\"model\"])\n",
    " \n",
    "# rfe_pipe_linear = Pipeline([\n",
    "#     (\"preprocess\", preprocessor_fe_clean),\n",
    "#     (\"rfe\", RFE(\n",
    "#         estimator=en_base,\n",
    "#         n_features_to_select=100,\n",
    "#         step=0.1,              # remove ~10% of features per iteration\n",
    "#         importance_getter=\"auto\"\n",
    "#     )),\n",
    "#     (\"model\", clone(en_base))\n",
    "# ])\n",
    " \n",
    "# # train / predict / evaluate\n",
    "# rfe_pipe_linear.fit(X_train, y_train)\n",
    "# val_pred_linear_rfe = rfe_pipe_linear.predict(X_val)\n",
    " \n",
    "# print(\"ElasticNet with RFE (no CV):\")\n",
    "# print_metrics(y_val, val_pred_linear_rfe)\n",
    " \n",
    "# # show which features were removed\n",
    "# rfe = rfe_pipe_linear.named_steps[\"rfe\"]\n",
    "# feat_names = feature_names_all\n",
    "# # (\n",
    "# #     deselect_features + # Mention the previously removed features here too to have the full list of removed features\n",
    "# #     list(rfe_pipe_linear.named_steps[\"preprocess\"]\n",
    "# #          .named_transformers_[\"cat\"].named_steps[\"encoder\"]\n",
    "# #          .get_feature_names_out(categorical_features))\n",
    "# # )\n",
    "# removed = [n for n, keep in zip(feat_names, rfe.support_) if not keep]\n",
    "\n",
    "# print(f\"\\nRemoved Features through RFE ({len(removed)}):\")\n",
    "# for feat in removed:\n",
    "#     print(f\"  - {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21989bd5-8ab1-4d43-a099-3b2f412c94e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reasoning**: We used 100 features as an initial, arbitrary cutoff for feature selection in the ElasticNet model. Preliminary experiments and insights from the EDA (see separate notebook) indicated that tree-based methods are likely to perform better. Therefore, we prioritized feature selection for the tree-based models based on SHAP values.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad5a0287-45b6-4022-8d47-cde8cc52469e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.2 HistGradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "998dab6d-7494-483c-ab9e-a075b4c75c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hgb_param_dist = {\n",
    "    \"model__learning_rate\": uniform(0.01, 0.15),       # samples values\n",
    "    \"model__max_leaf_nodes\": randint(50, 150),         \n",
    "    \"model__min_samples_leaf\": randint(2, 20),         # samples leaf sizes between 2–20\n",
    "    \"model__max_iter\": randint(200, 900),              # tries 200–900 iterations\n",
    "    \"model__l2_regularization\": uniform(0.0, 1.0),      # samples small regularization values\n",
    "    \"model__early_stopping\": [True],\n",
    "    \"model__validation_fraction\": [0.1],\n",
    "    \"model__n_iter_no_change\": [20],\n",
    "    \"model__random_state\":[42]\n",
    "}\n",
    "\n",
    "# optimized the parameter distributions based on previous runs to focus search space\n",
    "# hgb_param_dist = {\n",
    "#     \"model__learning_rate\": [0.06389789198396824],\n",
    "#     \"model__max_leaf_nodes\": [105],\n",
    "#     \"model__min_samples_leaf\": [3],\n",
    "#     \"model__max_iter\": [642],\n",
    "#     \"model__l2_regularization\": [0.942853570557981],\n",
    "#     \"model__early_stopping\": [True],\n",
    "#     \"model__validation_fraction\": [0.1],\n",
    "#     \"model__n_iter_no_change\": [20],\n",
    "#     \"model__random_state\":[42]\n",
    "# }\n",
    "\n",
    "hgb_best = model_hyperparameter_tuning(hgb_pipe_fe, hgb_param_dist) \n",
    "\n",
    "# Old preset hps (1min):\n",
    "# MAE: 1289.7294\n",
    "# RMSE: 2185.5006\n",
    "# R²: 0.9498\n",
    "\n",
    "# Reapplying RandomizedSearchCV (40mins):\n",
    "# MAE: 1289.6713\n",
    "# RMSE: 2181.5766\n",
    "# R²: 0.9500\n",
    "# Best Model params: {'model__early_stopping': True, 'model__l2_regularization': np.float64(0.14092422497476265), 'model__learning_rate': np.float64(0.08219772826786356), 'model__max_iter': 464, 'model__max_leaf_nodes': 108, 'model__min_samples_leaf': 8, 'model__n_iter_no_change': 20, 'model__random_state': 42, 'model__validation_fraction': 0.1}\n",
    "\n",
    "# Using transmission and fuelType as OHE instead of TE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70de2673-0443-4cc4-b1cd-f3813a969246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.3 RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "# Old parameter distribution\n",
    "rf_param_dist = {\n",
    "    \"model__n_estimators\": randint(200, 600),        # number of trees\n",
    "    \"model__max_depth\": randint(5, 40),              # depth of each tree\n",
    "    \"model__min_samples_split\": randint(2, 10),      # min samples to split an internal node\n",
    "    \"model__min_samples_leaf\": randint(1, 8),        # min samples per leaf\n",
    "    \"model__max_features\": [\"sqrt\", None],           # feature sampling strategy (sqrt performed better than log2 in previous tests)\n",
    "    \"model__bootstrap\": [False]                      # use bootstrapping or not (False performed better than True in previous tests)\n",
    "}\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "# rf_param_dist = {\n",
    "#     \"model__n_estimators\": [467],        \n",
    "#     \"model__max_depth\": [32],              \n",
    "#     \"model__min_samples_split\": [9],      \n",
    "#     \"model__min_samples_leaf\": [1],        \n",
    "#     \"model__max_features\": [\"sqrt\"],         \n",
    "#     \"model__bootstrap\": [False]                \n",
    "# }\n",
    "\n",
    "rf_best_rand = model_hyperparameter_tuning(rf_pipe_fe, rf_param_dist)\n",
    "# joblib.dump(rf_best_rand, \"rf_best_rand.pkl\")\n",
    "\n",
    "\n",
    "# Long Duration (~2min)\n",
    "\n",
    "# MAE: 1275.1518\n",
    "# RMSE: 2232.9070\n",
    "# R²: 0.9477\n",
    "\n",
    "# Reapplying RandomizedSearchCV (~120mins):\n",
    "# MAE: 1272.4144\n",
    "# RMSE: 2214.9228\n",
    "# R²: 0.9486\n",
    "# Best Model params: {'model__bootstrap': False, 'model__max_depth': 27, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 5, 'model__n_estimators': 328}\n",
    "\n",
    "# Using transmission and fuelType as OHE instead of TE:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae6d3b92-efc6-4d8a-b3e1-cb6aff36b84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.4 StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6f4ff12e-07a4-47f5-8c08-43b5d8d86942",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ExtraTrees"
    }
   },
   "outputs": [],
   "source": [
    "# Old parameter distribution\n",
    "stack_param_dist = {\n",
    "    \"final_estimator__learning_rate\": uniform(0.02, 0.1),\n",
    "    \"final_estimator__max_depth\": randint(3, 10),\n",
    "    \"final_estimator__min_samples_leaf\": randint(3, 20),\n",
    "    \"final_estimator__l2_regularization\": uniform(0.0, 1.0),\n",
    "}\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "# stack_param_dist = {\n",
    "#     \"final_estimator__learning_rate\": [0.0960571445127933],\n",
    "#     \"final_estimator__max_depth\": [5],\n",
    "#     \"final_estimator__min_samples_leaf\": [10],\n",
    "#     \"final_estimator__l2_regularization\": [0.3745401188473625]\n",
    "# }\n",
    "\n",
    "stack_best = model_hyperparameter_tuning(stack_pipe_fe, stack_param_dist, splits=3)\n",
    "# joblib.dump(stack_best, \"stack_best.pkl\")\n",
    "\n",
    "\n",
    "# Long Duration (~3mins)\n",
    "\n",
    "# MAE: 1351.8682\n",
    "# RMSE: 2498.2822\n",
    "# R²: 0.9342\n",
    "\n",
    "# After RandomizedSearchCV:\n",
    "# MAE: 1350.4717\n",
    "# RMSE: 2497.0474\n",
    "# R²: 0.9343\n",
    "# Best Model params: {'final_estimator__l2_regularization': np.float64(0.978892858275009), 'final_estimator__learning_rate': np.float64(0.06867421529594551), 'final_estimator__max_depth': 6, 'final_estimator__min_samples_leaf': 13}\n",
    "\n",
    "# Removed ElasticNet from stacking due to poor performance compared to RF and HGB alone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "852bd7bc-33d9-4ae4-a552-0850b200be11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Feature Importance of Tree Models (with SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c02463-def6-4c5c-93ce-5ca710b6eb7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  **Problem:** Current feature selection targets linear models\n",
    "  (ElasticNet), but we primarily use tree-based models (HGB,\n",
    "  RandomForest).\n",
    "\n",
    "  **Solution:** Use SHAP (SHapley Additive exPlanations) to\n",
    "  identify feature importance specifically for tree models\n",
    "\n",
    "  **Why SHAP for Trees:**\n",
    "  - Provides exact feature importance values for tree-based\n",
    "  models\n",
    "  - Tree models handle irrelevant features, but noise features\n",
    "  still impact performance\n",
    "  - Enables data-driven selection rather than statistical filter\n",
    "  methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1d70ae47-45bb-4fdd-ad8f-a47bad927b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to compute SHAP-based feature importance for any tree model\n",
    "def calculate_shap_values(best_pipeline, X_train, log_features, numeric_features, categorical_features, sample_size=1000, seed=42, label=None):\n",
    "    '''\n",
    "    We use SHAP's TreeExplainer to calculate feature importance values. TreeExplainer is specifically optimized for tree-based models and provides exact Shapley values efficiently.\n",
    "    '''\n",
    "    # Preprocess training data with the pipeline’s preprocessor\n",
    "    pre = best_pipeline.named_steps[\"preprocess\"]\n",
    "    X_train_proc = pre.transform(X_train)\n",
    "\n",
    "    # Build feature names in ColumnTransformer order: log, numeric, one-hot(cat)\n",
    "    # cat_names = pre.named_transformers_[\"cat\"].named_steps[\"encoder\"].get_feature_names_out(categorical_features) # TODO add cat_names back if trying multiple encodings ~J\n",
    "    feature_names_all = list(log_features) + list(numeric_features) #+ list(cat_names) # TODO add cat_names back if trying multiple encodings ~J\n",
    "    feature_names_all = [f for f in feature_names_all if f not in deselect_features]  # ensure deselected features are not included\n",
    "\n",
    "    # SHAP for tree models\n",
    "    model = best_pipeline.named_steps[\"model\"]\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "\n",
    "    # Sample for speed, reproducible\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = min(sample_size, len(X_train_proc))\n",
    "    idx = rng.choice(len(X_train_proc), n, replace=False)\n",
    "\n",
    "    shap_values = explainer.shap_values(X_train_proc[idx])\n",
    "    importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    shap_df = (pd.DataFrame({\"feature\": feature_names_all, \"importance\": importance})\n",
    "               .sort_values(\"importance\", ascending=False)\n",
    "               .reset_index(drop=True))\n",
    "\n",
    "    tag = label or model.__class__.__name__\n",
    "    print(f\"\\Most important features ({tag}):\")\n",
    "    print(shap_df.to_string(index=False))\n",
    "\n",
    "    return shap_df, feature_names_all, X_train_proc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b090388-f49d-4c7e-9a79-e9881c577c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# General function which can be called by the models to avoid redundant code and enable easy maintenance\n",
    "def train_model_on_best_features(baseline_mae, shap_importance, model, X_train_processed, X_val_processed, range_number_of_features, feature_names_all):\n",
    "    '''\n",
    "    We systematically test different numbers of top features to find the optimal subset:\n",
    "    We train the model with the same optimized hyperparameters but using only the most important features identified by SHAP\n",
    "    '''\n",
    "    # Track best model\n",
    "    results = []\n",
    "    best_model = None\n",
    "    best_mae = float(\"inf\")\n",
    "    best_n = None\n",
    "    best_features = None\n",
    "\n",
    "    # Find best feature counts\n",
    "    for n_features in range_number_of_features:\n",
    "        # Select top N features\n",
    "        top_features = shap_importance.head(n_features)[\"feature\"].tolist()\n",
    "        feature_indices = [i for i, fname in enumerate(feature_names_all) if fname in top_features]\n",
    "\n",
    "        X_train_subset = X_train_processed[:, feature_indices]\n",
    "        X_val_subset   = X_val_processed[:, feature_indices]\n",
    "\n",
    "        # Train and predict using selected amount of features (model uses tuned hyperparams)\n",
    "        model.fit(X_train_subset, y_train)\n",
    "        pred_subset = model.predict(X_val_subset)\n",
    "        mae_subset = mean_absolute_error(y_val, pred_subset)\n",
    "        results.append({\"n_features\": n_features, \"mae\": mae_subset})\n",
    "\n",
    "        # Check whether current mae is best so far\n",
    "        if mae_subset < best_mae:\n",
    "            best_mae = mae_subset\n",
    "            best_n = n_features\n",
    "            best_model = model\n",
    "            best_features = top_features\n",
    "\n",
    "        # Print MAE for each amount of features\n",
    "        if n_features in range_number_of_features:\n",
    "            improvement_rf = baseline_mae - mae_subset\n",
    "            print(f\"Top {n_features:3d} features: MAE: {mae_subset:.1f} (Δ: {improvement_rf:+.1f})\")\n",
    "\n",
    "\n",
    "    print(f\"\\nOptimal feature selection results:\")\n",
    "    print(f\"Best performance with {best_n} features: MAE: {best_mae:.2f}\")\n",
    "    print(f\"Improvement over baseline: {baseline_mae - best_mae:+.2f} MAE\\n\")\n",
    "\n",
    "    print(f\"Optimal {best_n} features for production model:\")\n",
    "    for i, feat in enumerate(best_features, start=1):\n",
    "        imp = shap_importance.loc[shap_importance['feature'] == feat, 'importance'].values[0]\n",
    "        print(f\"{i:2d}. {feat:25s} ({imp:.1f})\")\n",
    "    \n",
    "    # Retrain a fresh final estimator on the full training set restricted to best_features (guarantees correct input dimension)\n",
    "    selected_idx = [i for i, fname in enumerate(feature_names_all) if fname in best_features]\n",
    "    final_est = clone(model)\n",
    "    final_est.fit(X_train_processed[:, selected_idx], y_train)\n",
    "\n",
    "    return final_est, best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dac80f70-0890-45e0-b87f-33274924d2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_top_shap(shap_df, model_name, top_k=20):\n",
    "    top_df = shap_df.head(top_k).iloc[::-1]\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.barh(top_df[\"feature\"], top_df[\"importance\"], color=\"#4C72B0\")\n",
    "    ax.set_xlabel(\"Average |SHAP| value\")\n",
    "    ax.set_title(f\"Top {top_k} {model_name} features by SHAP\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7de8112-ad4c-4761-a868-9ed084f86174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.1 HGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cadbb5c-a5f6-4fb1-963c-70ff79b146f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 1: Baseline Performance with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "51a1844a-48ae-4a25-8f9a-5759a7d13a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_val_processed_hgb = hgb_best.named_steps[\"preprocess\"].transform(X_val)\n",
    "hgb_val_pred = hgb_best.named_steps[\"model\"].predict(X_val_processed_hgb)\n",
    "n_features_total = X_val_processed_hgb.shape[1]\n",
    "baseline_mae_hgb = mean_absolute_error(y_val, hgb_val_pred)\n",
    "\n",
    "print(\"Baseline Performance of HGB model after Hyperparameter Tuning:\\n\")\n",
    "print_metrics(y_val, hgb_val_pred)\n",
    "print(f\"\\nTotal features used: {n_features_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a381d64-0406-4daa-a147-95081efa4eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 2: SHAP Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fa44b8cb-0c89-4054-bbaf-38250ce56ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#function\n",
    "shap_importance_df_hgb, feature_names_all_hgb, X_train_processed_hgb = calculate_shap_values(\n",
    "    hgb_best, X_train, log_features, numeric_features, categorical_features,\n",
    "    sample_size=1000, seed=42, label=\"HGB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "355aaca1-92eb-42d9-82d5-e76c3a8b99bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 3: Automated Feature Selection Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "02c55af4-eea0-4be4-bae7-0e103184254c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define model with the same hyperparams\n",
    "hgb_model = hgb_best.named_steps[\"model\"]\n",
    "hgb_selected = HistGradientBoostingRegressor(**hgb_model.get_params())\n",
    "\n",
    "# Number of top SHAP features to try\n",
    "range_number_of_features_hgb = range(15, n_features_total + 1, 1) # After previous runs with higher step size, the range is now narrowed down\n",
    "\n",
    "# Train/evaluate on subsets of top features\n",
    "best_model_hgb, best_features_hgb = train_model_on_best_features(\n",
    "    baseline_mae_hgb, shap_importance_df_hgb,\n",
    "    hgb_selected,\n",
    "    X_train_processed_hgb, X_val_processed_hgb,\n",
    "    range_number_of_features_hgb,\n",
    "    feature_names_all_hgb\n",
    ")\n",
    "\n",
    "# Long Duration (ca 2mins)\n",
    "\n",
    "# Notes by Jan: (TODO to be removed)\n",
    "# start: Best performance with 17 features: MAE: 1288.12\n",
    "# removed ohe categoricals: Best performance with 19 features: MAE: 1282.81\n",
    "# After FS removal: Best performance with 19 features (all features bc deselection filtered the same features as SHAP): MAE: 1282.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aba33f2a-1703-4f17-a390-434d1e858b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# HGB SHAP bar plot\n",
    "plot_top_shap(shap_importance_df_hgb, \"HGB\", top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1e893a1a-3c61-4f13-9fd9-35715c65d492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build the final pipeline with feature selection included\n",
    "def select_best_features_hgb(X):\n",
    "    # X is the output of the preprocessing step: Matrix with all features after they have been scaled, encoded, and combined by the preprocessor\n",
    "    idx = [i for i, fname in enumerate(feature_names_all) if fname in best_features_hgb]\n",
    "    return X[:, idx]\n",
    "\n",
    "hgb_final_pipe = Pipeline([\n",
    "    (\"preprocess\", hgb_best.named_steps[\"preprocess\"]),\n",
    "    (\"feature_selector\", FunctionTransformer(select_best_features_hgb, validate=False)), # a flexible wrapper that applies a custom function to the data flow in a pipeline\n",
    "    (\"model\", best_model_hgb)\n",
    "])\n",
    "\n",
    "# Save the best model for later use\n",
    "joblib.dump(hgb_final_pipe, \"hgb_best_feature.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a16b675a-660c-46c8-8430-5b28d91b8c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.2 RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f65c9142-f110-4d6f-a31c-5764d2492fc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 1: Baseline Performance with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "95adb47f-fb45-47f0-b24d-e75d5ef1c68e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the tuned RF pipeline (rf_best_rand) and compute baseline on the validation set\n",
    "X_val_processed_rf = rf_best_rand.named_steps[\"preprocess\"].transform(X_val)\n",
    "rf_val_pred = rf_best_rand.named_steps[\"model\"].predict(X_val_processed_rf)\n",
    "n_features_total_rf = X_val_processed_rf.shape[1] # TODO cant we just use one val_processed and one n_features_total or why did we split that? ~J\n",
    "baseline_mae_rf = mean_absolute_error(y_val, rf_val_pred)\n",
    "\n",
    "print(\"Baseline Performance of RF model after Hyperparameter Tuning:\\n\")\n",
    "print_metrics(y_val, rf_val_pred)\n",
    "print(f\"\\nTotal features used: {n_features_total_rf}\")\n",
    "# Notes by Jan: (TODO to be removed)\n",
    "# start: MAE: 1322.4418 | RMSE: 4630733.9922 | R2: 0.9492 (Total features used: 155)\n",
    "# after removing ohe categoricals: MAE: 1282.0634 | RMSE: 4317505.8622 | R2: 0.9526 (Total features used: 22)\n",
    "# After FS removal: MAE: 1275.2603 | RMSE: 4274727.2255 | R2: 0.9531 (Total features used: 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eef64537-b8f4-4f70-b060-78606e96731b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 2: SHAP Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8dfcd356-3197-4fa0-b6bd-1eb3a0e1697a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap_importance_df_rf, feature_names_all_rf, X_train_processed_rf = calculate_shap_values(\n",
    "    rf_best_rand, X_train, log_features, numeric_features, categorical_features,\n",
    "    sample_size=100, seed=42, label=\"RF\"\n",
    ")\n",
    "\n",
    "# Long Duration (ca 4mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0af8a6-1394-464d-8e8c-d452d0138cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 3: Automated Feature Selection Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "da117a79-190d-4a3e-ba7f-86f6e166170d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the same processed validation data and reuse tuned RF hyperparameters\n",
    "rf_params = {k.replace(\"model__\", \"\"): v for k, v in rf_random.best_params_.items()}\n",
    "rf_selected = RandomForestRegressor(random_state=42, n_jobs=-1, **rf_params)\n",
    "range_number_of_features_rf = range(16, n_features_total_rf + 1, 1) # After previous runs with higher step size, the range is now narrowed down\n",
    "\n",
    "best_model_rf, best_features_rf = train_model_on_best_features(baseline_mae_rf, shap_importance_df_rf, rf_selected, X_train_processed_rf, X_val_processed_rf, range_number_of_features_rf, feature_names_all_rf)\n",
    "\n",
    "# Long Duration (ca 1min)\n",
    "\n",
    "# Notes by Jan: (TODO to be removed)\n",
    "# start: Best performance with 26 features: MAE: 1277.16\n",
    "# removed ohe categoricals: Best performance with 20 features: MAE: 1273.89\n",
    "# After FS removal: Best performance with 19 features (all features bc deselection filtered the same features as SHAP): MAE: 1275.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6613cd44-7c2e-49e9-a18b-ea098b686ac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RF SHAP bar plot\n",
    "plot_top_shap(shap_importance_df_rf,  \"RF\",  top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e8398f0e-737d-4ae1-8ba1-5f8f21aa211d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the best RF model for later use\n",
    "\n",
    "# Build the final RF pipeline with feature selection included\n",
    "def select_best_features_rf(X):\n",
    "    idx = [i for i, fname in enumerate(feature_names_all_rf) if fname in best_features_rf]\n",
    "    return X[:, idx]\n",
    "\n",
    "final_rf_pipe = Pipeline([\n",
    "    (\"preprocess\", rf_best_rand.named_steps[\"preprocess\"]),\n",
    "    (\"feature_selector\", FunctionTransformer(select_best_features_rf, validate=False)),\n",
    "    (\"model\", best_model_rf)\n",
    "])\n",
    "\n",
    "joblib.dump(final_rf_pipe, \"rf_best_feature.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f248b6e7-a945-45ee-aa5c-5ffd34c00cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.3 Build Final Stacking Regressor to mix tuned and feature selected HGB and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4ef23a-6313-43cc-91fc-34e3c7842684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stack_pipe_final = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"hgb_final\", hgb_final_pipe),   # tuned HGB pipeline (preprocessor + model)\n",
    "        (\"rf_final\",  final_rf_pipe),    # tuned RF pipeline (preprocessor + model)\n",
    "    ],\n",
    "    final_estimator=LinearRegression(),  # simple, perfect for 2 base preds\n",
    "    passthrough=False,                   # meta model sees only base predictions\n",
    "    cv=5,                                # proper OOF stacking\n",
    "    n_jobs=1                             # no BrokenProcessPool on Databricks\n",
    ")\n",
    "\n",
    "stack_pipe_final.fit(X_train, y_train)\n",
    "stack_val_pred = stack_pipe_final.predict(X_val)\n",
    "print_metrics(y_val, stack_val_pred)\n",
    "\n",
    "joblib.dump(stack_pipe_final, \"stack_pipe.pkl\")\n",
    "\n",
    "# MAE: 1255.3112 | RMSE: 4157099.9081 | R2: 0.9544\n",
    "\n",
    "# Kaggle Score submit 1274 !! OVERFITTED\n",
    "\n",
    "# Long Duration (ca 3mins)\n",
    "\n",
    "# Notes by Jan: (TODO to be removed)\n",
    "# start: MAE: 1256.5922 | RMSE: 4154147.3742 | R2: 0.9544\n",
    "# removed ohe categoricals: MAE: 1252.2718 | RMSE: 4146270.8422 | R2: 0.9545\n",
    "# After FS removal: MAE: 1252.9558 | RMSE: 4145097.0520 | R2: 0.9545"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f82fb46-4c09-4bd8-b405-7c99d4850eca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Final SR of the tuned HGB and RF models, did improve over the best single HGB and RF models on the validation set (MAE 1258 vs 1281/1289). \n",
    "\n",
    "However, it seems to be overfitted, Kaggle Score is only 1274\n",
    "\n",
    "Therefore we will keep the RF/HGB model => with such small difference in MAE, we further need to evaluate them both + the Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "135acfad-1ad6-49f9-97fc-f1a81d0c3187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb68e758-d7b1-4f80-9dc4-ed985f721c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Extra Task (1 Point): Be in the Top 5 Groups on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test(model_pipeline, model_name):\n",
    "    # Load best model from Joblib and predict on validation set to verify\n",
    "    pipe_best = joblib.load(model_pipeline)\n",
    "    pred_loaded = pipe_best.predict(X_val)\n",
    "    print(f\"Loaded {model_name}-model MAE on validation set: {mean_absolute_error(y_val, pred_loaded):.2f}\")\n",
    "\n",
    "    # Predict on test set\n",
    "    df_cars_test['price'] = pipe_best.predict(df_cars_test)\n",
    "    df_cars_test['price'].to_csv(f'Group05_{model_name}_Version10.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_test(\"hgb_best_feature.pkl\", \"HGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_test(\"rf_best_feature.pkl\", \"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_test(\"stack_pipe.pkl\", \"Stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b18805-e662-4807-bdb8-758df6027d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Submission with API push"
    }
   },
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c cars4you -f Group05_Version05.csv -m \"Message\" # Uncomment to submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350057a3-c771-4bf0-8fac-19b49be17e35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Score Check"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c cars4you"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "group05_main_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
