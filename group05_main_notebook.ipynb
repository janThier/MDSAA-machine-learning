{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156fc835-43ad-4e58-a2ea-e12bb559c844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Open Questions (maybe ask Ricardo):\n",
    "# - Is there downside of using sklearn pipelines or is it best practice if we understood it and can explain it well?\n",
    "# - How far can we go with cleaninig the data so before it becomes outlier handling? (e.g. only keeping cars with mpg between 5 and 150 vs. between 10 and 80)\n",
    "# - can we remove hasDamage before the actual feature selection or is it only allowed to remove features in the FS part even though this would lead to unnecessary steps and wasting computing power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0d9d0c-5598-4a4e-b28b-a2921d27ce73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Your work will be evaluated according to the following criteria:**\n",
    "- Project Structure and Notebook(s) Quality (4/20)\n",
    "- Data Exploration & Initial Preprocessing (4/20)\n",
    "- Regression Benchmarking and Optimization (7/20)\n",
    "- Open-Ended Section (4/20)\n",
    "- Deployment (1/20)\n",
    "- Extra Point: Have Project Be Publicly Available on GitHub (1/20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a937a442-783b-4e36-a847-80976966adeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Project Timeline**\n",
    "- 22.11.: Preprocessing and Model Preparation\n",
    "    - Finish clean preprocessing all included in pipeline\n",
    "    - Finish clean Hyperparameter Tuning\n",
    "- 29.11.: Feature Selection\n",
    "    - Clean and structured approach for feature selection for all models (best case: consistent approach imo)\n",
    "- 29.11.: Regression Benchmarking and Optimization\n",
    "    - Automize Optimization (add something like mlflow)\n",
    "- 06.12.: Open-End Section and Deployment\n",
    "    - Added 4 open-end-experiments\n",
    "    - Deployment\n",
    "- 13.12.: Notebook Feinschliff\n",
    "    - Super clean notebook structure similar to lab-notebooks by Ricardo\n",
    "    - Show and explain results of different models clearly in markdown tables etc. (see the lab-notebooks)\n",
    "- 14.12.: Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d89fe94-6eed-4612-ad0a-40032c4dd67a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO Open End Section:\n",
    "# Interface for new Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4467ab3-7594-4e02-a19f-8c5e47fb14e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"\n",
    "    background: rgba(25, 25, 25, 0.55);\n",
    "    backdrop-filter: blur(16px) saturate(150%);\n",
    "    -webkit-backdrop-filter: blur(16px) saturate(150%);\n",
    "    border: 1px solid rgba(255, 255, 255, 0.12);\n",
    "    border-radius: 18px;\n",
    "    padding: 45px 30px;\n",
    "    text-align: center;\n",
    "    font-family: 'Inter', 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;\n",
    "    color: #e0e0e0;\n",
    "    box-shadow: 0 0 30px rgba(0, 0, 0, 0.35);\n",
    "    margin: 40px auto;\n",
    "    max-width: 800px;\n",
    "\">\n",
    "\n",
    "  <h1 style=\"\n",
    "      font-size: 2.8em;\n",
    "      font-weight: 700;\n",
    "      margin: 0 0 8px 0;\n",
    "      letter-spacing: -0.02em;\n",
    "      background: linear-gradient(90deg, #00e0ff, #9c7eff);\n",
    "      -webkit-background-clip: text;\n",
    "      -webkit-text-fill-color: transparent;\n",
    "  \">\n",
    "      Machine Learning Project\n",
    "  </h1>\n",
    "\n",
    "  <h2 style=\"\n",
    "      font-size: 1.6em;\n",
    "      font-weight: 500;\n",
    "      margin: 0 0 25px 0;\n",
    "      color: #b0b0b0;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Cars 4 You - Predicting Car Prices\n",
    "  </h2>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.25em;\n",
    "      font-weight: 500;\n",
    "      color: #c0c0c0;\n",
    "      margin-bottom: 6px;\n",
    "  \">\n",
    "      Group 5 - Lukas Belser, Samuel Braun, Elias Karle, Jan Thier\n",
    "  </p>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.05em;\n",
    "      font-weight: 400;\n",
    "      color: #8a8a8a;\n",
    "      font-style: italic;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Machine Learning End Results Â· 22.12.2025\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/process_ML.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372c87aa-b7f3-4a34-a596-bb5e584db114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Table of Contents**\n",
    " \n",
    "- [1. Import Packages and Data](#1-import-packages-and-data)  \n",
    "  - [1.1 Import Required Packages](#11-import-required-packages)  \n",
    "  - [1.2 Load Datasets](#12-load-datasets)  \n",
    "  - [1.3 Kaggle Setup](#13-kaggle-setup)  \n",
    "- [2. Preprocessing](#2-data-cleaning-feature-engineering-split--preprocessing)  \n",
    "  - [2.1 Data Cleaning](#21-data-cleaning)  \n",
    "  - [2.2 Feature Engineering](#22-feature-engineering)  \n",
    "  - [2.3 (No) Data Split](#23-data-split)  \n",
    "  - [2.4 Encoding, Transforming and Scaling](#24-preprocessing)  \n",
    "  - [2.5. Feature Selection](#3-feature-selection)  \n",
    "- [4. Model Evaluation Metrics, Baselining, Setup](#4-model-evaluation-metrics-baselining-setup)  \n",
    "- [5. Hyperparameter Tuning and Model Evaluation](#5-hyperparameter-tuning-and-model-evaluation)  \n",
    "  - [5.1 ElasticNet](#51-elasticnet)  \n",
    "  - [5.2 HistGradientBoost](#52-histgradientboost)  \n",
    "  - [5.3 RandomForest](#53-randomforest)  \n",
    "  - [5.4 ExtraTrees](#54-extratrees)  \n",
    "- [6. Feature Importance of Tree Models (with SHAP)](#6-feature-importance-of-tree-models-with-shap)  \n",
    "  - [6.1 HGB](#61-hgb)  \n",
    "  - [6.2 RF](#62-rf)  \n",
    "- [7. Kaggle Competition](#7-kaggle-competition)  \n",
    "\n",
    "TODO finish + update toc > at the end of project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fff3e4-624f-4b1f-b258-fbbe7b69cb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Import Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e6aadb-727a-4399-8888-9d6d4e593898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aa35af5d-8257-4fb6-a380-86bbbfac3244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!pip install shap\n",
    "!pip install -U scikit-learn\n",
    "!pip install category_encoders\n",
    "!pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f54ab11e-fcfc-4278-8997-89a92adf8eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import and load Data"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({\"figure.max_open_warning\": 0, \"figure.dpi\": 100})\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE\n",
    "from scipy.stats import spearmanr, uniform, randint\n",
    "from sklearn.metrics import mean_absolute_error\n",
    " \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder, StandardScaler, FunctionTransformer, RobustScaler\n",
    "from sklearn.base import clone, BaseEstimator, TransformerMixin\n",
    " \n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, GridSearchCV, cross_validate\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from category_encoders import QuantileEncoder # used for median target encoding (sklearn only supports mean target encoding with their TargetEncoder class)\n",
    " \n",
    "from car_functions import clean_car_dataframe\n",
    "from pipeline_functions import GroupImputer, m_estimate_mean, CarFeatureEngineer, NamedFunctionTransformer, to_float_array, model_hyperparameter_tuning, DebugTransformer, MajorityVoteSelectorTransformer, CorrelationThresholdSelector, MutualInfoThresholdSelector, plot_selector_agreement\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.inspection import permutation_importance\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9737aef-d2bd-4748-89ed-9b0e2cea8637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.2 Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3da1ad2b-8ee0-48b0-917d-545c70633ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cars_train = pd.read_csv(\"train.csv\")\n",
    "df_cars_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab6500b5-94a1-4a0e-b7f6-ea9d35c787eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Kaggle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "556ffc24-70f7-48c9-96a4-3a19380f91db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Connect"
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle API Connect\n",
    "\n",
    "# Folder containing kaggle.json\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/Workspace/Users/20250355@novaims.unl.pt\" #add your own kaggle.json api token\n",
    "\n",
    "# Test\n",
    "!echo $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75ee214-0996-406f-b77f-9e72fa7153e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53a2a97-4662-40a3-8b24-b238dad5138e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6fa0cfe9-be7e-4e90-ac1c-cd600bb7d13f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Numerical / Categorical Data Cleaning: with Pandas"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier Preprocessing, Missing Value Handling and Decision justifying happens here\n",
    "# TODO add clean_car_dataframe to pipeline\n",
    "df_cars_train = clean_car_dataframe(df_cars_train)\n",
    "df_cars_test = clean_car_dataframe(df_cars_test)\n",
    "\n",
    "# Safety Check: print unique values of all columns of df_cars_train // df_cars_test to see if data cleaning worked and if there are still odd values\n",
    "for col in df_cars_train.columns:\n",
    "    print(col, df_cars_train[col].unique())\n",
    "print(\"X\"*150)\n",
    "for col in df_cars_test.columns:\n",
    "    print(col, df_cars_test[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8cfe04-5941-4c6b-a019-968a568855cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.2 (No) Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b11b9ba-9575-4f9c-bbcc-a0552567ff47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- Train: use cross-validation in the sklearn pipeline on the training data\n",
    "- Test: Use external hold-out set from kaggle as final test set    \n",
    "-> An additional val set is therefore not necessary and would waste training data\n",
    "\n",
    "\n",
    "<u>Place in the pipe:</u> The split is decided here because the data has to be split before all of the following preprocessing steps to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadf736b-afb0-4033-b6a6-dab806fe32c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train = df_cars_train.drop(columns='price')\n",
    "y_train = df_cars_train['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0bafff7-43a2-4c3c-81cf-1e54efd19ffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.3 Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "485fa926-0a42-4739-8d1e-f45287b9d348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- `Group Imputer`: We use a custom GroupImputer that imputes the missing values to be the median of entries within the same group\n",
    "    - For that we use a hierarchical structure to identify the most similar group to the one with the missing value:\n",
    "        - 1st level: ...\n",
    "        - 2nd level: ...\n",
    "        - ...\n",
    "        - 4th level: Model\n",
    "        - 5th level: Brand\n",
    "\n",
    "<u>Place in the pipe:</u> The Imputation is decided here because the data has to be imputed on original values before engineering new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Outlier Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our approach:**\n",
    "- Outlier detection through multiple methods to increase the probability that it's actually an outlier\n",
    "- ...\n",
    "\n",
    "\n",
    "<u>Place in the pipe</u>: After imputation to have no missing values left and before FE to not create new features based on (unrealistic) outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO maybe even before Imputation: \"If you impute first, you fill the original gaps based on a distribution that includes the massive outliers (skewing the mean/median). If you kill the outliers first (set to NaN), the imputation for everyone becomes cleaner.\"\n",
    "\n",
    "# TODO\n",
    "# e.g. how to handle Zeros in tax (use groupimputer?) -> features that are computed with tax are also affected and need to be handled then ~J\n",
    "# e.g. maybe outlier handling per model (if sample size big enough) ~J\n",
    "\n",
    "# TODO maybe use different outlier handling pipes for tree-based vs. linear vs. NN models to show that we understand the differences ~J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b198720e-b5a0-46ef-8055-4ac33b852410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.5 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd8845f0-241e-40a4-8c24-3e876d89b02b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- We use the class CarFeatureEngineer to be able create the engineered features within the pipeline to prevent data leakage\n",
    "\n",
    "<u>Base Feature Creation</u>\n",
    "\n",
    "These are foundational features derived directly from the original data, often to create linear relationships or capture interactions.\n",
    "- `age`: Calculated as `2020 - year`. Creates a simple linear feature representing the car's age. Newer cars (lower age) generally have higher prices.\n",
    "- `miles_per_year`: Calculated as `mileage / age`. This normalizes the car's usage, preventing high correlation (multicollinearity) between `mileage` and `age`. A 3-year-old car with 60,000 miles is different from a 6-year-old car with 60,000 miles.\n",
    "- `age_x_engine`: An interaction term `age * engineSize`. This helps the model capture non-linear relationships, such as the possibility that the value of cars with large engines might depreciate faster (or slower) than cars with small engines.\n",
    "- `mpg_x_engine`: An interaction term `mpg * engineSize`. This captures the combined effect of fuel efficiency and engine power.\n",
    "- `tax_per_engine`: Calculated as `tax / engineSize`. This feature represents the tax cost relative to the engine's power, which could be an indicator of overall running costs or vehicle class.\n",
    "- `mpg_per_engine`: Calculated as `mpg / engineSize`. This creates an \"efficiency\" metric, representing how many miles per gallon the car achieves for each unit of engine size.\n",
    "\n",
    "\n",
    "<u>Popularity & Demand Features</u>\n",
    "\n",
    "These features attempt to quantify a car's popularity or market demand, which directly influences price.\n",
    "- `model_freq`: Calculates the frequency (percentage) of each `model` in the training dataset. Popular, common models often have more stable and predictable pricing and demand.\n",
    "\n",
    "\n",
    "<u>Price Anchor Features</u>\n",
    "\n",
    "These features \"anchor\" a car's price relative to its group. They provide a strong baseline price signal based on brand, model, and configuration.\n",
    "- `brand_med_price`: The median price for the car's `Brand` (e.g., the typical price for a BMW vs. a Skoda). This captures overall brand positioning.\n",
    "- `model_med_price`: The median price for the car's `model` (e.g., the typical price for a 3-Series vs. a 1-Series). This captures the model's positioning within the brand.\n",
    "- `brand_fuel_med_price`: The median price for the car's specific `Brand` and `fuelType` combination (e.g., a Diesel BMW vs. a Petrol BMW).\n",
    "- `brand_trans_med_price`: The median price for the `Brand` and `transmission` combination (e.g., an Automatic BMW vs. a Manual BMW).\n",
    "\n",
    "\n",
    "<u>Normalized & Relative Features</u>\n",
    "\n",
    "These features compare a car to its peers rather than using absolute values.\n",
    "- `*_anchor` (e.g., `brand_med_price_anchor`): Created by dividing the median price features (from section 3) by the `overall_mean_price`. This makes the feature dimensionless and represents the group's price *relative* to the entire market (e.g., \"this brand is 1.5x the market average\").\n",
    "- `age_rel_brand`: Calculated as `age - brand_median_age`. This shows if a car is newer or older than the *typical* car for that specific brand, capturing relative age within its own group.\n",
    "\n",
    "\n",
    "<u>CV-Safe Target Encodings</u>\n",
    "\n",
    "This is an advanced technique to encode categorical variables (like `model` or `Brand`) using information from the target variable (`price`) without causing data leakage.\n",
    "- `*_te` (e.g., `model_te`): Represents the *average price* for that category (e.g., the average price for a \"Fiesta\").\n",
    "- **Why is it \"CV-Safe\"?** Instead of just calculating the global average price for \"Fiesta\" and applying it to all rows (which leaks target information), this method uses K-Fold cross-validation. For each fold of the data, the target encoding is calculated *only* from the *other* folds. This ensures the encoding for any given row never includes its own price, preventing leakage and leading to a more robust model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO maybe add ('poly', PolynomialFeatures(degree=2)) to the pipeline for interaction terms ~J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9425a7c0-ec96-4458-a0da-8ef65acb3f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.6 Encoding, Transforming and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d59f995-10c7-4391-a5ce-759858b69889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- We identify different treatments for different groups of variables and combine all of them in the ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4562e8d4-fe9a-4262-b3fc-c2cf696dff58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Original features\n",
    "orig_numeric_features = [\"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\", \"previousOwners\", \"hasDamage\"]\n",
    "# TODO create origic_boolean_features for hasDamage ~J\n",
    "orig_categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91ca286-c202-45ac-afdf-c9631586ca8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### [2.6.0 Baseline Pipe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a6ee02c-baab-438c-95a6-7f0124e3af0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- We create two different pipelines to compare our fully adjusted model to the baseline\n",
    "    - original features with simple imputer\n",
    "    - engineered features with group imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "362f52e2-4378-4266-97b7-a4038b588e6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PIPELINE WITH preprocessor_orig CONTAINING ONLY ORIGINAL FEATURES and USING SIMPLE IMPUTATION\n",
    "numeric_transformer_orig = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),    # simple global median imputation\n",
    "    (\"to_float\", FunctionTransformer()), # TODO dont we have to add the to_float_array function here too? ~J\n",
    "    (\"scaler\", RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer_orig = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # fill by mode instead of Unknown\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))  # One-hot encoding\n",
    "])\n",
    "\n",
    "preprocessor_orig = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer_orig, orig_numeric_features),\n",
    "    (\"cat\", categorical_transformer_orig, orig_categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1778a80-df4f-481e-9034-b59f1c1343ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2.6.1 Categorize Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f684525-467f-4de3-97e4-3c65b414f22b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- Numeric Features:\n",
    "    - For scaling:\n",
    "        - ...\n",
    "    - For log-transformation before scaling (due to right-skew identified in EDA):\n",
    "        - ...\n",
    "- Categorical Features:\n",
    "    - For OHE:\n",
    "        - ...\n",
    "    - For TE:\n",
    "        - ...\n",
    "- Unused Features:\n",
    "    - `year`: dropped because replaced by derived feature 'age'\n",
    "    - `hasDamage`: droppped because unclear what 0 and NaN mean\n",
    "    - `paintQuality`: dropped because added by mechanic so not available for our predictions in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39142374-d485-42e7-a6cb-83ecffae0091",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocessing: with sklearn Pipeline & Column Transformer"
    }
   },
   "outputs": [],
   "source": [
    "# TODO maybe this step can also be automated within the pipeline ~J\n",
    "numeric_features = [\n",
    "    \"hasDamage\",\n",
    "    \"age\", \"tax\", \"mpg\", \"engineSize\", \"previousOwners\",        # Original features (mileage is handled separately because of log transformation)\n",
    "    \"mpg_x_engine\",                                             # TODO this feature does not really make sense, however it improves MAE slightly (3) ~J\n",
    "    \"engine_x_age\", \"mpg_x_age\", \"tax_x_age\",                   # multiplication interaction features (multiplying for amplification)                                   \n",
    "    \"engine_per_mpg\", \"tax_per_mpg\",                            # division interaction features (division for normalization for ratios (efficiency))                 \n",
    "    \"model_freq\",\n",
    "    \"age_rel_brand\", \"age_rel_model\", \"engine_rel_model\"\n",
    "]\n",
    "numeric_features_for_log = [\"mileage\", \"miles_per_year\"] #, \"mileage_x_age\"] # mileage_x_age decreases performance slightly\n",
    "boolean_features = [\"hasDamage\"]                                # TODO create logic for boolean features in GroupImputer and ColumnTransformer\n",
    "categorical_features_ohe = [\"transmission\", \"fuelType\"]\n",
    "# categorical_features_te_mean = [\"Brand\", \"model\"]             # TODO currently not used because median TE is used\n",
    "categorical_features_te_median = [\"Brand\", \"model\",             # original features\n",
    "                                  \"brand_fuel\", \"brand_trans\"]  # engineered features for anchors\n",
    "unused_columns = [\"year\"]                                       # replaced by age\n",
    "\n",
    "all_feature_names_before_encoding = numeric_features + numeric_features_for_log + boolean_features + categorical_features_ohe + categorical_features_te_median\n",
    "print(len(all_feature_names_before_encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dbdfc5f-b865-4c1a-8093-fe021c5ed311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2.6.2 Create Pipelines for each feature type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4385f8f4-0a8b-4b53-8350-52ee453af34a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- `Transformation` for skewed Numerics:\n",
    "    - Log-transform for right-skewed variables\n",
    "    - box-cox not used in final pipe because ...\n",
    "- `Scaler` for Numerics:\n",
    "    - StandardScaler because ...\n",
    "    - MinMaxScaler performed worse because ...\n",
    "    - RobustScaler performed worse because ...\n",
    "- `Encoding` for Categoricals:\n",
    "    - Low cardinality:\n",
    "        - OHE because best performance with tree-based models\n",
    "    - High Cardinality:\n",
    "        - Median TE on categorical features because performs better than Mean TE\n",
    "- `ColumnTransformer`: All operations are combined in the ColumnTransformer which applies the different steps to different columns of the data in one unified pipeline (reproducible and prevents data leakage)    \n",
    "  -> outputs a combined feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc19882e-d4db-4c22-a2d6-7bb290b42eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO two different scalers are used here. Isnt it better to use exactly one? ~J \n",
    "log_transformer_and_scaler = Pipeline([\n",
    "    (\"to_float\", NamedFunctionTransformer(to_float_array, feature_names=numeric_features_for_log, validate=False)),\n",
    "    (\"log\",    NamedFunctionTransformer(np.log1p, feature_names=numeric_features_for_log, validate=False)),  # log1p handles zeros safely\n",
    "    (\"scaler\", RobustScaler()),\n",
    "])\n",
    "\n",
    "numeric_scaler = Pipeline([\n",
    "    (\"to_float\", NamedFunctionTransformer(to_float_array, feature_names=numeric_features, validate=False)),\n",
    "    (\"scaler\", RobustScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer_ohe = Pipeline([\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "# Keep mean target encoder in the code but dont use it for now because median TE seems more robust and we use only one method for consistency ~J\n",
    "# categorical_transformer_te_mean = Pipeline([ \n",
    "#     (\"encoder\", TargetEncoder(target_type='continuous', cv=5, smooth='auto', random_state=42)), # Prevents data leakage with CV (e.g. for the samples in Fold 1, it calculates the target mean using the data from Folds 2, 3, 4, and 5) # TODO If it overfits test data too much, increasing the smoothing parameter can help\n",
    "#     (\"scaler\", StandardScaler()),\n",
    "# ])\n",
    "\n",
    "# Names for median-TE features (one per input column, since QuantileEncoder outputs 1 column per feature)\n",
    "median_te_feature_names = [f\"{col}_median_te\" for col in categorical_features_te_median]\n",
    "categorical_transformer_te_median = Pipeline(steps=[\n",
    "    ('median_encoder', QuantileEncoder(quantile=0.5, m=10.0)), # not specifying the cols means it encodes all columns (m is the smoothing parameter -> smoothing mitigates but doesnt eliminate leakage) # TODO tune m?\n",
    "    # TODO Why is to_float not needed here but in other pipelines? ~J\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('name_wrapper', NamedFunctionTransformer(feature_names=median_te_feature_names,\n",
    "                                              validate=False)),\n",
    "])\n",
    "\n",
    "enc_transf_scale = ColumnTransformer([\n",
    "    (\"log\", log_transformer_and_scaler, numeric_features_for_log),\n",
    "    (\"num\", numeric_scaler, numeric_features),\n",
    "    (\"cat\", categorical_transformer_ohe, categorical_features_ohe),\n",
    "    # (\"mean_te\", categorical_transformer_te_mean, categorical_features_te_mean), # Mean TE is currently not used but we keep it in the code for reference or later experimenting ~J\n",
    "    (\"median_te\", categorical_transformer_te_median, categorical_features_te_median)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fbfaa0a-e16e-4211-9be6-18192720905a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.7 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98263698-5b76-4d51-b2fb-11e10fdd81b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- We try to generalize feature selection as much as possible to find features that are most likely actually irrelevant/redundant and therefore generate noise in the model that might lead to overfitting\n",
    "- To achieve that goal we use a two-step technique: (1) We filter, (2) We use majority voting of multiple techniques:\n",
    "    - *Filter* methods to make an initial screening of the statistical properties of the data: \n",
    "        - `VarianceThreshold` to remove constant features. It serves as a cheap, fast garbage collector.\n",
    "        - `Correlation Indices` to identify redundant features and analyze the relationship with the target to identify irrelevant features (leakage-proof)\n",
    "        - `Statistical Hypothesis Testing` to identify irrelevant features by measuring the relationship with the target\n",
    "        - `Mutual Information` also considers nonlinear features and is implemented in the following way in sklearn (KNN approach for regression)\n",
    "            1. It looks at a data point in the Feature space (e.g., a specific car's mileage)\n",
    "            2. It finds the k closest neighbors (other cars with similar mileage)\n",
    "            3. It checks if those neighbors also have similar Prices (Target)\n",
    "            4. If neighbors in Mileage are consistently neighbors in Price, the MI score goes up.\n",
    "    - *Wrapper*:\n",
    "        - `Recursive Feature Elimination` to eliminate weak features stepwise \n",
    "    - *Embedded*: SelectFromModel(RandomForest) inside each model pipeline as a supervised feature selector.\n",
    "        - This selector is trained within cross-validation and shared across all models, ensuring: _No data leakage, Consistent feature selection logic, Model-agnostic, non-linear evaluation of feature relevance._\n",
    "        - We only use this supervised selector for our models that are more sensitive to high dimensionality and collinearity (ElasticNet, SVR)\n",
    "\n",
    "<u>Place in the pipe:</u> The Feature Selection is placed after the scaling to have the features on one scale (just like in the lab)\n",
    "\n",
    "**Our findings:**\n",
    "- For tree-based ensemble models (RF, ET, HGB, and SR), our runs showed that explicit feature selection did **not improve** performance, as these models already handle redundant features well through mechanisms like greedy node splitting, feature subsampling (bagging), and iterative error correction.\n",
    "\n",
    "-----\n",
    "\n",
    "SKlearn elements we also considered:\n",
    "- Filter Methods: SelectKBest & SelectPercentile\n",
    "- Embedded: No more than SelectFromModel\n",
    "- Wrapper Methods: RFE, RFECV, SequentialFeatureSelector (too expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd45610f-7196-4ca8-a5e2-f43bb8bffe8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO maybe use different pipes for tree-based vs. linear vs. NN models to show that we understand the differences ~J\n",
    "\n",
    "######################## Filter methods ########################\n",
    "# Variance Threshold (If using a different value than threshold 0, VarianceThreshold has to be applied before scaling while most of the other FS techniques should be applied after scaling)\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "\n",
    "# Select top features based on importance from model (Random Forest)\n",
    "fs_estimator_for_fs = RandomForestRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "######### Define voters\n",
    "### SelectKBest is a is a univariate filter -> runs a statistical test on each feature individually (e.g. corr with target) and takes the top k\n",
    "# Voter: Statistical linear Correlation -> misses nonlinear relationships so has to be combined with other techniques in majority voting\n",
    "stat_voter_linear_corr = CorrelationThresholdSelector(threshold=0.05)                     # TODO tune threshold? # Use custom correlation threshold selector instead of SelectKBest(score_func=f_regression, k=x) because we dont want to fix that k\n",
    "\n",
    "# Voter: Mutual Information (Non-Linear Dependency) MI calculates how much the \"uncertainty\" (Entropy) drops when using different features ()\n",
    "# stat_voter_nonlinear_mi = SelectKBest(score_func=mutual_info_regression(n_neighbors=10), k=20)     # TODO tune k? # Increasing n_neighbors makes the estimation more stable but computationally slower\n",
    "stat_voter_nonlinear_mi = MutualInfoThresholdSelector(threshold=0.01, n_neighbors=10)                # Use custom instead of SelectKBest because we dont want to fix k\n",
    "\n",
    "### SelectFromModel is a model-based feature selector that uses model importance scores\n",
    "# Voter: Linear Model (Lasso) (only selects linear features and will kill interaction terms) (!Lasso needs scaling beforehand!)\n",
    "lasso_for_fs = Lasso(alpha=0.1) # TODO do I have to use LassoCV?\n",
    "select_from_lasso = SelectFromModel(lasso_for_fs, threshold=0.01)         # Very low threshold to filter out the values set to 0 by lasso. Default prefit=False to fit in the pipeline\n",
    "\n",
    "# Voter: Tree Importance\n",
    "rf_for_fs = RandomForestRegressor(max_depth=5)\n",
    "select_from_rf = SelectFromModel(rf_for_fs, threshold='0.001*mean')         # threshold relative because it sums to 1 and if we have many features, many features will have a low importance but are still important\n",
    "\n",
    "\n",
    "# Voter: RFE (Recursive Feature Elimination) is excluded for now because it is very computationally expensive\n",
    "\n",
    "\n",
    "##### Initialize Custom Selector\n",
    "majority_selector = MajorityVoteSelectorTransformer(\n",
    "    selectors=[stat_voter_linear_corr,\n",
    "               stat_voter_nonlinear_mi,\n",
    "               select_from_lasso,\n",
    "               select_from_rf],\n",
    "    min_votes=2                                     # TODO can be tuned in hyperparameter tuning ~J\n",
    ")\n",
    "\n",
    "\n",
    "# ==> Final FS pipeline\n",
    "fs_pipe = Pipeline([\n",
    "    (\"vt\", vt), # Apply VT first to remove constant features (it serves as a \"dictator\" and not a \"voter\" in our pipeline)\n",
    "    ('selector', majority_selector),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0f4406-27be-4204-bd85-653c8d5ac173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.9 Create Final Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a716cfa0-d1e3-4978-83f2-183c1a346b94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- The `Pipeline` combines feature engineering, group imputation and the column transformer into the final preprocessing pipe\n",
    "    - Feature Engineering: see Markdown in Section 2.2\n",
    "    - Group Imputer: see Markdown in ...\n",
    "    - Column Transformer: see explanation in line 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6949511c-a9e8-44f6-b9d4-a4d2950fff69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preprocessor_pipe = Pipeline([\n",
    "    (\"group_imputer\", GroupImputer(\n",
    "        group_cols=(\"Brand\", \"model\"),\n",
    "        num_cols=orig_numeric_features,                         # We have to use the original features here because the others are engineered in the next step\n",
    "        cat_cols=orig_categorical_features,                     # We have to use the original features here because the others are engineered in the next step\n",
    "        fallback=\"__MISSING__\",\n",
    "    )),\n",
    "    # TODO add outlier handling step here (maybe even before Imputation)\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", enc_transf_scale),\n",
    "    (\"fs\", fs_pipe)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b9f0e68-668b-4730-8cd6-800efce68e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize outputs of each step in the preprocessing pipeline\n",
    "enc_transf_scale.set_output(transform=\"pandas\")\n",
    "fs_pipe.set_output(transform=\"pandas\")\n",
    "\n",
    "show_data = True # needs to be set to False when running the models because 'display' is used\n",
    "y_data_profiling = False\n",
    "debug_preprocessor_pipe = Pipeline([\n",
    "    ('debug_start', DebugTransformer('START', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "    (\"group_imputer\", GroupImputer(\n",
    "        group_cols=(\"Brand\", \"model\"),\n",
    "        num_cols=orig_numeric_features, # numeric_features + numeric_features_for_log,                      # We have to use the original features here because the others are engineered in the next step\n",
    "        cat_cols=orig_categorical_features, # categorical_features_ohe + categorical_features_te_median,    # We have to use the original features here because the others are engineered in the next step\n",
    "        fallback=\"__MISSING__\",\n",
    "    )),\n",
    "    ('debug_after_impute', DebugTransformer('AFTER IMPUTATION', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    ('debug_after_fe', DebugTransformer('AFTER FEATURE ENGINEERING', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "    (\"ct\", (enc_transf_scale)),\n",
    "    ('debug_after_ct', DebugTransformer('AFTER COLUMN TRANSFORMER', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "    (\"fs\", (fs_pipe)),\n",
    "    ('debug_after_fs', DebugTransformer('AFTER FEATURE SELECTION', show_data=show_data, y_data_profiling=y_data_profiling))\n",
    "])\n",
    "\n",
    "print(\"Show outputs of each step in the preprocessing pipeline:\") # Set show_data=True in DebugTransformer to see the data at each step\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)\n",
    "\n",
    "# Reset output to default (numpy arrays) for model training\n",
    "enc_transf_scale.set_output(transform=\"default\")\n",
    "fs_pipe.set_output(transform=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the feed names after VT because VT is applied before the majority voting to remove constant features\n",
    "feature_names_after_vt = debug_preprocessor_pipe.named_steps['fs'].named_steps['vt'].get_feature_names_out()\n",
    "plot_selector_agreement(\n",
    "    majority_selector = debug_preprocessor_pipe.named_steps['fs'].named_steps['selector'], \n",
    "    feature_names = feature_names_after_vt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "32efebf2-72f6-448e-b5a1-3fa27bd272bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # TODO delete following cell later - this is for us to see if the group imputer works - but it is GPT slop\n",
    "\n",
    "# brand = \"VW\"\n",
    "# model = \"golf\"\n",
    "\n",
    "# # 1) Get the fitted steps from preprocessor_pipe\n",
    "# preprocessor_pipe.fit(X_train, y_train)\n",
    "# fe = preprocessor_pipe.named_steps[\"fe\"]              # CarFeatureEngineer\n",
    "# imp = preprocessor_pipe.named_steps[\"group_imputer\"]  # GroupImputer\n",
    "\n",
    "# # 2) Inspect GroupImputer internal numeric stats\n",
    "# pair_table = getattr(imp, \"num_pair_\", None)    # indexed by (_g0, _g1) = (Brand, model)\n",
    "# brand_table = getattr(imp, \"num_first_\", None)  # indexed by _g0 = Brand\n",
    "# global_med = getattr(imp, \"num_global_\", None)  # Series of global medians\n",
    "\n",
    "# print(\"Has pair-level medians table:\",\n",
    "#       pair_table is not None and not getattr(pair_table, \"empty\", True))\n",
    "# print(\"Has brand-level medians table:\",\n",
    "#       brand_table is not None and not getattr(brand_table, \"empty\", True))\n",
    "# print(\"Has global median:\",\n",
    "#       global_med is not None and not global_med.empty if global_med is not None else False)\n",
    "# print()\n",
    "\n",
    "# _g0 = brand\n",
    "# _g1 = model\n",
    "\n",
    "# # 2a) Pair-level\n",
    "# if pair_table is not None and (_g0, _g1) in pair_table.index:\n",
    "#     print(f\"Pair-level median FOUND for ({brand}, {model}):\")\n",
    "#     display(pair_table.loc[(_g0, _g1)])\n",
    "# else:\n",
    "#     print(f\"No pair-level median for ({brand}, {model}).\")\n",
    "#     if pair_table is not None and not pair_table.empty:\n",
    "#         print(\"Sample of pair-level medians (top 5):\")\n",
    "#         display(pair_table.head())\n",
    "\n",
    "# # 2b) Brand-level\n",
    "# if brand_table is not None and _g0 in brand_table.index:\n",
    "#     print(f\"\\nBrand-level median for {brand}:\")\n",
    "#     display(brand_table.loc[_g0])\n",
    "# else:\n",
    "#     print(\"\\nNo brand-level median for\", brand)\n",
    "#     if brand_table is not None and not brand_table.empty:\n",
    "#         print(\"Sample of brand-level medians (top 5):\")\n",
    "#         display(brand_table.head())\n",
    "\n",
    "# # 2c) Global medians\n",
    "# print(\"\\nGlobal median (fallback):\")\n",
    "# display(global_med)\n",
    "\n",
    "# # 3) Apply CarFeatureEngineer + GroupImputer to VW Golf rows and compare\n",
    "# #    (GroupImputer was fitted after CarFeatureEngineer, so we must mimic that order)\n",
    "\n",
    "# # 3a) Feature engineering on full X_train\n",
    "# X_train_fe = fe.transform(X_train)\n",
    "\n",
    "# # 3b) Filter for VW Golf in the feature-engineered space\n",
    "# vw_golf = X_train_fe[(X_train_fe[\"Brand\"] == brand) & (X_train_fe[\"model\"] == model)].copy()\n",
    "\n",
    "# if vw_golf.empty:\n",
    "#     print(\"\\nNo VW Golf rows found in X_train.\")\n",
    "# else:\n",
    "#     print(f\"\\nFound {len(vw_golf)} VW Golf rows in X_train.\")\n",
    "\n",
    "#     # 3c) GroupImputer expects the columns it saw at fit time\n",
    "#     cols_for_imp = imp.feature_names_in_\n",
    "#     vw_input = vw_golf.loc[:, cols_for_imp]\n",
    "\n",
    "#     vw_imp = imp.transform(vw_input)\n",
    "#     vw_imp_df = pd.DataFrame(vw_imp, columns=cols_for_imp, index=vw_golf.index)\n",
    "\n",
    "#     print(\"\\nImputed data (first 8 rows):\")\n",
    "#     display(vw_imp_df[[\"mpg\", \"mileage\", \"tax\"]].head(8))\n",
    "\n",
    "#     # 4) Build comparison table (original vs imputed, for selected columns)\n",
    "#     comp = pd.DataFrame(index=vw_golf.index)\n",
    "#     comp[\"orig_mpg\"] = vw_golf[\"mpg\"]\n",
    "#     comp[\"imp_mpg\"] = vw_imp_df[\"mpg\"]\n",
    "#     comp[\"orig_tax\"] = vw_golf[\"tax\"]\n",
    "#     comp[\"imp_tax\"] = vw_imp_df[\"tax\"]\n",
    "#     comp[\"orig_mileage\"] = vw_golf[\"mileage\"]\n",
    "#     comp[\"imp_mileage\"] = vw_imp_df[\"mileage\"]\n",
    "\n",
    "#     print(\"\\nOriginal vs imputed (first 12 rows):\")\n",
    "#     display(comp.head(12))\n",
    "\n",
    "#     # 5) Determine imputation source per row\n",
    "#     def source_of_imputation(col):\n",
    "#         srcs = []\n",
    "#         for idx, row in comp.iterrows():\n",
    "#             val = row[f\"imp_{col}\"]\n",
    "#             src = \"other\"\n",
    "\n",
    "#             # Pair-level\n",
    "#             if pair_table is not None and (_g0, _g1) in pair_table.index and col in pair_table.columns:\n",
    "#                 pair_val = pair_table.loc[(_g0, _g1), col]\n",
    "#                 if pd.notna(pair_val) and pd.notna(val) and val == pair_val:\n",
    "#                     src = \"pair\"\n",
    "\n",
    "#             # Brand-level\n",
    "#             if src == \"other\" and brand_table is not None and _g0 in brand_table.index and col in brand_table.columns:\n",
    "#                 brand_val = brand_table.loc[_g0, col]\n",
    "#                 if pd.notna(brand_val) and pd.notna(val) and val == brand_val:\n",
    "#                     src = \"brand\"\n",
    "\n",
    "#             # Global\n",
    "#             if src == \"other\" and global_med is not None and col in global_med.index:\n",
    "#                 glob_val = global_med[col]\n",
    "#                 if pd.notna(glob_val) and pd.notna(val) and val == glob_val:\n",
    "#                     src = \"global\"\n",
    "\n",
    "#             srcs.append(src)\n",
    "#         return srcs\n",
    "\n",
    "#     comp[\"src_mpg\"] = source_of_imputation(\"mpg\")\n",
    "#     comp[\"src_tax\"] = source_of_imputation(\"tax\")\n",
    "#     comp[\"src_mileage\"] = source_of_imputation(\"mileage\")\n",
    "\n",
    "#     print(\"\\nImputation sources for the shown rows:\")\n",
    "#     display(comp.head(12))\n",
    "\n",
    "#     # 6) Summary counts: NaN before vs after imputation\n",
    "#     print(\"\\nSummary counts: NaN before -> NaN after\")\n",
    "#     before = vw_golf[[\"mpg\", \"mileage\", \"tax\"]].isna().sum()\n",
    "#     after = pd.DataFrame({\n",
    "#         \"mpg\": comp[\"imp_mpg\"],\n",
    "#         \"mileage\": comp[\"imp_mileage\"],\n",
    "#         \"tax\": comp[\"imp_tax\"],\n",
    "#     }).isna().sum()\n",
    "#     display(pd.DataFrame({\"na_before\": before, \"na_after\": after}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0dc6124-f736-46fb-9f73-895b930faae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Model Evaluation Metrics, Baselining, Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "225688cc-6157-4613-bf28-9e430473a3a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.1 Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6297f034-638d-4db6-a681-597f20036432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definition of Function for Model Assessment"
    }
   },
   "source": [
    "**MAE (Mean Absolute Error):**\n",
    "- average absolute deviation between predicted and true car prices\n",
    "- easy to interpret in pounds, same metric used by Kaggle competition\n",
    "\n",
    "**RMSE (Root Mean Squared Error):**\n",
    "- sensitive to outliers, helps identify large prediction errors\n",
    "\n",
    "**RÂ²:**\n",
    "- Coefficient of determination: proportion of variance explained by the model\n",
    "- 1.0 = perfect predictions, 0.0 = same as predicting mean, < 0.0 = worse than mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "653096f8-6a05-419c-a430-a43d15401295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.2 Baseline (median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b26348e-a5ff-4dc9-9d61-7f59d0ea335d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Baselining median"
    }
   },
   "outputs": [],
   "source": [
    "# Baseline: DummyRegressor using the median price as prediction\n",
    "baseline_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_pipe),  \n",
    "    (\"model\", DummyRegressor(strategy=\"median\")),\n",
    "])\n",
    "\n",
    "baseline_cv = cross_validate(\n",
    "    baseline_pipe,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring={\n",
    "        \"neg_mae\": \"neg_mean_absolute_error\",\n",
    "        \"neg_mse\": \"neg_mean_squared_error\",\n",
    "        \"r2\": \"r2\",\n",
    "    },\n",
    "    n_jobs=-2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "baseline_mae = -baseline_cv[\"test_neg_mae\"].mean()\n",
    "baseline_rmse = np.sqrt(-baseline_cv[\"test_neg_mse\"].mean())\n",
    "baseline_r2 = baseline_cv[\"test_r2\"].mean()\n",
    "\n",
    "print(\"Baseline (median) performance on CV:\")\n",
    "print(f\"MAE:  {baseline_mae:,.4f}\")\n",
    "print(f\"RMSE: {baseline_rmse:,.4f}\")\n",
    "print(f\"R2:   {baseline_r2:,.4f}\")\n",
    "\n",
    "# Baseline (median) with engineered features & CV:\n",
    "# MAE:  6,801.8131\n",
    "# RMSE: 9,981.0186\n",
    "# R2:   -0.0508"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c31fd5-30e4-4f93-a6e4-8e67e0fdd0c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.3 Pipeline Definitions (preprocessor + model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc50f05c-4c02-4ff6-9a48-319af5baecfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 4.3.1 Using Baseline Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef8bd00-b39c-421d-8955-b0e40f439d25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "elastic_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", ElasticNet(\n",
    "        alpha=0.01,\n",
    "        l1_ratio=0.5,\n",
    "        max_iter=30000,\n",
    "        tol=1e-4,\n",
    "        selection=\"cyclic\",\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "hgb_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "rf_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "et_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "svr_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", SVR(\n",
    "        kernel=\"rbf\",\n",
    "        C=10,\n",
    "        epsilon=0.1,\n",
    "        gamma=\"scale\",\n",
    "    )),\n",
    "])\n",
    "\n",
    "stack_pipe_orig = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"elastic_orig\", elastic_pipe_orig),\n",
    "        (\"hgb_orig\", hgb_pipe_orig),\n",
    "        (\"rf_orig\", rf_pipe_orig),\n",
    "    ],\n",
    "    final_estimator=HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    passthrough=False,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a54c7598-3c37-43b9-995d-62ee95b329e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 4.3.2 Using optimized Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "04d3aa5b-36ca-4dba-9592-24edd345897d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Models Setup (inkl. Prepro in Pipeline)"
    }
   },
   "outputs": [],
   "source": [
    "def create_model_pipe(prepro_pipe, model):\n",
    "    model_pipe = Pipeline([\n",
    "        (\"preprocess\", prepro_pipe),\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "    return model_pipe\n",
    "\n",
    "\n",
    "### LINEAR MODEL\n",
    "# ElasticNet\n",
    "elastic_net_model = ElasticNet(\n",
    "        alpha=0.01,\n",
    "        l1_ratio=0.5,\n",
    "        max_iter=30000,\n",
    "        tol=1e-4,\n",
    "        selection=\"cyclic\",\n",
    "        random_state=42,\n",
    "    )\n",
    "elastic_pipe_fe = create_model_pipe(preprocessor_pipe, elastic_net_model)\n",
    "\n",
    "### TREE MODELS\n",
    "# HistGradientBoostingRegressor\n",
    "hgb_model = HistGradientBoostingRegressor(\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42,\n",
    "    )\n",
    "hgb_pipe_fe = create_model_pipe(preprocessor_pipe, hgb_model)\n",
    "\n",
    "# RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )\n",
    "rf_pipe_fe = create_model_pipe(preprocessor_pipe, rf_model)\n",
    "\n",
    "# ExtraTreesRegressor\n",
    "et_model = ExtraTreesRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )\n",
    "et_pipe_fe = create_model_pipe(preprocessor_pipe, et_model)\n",
    "\n",
    "### KERNEL-BASED MODEL (SVR)\n",
    "svr_model = SVR(\n",
    "        kernel=\"rbf\",\n",
    "        C=10,\n",
    "        epsilon=0.1,\n",
    "        gamma=\"scale\",\n",
    "    )\n",
    "svr_pipe_fe = create_model_pipe(preprocessor_pipe, svr_model)\n",
    "\n",
    "### ENSEMBLE META MODEL (Stacking)\n",
    "stack_pipe_fe = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"hgb_fe\", hgb_pipe_fe),\n",
    "        (\"rf_fe\", rf_pipe_fe),\n",
    "        (\"et_fe\", et_pipe_fe),\n",
    "    ],\n",
    "    final_estimator=HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    passthrough=False,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a21d8b-3aeb-4155-b6db-17475e30c3b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.4 First run of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "80a5dc82-4f35-4d66-9e97-e249166949e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # TODO uncomment (currently its commented to save time during experimentation)\n",
    "\n",
    "# # First evaluation of metrics based on original and engineered feature pipeline to decide how to proceed\n",
    "\n",
    "\n",
    "# models_orig = {\n",
    "#     # \"ElasticNet_orig\": elastic_pipe_orig,\n",
    "#     \"HGB_orig\": hgb_pipe_orig,\n",
    "#     \"RF_orig\": rf_pipe_orig,\n",
    "#     \"ET_orig\": et_pipe_orig,\n",
    "#     \"SVR_orig\": svr_pipe_orig,\n",
    "#     \"Stack_orig\": stack_pipe_orig,\n",
    "# }\n",
    "\n",
    "# models_fe = {\n",
    "#     # \"ElasticNet_fe\": elastic_pipe_fe,\n",
    "#     \"HGB_fe\": hgb_pipe_fe,\n",
    "#     \"RF_fe\": rf_pipe_fe,\n",
    "#     \"ET_fe\": et_pipe_fe,\n",
    "#     \"SVR_fe\": svr_pipe_fe,\n",
    "#     \"Stack_fe\": stack_pipe_fe,\n",
    "# }\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # for name, model in {**models_orig, **models_fe}.items():\n",
    "#     print(f\"Fitting {name} with cross-validation...\")\n",
    "    \n",
    "#     # Perform cross-validation on the entire training set\n",
    "#     cv_results = cross_validate(\n",
    "#         model, \n",
    "#         X_train, \n",
    "#         y_train,\n",
    "#         cv=3,\n",
    "#         scoring={\n",
    "#             'neg_mae': 'neg_mean_absolute_error',\n",
    "#             'neg_mse': 'neg_mean_squared_error',\n",
    "#             'r2': 'r2'\n",
    "#         },\n",
    "#         return_train_score=False,\n",
    "#         verbose=3,\n",
    "#         n_jobs=-2\n",
    "#     )\n",
    "    \n",
    "#     # Calculate mean metrics across folds\n",
    "#     mae = -cv_results['test_neg_mae'].mean()\n",
    "#     rmse = np.sqrt(-cv_results['test_neg_mse'].mean())\n",
    "#     r2 = cv_results['test_r2'].mean()\n",
    "    \n",
    "#     results.append({\n",
    "#         \"model\": name,\n",
    "#         \"feature_set\": \"original\" if name.endswith(\"_orig\") else \"engineered\",\n",
    "#         \"MAE\": mae,\n",
    "#         \"RMSE\": rmse,\n",
    "#         \"R2\": r2,\n",
    "#     })\n",
    "\n",
    "# results_df = (\n",
    "#     pd.DataFrame(results)\n",
    "#       .sort_values([\"feature_set\", \"MAE\"])\n",
    "#       .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# print(results_df)\n",
    "\n",
    "# # Long Duration (with orig ca 25mins VS without orig ca 6mins VS with CV ca 16mins VS with njobs=-1 ca )\n",
    "\n",
    "# # Predicted on hold-out val set (20%):\n",
    "# #       model feature_set          MAE          RMSE        R2\n",
    "# # 0     RF_fe  engineered  1299.728938  4.509435e+06  0.950490\n",
    "# # 1  Stack_fe  engineered  1321.130612  4.831609e+06  0.946953\n",
    "# # 2     ET_fe  engineered  1328.051439  4.707534e+06  0.948315\n",
    "# # 3    HGB_fe  engineered  1534.496164  5.609255e+06  0.938415\n",
    "# # 4    SVR_fe  engineered  2955.064750  3.242891e+07  0.643956\n",
    "\n",
    "# # Predicted using 3-fold CV on entire data:\n",
    "# #       model feature_set          MAE         RMSE        R2\n",
    "# # 0     RF_fe  engineered  1336.806163  2375.850617  0.940424\n",
    "# # 1  Stack_fe  engineered  1357.266391  2505.029128  0.933786\n",
    "# # 2     ET_fe  engineered  1364.212656  2399.654669  0.939223\n",
    "# # 3    HGB_fe  engineered  1551.419964  2503.445871  0.933858\n",
    "# # 4    SVR_fe  engineered  3068.524237  6130.420383  0.603579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Hyperparameter Tuning and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0eb17d8-4b9a-433f-98f6-26777d49fa3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- After first experiments we decided to skip hyperparameter-tuning for SVR and ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c880d2d-c7aa-478f-8d34-90aa6e33a172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.1 ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "18042056-7ef9-489f-afc3-958fe02fbfc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ElasticNet"
    }
   },
   "outputs": [],
   "source": [
    "# TODO this cell is commented because we dont evaluate elasticnet for final performance (save time)\n",
    "# # Hyperparameter Tuning: ElasticNet\n",
    "\n",
    "# elastic_param_grid = {\n",
    "#     \"model__alpha\": [0.001],    # also tried 0.01, 0.05, 0.1, 0.5\n",
    "#     \"model__l1_ratio\": [0.9]    # also tried 0.1, 0.3, 0.5, 0.7  \n",
    "# }\n",
    "\n",
    "# # CV: Calculate all metrics but use MAE for selecting best model\n",
    "# elastic_grid = GridSearchCV(\n",
    "#     elastic_pipe_fe, \n",
    "#     param_grid=elastic_param_grid,\n",
    "#     cv=5,\n",
    "#     scoring={\n",
    "#         'mae': 'neg_mean_absolute_error',\n",
    "#         'mse': 'neg_mean_squared_error',\n",
    "#         'r2': 'r2'\n",
    "#     },\n",
    "#     refit='mae', # Refit the best model based on MAE on the whole training set\n",
    "#     n_jobs=-2,\n",
    "#     verbose=3,\n",
    "#     return_train_score=False\n",
    "# )\n",
    "# elastic_grid.fit(X_train, y_train)\n",
    "\n",
    "# # Get mean metrics across folds\n",
    "# mae = -elastic_grid.cv_results_['mean_test_mae'][elastic_grid.best_index_]\n",
    "# mse = -elastic_grid.cv_results_['mean_test_mse'][elastic_grid.best_index_]\n",
    "# rmse = np.sqrt(mse)\n",
    "# r2 = elastic_grid.cv_results_['mean_test_r2'][elastic_grid.best_index_]\n",
    "# print(\"ElasticNet Results (CV on entire train set):\")\n",
    "# print(f\"MAE: {mae:.4f}\")\n",
    "# print(f\"RMSE: {rmse:.4f}\")\n",
    "# print(f\"RÂ²: {r2:.4f}\")\n",
    "# print(\"Best ElasticNet params:\", elastic_grid.best_params_)\n",
    "\n",
    "# elastic_best = elastic_grid.best_estimator_ # Final model trained on entire training set with best hyperparameters minimizing MAE\n",
    "\n",
    "# # Long Duration (Before removal of OHE-categoricals interrupted kernel after 64mins VS after removal ca 1min -> now 15secs with njobs=-2)\n",
    "\n",
    "# # ElasticNet Results: \n",
    "# # MAE: 2353.9112 | RMSE: 13356867.7860 | R2: 0.8534\n",
    "# # Best ElasticNet params: {'model__alpha': 0.001, 'model__l1_ratio': 0.9}\n",
    "\n",
    "# # MAE: 2589.6100\n",
    "# # RMSE: 4104.4515\n",
    "# # RÂ²: 0.8222\n",
    "# # Best ElasticNet params: {'model__alpha': 0.001, 'model__l1_ratio': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8affc096-36be-4561-8aaf-e0c642a48684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # TODO this cell is commented because of time constraints\n",
    "# # Use GridSearchCV for features_to_select\n",
    "# # Base model: tuned ElasticNet from above\n",
    "# en_base = clone(elastic_best.named_steps[\"model\"])\n",
    "\n",
    "# # Pipeline: clean preprocessing -> RFE -> model\n",
    "# rfe_pipe_linear = Pipeline([\n",
    "#     (\"preprocess\", preprocessor_fe_clean),\n",
    "#     (\"rfe\", RFE(\n",
    "#         estimator=en_base,\n",
    "#         step=0.5,               # drop ~20% per iteration\n",
    "#         importance_getter=\"auto\"\n",
    "#     )),\n",
    "#     (\"model\", clone(en_base))\n",
    "# ])\n",
    "\n",
    "# # Try a few target feature counts (adjust as needed)\n",
    "# number_of_all_features = preprocessor_fe_clean.transform(X_train).shape[1]\n",
    "# rfe_param_grid = {\n",
    "#     \"rfe__n_features_to_select\": [int(number_of_all_features*0.5)]# , int(number_of_all_features*1)] # use only these two extremes to save time ~J\n",
    "# }\n",
    "\n",
    "# rfe_grid = GridSearchCV(\n",
    "#     rfe_pipe_linear,\n",
    "#     param_grid=rfe_param_grid,\n",
    "#     cv=2,\n",
    "#     scoring=\"neg_mean_absolute_error\",\n",
    "#     n_jobs=-1,\n",
    "#     verbose=3,\n",
    "#     return_train_score=False,\n",
    "# )\n",
    "\n",
    "# rfe_grid.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best n_features_to_select:\", rfe_grid.best_params_[\"rfe__n_features_to_select\"])\n",
    "# print(\"MAE (CV):\", -rfe_grid.best_score_)\n",
    "# rfe_best = rfe_grid.best_estimator_\n",
    "\n",
    "# # list kept features\n",
    "# best_rfe = rfe_best.named_steps[\"rfe\"]\n",
    "# all_feats = rfe_best.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "# kept = [f for f, keep in zip(all_feats, best_rfe.support_) if keep]\n",
    "# print(\"Kept features:\", kept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21989bd5-8ab1-4d43-a099-3b2f412c94e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reasoning**: We used 100 features as an initial, arbitrary cutoff for feature selection in the ElasticNet model. Preliminary experiments and insights from the EDA (see separate notebook) indicated that tree-based methods are likely to perform better. Therefore, we prioritized feature selection for the tree-based models based on SHAP values.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5a0287-45b6-4022-8d47-cde8cc52469e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.2 HistGradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "998dab6d-7494-483c-ab9e-a075b4c75c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hgb_param_dist = {\n",
    "    \"preprocess__fs__vt__threshold\": [0.0, 0.005, 0.01],\n",
    "    # \"preprocessor__fs__selector__min_votes\": [1, 2, 3],  # TODO try different vote thresholds for MajorityVoteSelectorTransformer in FS pipeline\n",
    "    \"model__learning_rate\": uniform(0.01, 0.15),       # samples values\n",
    "    \"model__max_leaf_nodes\": randint(50, 170),         \n",
    "    \"model__min_samples_leaf\": randint(2, 20),         # samples leaf sizes between 2â20\n",
    "    \"model__max_iter\": randint(200, 900),              # tries 200â900 iterations\n",
    "    \"model__l2_regularization\": uniform(0.0, 1.0),      # samples small regularization values\n",
    "    \"model__early_stopping\": [True],\n",
    "    \"model__validation_fraction\": [0.1],\n",
    "    \"model__n_iter_no_change\": [20],\n",
    "    \"model__random_state\":[42]\n",
    "}\n",
    "\n",
    "# optimized the parameter distributions based on previous runs to focus search space\n",
    "hgb_param_dist = {\n",
    "    \"preprocess__fs__vt__threshold\": [0.0],\n",
    "    # \"preprocessor__fs__selector__min_votes\": [1, 2, 3],  # TODO try different vote thresholds for MajorityVoteSelectorTransformer in FS pipeline\n",
    "    \"model__learning_rate\": [0.05889383578028271],\n",
    "    \"model__max_leaf_nodes\": [139],\n",
    "    \"model__min_samples_leaf\": [4],\n",
    "    \"model__max_iter\": [602],\n",
    "    \"model__l2_regularization\": [0.8583588048137198],\n",
    "    \"model__early_stopping\": [True],\n",
    "    \"model__validation_fraction\": [0.1],\n",
    "    \"model__n_iter_no_change\": [20],\n",
    "    \"model__random_state\":[42]\n",
    "}\n",
    "    \n",
    "hgb_best = model_hyperparameter_tuning(X_train, y_train, hgb_pipe_fe, hgb_param_dist, n_iter=100, splits=5)\n",
    "\n",
    "# Before FS (last step was: Add \"mpg_x_age\", \"tax_x_age\")\n",
    "# MAE: 1335.6473\n",
    "# RMSE: 2308.0557\n",
    "# RÂ²: 0.9439\n",
    "# Best Model params: {'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 4, 'model__max_leaf_nodes': 139, 'model__max_iter': 602, 'model__learning_rate': 0.05889383578028271, 'model__l2_regularization': 0.8583588048137198, 'model__early_stopping': True, 'fs__vt__threshold': 0.0}\n",
    "\n",
    "# use mean instead of median in  \"age_rel_brand\" because most of the values were 0 otherwise\n",
    "# MAE: 1323.1182\n",
    "# RMSE: 2300.0353\n",
    "# RÂ²: 0.9443\n",
    "# Best Model params: {'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 4, 'model__max_leaf_nodes': 139, 'model__max_iter': 602, 'model__learning_rate': 0.05889383578028271, 'model__l2_regularization': 0.8583588048137198, 'model__early_stopping': True, 'fs__vt__threshold': 0.0}\n",
    "\n",
    "# Add 'age_rel_model'\n",
    "# MAE: 1323.7268\n",
    "# RMSE: 2307.2668\n",
    "# RÂ²: 0.9439\n",
    "# Best Model params: {'preprocess__fs__vt__threshold': 0.0, 'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 4, 'model__max_leaf_nodes': 139, 'model__max_iter': 602, 'model__learning_rate': 0.05889383578028271, 'model__l2_regularization': 0.8583588048137198, 'model__early_stopping': True}\n",
    "\n",
    "# Add 'engine_rel_model'\n",
    "# MAE: 1314.7631\n",
    "# RMSE: 2300.8177\n",
    "# RÂ²: 0.9442\n",
    "# Best Model params: {'preprocess__fs__vt__threshold': 0.0, 'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 4, 'model__max_leaf_nodes': 139, 'model__max_iter': 602, 'model__learning_rate': 0.05889383578028271, 'model__l2_regularization': 0.8583588048137198, 'model__early_stopping': True}\n",
    "\n",
    "# Use engine_per_mpg instead of mpg_per_engine\n",
    "# MAE: 1319.4956\n",
    "# RMSE: 2301.7438\n",
    "# RÂ²: 0.9442\n",
    "# Best Model params: {'preprocess__fs__vt__threshold': 0.0, 'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 4, 'model__max_leaf_nodes': 139, 'model__max_iter': 602, 'model__learning_rate': 0.05889383578028271, 'model__l2_regularization': 0.8583588048137198, 'model__early_stopping': True}\n",
    "\n",
    "# After adding FS pipe\n",
    "# MAE: 1305.3789\n",
    "# RMSE: 2286.3747\n",
    "# RÂ²: 0.9449\n",
    "# Best Model params: {'preprocess__fs__vt__threshold': 0.0, 'model__validation_fraction': 0.1, 'model__random_state': 42, 'model__n_iter_no_change': 20, 'model__min_samples_leaf': 4, 'model__max_leaf_nodes': 139, 'model__max_iter': 602, 'model__learning_rate': 0.05889383578028271, 'model__l2_regularization': 0.8583588048137198, 'model__early_stopping': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70de2673-0443-4cc4-b1cd-f3813a969246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.3 RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "# Old parameter distribution\n",
    "rf_param_dist = {\n",
    "    \"preprocess__fs__vt__threshold\": [0.0, 0.005, 0.01],\n",
    "    \"model__n_estimators\": randint(200, 600),        # number of trees\n",
    "    \"model__max_depth\": randint(5, 40),              # depth of each tree\n",
    "    \"model__min_samples_split\": randint(2, 10),      # min samples to split an internal node\n",
    "    \"model__min_samples_leaf\": randint(1, 8),        # min samples per leaf\n",
    "    \"model__max_features\": [\"sqrt\"],           # feature sampling strategy (sqrt performed better than log2 and None in previous tests)\n",
    "    \"model__bootstrap\": [False]                      # use bootstrapping or not (False performed better than True in previous tests)\n",
    "}\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "rf_param_dist = {\n",
    "    \"preprocess__fs__vt__threshold\": [0.005],\n",
    "    \"model__n_estimators\": [328],\n",
    "    \"model__max_depth\": [20],\n",
    "    \"model__min_samples_split\": [5],\n",
    "    \"model__min_samples_leaf\": [1],\n",
    "    \"model__max_features\": [\"sqrt\"],\n",
    "    \"model__bootstrap\": [False],\n",
    "}\n",
    "\n",
    "rf_best_rand = model_hyperparameter_tuning(X_train, y_train, rf_pipe_fe, rf_param_dist)\n",
    "\n",
    "joblib.dump(rf_best_rand, \"rf_best_rand.pkl\")\n",
    "\n",
    "\n",
    "# Long Duration (~1min)\n",
    "\n",
    "# Before FS (last step was: Add \"mpg_x_age\", \"tax_x_age\")\n",
    "# MAE: 1310.7979\n",
    "# RMSE: 2313.0862\n",
    "# RÂ²: 0.9437\n",
    "# Best Model params: {'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False, 'fs__vt__threshold': 0.005}\n",
    "\n",
    "# use mean instead of median in  \"age_rel_brand\" because most of the values were 0 otherwise\n",
    "# MAE: 1309.6709\n",
    "# RMSE: 2309.8228\n",
    "# RÂ²: 0.9438\n",
    "# Best Model params: {'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False, 'fs__vt__threshold': 0.005}\n",
    "\n",
    "# Add 'age_rel_model' \n",
    "# MAE: 1304.7703\n",
    "# RMSE: 2302.0800\n",
    "# RÂ²: 0.9442\n",
    "# Best Model params: {'preprocess__fs__vt__threshold': 0.005, 'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "\n",
    "# Add 'engine_rel_model'\n",
    "# MAE: 1298.8499\n",
    "# RMSE: 2299.7010\n",
    "# RÂ²: 0.9443\n",
    "# Best Model params: {'preprocess__fs__vt__threshold': 0.005, 'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "\n",
    "# Use engine_per_mpg instead of mpg_per_engine\n",
    "# MAE: 1298.6139\n",
    "# RMSE: 2299.5233\n",
    "# RÂ²: 0.9443\n",
    "# Best Model params: {'preprocess__fs__vt__threshold': 0.005, 'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n",
    "\n",
    "# After adding FS pipe\n",
    "# MAE: 1300.1077\n",
    "# RMSE: 2299.1285\n",
    "# RÂ²: 0.9443\n",
    "# Best Model params: {'preprocess__fs__vt__threshold': 0.005, 'model__n_estimators': 328, 'model__min_samples_split': 5, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20, 'model__bootstrap': False}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d0fce47e-39a6-480f-979f-7f91422734dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipe = rf_best_rand if hasattr(rf_best_rand, \"named_steps\") else rf_best_rand[0]\n",
    "\n",
    "# Use the debug preprocessor pipeline to get final feature names by hierarchically accessing each step\n",
    "feature_names_after_fs = debug_preprocessor_pipe.named_steps['fs'].get_feature_names_out()\n",
    "feat_names = feature_names_after_fs\n",
    "\n",
    "importances = pipe.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {\"feature\": feat_names, \"importance\": importances}\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for _, row in feature_importance_df.iterrows():\n",
    "    print(f\"{row['feature']:30s}: {row['importance']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a86655-becc-4ac3-80a1-9655769bde75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4 Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old parameter distribution\n",
    "et_param_dist = {\n",
    "    \"preprocess__fs__vt__threshold\": [0.0, 0.005, 0.01],\n",
    "    \"model__n_estimators\": randint(200, 600),        # number of trees\n",
    "    \"model__max_depth\": randint(5, 40),              # depth of each tree\n",
    "    \"model__min_samples_split\": randint(2, 10),      # min samples to split an internal node\n",
    "    \"model__min_samples_leaf\": randint(1, 8),        # min samples per leaf\n",
    "    \"model__max_features\": [\"sqrt\"],           # feature sampling strategy (sqrt performed better than log2 and None in previous tests)\n",
    "    \"model__bootstrap\": [False]                      # use bootstrapping or not (False performed better than True in previous tests)\n",
    "}\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "et_param_dist = {\n",
    "    \"preprocess__fs__vt__threshold\": [0.005],\n",
    "    \"model__n_estimators\": [328],\n",
    "    \"model__max_depth\": [20],\n",
    "    \"model__min_samples_split\": [5],\n",
    "    \"model__min_samples_leaf\": [1],\n",
    "    \"model__max_features\": [\"sqrt\"],\n",
    "    \"model__bootstrap\": [False],\n",
    "}\n",
    "\n",
    "et_best_rand = model_hyperparameter_tuning(X_train, y_train, et_pipe_fe, et_param_dist)\n",
    "\n",
    "joblib.dump(et_best_rand, \"et_best_rand.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6d3b92-efc6-4d8a-b3e1-cb6aff36b84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.4 StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "37c25558-2e97-4565-9a57-8a46800b59ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Old parameter distribution\n",
    "stack_param_dist = {\n",
    "    \"final_estimator__learning_rate\": uniform(0.02, 0.1),\n",
    "    \"final_estimator__max_depth\": randint(3, 10),\n",
    "    \"final_estimator__min_samples_leaf\": randint(3, 20),\n",
    "    \"final_estimator__l2_regularization\": uniform(0.0, 1.0),\n",
    "}\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "stack_param_dist = {\n",
    "    \"final_estimator__learning_rate\": [0.061135390505667866],\n",
    "    \"final_estimator__max_depth\": [5],\n",
    "    \"final_estimator__min_samples_leaf\": [10],\n",
    "    \"final_estimator__l2_regularization\": [0.19438003399487302]\n",
    "}\n",
    "\n",
    "stack_best = model_hyperparameter_tuning(X_train, y_train, stack_pipe_fe, stack_param_dist, splits=3)\n",
    "# joblib.dump(stack_best, \"stack_best.pkl\")\n",
    "\n",
    "\n",
    "# Long Duration (~3mins)\n",
    "\n",
    "# MAE: 1351.8682\n",
    "# RMSE: 2498.2822\n",
    "# RÂ²: 0.9342\n",
    "\n",
    "# After RandomizedSearchCV:\n",
    "# MAE: 1350.4717\n",
    "# RMSE: 2497.0474\n",
    "# RÂ²: 0.9343\n",
    "# Best Model params: {'final_estimator__l2_regularization': np.float64(0.978892858275009), 'final_estimator__learning_rate': np.float64(0.06867421529594551), 'final_estimator__max_depth': 6, 'final_estimator__min_samples_leaf': 13}\n",
    "\n",
    "# Removed ElasticNet from stacking due to poor performance compared to RF and HGB alone\n",
    "# canceled but the cv scores didnt seem to show much improvement\n",
    "\n",
    "# Using transmission and fuelType as OHE instead of TE():\n",
    "# MAE: 1357.4291\n",
    "# RMSE: 2516.5470\n",
    "# RÂ²: 0.9333\n",
    "# Best Model params: {'final_estimator__l2_regularization': np.float64(0.19438003399487302), 'final_estimator__learning_rate': np.float64(0.061135390505667866), 'final_estimator__max_depth': 5, 'final_estimator__min_samples_leaf': 10}\n",
    "\n",
    "\n",
    "# Removed fillna(0) in feature engineering for a_x_b and model_freq():\n",
    "# was worse for hgb and rf so not tested for stacking\n",
    "\n",
    "# ...\n",
    "\n",
    "# implemented GroupModeImputer\n",
    "# MAE: 1329.2379\n",
    "# RMSE: 2453.0239\n",
    "# RÂ²: 0.9366\n",
    "# Best Model params: {'final_estimator__min_samples_leaf': 10, 'final_estimator__max_depth': 5, 'final_estimator__learning_rate': 0.061135390505667866, 'final_estimator__l2_regularization': 0.19438003399487302}\n",
    "\n",
    "# Fixed GroupImputer and added Feature Engineering to pipeline\n",
    "# MAE: 1369.6876\n",
    "# RMSE: 2516.2583\n",
    "# RÂ²: 0.9333\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852bd7bc-33d9-4ae4-a552-0850b200be11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Feature Importance of Tree Models (with SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dc7da9e-7a7e-479c-830e-a840f78990bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use SHAP (SHapley Additive exPlanations) to\n",
    "  identify feature importance specifically for our selected tree models (HGB&RF)\n",
    "\n",
    "  **Why SHAP for Trees:**\n",
    "  - Provides exact feature importance values for tree-based\n",
    "  models\n",
    "  - Tree models handle irrelevant features, but noise features\n",
    "  still impact performance\n",
    "  - Enables data-driven selection rather than statistical filter\n",
    "  methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae2ab40-c379-41d5-a60e-2bbb51be7a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.1 Define needed Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "29f2979d-5324-4aa7-929f-85c2e67f09b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get Feature names aligned with X_proc\n",
    "def get_pipeline_feature_matrix(pipe, X):\n",
    "    \"\"\"\n",
    "    Given a fitted pipeline with steps:\n",
    "      'preprocess' -> optional 'vt' -> optional 'fs' -> 'model'\n",
    "    return:\n",
    "      X_proc: 2D numpy array of features just before the model step\n",
    "      feat_names: 1D np.array of feature names aligned with X_proc columns\n",
    "    \"\"\"\n",
    "    X_proc = pipe.named_steps[\"preprocess\"].transform(X)\n",
    "    feat_names = debug_preprocessor_pipe.named_steps[\"fs\"].get_feature_names_out()\n",
    "\n",
    "    return X_proc, feat_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c530d767-6718-467b-916e-96e6250bab0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compute Shap Importance\n",
    "def compute_shap_importance(\n",
    "    pipe,\n",
    "    X,\n",
    "    sample_size=1000,\n",
    "    seed=42,\n",
    "    model_name=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute global SHAP feature importances for a fitted pipeline.\n",
    "\n",
    "    Steps:\n",
    "      - Transform X with the pipeline up to just before the model.\n",
    "      - Subsample up to `sample_size` rows.\n",
    "      - Use TreeExplainer on the model (tree-based models).\n",
    "      - Return a DataFrame with mean |SHAP| per feature.\n",
    "\n",
    "    Returns:\n",
    "      shap_df: DataFrame with columns ['feature', 'importance']\n",
    "      feat_names: np.array of feature names aligned with importances\n",
    "    \"\"\"\n",
    "    # Extract processed feature matrix and names\n",
    "    X_proc, feat_names = get_pipeline_feature_matrix(pipe, X)\n",
    "\n",
    "    # Subsample rows for SHAP (for speed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = min(sample_size, len(X_proc))\n",
    "    idx = rng.choice(len(X_proc), n, replace=False)\n",
    "    X_sample = X_proc[idx]\n",
    "\n",
    "    # Underlying model (last step in pipeline)\n",
    "    model = pipe.named_steps[\"model\"]\n",
    "    tag = model_name or model.__class__.__name__\n",
    "\n",
    "    # TreeExplainer is appropriate for tree ensembles (RF, ET, HGB, GB, etc.)\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_vals = explainer.shap_values(X_sample)\n",
    "\n",
    "    # For regression, shap_vals is (n_samples, n_features)\n",
    "    importance = np.abs(shap_vals).mean(axis=0)\n",
    "\n",
    "    shap_df = (\n",
    "        pd.DataFrame({\"feature\": feat_names, \"importance\": importance})\n",
    "        .sort_values(\"importance\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(f\"Top 20 features by SHAP for {tag}:\")\n",
    "    print(shap_df.head(20).to_string(index=False))\n",
    "\n",
    "    return shap_df, feat_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1e73659f-6a4c-428c-9286-a3efcc425fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot Shap Importance\n",
    "def plot_top_shap(shap_df, model_name, top_k=20):\n",
    "    \"\"\"\n",
    "    Horizontal bar plot of top_k features by mean |SHAP|.\n",
    "    \"\"\"\n",
    "    top_df = shap_df.head(top_k).iloc[::-1]  # reverse for nicer barh order\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.barh(top_df[\"feature\"], top_df[\"importance\"])\n",
    "    ax.set_xlabel(\"Average |SHAP| value\")\n",
    "    ax.set_title(f\"Top {top_k} features by SHAP â {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9b0df25d-7c96-489f-ba57-e6c54e0d2083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# See if smaller k features improve MAE (top k)\n",
    "def cv_mae_topk_from_shap(\n",
    "    pipe,\n",
    "    shap_importance,\n",
    "    X,\n",
    "    y,\n",
    "    n_features_list,\n",
    "    folds=5,\n",
    "    seed=42,\n",
    "    model_name=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    For a fitted pipeline `pipe` and its SHAP importances:\n",
    "      - Build X_proc, feat_names from the pipeline.\n",
    "      - For each k in n_features_list:\n",
    "          * Take top-k features by SHAP.\n",
    "          * Run KFold CV on X_proc[:, idx] with the pipeline's final estimator.\n",
    "      - Print MAE per k and return the best (k, model, feature list).\n",
    "\n",
    "    Returns:\n",
    "      best_model: fitted estimator on full X_proc restricted to best-k features\n",
    "      best_features: list of feature names used\n",
    "    \"\"\"\n",
    "    # 1) Get processed features and names\n",
    "    X_proc, feat_names = get_pipeline_feature_matrix(pipe, X)\n",
    "    feat_names = np.asarray(feat_names, dtype=object)\n",
    "\n",
    "    # 2) SHAP ranking\n",
    "    shap_sorted = shap_importance.sort_values(\"importance\", ascending=False)\n",
    "    shap_order = shap_sorted[\"feature\"].tolist()\n",
    "\n",
    "    # helper: indices of top-k by SHAP\n",
    "    def indices_for_topk(k):\n",
    "        top_feats = shap_order[:k]\n",
    "        return [i for i, fname in enumerate(feat_names) if fname in top_feats]\n",
    "\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    model_proto = pipe.named_steps[\"model\"]\n",
    "    tag = model_name or model_proto.__class__.__name__\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for k in n_features_list:\n",
    "        idx = indices_for_topk(k)\n",
    "        if len(idx) == 0:\n",
    "            print(f\"Skipping k={k}: no matching feature indices.\")\n",
    "            continue\n",
    "\n",
    "        mae_folds = []\n",
    "\n",
    "        for train_idx, val_idx in kf.split(X_proc):\n",
    "            X_tr, X_val = X_proc[train_idx][:, idx], X_proc[val_idx][:, idx]\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            est = clone(model_proto)\n",
    "            est.fit(X_tr, y_tr)\n",
    "            y_pred = est.predict(X_val)\n",
    "            mae_folds.append(mean_absolute_error(y_val, y_pred))\n",
    "\n",
    "        mae_mean = float(np.mean(mae_folds))\n",
    "        results.append({\"k\": k, \"mae\": mae_mean, \"idx\": idx})\n",
    "\n",
    "    # pick best k\n",
    "    if not results:\n",
    "        raise RuntimeError(\"No valid k in n_features_list produced results.\")\n",
    "\n",
    "    best = min(results, key=lambda r: r[\"mae\"])\n",
    "    best_k = best[\"k\"]\n",
    "    best_mae = best[\"mae\"]\n",
    "    best_idx = best[\"idx\"]\n",
    "    best_features = [feat_names[i] for i in best_idx]\n",
    "\n",
    "    print(f\"\\nTop-k SHAP feature CV â {tag}\")\n",
    "    for r in results:\n",
    "        print(f\"  k={r['k']:3d} | MAE={r['mae']:.2f}\")\n",
    "    print(f\"Best: k={best_k} | MAE={best_mae:.2f}\")\n",
    "\n",
    "    # fit final estimator on full X_proc restricted to best_k features\n",
    "    final_est = clone(model_proto)\n",
    "    final_est.fit(X_proc[:, best_idx], y)\n",
    "\n",
    "    return final_est, best_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bb548f4f-6cc4-44ea-b4ee-86210409f74a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ShapTopKColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer that selects a fixed subset of columns by name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_features : list of str\n",
    "        Feature names (after preprocessing) to keep.\n",
    "\n",
    "    all_feature_names : array-like of str\n",
    "        Full list of feature names aligned with the columns of X after preprocessing.\n",
    "        These are typically obtained from get_pipeline_feature_matrix(...).\n",
    "    \"\"\"\n",
    "    def __init__(self, selected_features, all_feature_names):\n",
    "        self.selected_features = list(selected_features)\n",
    "        self.all_feature_names = np.asarray(all_feature_names, dtype=object)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Compute the column indices corresponding to selected_features\n",
    "        name_to_idx = {name: i for i, name in enumerate(self.all_feature_names)}\n",
    "        self.idx_ = [\n",
    "            name_to_idx[name]\n",
    "            for name in self.selected_features\n",
    "            if name in name_to_idx\n",
    "        ]\n",
    "        if len(self.idx_) == 0:\n",
    "            raise ValueError(\n",
    "                \"ShapTopKColumnSelector: none of the selected_features were found \"\n",
    "                \"in all_feature_names.\"\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X is the matrix after preprocessing; select only the chosen columns\n",
    "        return X[:, self.idx_]\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        # For consistency with sklearn's feature-name API\n",
    "        return np.asarray(self.selected_features, dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6bbb30d-d43d-4b09-b9f4-9351b4315c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_shap_topk_pipeline(\n",
    "    base_pipe,\n",
    "    best_features,\n",
    "    all_feature_names,\n",
    "    step_model_name=\"model\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a final pipeline that:\n",
    "      - reuses the preprocessing (and vt/fs if present) from `base_pipe`\n",
    "      - inserts a ShapTopKColumnSelector to keep only `best_features`\n",
    "      - uses a fresh clone of the base model as final estimator\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_pipe : sklearn Pipeline\n",
    "        Fitted pipeline with steps: 'preprocess' -> optional 'vt'/'fs' -> 'model'.\n",
    "\n",
    "    best_features : list of str\n",
    "        Names of the features (after preprocessing) to keep.\n",
    "\n",
    "    all_feature_names : array-like of str\n",
    "        Full list of feature names aligned with the output of preprocessing\n",
    "        (and vt/fs if they were applied when computing SHAP).\n",
    "\n",
    "    step_model_name : str, default=\"model\"\n",
    "        Name of the final estimator step in base_pipe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    final_pipe : sklearn Pipeline\n",
    "        Unfitted pipeline. Call final_pipe.fit(X, y) to train on full data.\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "\n",
    "    # 1) Preprocess step (clone so we refit on full data)\n",
    "    pre = base_pipe.named_steps[\"preprocess\"]\n",
    "    steps.append((\"preprocess\", clone(pre)))\n",
    "\n",
    "    # 2) Optional VarianceThreshold\n",
    "    if \"vt\" in base_pipe.named_steps and base_pipe.named_steps[\"vt\"] is not None:\n",
    "        steps.append((\"vt\", clone(base_pipe.named_steps[\"vt\"])))\n",
    "\n",
    "    # 3) SHAP-based column selector\n",
    "    shap_selector = ShapTopKColumnSelector(\n",
    "        selected_features=best_features,\n",
    "        all_feature_names=all_feature_names,\n",
    "    )\n",
    "    steps.append((\"shap_select\", shap_selector))\n",
    "\n",
    "    # 4) Final estimator â fresh clone of the base model\n",
    "    base_model = base_pipe.named_steps[step_model_name]\n",
    "    steps.append((\"model\", clone(base_model)))\n",
    "\n",
    "    final_pipe = Pipeline(steps)\n",
    "    return final_pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee942b45-691c-4f76-8bb8-8ed030f5cecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.2 Feature Importance HGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7097e769-b969-4fea-8fcc-7f0d9cf7fda4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unpack tuned pipeline and search object\n",
    "hgb_pipe, hgb_search = hgb_best\n",
    "\n",
    "# Best CV scores from the search\n",
    "idx = hgb_search.best_index_\n",
    "mae_cv  = -hgb_search.cv_results_['mean_test_mae'][idx]\n",
    "rmse_cv = np.sqrt(-hgb_search.cv_results_['mean_test_mse'][idx])\n",
    "r2_cv   = hgb_search.cv_results_['mean_test_r2'][idx]\n",
    "\n",
    "# Feature count after preprocess (+ VT)\n",
    "X_proc_hgb, feat_names_hgb = get_pipeline_feature_matrix(hgb_pipe, X_train)\n",
    "n_features_total_hgb = X_proc_hgb.shape[1]\n",
    "\n",
    "print(\"Baseline HGB (CV on train):\")\n",
    "print(f\"MAE:  {mae_cv:.4f}\")\n",
    "print(f\"RMSE: {rmse_cv:.4f}\")\n",
    "print(f\"RÂ²:   {r2_cv:.4f}\")\n",
    "print(f\"Total features used: {n_features_total_hgb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e4b6b7-986d-48c2-bf7a-b07afb052c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap_importance_hgb, feat_names_hgb = compute_shap_importance(\n",
    "    hgb_pipe,\n",
    "    X_train,\n",
    "    sample_size=1000,\n",
    "    seed=42,\n",
    "    model_name=\"HGB\",\n",
    ")\n",
    "\n",
    "plot_top_shap(shap_importance_hgb, model_name=\"HGB\", top_k=n_features_total_hgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03630ed3-21fc-48f8-90de-1e4b14f94eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if a smaller feature amount gives better MAE\n",
    "n_features_list_hgb = list(range(10, n_features_total_hgb)) \n",
    "if n_features_total_hgb not in n_features_list_hgb:\n",
    "    n_features_list_hgb.append(n_features_total_hgb)\n",
    "\n",
    "best_model_hgb, best_features_hgb = cv_mae_topk_from_shap(\n",
    "    pipe=hgb_pipe,\n",
    "    shap_importance=shap_importance_hgb,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    n_features_list=n_features_list_hgb,\n",
    "    folds=5,\n",
    "    seed=42,\n",
    "    model_name=\"HGB\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d87a871e-4968-4787-b9cf-04838dd4a0c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final Pipe with best k and MAE\n",
    "hgb_final_shap_pipe = build_shap_topk_pipeline(\n",
    "    base_pipe=hgb_pipe,\n",
    "    best_features=best_features_hgb,\n",
    "    all_feature_names=feat_names_hgb,\n",
    "    step_model_name=\"model\",   # name of the final estimator step in hgb_pipe\n",
    ")\n",
    "\n",
    "# Fit on full training data\n",
    "hgb_final_shap_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Save for later use\n",
    "joblib.dump(hgb_final_shap_pipe, \"hgb_final_shap_pipe.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f76646-5cae-49f3-a442-b50d7ed7bbe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.3 Feature Importance RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c9afd6-5d67-4f5c-b379-4d1bf9a5b6ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unpack tuned RF pipeline and search object\n",
    "rf_pipe, rf_search = rf_best_rand  # result from model_hyperparameter_tuning\n",
    "\n",
    "# Best CV scores from the search\n",
    "idx = rf_search.best_index_\n",
    "mae_cv_rf  = -rf_search.cv_results_['mean_test_mae'][idx]\n",
    "rmse_cv_rf = np.sqrt(-rf_search.cv_results_['mean_test_mse'][idx])\n",
    "r2_cv_rf   =  rf_search.cv_results_['mean_test_r2'][idx]\n",
    "\n",
    "# Feature matrix + names after preprocess (+ vt/fs if present)\n",
    "X_proc_rf, feat_names_rf = get_pipeline_feature_matrix(rf_pipe, X_train)\n",
    "n_features_total_rf = X_proc_rf.shape[1]\n",
    "\n",
    "print(\"Baseline RandomForest (CV on train):\")\n",
    "print(f\"MAE:  {mae_cv_rf:.4f}\")\n",
    "print(f\"RMSE: {rmse_cv_rf:.4f}\")\n",
    "print(f\"RÂ²:   {r2_cv_rf:.4f}\")\n",
    "print(f\"Total features used: {n_features_total_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9b167904-22f7-4484-9644-f349d857f798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap_importance_rf, feat_names_rf_check = compute_shap_importance(\n",
    "    rf_pipe,\n",
    "    X_train,\n",
    "    sample_size=1000,\n",
    "    seed=42,\n",
    "    model_name=\"RandomForest\",\n",
    ")\n",
    "\n",
    "plot_top_shap(shap_importance_rf, model_name=\"RandomForest\", top_k=n_features_total_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604db5f2-8d4a-48a6-b6ae-8586fa3b0511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if a smaller feature amount gives better MAE\n",
    "n_features_list_rf = list(range(10, n_features_total_rf)) \n",
    "if n_features_total_rf not in n_features_list_rf:\n",
    "    n_features_list_rf.append(n_features_total_rf)\n",
    "\n",
    "best_model_rf, best_features_rf = cv_mae_topk_from_shap(\n",
    "    pipe=rf_pipe,\n",
    "    shap_importance=shap_importance_rf,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    n_features_list=n_features_list_rf,\n",
    "    folds=5,\n",
    "    seed=42,\n",
    "    model_name=\"RandomForest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3f172be2-4464-4404-8802-7847ac905b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build final pipeline:\n",
    "#   preprocess -> (vt/fs) -> shap_select(best_features_rf) -> RF\n",
    "rf_final_shap_pipe = build_shap_topk_pipeline(\n",
    "    base_pipe=rf_pipe,\n",
    "    best_features=best_features_rf,\n",
    "    all_feature_names=feat_names_rf,\n",
    "    step_model_name=\"model\",   # name of the RF step in rf_pipe\n",
    ")\n",
    "\n",
    "# Fit final RF SHAP-top-k pipeline on full training data\n",
    "rf_final_shap_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Optionally save for later use\n",
    "joblib.dump(rf_final_shap_pipe, \"rf_final_shap_pipe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135acfad-1ad6-49f9-97fc-f1a81d0c3187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb68e758-d7b1-4f80-9dc4-ed985f721c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Extra Task (1 Point): Be in the Top 5 Groups on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9d0cb5aa-be61-4a2e-af5d-0f7e63f604f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict_on_test(model_pipeline, model_name):\n",
    "    # Load best model from Joblib and predict on validation set to verify\n",
    "    pipe_best = joblib.load(model_pipeline)\n",
    "    \n",
    "    # Predict on test set\n",
    "    df_cars_test['price'] = pipe_best.predict(df_cars_test)\n",
    "    df_cars_test['price'].to_csv(f'Group05_{model_name}_Version10.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9d9d9f8a-642e-405d-9de2-b8c802a90919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predict_on_test(\"hgb_final_shap_pipe.pkl\", \"HGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3745830-e49d-43f8-8910-09e6bad342d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predict_on_test(\"rf_final_shap_pipe.pkl\", \"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f119b3-d572-4c5b-830e-6f396cd167a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# predict_on_test(\"stack_pipe.pkl\", \"Stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b18805-e662-4807-bdb8-758df6027d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Submission with API push"
    }
   },
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c cars4you -f Group05_Version05.csv -m \"Message\" # Uncomment to submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350057a3-c771-4bf0-8fac-19b49be17e35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Score Check"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c cars4you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bfd1b4a-0e13-443a-aeb7-fa6324f801e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. Open-Ended-Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a3ac74-bda2-40f7-97f1-ca1574ae7896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Open-ended Section: Global vs Brand- and Model-Specific Models\n",
    "\n",
    "##### a) Objective and motivation (0.5v)\n",
    "\n",
    "We investigated how far Cars4You should specialize its pricing models:\n",
    "\n",
    "1. **Brand level:** Is a single global price model for all brands sufficient, or do separate brand-specific models reduce pricing error?\n",
    "2. **Brandâmodel level:** For frequent models (e.g. âSkoda Octaviaâ, âVW Golfâ), does an even more specialized model per (brand, model) segment bring additional improvements, or does it overfit?\n",
    "\n",
    "Concretely, we started from our final production pipeline `hgb_final_shap_pipe` (full preprocessing + SHAP-based feature selection + HGB regressor) and compared:\n",
    "\n",
    "- **Global model:** trained on all cars, evaluated only on a given segment.\n",
    "- **Brand-specific model:** same preprocessing and SHAP selector, but the regressor re-fitted only on cars of a given brand.\n",
    "- **Brandâmodel-specific model:** same preprocessing and SHAP selector, but the regressor re-fitted only on cars of a given (brand, model) pair.\n",
    "\n",
    "We measured mean absolute error (MAE) and root mean squared error (RMSE) per segment. This answers how much performance we gain by moving from:\n",
    "\n",
    "> one global model â several brand models â many brandâmodel models.\n",
    "\n",
    "---\n",
    "\n",
    "##### b) Difficulty of tasks (1v)\n",
    "\n",
    "Extending the existing solution to this multi-level comparison was non-trivial:\n",
    "\n",
    "- **Complex pipeline with a custom SHAP selector**  \n",
    "  The final pipeline contains a `ShapTopKColumnSelector` that is not clone-compatible. Standard `cross_val_score` + `clone` would fail. We therefore implemented manual cross-validation:\n",
    "  - reuse the fitted preprocessing + SHAP selector from `hgb_final_shap_pipe`;\n",
    "  - only re-fit the final regressor for each fold and segment.\n",
    "\n",
    "- **Consistent and fair evaluation protocol**  \n",
    "  We reused the same 5-fold KFold strategy (`n_splits=5`, `shuffle=True`, `random_state=42`) and the same target (`price`) as in the main project. For each fold and segment:\n",
    "  - the global model is trained on all training rows but evaluated only on validation rows belonging to that segment;\n",
    "  - the segment-specific model is trained and evaluated only on that segmentâs rows.\n",
    "\n",
    "- **Handling data imbalance**  \n",
    "  Data is unevenly distributed across brands and models. We therefore:\n",
    "  - restricted the analysis to brands with at least 500 training samples;\n",
    "  - for brandâmodel analysis, kept only frequent pairs (e.g. Skoda Octavia, VW Golf) with a minimum sample threshold per segment;\n",
    "  - enforced additional checks per fold (minimum training size) to avoid fits on a handful of cars.\n",
    "\n",
    "- **Manual metric computation**  \n",
    "  Due to an older `sklearn` version (no `squared=` parameter), RMSE had to be computed manually as `sqrt(MSE)` inside the CV loops instead of relying on built-in scorers.\n",
    "\n",
    "Overall, the task required custom CV logic, careful reuse of the production pipeline, and multiple levels of segment-wise filtering.\n",
    "\n",
    "---\n",
    "\n",
    "##### c) Correctness and efficiency of implementation (1v)\n",
    "\n",
    "To keep the analysis correct and reasonably efficient we:\n",
    "\n",
    "- **Reused the production pipeline as-is**  \n",
    "  All preprocessing (imputation, scaling, encoding, price anchors) and SHAP-based feature selection are exactly the same as in the final model used on the test set. Only the last regressor is re-fit for segment-specific models.\n",
    "\n",
    "- **Used a single CV design for all comparisons**  \n",
    "  The same KFold splits (`splits = list(KFold(...).split(X_train, y_train))`) are reused for:\n",
    "  - global per-brand evaluation;\n",
    "  - brand-specific evaluation;\n",
    "  - global per (brand, model) evaluation;\n",
    "  - brandâmodel-specific evaluation.  \n",
    "  This removes extra randomness and makes differences directly comparable.\n",
    "\n",
    "- **Implemented clear separation between global and segment-specific training**  \n",
    "  - For brands:  \n",
    "    - global: fit on all brands, compute metrics only on that brandâs validation rows;  \n",
    "    - brand-specific: use the fixed preprocessor, fit a fresh regressor only on that brandâs transformed data.\n",
    "  - For (brand, model) pairs:  \n",
    "    - global: fit on all cars, compute metrics only on that (brand, model) validation subset;  \n",
    "    - brandâmodel-specific: fixed preprocessor + fresh regressor only on that pair.\n",
    "\n",
    "- **Guarded against tiny segments**  \n",
    "  Only segments with enough rows at dataset level and per fold are evaluated. Otherwise, metrics are set to NaN and those segments are excluded via `dropna`.\n",
    "\n",
    "This design produces stable segment-wise estimates without changing the core production pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "##### d) Discussion of results (1v)\n",
    "\n",
    "#### Brand-level comparison\n",
    "\n",
    "For the main brands, the final summary table (MAE in GBP) is:\n",
    "\n",
    "| Brand    | MAE (global) | MAE (brand) | ÎMAE (brand â global) | n_samples |\n",
    "|----------|--------------|-------------|------------------------|-----------|\n",
    "| Ford     | 966.7        | 929.2       | -37.6                  | 16,371    |\n",
    "| BMW      | 1,828.0      | 1,792.8     | -35.2                  | 7,540     |\n",
    "| Mercedes | 1,968.7      | 1,934.6     | -34.1                  | 11,899    |\n",
    "| VW       | 1,299.7      | 1,287.8     | -11.9                  | 10,572    |\n",
    "| Audi     | 1,806.0      | 1,794.5     | -11.5                  | 7,456     |\n",
    "| Skoda    | 1,174.6      | 1,165.9     | -8.7                   | 4,380     |\n",
    "| Toyota   |   926.7      |   920.9     | -5.9                   | 4,714     |\n",
    "| Opel     |   777.1      |   774.5     | -2.6                   | 9,530     |\n",
    "\n",
    "Key observations:\n",
    "\n",
    "- **High-volume premium brands benefit the most from brand-specific models.**  \n",
    "  Ford, BMW and Mercedes gain about 35â38 GBP lower MAE per car (â 2â4% relative improvement). This is meaningful at scale and based on large sample sizes.\n",
    "\n",
    "- **Moderate gains for VW, Audi, Skoda, Toyota.**  \n",
    "  MAE improvements are smaller (5â12 GBP, typically <1% relative), but still consistent in sign.\n",
    "\n",
    "- **Minimal benefit for Opel.**  \n",
    "  The improvement for Opel (â 2.6 GBP) is negligible relative to its base MAE. The global model already captures Opelâs pricing patterns.\n",
    "\n",
    "- **RMSE sometimes increases slightly for brand-specific models.**  \n",
    "  For some brands, RMSE is marginally higher, indicating that brand-specific models reduce typical errors but can perform worse on rare/extreme cases, hinting at mild overfitting in the tails.\n",
    "\n",
    "Overall, moving from a global to a brand-specific layer consistently does not harm MAE and clearly helps for some large brands, but the absolute gains are moderate.\n",
    "\n",
    "#### Brandâmodel-level comparison\n",
    "\n",
    "For frequent (brand, model) pairs, the analysis shows a more mixed picture. A selection of results (MAE in GBP):\n",
    "\n",
    "| Brand   | Model        | MAE global | MAE seg | ÎMAE (seg â global) | n_samples |\n",
    "|---------|--------------|-----------:|--------:|---------------------:|----------:|\n",
    "| Skoda   | kamiq        | 1,418.6    | 1,107.1 | -311.5               | 109       |\n",
    "| VW      | amarok       | 2,988.7    | 2,801.3 | -187.4               | 83        |\n",
    "| Mercedes| x-class      | 3,592.8    | 3,448.9 | -144.0               | 59        |\n",
    "| Skoda   | scala        | 1,175.7    | 1,100.5 | -75.2                | 147       |\n",
    "| Ford    | b-max        |   640.2    |   578.1 | -62.1                | 248       |\n",
    "| Skoda   | octavia      | 1,089.4    | 1,031.9 | -57.5                | 1,021     |\n",
    "| Skoda   | fabia        |   845.8    |   795.1 | -50.6                | 1,069     |\n",
    "| VW      | up           |   645.2    |   608.1 | -37.1                | 608       |\n",
    "| BMW     | 1 series     | 1,158.2    | 1,130.0 | -28.1                | 1,358     |\n",
    "| VW      | golf         | 1,151.0    | 1,155.8 |  +4.8                | 3,515     |\n",
    "| Ford    | fiesta       |   753.3    |   762.7 |  +9.3                | 4,470     |\n",
    "| Toyota  | aygo         |   557.1    |   576.7 | +19.6                | 1,381     |\n",
    "| BMW     | 7 series     | 3,146.7    | 4,751.9 | +1,605.2             | 71        |\n",
    "| Mercedes| gls class    | 3,295.8    | 5,906.4 | +2,610.7             | 54        |\n",
    "\n",
    "Patterns:\n",
    "\n",
    "- **Some compact, relatively frequent models benefit from model-level specialization.**  \n",
    "  Examples: Skoda Kamiq, Scala, Octavia and Fabia; VW up; Ford B-MAX.  \n",
    "  These segments see large MAE reductions (50â300 GBP), and RMSE also tends to decrease. Here, the model-level regressor can exploit consistent, model-specific patterns.\n",
    "\n",
    "- **For many common volume models, gains are small or negative.**  \n",
    "  VW Golf, Ford Fiesta, Opel Corsa, Toyota Yaris, etc. often show small positive ÎMAE and/or higher RMSE. For these, splitting by model does not significantly improve typical error and can worsen extreme cases.\n",
    "\n",
    "- **For rare, high-priced models, model-specific fits severely overfit.**  \n",
    "  BMW 7 series, BMW X6, Mercedes GLS/S/SL/CLS class, VW Beetle, Toyota Avensis/Verso and others exhibit very large increases in MAE (hundreds to thousands of GBP) and often huge increases in RMSE.  \n",
    "  These models have small sample sizes (often <100 cars), so a separate model per (brand, model) is clearly not robust.\n",
    "\n",
    "In short:\n",
    "\n",
    "- Moving from **global â brand** is often beneficial and relatively safe for high-volume brands.\n",
    "- Moving further from **brand â brandâmodel** brings strong improvements only for a small subset of frequent models; for many others, especially rare premium models, it clearly overfits.\n",
    "\n",
    "---\n",
    "\n",
    "##### e) Alignment with objectives (0.5v)\n",
    "\n",
    "This extended open-ended study:\n",
    "\n",
    "- Directly addresses and expands a suggested topic (âglobal vs brand-specific modelsâ), and pushes it one step further to **brandâmodel** specialization.\n",
    "- Uses fully the final production pipeline and a consistent CV protocol, so the conclusions are directly relevant for deployment.\n",
    "- Provides a **clear design recommendation**:\n",
    "  - Use a **single global model** as the base.\n",
    "  - Optionally introduce **brand-level specialization** for a small set of high-volume brands (e.g. Ford, BMW, Mercedes) where MAE improvements are meaningful.\n",
    "  - Avoid full **brandâmodel specialization** except potentially for a handful of very frequent models with demonstrated gains; for most models, especially rare and expensive ones, splitting further clearly overfits.\n",
    "\n",
    "This shows that we not only tuned a strong model, but also explored the trade-off between model complexity and robustness in a structured, data-driven way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "44783a34-5a57-4037-a7d8-54b387152db9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load final production pipeline (preprocessing + SHAP + HGB)\n",
    "# hgb_final_shap_pipe = load(\"hgb_final_shap_pipe.pkl\")\n",
    "pipe_global = hgb_final_shap_pipe \n",
    "\n",
    "assert \"X_train\" in globals() and \"y_train\" in globals(), \"Define X_train and y_train before proceeding.\"\n",
    "\n",
    "# Identify the brand column (name may be 'Brand' or 'brand')\n",
    "brand_col = \"Brand\" if \"Brand\" in X_train.columns else \"brand\"\n",
    "assert brand_col in X_train.columns, (\n",
    "    f\"Brand column not found in X_train. \"\n",
    "    f\"First columns: {X_train.columns.tolist()[:20]}\"\n",
    ")\n",
    "\n",
    "print(\"Using brand column:\", brand_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dc47c72d-b82a-4b94-aa47-638eda95115a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inspect brand frequencies\n",
    "brand_counts = X_train[brand_col].value_counts()\n",
    "print(\"Top brands by count:\")\n",
    "print(brand_counts.head(15))\n",
    "\n",
    "# Select candidate brands\n",
    "#    - TOP_K: max number of brands to compare.\n",
    "#    - MIN_SAMPLES: minimum number of rows per brand.\n",
    "\n",
    "TOP_K = 8\n",
    "MIN_SAMPLES = 500  # adjust if needed\n",
    "\n",
    "candidate_brands = [\n",
    "    b for b, cnt in brand_counts.items()\n",
    "    if cnt >= MIN_SAMPLES\n",
    "][:TOP_K]\n",
    "\n",
    "print(\"\\nCandidate brands used in the comparison:\")\n",
    "print(candidate_brands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f0f1d273-7c33-4818-94d6-52585c8d9196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cross-validation setup: We reuse the same KFold splits for all evaluations to keep comparisons fair and to reduce randomness.\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "splits = list(cv.split(X_train, y_train))\n",
    "\n",
    "\n",
    "def eval_global_for_brand(model, X, y, brand_col, brand, splits):\n",
    "    \"\"\"\n",
    "    Evaluate the global pipeline for a single brand.\n",
    "\n",
    "    The model is trained on all brands in each fold, but the error\n",
    "    is computed only on validation rows belonging to the given brand.\n",
    "    \"\"\"\n",
    "    maes, rmses = [], []\n",
    "    n_obs = 0\n",
    "\n",
    "    for train_idx, val_idx in splits:\n",
    "        # Split data for this fold\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Train global model on ALL brands in this fold\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Restrict metrics to the target brand in validation\n",
    "        mask_b = (X_val[brand_col] == brand)\n",
    "        if mask_b.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        y_val_b = y_val[mask_b]\n",
    "        y_pred_b = y_pred[mask_b]\n",
    "\n",
    "        mae = mean_absolute_error(y_val_b, y_pred_b)\n",
    "        mse = mean_squared_error(y_val_b, y_pred_b)\n",
    "        rmse = float(np.sqrt(mse))\n",
    "\n",
    "        maes.append(mae)\n",
    "        rmses.append(rmse)\n",
    "        n_obs += mask_b.sum()\n",
    "\n",
    "    return {\n",
    "        \"MAE_mean\": float(np.mean(maes)),\n",
    "        \"MAE_std\":  float(np.std(maes)),\n",
    "        \"RMSE_mean\": float(np.mean(rmses)),\n",
    "        \"RMSE_std\":  float(np.std(rmses)),\n",
    "        \"n\": int(n_obs),\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_brand_specific(pipe_global, X, y, brand_col, brand, splits,\n",
    "                        min_train_per_fold=50):\n",
    "    \"\"\"\n",
    "    Evaluate a brand-specific model for a single brand.\n",
    "\n",
    "    Preprocessing + SHAP selection are kept fixed (from pipe_global).\n",
    "    In each fold:\n",
    "      - Transform the brand's data with the fixed preprocessor.\n",
    "      - Fit a fresh regressor (clone of the final step) only on that brand.\n",
    "      - Evaluate on validation rows of that brand.\n",
    "    \"\"\"\n",
    "    # Split the pipeline into:\n",
    "    # - preproc: all steps except the final regressor\n",
    "    # - base_reg: the final regressor template\n",
    "    preproc = pipe_global[:-1]\n",
    "    base_reg = pipe_global[-1]\n",
    "\n",
    "    maes, rmses = [], []\n",
    "    n_obs = 0\n",
    "\n",
    "    for train_idx, val_idx in splits:\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Keep only this brand in train/val\n",
    "        mask_tr = (X_tr[brand_col] == brand)\n",
    "        mask_val = (X_val[brand_col] == brand)\n",
    "\n",
    "        if mask_val.sum() == 0:\n",
    "            # No validation examples of this brand in this fold\n",
    "            continue\n",
    "        if mask_tr.sum() < min_train_per_fold:\n",
    "            # Too few training examples for a stable brand-specific fit\n",
    "            continue\n",
    "\n",
    "        X_tr_b, y_tr_b = X_tr[mask_tr], y_tr[mask_tr]\n",
    "        X_val_b, y_val_b = X_val[mask_val], y_val[mask_val]\n",
    "\n",
    "        # Do NOT refit the preprocessor; just transform with the fitted one\n",
    "        X_tr_b_proc = preproc.transform(X_tr_b)\n",
    "        X_val_b_proc = preproc.transform(X_val_b)\n",
    "\n",
    "        # Fresh regressor for this fold\n",
    "        reg = clone(base_reg)\n",
    "        reg.fit(X_tr_b_proc, y_tr_b)\n",
    "\n",
    "        y_pred_b = reg.predict(X_val_b_proc)\n",
    "\n",
    "        mae = mean_absolute_error(y_val_b, y_pred_b)\n",
    "        mse = mean_squared_error(y_val_b, y_pred_b)\n",
    "        rmse = float(np.sqrt(mse))\n",
    "\n",
    "        maes.append(mae)\n",
    "        rmses.append(rmse)\n",
    "        n_obs += len(y_val_b)\n",
    "\n",
    "    return {\n",
    "        \"MAE_mean\": float(np.mean(maes)) if maes else np.nan,\n",
    "        \"MAE_std\":  float(np.std(maes))  if maes else np.nan,\n",
    "        \"RMSE_mean\": float(np.mean(rmses)) if rmses else np.nan,\n",
    "        \"RMSE_std\":  float(np.std(rmses))  if rmses else np.nan,\n",
    "        \"n\": int(n_obs),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "68fac82c-6faf-4fcd-9a77-853202ce992e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate both models for each candidate brand\n",
    "\n",
    "pipe_global = hgb_final_shap_pipe \n",
    "\n",
    "global_results = []\n",
    "brand_specific_results = []\n",
    "\n",
    "for brand in candidate_brands:\n",
    "    print(\"Evaluating brand:\", brand)\n",
    "\n",
    "    # 1) Global: train on all brands, measure only this brand in validation\n",
    "    res_g = eval_global_for_brand(\n",
    "        pipe_global,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        brand_col=brand_col,\n",
    "        brand=brand,\n",
    "        splits=splits,\n",
    "    )\n",
    "    res_g.update({\n",
    "        \"brand\": brand,\n",
    "        \"model_type\": \"global\",\n",
    "    })\n",
    "    global_results.append(res_g)\n",
    "\n",
    "    # 2) Brand-specific: preproc fixed, regressor trained only on this brand\n",
    "    res_b = eval_brand_specific(\n",
    "        pipe_global,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        brand_col=brand_col,\n",
    "        brand=brand,\n",
    "        splits=splits,\n",
    "    )\n",
    "    res_b.update({\n",
    "        \"brand\": brand,\n",
    "        \"model_type\": \"brand_specific\",\n",
    "    })\n",
    "    brand_specific_results.append(res_b)\n",
    "\n",
    "# Collect results into DataFrames\n",
    "df_global = pd.DataFrame(global_results)\n",
    "df_brand = pd.DataFrame(brand_specific_results)\n",
    "\n",
    "print(\"\\nGlobal model results per brand:\")\n",
    "display(df_global)\n",
    "\n",
    "print(\"\\nBrand-specific model results per brand:\")\n",
    "display(df_brand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3ae57229-677c-4848-a3e2-55f45bf905ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean results and compute performance differences\n",
    "\n",
    "# Drop any brands where evaluation failed (NaNs)\n",
    "df_global = df_global.dropna(subset=[\"MAE_mean\", \"RMSE_mean\"])\n",
    "df_brand  = df_brand.dropna(subset=[\"MAE_mean\", \"RMSE_mean\"])\n",
    "\n",
    "# Merge global vs brand-specific results\n",
    "df_compare = df_global.merge(\n",
    "    df_brand,\n",
    "    on=\"brand\",\n",
    "    suffixes=(\"_global\", \"_brand\"),\n",
    ")\n",
    "\n",
    "# Compute deltas:\n",
    "#   delta_MAE  < 0  -> brand-specific has lower MAE (better)\n",
    "#   delta_RMSE < 0  -> brand-specific has lower RMSE (better)\n",
    "df_compare[\"delta_MAE\"]  = df_compare[\"MAE_mean_brand\"]  - df_compare[\"MAE_mean_global\"]\n",
    "df_compare[\"delta_RMSE\"] = df_compare[\"RMSE_mean_brand\"] - df_compare[\"RMSE_mean_global\"]\n",
    "\n",
    "# Sort by delta_MAE (most improvement first)\n",
    "df_compare_sorted = df_compare.sort_values(\"delta_MAE\")\n",
    "\n",
    "print(\"Per-brand comparison (head):\")\n",
    "display(df_compare_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1ffeb757-0371-4df0-81dd-981148360a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualizations: bar plots for MAE and ÎMAE\n",
    "\n",
    "# Global vs Brand-specific MAE per brand\n",
    "plt.figure(figsize=(8, 4))\n",
    "x = np.arange(len(df_compare_sorted))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(\n",
    "    x - width / 2,\n",
    "    df_compare_sorted[\"MAE_mean_global\"],\n",
    "    width,\n",
    "    label=\"Global model\",\n",
    ")\n",
    "plt.bar(\n",
    "    x + width / 2,\n",
    "    df_compare_sorted[\"MAE_mean_brand\"],\n",
    "    width,\n",
    "    label=\"Brand-specific model\",\n",
    ")\n",
    "\n",
    "plt.xticks(x, df_compare_sorted[\"brand\"], rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"MAE (GBP)\")\n",
    "plt.title(\"Global vs Brand-specific models (MAE per brand)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 1ÎMAE per brand (negative = improvement with specialization)\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.bar(df_compare_sorted[\"brand\"], df_compare_sorted[\"delta_MAE\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Î MAE (brand - global)\")\n",
    "plt.title(\"Effect of model specialization per brand\\n(negative = brand-specific MAE is lower)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1614895-99f2-43b7-b656-c540e7eeda77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Brand-Model Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "403e36df-4193-4b30-be8f-d6eb93b1889a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation helpers for brandâmodel segments\n",
    "\n",
    "def eval_global_for_brand_model(model, X, y, brand_col, model_col,\n",
    "                                brand, model_name, splits):\n",
    "    \"\"\"\n",
    "    Evaluate the global pipeline for a specific (brand, model) pair.\n",
    "\n",
    "    In each fold:\n",
    "      - Train on all cars.\n",
    "      - Compute MAE / RMSE only on validation rows where\n",
    "        Brand == brand AND model == model_name.\n",
    "    \"\"\"\n",
    "    maes, rmses = [], []\n",
    "    n_obs = 0\n",
    "\n",
    "    for train_idx, val_idx in splits:\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Train global model on ALL brands and models\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Restrict to this brandâmodel in validation\n",
    "        mask_seg = (\n",
    "            (X_val[brand_col] == brand) &\n",
    "            (X_val[model_col] == model_name)\n",
    "        )\n",
    "        if mask_seg.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        y_val_seg = y_val[mask_seg]\n",
    "        y_pred_seg = y_pred[mask_seg]\n",
    "\n",
    "        mae = mean_absolute_error(y_val_seg, y_pred_seg)\n",
    "        mse = mean_squared_error(y_val_seg, y_pred_seg)\n",
    "        rmse = float(np.sqrt(mse))\n",
    "\n",
    "        maes.append(mae)\n",
    "        rmses.append(rmse)\n",
    "        n_obs += mask_seg.sum()\n",
    "\n",
    "    return {\n",
    "        \"MAE_mean\": float(np.mean(maes)),\n",
    "        \"MAE_std\":  float(np.std(maes)),\n",
    "        \"RMSE_mean\": float(np.mean(rmses)),\n",
    "        \"RMSE_std\":  float(np.std(rmses)),\n",
    "        \"n\": int(n_obs),\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_brand_model_specific(pipe_global, X, y, brand_col, model_col,\n",
    "                              brand, model_name, splits,\n",
    "                              min_train_per_fold=40):\n",
    "    \"\"\"\n",
    "    Evaluate a brandâmodel-specific regressor.\n",
    "\n",
    "    Preprocessing + SHAP selection stay fixed (from pipe_global).\n",
    "    In each fold:\n",
    "      - Keep only rows with this (brand, model).\n",
    "      - Transform them with the fixed preprocessor.\n",
    "      - Fit a fresh regressor only on this segment.\n",
    "      - Evaluate on validation rows of the same segment.\n",
    "    \"\"\"\n",
    "    preproc = pipe_global[:-1]\n",
    "    base_reg = pipe_global[-1]\n",
    "\n",
    "    maes, rmses = [], []\n",
    "    n_obs = 0\n",
    "\n",
    "    for train_idx, val_idx in splits:\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Restrict to this brandâmodel in train/val\n",
    "        mask_tr = (\n",
    "            (X_tr[brand_col] == brand) &\n",
    "            (X_tr[model_col] == model_name)\n",
    "        )\n",
    "        mask_val = (\n",
    "            (X_val[brand_col] == brand) &\n",
    "            (X_val[model_col] == model_name)\n",
    "        )\n",
    "\n",
    "        if mask_val.sum() == 0:\n",
    "            continue\n",
    "        if mask_tr.sum() < min_train_per_fold:\n",
    "            continue\n",
    "\n",
    "        X_tr_seg, y_tr_seg = X_tr[mask_tr], y_tr[mask_tr]\n",
    "        X_val_seg, y_val_seg = X_val[mask_val], y_val[mask_val]\n",
    "\n",
    "        # Transform with fixed preprocessor\n",
    "        X_tr_seg_proc = preproc.transform(X_tr_seg)\n",
    "        X_val_seg_proc = preproc.transform(X_val_seg)\n",
    "\n",
    "        # Fresh regressor for this fold\n",
    "        reg = clone(base_reg)\n",
    "        reg.fit(X_tr_seg_proc, y_tr_seg)\n",
    "        y_pred_seg = reg.predict(X_val_seg_proc)\n",
    "\n",
    "        mae = mean_absolute_error(y_val_seg, y_pred_seg)\n",
    "        mse = mean_squared_error(y_val_seg, y_pred_seg)\n",
    "        rmse = float(np.sqrt(mse))\n",
    "\n",
    "        maes.append(mae)\n",
    "        rmses.append(rmse)\n",
    "        n_obs += len(y_val_seg)\n",
    "\n",
    "    return {\n",
    "        \"MAE_mean\": float(np.mean(maes)) if maes else np.nan,\n",
    "        \"MAE_std\":  float(np.std(maes))  if maes else np.nan,\n",
    "        \"RMSE_mean\": float(np.mean(rmses)) if rmses else np.nan,\n",
    "        \"RMSE_std\":  float(np.std(rmses))  if rmses else np.nan,\n",
    "        \"n\": int(n_obs),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fb73408c-9253-4166-ba56-49eb7d898d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run evaluation for each (brand, model) pair\n",
    "\n",
    "bm_global_results = []\n",
    "bm_specific_results = []\n",
    "\n",
    "for (brand, model_name), cnt in candidate_pairs.items():\n",
    "    print(f\"Evaluating pair: {brand} / {model_name} (n={cnt})\")\n",
    "\n",
    "    # Global model on this brandâmodel segment\n",
    "    res_g = eval_global_for_brand_model(\n",
    "        pipe_global,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        brand_col=brand_col,\n",
    "        model_col=model_col,\n",
    "        brand=brand,\n",
    "        model_name=model_name,\n",
    "        splits=splits,\n",
    "    )\n",
    "    res_g.update({\n",
    "        \"brand\": brand,\n",
    "        \"model\": model_name,\n",
    "        \"segment_type\": \"global\",\n",
    "    })\n",
    "    bm_global_results.append(res_g)\n",
    "\n",
    "    # Brandâmodel-specific regressor\n",
    "    res_bm = eval_brand_model_specific(\n",
    "        pipe_global,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        brand_col=brand_col,\n",
    "        model_col=model_col,\n",
    "        brand=brand,\n",
    "        model_name=model_name,\n",
    "        splits=splits,\n",
    "    )\n",
    "    res_bm.update({\n",
    "        \"brand\": brand,\n",
    "        \"model\": model_name,\n",
    "        \"segment_type\": \"brand_model_specific\",\n",
    "    })\n",
    "    bm_specific_results.append(res_bm)\n",
    "\n",
    "df_bm_global = pd.DataFrame(bm_global_results)\n",
    "df_bm_spec   = pd.DataFrame(bm_specific_results)\n",
    "\n",
    "print(\"\\nGlobal results per (brand, model):\")\n",
    "display(df_bm_global)\n",
    "\n",
    "print(\"\\nBrandâmodel-specific results:\")\n",
    "display(df_bm_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b457ac57-92eb-4403-94c7-b8ef15bafc87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare global vs brandâmodel-specific performance\n",
    "\n",
    "# Drop failed / NaN segments\n",
    "df_bm_global = df_bm_global.dropna(subset=[\"MAE_mean\", \"RMSE_mean\"])\n",
    "df_bm_spec   = df_bm_spec.dropna(subset=[\"MAE_mean\", \"RMSE_mean\"])\n",
    "\n",
    "df_bm_compare = df_bm_global.merge(\n",
    "    df_bm_spec,\n",
    "    on=[\"brand\", \"model\"],\n",
    "    suffixes=(\"_global\", \"_bm\"),\n",
    ")\n",
    "\n",
    "df_bm_compare[\"delta_MAE\"]  = df_bm_compare[\"MAE_mean_bm\"]  - df_bm_compare[\"MAE_mean_global\"]\n",
    "df_bm_compare[\"delta_RMSE\"] = df_bm_compare[\"RMSE_mean_bm\"] - df_bm_compare[\"RMSE_mean_global\"]\n",
    "\n",
    "df_bm_sorted = df_bm_compare.sort_values(\"delta_MAE\")\n",
    "\n",
    "print(\"Brandâmodel comparison (most improvement first):\")\n",
    "display(df_bm_sorted)\n",
    "\n",
    "# Optional readable table\n",
    "bm_display_cols = [\n",
    "    \"brand\", \"model\",\n",
    "    \"MAE_mean_global\", \"MAE_mean_bm\", \"delta_MAE\",\n",
    "    \"RMSE_mean_global\", \"RMSE_mean_bm\", \"delta_RMSE\",\n",
    "    \"n_global\",\n",
    "]\n",
    "df_bm_display = (\n",
    "    df_bm_sorted[bm_display_cols]\n",
    "    .copy()\n",
    "    .rename(columns={\"n_global\": \"n_samples\"})\n",
    ")\n",
    "\n",
    "for c in df_bm_display.columns:\n",
    "    if \"MAE\" in c or \"RMSE\" in c or \"delta\" in c:\n",
    "        df_bm_display[c] = df_bm_display[c].round(1)\n",
    "\n",
    "print(\"\\nReadable brandâmodel summary:\")\n",
    "display(df_bm_display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "24c38ee2-baf5-4ee9-a269-cfc88390fccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extra plots for brandâmodel specialization\n",
    "\n",
    "# Focus on segments with at least 100 samples for more stable numbers\n",
    "df_bm_plot = df_bm_display[df_bm_display[\"n_samples\"] >= 100]\n",
    "\n",
    "# Sort by delta_MAE (most improvement first)\n",
    "df_bm_plot = df_bm_plot.sort_values(\"delta_MAE\")\n",
    "\n",
    "# 1) Bar plot of ÎMAE for brandâmodel segments (filtered)\n",
    "plt.figure(figsize=(10, 4))\n",
    "x = np.arange(len(df_bm_plot))\n",
    "plt.bar(x, df_bm_plot[\"delta_MAE\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xticks(x, [f\"{b} {m}\" for b, m in zip(df_bm_plot[\"brand\"], df_bm_plot[\"model\"])],\n",
    "           rotation=90, ha=\"right\")\n",
    "plt.ylabel(\"Î MAE (brandâmodel - global)\")\n",
    "plt.title(\"Effect of brandâmodel specialization\\n(negative = lower MAE than global)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) Scatter plot: n_samples vs ÎMAE to visualise overfitting at low sample sizes\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(df_bm_display[\"n_samples\"], df_bm_display[\"delta_MAE\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xlabel(\"Number of samples per (brand, model)\")\n",
    "plt.ylabel(\"Î MAE (brandâmodel - global)\")\n",
    "plt.title(\"ÎMAE vs segment size\\n(negative = brandâmodel-specific is better)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "group05_main_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
