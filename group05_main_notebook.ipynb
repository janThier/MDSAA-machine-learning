{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4467ab3-7594-4e02-a19f-8c5e47fb14e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"\n",
    "    background: rgba(25, 25, 25, 0.55);\n",
    "    backdrop-filter: blur(16px) saturate(150%);\n",
    "    -webkit-backdrop-filter: blur(16px) saturate(150%);\n",
    "    border: 1px solid rgba(255, 255, 255, 0.12);\n",
    "    border-radius: 18px;\n",
    "    padding: 45px 30px;\n",
    "    text-align: center;\n",
    "    font-family: 'Inter', 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;\n",
    "    color: #e0e0e0;\n",
    "    box-shadow: 0 0 30px rgba(0, 0, 0, 0.35);\n",
    "    margin: 40px auto;\n",
    "    max-width: 800px;\n",
    "\">\n",
    "\n",
    "  <h1 style=\"\n",
    "      font-size: 2.8em;\n",
    "      font-weight: 700;\n",
    "      margin: 0 0 8px 0;\n",
    "      letter-spacing: -0.02em;\n",
    "      background: linear-gradient(90deg, #00e0ff, #9c7eff);\n",
    "      -webkit-background-clip: text;\n",
    "      -webkit-text-fill-color: transparent;\n",
    "  \">\n",
    "      Machine Learning Project\n",
    "  </h1>\n",
    "\n",
    "  <h2 style=\"\n",
    "      font-size: 1.6em;\n",
    "      font-weight: 500;\n",
    "      margin: 0 0 25px 0;\n",
    "      color: #b0b0b0;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Cars 4 You - Predicting Car Prices\n",
    "  </h2>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.25em;\n",
    "      font-weight: 500;\n",
    "      color: #c0c0c0;\n",
    "      margin-bottom: 6px;\n",
    "  \">\n",
    "      Group 5 - Lukas Belser, Samuel Braun, Elias Karle, Jan Thier\n",
    "  </p>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.05em;\n",
    "      font-weight: 400;\n",
    "      color: #8a8a8a;\n",
    "      font-style: italic;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Machine Learning End Results · 22.12.2025\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372c87aa-b7f3-4a34-a596-bb5e584db114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Table of Contents**\n",
    "\n",
    "- [1. Import Packages and Data](#1-import-packages-and-data)  \n",
    "\n",
    "  - [1.1 Import Required Packages](#11-import-required-packages)  \n",
    "\n",
    "  - [1.2 Load Datasets](#12-load-datasets)  \n",
    "\n",
    "  - [1.3 Kaggle Setup](#13-kaggle-setup)  \n",
    "\n",
    "- [2. Data Preparation](#2-data-preparation)  \n",
    "\n",
    "  - [2.1 Data Cleaning](#21-data-cleaning)  \n",
    "\n",
    "  - [2.2 Data Imputation](#22-data-imputation)  \n",
    "\n",
    "  - [2.3 Outlier Handling](#23-outlier-handling)  \n",
    "\n",
    "  - [2.4 Missing Values Handling](#24-missing-values-handling)  \n",
    "\n",
    "  - [2.5 Feature Engineering](#25-feature-engineering)  \n",
    "\n",
    "  - [2.6 Encoding, Transforming and Scaling](#26-encoding-transforming-and-scaling)  \n",
    "\n",
    "  - [2.7 Feature Selection](#27-feature-selection)  \n",
    "\n",
    "  - [2.8 Create Final Preprocessing Pipeline](#28-create-final-preprocessing-pipeline)  \n",
    "\n",
    "- [3. Model Assessment Strategy and Metrics](#3-model-assessment-strategy-and-metrics)  \n",
    "\n",
    "- [4. Baseline Model Comparison](#4-baseline-model-comparison)  \n",
    "\n",
    "  - [4.1 Setup Default Models (Original vs. Optimized Preprocessing)](#41-setup-default-models-original-vs-optimized-preprocessing)  \n",
    "\n",
    "  - [4.2 Run the Models](#42-run-the-models)  \n",
    "\n",
    "  - [4.3 Baseline Model Comparison Discussion of Results](#43-baseline-model-comparison-discussion-of-results)  \n",
    "\n",
    "- [5. Hyperparameter Tuning of Top Candidates](#5-hyperparameter-tuning-of-top-candidates)  \n",
    "\n",
    "  - [5.1 Tree-Based RandomForest](#51-tree-based-randomforest)  \n",
    "\n",
    "  - [5.2 Tree-Based Extra Trees](#52-tree-based-extra-trees)  \n",
    "\n",
    "- [6. Comparison of Tuned Models](#6-comparison-of-tuned-models)  \n",
    "\n",
    "- [7. Deployment and Prediction on Test Set](#7-deployment-and-prediction-on-test-set)  \n",
    "\n",
    "- [8. Visualizations and Insights of Best Model and the Data Preparation Pipeline](#8-visualizations-and-insights-of-best-model-and-the-data-preparation-pipeline)  \n",
    "\n",
    "  - [8.1 Data Preparation Pipeline](#81-data-preparation-pipeline)  \n",
    "\n",
    "    - [8.1.1 Start](#811-start)  \n",
    "\n",
    "    - [8.1.2 Cleaning](#812-cleaning)  \n",
    "\n",
    "    - [8.1.3 Unused Outlier Handling](#813-unused-outlier-handling)  \n",
    "\n",
    "    - [8.1.4 Missing Values Handling](#814-missing-values-handling)  \n",
    "\n",
    "    - [8.1.5 Feature Engineering](#815-feature-engineering)  \n",
    "\n",
    "    - [8.1.6 Transformation, Scaling, Encoding](#816-transformation-scaling-encoding)  \n",
    "\n",
    "    - [8.1.7 Feature Selection](#817-feature-selection)  \n",
    "\n",
    "    - [8.1.8 Entire Pipeline with all outputs](#818-entire-pipeline-with-all-outputs)  \n",
    "\n",
    "  - [8.2 Model](#82-model)  \n",
    "\n",
    "- [9. Discussion and Outlook](#9-discussion-and-outlook)  \n",
    "\n",
    "- [10. Open-Ended-Section](#10-open-ended-section)  \n",
    "\n",
    "  - [10.1 SHAP Interpretability for Our Final Tree Model (Informative Only)](#101-shap-interpretability-for-our-final-tree-model-informative-only)  \n",
    "\n",
    "  - [10.2 Global vs Brand- and Model-Specific Models](#102-global-vs-brand--and-model-specific-models)  \n",
    "\n",
    "  - [10.3 Price Predictor Web Application](#103-price-predictor-web-application)  \n",
    "\n",
    "  - [10.4 Deep Learning Experiment: Pre-Training vs Training from Scratch](#104-deep-learning-experiment-pre-training-vs-training-from-scratch)  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Group Member Contribution**    \n",
    " \n",
    "Jan (25%):\n",
    "- EDA\n",
    "- Pipeline structure (Data Split)\n",
    "- Missing Values Handling\n",
    "- Feature Engineering\n",
    "- Encoding, Transforming and Scaling\n",
    "- Feature Selection\n",
    "- Model Assessment Strategy\n",
    "- Baseline Model Comparison\n",
    "- Hyperparameter Tuning of Top Candidates\n",
    "- Comparison of Tuned Models\n",
    "- Deployment and Prediction on Test Set\n",
    "- Visualizations and Insights of the Data Preparation Pipeline\n",
    "- Discussion and Outlook\n",
    " \n",
    "Samu (25%):\n",
    "- Pipeline chart and Introduction Markdowns\n",
    "- Data Cleaning\n",
    "- Outlier Handling\n",
    "- Feature Engineering\n",
    "- Encoding, Transforming and Scaling\n",
    "- Model Assessment Strategy\n",
    "- Baseline Model Comparison\n",
    "- Deployment and Prediction on Test Set\n",
    "- Open End: SHAP Feature Importance\n",
    "- Open End: Brand-specific model comparison\n",
    " \n",
    "Lukas (25%):\n",
    "- EDA\n",
    "- Missing Values Handling\n",
    "- Feature Engineering\n",
    "- Encoding, Transforming and Scaling\n",
    "- Feature Selection\n",
    "- Model Assessment Strategy\n",
    "- Baseline Model Comparison\n",
    "- Hyperparameter Tuning of Top Candidates\n",
    "- Open End: Brand-specific model comparison\n",
    " \n",
    "Elias (25%):\n",
    "- Baseline Model Comparison\n",
    "- Hyperparameter Tuning of Top Candidates\n",
    "- Comparison of Tuned Models\n",
    "- Deployment and Prediction on Test Set\n",
    "- Visualizations and Insights of Best Model and the Data\n",
    "- Open End: SHAP Feature Importance\n",
    "- Open End: Analytical Interface\n",
    "- Open End: Deep-Learning\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract\n",
    "\n",
    "The Cars4You project aims to accelerate and standardize used-car price evaluations by replacing manual, subjective pricing with a production-ready machine learning pipeline. Our objective was to optimize predictive accuracy (MAE) on unseen cars while ensuring robustness to wrong inputs and a leakage-free evaluation.\n",
    "\n",
    "Our EDA containing univariate, bivariate and multivariate analysis showed three dominant challenges: (1) inconsistent raw entries (typos, invalid ranges, sparse categories), (2) strong segmentation effects by brand/model, and (3) heavy-tailed numeric distributions (notably mileage). \n",
    "\n",
    "We addressed these with a custom engineered and reproducible sklearn pipeline. It follows the state of the art pipeline architecture and consists the following transformers: \n",
    "\n",
    "deterministic cleaning and category canonicalization with `CarDataCleaner` and hierarchical imputation with `IndividualHierarchyImputer`. We then added domain-informed feature engineering with `CarFeatureEngineer` to encode depreciation, usage intensity, efficiency/performance ratios, interaction effects, and relative positioning within brand/model segments.\n",
    "\n",
    "Encoding and scaling were consolidated in a `ColumnTransformer` combining selective log transforms, `RobustScaler`, one-hot encoding, and median target encoding for high-signal categorical structure.\n",
    "\n",
    "To reduce noise and improve generalization, we implemented automated feature selection as a dedicated pipeline stage: VarianceThreshold followed by majority voting across complementary selectors (Spearman relevance+redundancy, mutual information, and tree-based importance). SHAP was used strictly for interpretability and diagnostics in the end.\n",
    "\n",
    "All model selection and tuning followed a consistent 5-fold cross-validation protocol. Primary evaluation metric MAE was set at the beginning of the project, we also evaluated RMSE and R2. After a first run of different models on original and engineered features, further hyperparameter tuning on the tree-based models HistGradientBoost and RandomForest was decided. The final tuned RF pipeline improved substantially over a naive median baseline (MAE ≈ 6.8k), achieving approximately **£1.2k MAE** in cross-validation.\n",
    "\n",
    "For detailed methodological reasoning, trade-offs and further findings please refer to the respective sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual representation of the approach we followed in this notebook:\n",
    "<img src=\"images/process_ML.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual representation of our technical pipeline:\n",
    "\n",
    "<img src=\"images/pipeline-chart.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!pip install shap\n",
    "!pip install -U scikit-learn\n",
    "!pip install category_encoders\n",
    "!pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    " \n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder, StandardScaler, FunctionTransformer, RobustScaler\n",
    "from category_encoders import QuantileEncoder\n",
    "from category_encoders.wrapper import NestedCVWrapper\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
    " \n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({\"figure.max_open_warning\": 0, \"figure.dpi\": 100})\n",
    "import shap\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "from pipeline_functions import CarDataCleaner, IndividualHierarchyImputer, CarFeatureEngineer, DebugTransformer, MajorityVoteSelectorTransformer, MutualInfoThresholdSelector, SpearmanRelevancyRedundancySelector, create_model_pipe, get_cv_results, model_hyperparameter_tuning, SetOutputCompatibleWrapper\n",
    "from visualization_functions import plot_selector_agreement, plot_train_val_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_train = pd.read_csv(\"train.csv\").rename(columns={\"Brand\": \"brand\",\n",
    "                                                        \"paintQuality%\": \"paintQuality\"})\n",
    "df_cars_test = pd.read_csv(\"test.csv\").rename(columns={\"Brand\": \"brand\",\n",
    "                                                       \"paintQuality%\": \"paintQuality\"})\n",
    "\n",
    "# Check for duplicates in carID column\n",
    "print(f\"Number of duplicate carIDs in training data: {df_cars_train['carID'].duplicated().sum()}\")\n",
    "print(f\"Total rows in training data: {len(df_cars_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Kaggle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing kaggle.json (add kaggle.json api token)\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/Workspace/Users/X@novaims.unl.pt\"\n",
    "\n",
    "# Test\n",
    "!echo $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Identifying Business Needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context**       \n",
    "Cars 4 You is an online car-resale company that buys cars from many brands. Sellers submit car details online, and the company traditionally relies on a mechanic inspection before making a purchase offer. Company growth has increased waiting lists for inspections, which risks losing potential customers to competitors. The business need is therefore to **expedite pricing** by generating a reliable **pre-inspection price estimate** directly from user-provided inputs.\n",
    "\n",
    "**Main goals**     \n",
    "**Business goal:** provide a fast, consistent price estimate at intake to reduce inspection bottlenecks and improve conversion.\n",
    "\n",
    "**ML goal:** train a supervised **regression** model to predict `price` (GBP) from features available at form submission time. Because `paintQuality%` is filled by a mechanic during evaluation, it is treated as **non-production** input and is excluded from the deployed prediction path.\n",
    "\n",
    "**Overall process (end-to-end)**      \n",
    "1. **Data intake:** use the 2020 training dataset (features + target `price`) to develop and validate models; use the provided test dataset (features only) for final predictions and Kaggle submission.\n",
    "2. **EDA → preprocessing decisions:** identify data inconsistencies, missingness patterns, segmentation by brand/model, and heavy-tailed variables; translate insights into pipeline steps.\n",
    "3. **Leakage-safe pipeline:** implement cleaning, outlier handling, hierarchical imputation, feature engineering, encoding/scaling, and feature selection as sklearn transformers inside a single `Pipeline` so that every step is fitted only on training folds.\n",
    "4. **Model benchmarking & optimization:** compare candidate regressors on a consistent protocol; tune the most promising models; select the most generalizable pipeline based on validation performance.\n",
    "5. **Deployment output:** fit the selected pipeline on the full training set and generate predictions for `test.csv` to produce the final submission `.csv`.\n",
    "\n",
    "**Model assessment strategy:**     \n",
    "We adopt **5-fold cross-validation (CV)** on the training set as the single assessment strategy used throughout benchmarking and tuning. The primary metric is **MAE** (business-interpretable error in GBP), with RMSE and R² reported as complementary diagnostics. The Kaggle test set functions as an **external holdout** for the final chosen pipeline (labels hidden; performance observed via leaderboard score).\n",
    "\n",
    "For more details on the respective steps in the pipeline, refer to the specific part in the notebook below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Exploration:** For the analysis of the original features including the consequences for preprocessing, refer to notebook `group05_exploratory_data_analysis.ipynb`. These insights are then used to clean and prepare the data. In addition we redid the EDA after each preprocessing step to visualize the impact and debug the pipeline. This process can be redone in Section 3.5.     \n",
    "\n",
    "**Top 3 EDA Key Insights:**\n",
    "- Multiple **logical inconsistencies** that will be handled in Data Cleaning (e.g. mileage < 0)\n",
    "- Some features, especially mileage and the target price, are **right-skewed** -> log-transform\n",
    "- The target has high **correlations** with year, mileage and engineSize while other features (previousOwners) have no correlation. There is also a high correlation between features (e.g. spearman of roughly -0.8 for mileage and year)\n",
    "\n",
    "**Preprocessing:** The steps taken to clean and prepare the data based on exploration are described and justified in the following respective Subsections on Data Cleaning. The inner-workings of these steps are later visualized in Section 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7890fb35-e1ca-463b-b83b-262df6947367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c7ca8f-1b74-4848-89b9-2bac509cb4db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Train and Val**: We use `Cross-Validation` in the `sklearn pipeline` on the available training data to make use of all data while validating different approaches.    \n",
    "-> We fix the random states everywhere to ensure that all models use the same split to ensure a fair model comparison\n",
    "\n",
    "**Test** Use external hold-out set from kaggle as final test set (remains completely unseen to avoid leakage)\n",
    "-> An additional val set is therefore not necessary and would waste training data\n",
    "\n",
    "Final setup:\n",
    "1. **Training Set (n-1 folds from CV)**: Used to fit models.\n",
    "2. **Validation Set (1 fold from CV)**: Used to evaluate performance of models and tune hyperparameters, detect overfitting. \n",
    "3. **Test Set (Kaggle)**: Used only once at the end of the entire process to evaluate final model performance. Not considered before to prevent leakage.\n",
    "\n",
    "\n",
    "\n",
    "<u>Place in the pipe:</u> The split is decided here because the data has to be split before the preprocessing steps to avoid data leakage. All of the following steps are part of the sklearn pipeline while the CV is not an explicit part of the pipeline but rather the technique that calls the pipeline with its separate folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c64124-1444-462c-a986-0d06cdb71d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create CV (shuffle to ensure randomness in splits, random_state to make it reproducible and comparable across models)\n",
    "rs = 5\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=rs)\n",
    "# => This cv will be passed for hyperparameter tuning later when training the models\n",
    "\n",
    "# Split features and target\n",
    "X_train = df_cars_train.drop(columns='price')\n",
    "y_train = df_cars_train['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8287784a-0d6e-4a6b-87d1-ee92ec55485f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**\n",
    "- CV achieves better results than using a hold-out set\n",
    "\n",
    "**Consequences/Interpretation:**\n",
    "- Usage of all available data is better for the model than 'wasting' training data for a hold-out set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53a2a97-4662-40a3-8b24-b238dad5138e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our approach:**\n",
    "- We `clean data inconsistencies` and data entry errors that we found in the EDA\n",
    "- These columns will be `set to NaN` for that specific entry to not lose rows in the data due to removing\n",
    "- Afterwards, these values will be imputed (see Section 2.3)\n",
    "\n",
    "____\n",
    "\n",
    "**Numerical Features**\n",
    "\n",
    "| **Feature** | **Allowed thresholds** | **Reasoning** | **# filtered below threshold** | **# filtered above threshold** |\n",
    "| :--- | :--- | :--- | :---: | :---: |\n",
    "| `year` | 1886 to 2020 | The first automobile is dated to 1886 (Benz Patent Motor Car), so earlier values are implausible; the dataset is from 2020, so newer model years are logically impossible. [1] | 0 | 358 |\n",
    "| `mileage` | ≥ 0 | Negative mileage is not possible. | 369 | - |\n",
    "| `tax` | ≥ 0 | Negative tax is not possible. | 378 | - |\n",
    "| `mpg` | 5 to 150 | Lower bound 5 mpg is a conservative “sanity floor” below the least-efficient passenger car on FuelEconomy.gov’s list (Bugatti Mistral at 9 mpg combined), so we avoid removing valid low-efficiency cars while filtering implausible entries. [2] Upper bound 150 is a pragmatic cap to reduce leverage from extreme values and potential metric-mixing (e.g., MPGe-style values are defined as an energy-equivalent MPG for plug-in vehicles). [3] Reference point for high-efficiency non-EVs: Prius variants are ~50–56 mpg combined on FuelEconomy.gov. [4] | 49 | 221 |\n",
    "| `engineSize` | 0.1 to 12.7 | Practical bounds: kei-class cars are capped at 660cc (0.66L), giving a grounded “small production car” reference point. [4] Very large historical production engines reach ~12.763L (Bugatti Type 41 / Royale). [5] Lower bound reduced to 0.1L as a conservative data-sanity floor (primarily to remove obvious errors) while avoiding unnecessary loss of potentially valid small-displacement entries. | 264 | 0 |\n",
    "| `paintQuality` | 0 to 100 | Percentage values must be between 0 and 100. | 0 | 367 |\n",
    "| `previousOwners` | ≥ 0 | Negative owner counts are not possible. | 371 | - |\n",
    "| `hasDamage` | - | Only 0 and NaN values in the data -> no thresholding | . | . |\n",
    "\n",
    "\n",
    "[[1]: https://group.mercedes-benz.com/company/tradition/company-history/1885-1886.html [2]: https://www.fueleconomy.gov/feg/best-worst.shtml https://www.fueleconomy.gov/feg/PowerSearch.do?action=noform&baseModel=Prius&make=Toyota&path=1&srchtyp=ymm&year1=2020&year2=2020 [3]: https://www.epa.gov/greenvehicles/fuel-economy-and-ev-range-testing [4]: https://www.fueleconomy.gov/feg/PowerSearch.do?action=noform&baseModel=Prius&make=Toyota&path=1&srchtyp=ymm&year1=2020&year2=2020 [5]: https://www.motortrend.com/features/what-is-a-kei-car [6]: https://www.bugatti-trust.co.uk/bugatti-type-41/]\n",
    "\n",
    "----\n",
    "\n",
    "**Categorial Features**\n",
    "\n",
    "We fix data entry noise (typos, truncations, inconsistent casing) in two stages:\n",
    "\n",
    "**1) Deterministic canonicalization:**\n",
    "\n",
    "We first apply **static mapping tables** (built from our EDA findings on the training data) that collapse variants into one canonical label\n",
    "\n",
    "- normalize formatting: lowercase + trim whitespace  \n",
    "- map known variants/typos → one canonical category  \n",
    "  - example: `\"AUDI\"`, `\"udi\"`, `\"Aud\"` → `Audi`  \n",
    "  - example: `\"semi-aut\"`, `\"emi-auto\"` → `Semi-Auto`\n",
    "\n",
    "This step is **fully deterministic**.\n",
    "\n",
    "**2) Strict validity check:**\n",
    "\n",
    "After canonicalization, we apply a **valid-category filter**:\n",
    "\n",
    "- for high-cardinality `model`, we accept only **known valid model names** (our static canonical model vocabulary)\n",
    "- any model token not in that vocabulary is considered unreliable and is set to **NaN**\n",
    "- this prevents “garbage categories” (rare or malformed strings) from becoming real categories and harming generalization\n",
    "\n",
    "Examples of what gets rejected (→ NaN):\n",
    "- truncations like `\"scirocc\"` (intended: `scirocco`)\n",
    "- incomplete single-letter tokens like `\"a\"`, `\"q\"`, `\"x\"` without enough context\n",
    "\n",
    "**3) Special-rule buckets for ambiguous 1-letter tokens:**\n",
    "\n",
    "Some one-letter model tokens are ambiguous and must not be guessed:\n",
    "\n",
    "- if `brand == Audi` and `model == \"a\"` → set to `a_unknown`\n",
    "- if `brand == Audi` and `model == \"q\"` → set to `q_unknown`\n",
    "- if `brand == BMW` and `model == \"x\"` → set to `x_unknown`\n",
    "\n",
    "If the brand is missing and the token is one of `{a, q, x}`, we do **not** guess and force it to **NaN**.\n",
    "\n",
    "**4) Conservative fuzzy rescue (only for values that are still missing):**\n",
    "\n",
    "Only after steps (1)–(3), we run a very strict “rescue step” to recover obvious typos:\n",
    "\n",
    "- applied **only where the value is still NaN**\n",
    "- candidate choices are restricted to **valid canonical categories** (never invent new labels)\n",
    "- acceptance requires very high similarity (strict cutoff) and/or a unique prefix match  \n",
    "  - example: `\"pum\"` → `puma` (unique prefix, safe)\n",
    "  - example: `\"sl clas\"` → `sl class` (high similarity, safe)\n",
    "- if no safe match exists, the value remains **NaN**\n",
    "\n",
    "**5) Final handling of remaining missing categories:**\n",
    "\n",
    "Any categorical value that is still missing after all steps remains **NaN** and is filled later by our **Imputer** (Section 2.3).\n",
    "\n",
    "_____\n",
    "\n",
    "**Leakage safety:**\n",
    "All “data-driven vocabularies” used in the cleaner are learned inside `fit()` on the training fold only, so the pipeline remains leakage-safe under CV:\n",
    "- the validation fold is never used to decide which categories are “valid”\n",
    "- the same cleaning logic is applied consistently in train/validation/test\n",
    "- the static mapping includes misspellings independent of the training fold and is included for every fold to prevent unnecessary pitfalls in model learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f5aae4cf-4df3-4f71-bff7-8a736d9a5b52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the data cleaning by running it on raw df and inspect uniques\n",
    "cleaner = CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)\n",
    "\n",
    "df_cars_train_clean = cleaner.fit_transform(df_cars_train)\n",
    "df_cars_test_clean  = cleaner.transform(df_cars_test)\n",
    "\n",
    "# Print unique values for visualization after cleaning (more detailed inspection before and after Cleaning can be found in the EDA)\n",
    "print(\"CLEANED TRAIN uniques\")\n",
    "for col in df_cars_train_clean.columns:\n",
    "    print(col, df_cars_train_clean[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393a3fff-9a33-43c1-979f-e31b6d1c5574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**     \n",
    "The findings are already included in the table above for easier overview and direct comparison\n",
    "    - 'Number of filtered values below threshold'\n",
    "    - 'Number of filtered values above threshold'\n",
    "\n",
    "==> In total, [TODO] values are identified as data errors in the available training data and are set to NaN\n",
    "\n",
    "**Consequences/Interpretation:**     \n",
    "Handling data erros is crucial for effective model training. To identify these data erros, an extensive EDA is inevitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e48e30f-ff20-46b7-94e1-d318088733da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.3 Outlier Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical considerations:**\n",
    "\n",
    "We tried different outlier handling techniques (see `unused_experiments.ipynb`) but decided to not use an explicit technique because they hurt our performance.\n",
    "Beside this explicit outlier handling between Cleaning and Imputation, we use the [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) for scaling the features which handles outliers by centering and scaling the data based on the median and IQR (refer to Section 2.6).\n",
    "\n",
    "<u>Place in the pipe</u>: \n",
    "- Before imputation to use original distribution for identifying the outliers (otherwise we would inflate the distributions with the imputed values)\n",
    "- Then in imputation, fill the original gaps based on a distribution that does not includes the massive outliers (skewing the mean/median)    \n",
    "  -> kill the outliers first (set to NaN) so the imputation for everyone becomes cleaner\n",
    "\n",
    "**Findings**:      \n",
    "\n",
    "As mentioned, using outlier handling significantly **hurt our best MAE**. The tried techniques are descripted in detail in the `unused_experiments.ipynb` file.\n",
    "\n",
    "**Consequences/Interpretation:**   \n",
    "\n",
    "Due to performance reasons we decided to use **no explicit outlier handling**.      \n",
    "The worse performance due to outlier handling can be explained by the **importance of extreme cars for the algorithm**. When winsorizing or imputing these extreme values, we remove valuable signals. The specific decision to not use outlier handling is based on our best performing tree-based models which are not as sensitive to outliers like other models. When using other approaches that are more sensitive to extreme values, outlier handling can indeed be a valuable technique to improve performance.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0bafff7-43a2-4c3c-81cf-1e54efd19ffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.4 Missing Values Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inidividual Hierarchy per Feature**\n",
    "\n",
    "To impute the NaNs in each cell, an **individual hierarchical approach** is applied for each feature. For this, we compute group statistics (median and mode) of features that are highly correlated with the feature in the missing cell. If one of the required features is not available, we fallback to the next level until finally the global dataset statistic (median or mode) is used as a last fallback.    \n",
    "\n",
    "Examples (for an extensive explanation refer to the IndividualHierarchyImputer class in pipeline_functions.py):\n",
    "- `brand`: impute the brand using the model and if the model is NaN too, we impute using the grouped mode of (fuelType, transmission) because we identfied interaction between these two features and brand in the EDA.   \n",
    "- `mileage`: we impute mileage with the median of the year because of high spearman corr identified in the EDA (~0.8).\n",
    "\n",
    "The group statistics are only computed on the respective train folds and transformed on the val set to prevent leakage.   \n",
    "-> When refitting the entire model, the entire train set is used to fit and the kaggle test set is transformed using the fitted values\n",
    "\n",
    "<u>Place in the pipe:</u> The Imputation is decided here because the data has to be imputed on original values before engineering new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c48d8a2-2615-4533-9a66-0375bc994fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**       \n",
    "The IndividualHierarchyImputer improves performance over the best of our other tried imputers SimpleImputer (mode and median), KNN and the custom GroupImputer by ~30 MAE on the best baseline models.\n",
    "\n",
    "**Consequences/Interpretation:**       \n",
    "The improved performance over SimpleImputer and the custom GroupImputer show that each feature is **best approximated by a different feature (combination)**. There is **no golden rule** of imputing everything by the median or mode of the model. Instead, the **interaction** (e.g. spearman corr) of the missing feature and the other features should be regarded (e.g. impute mileage with median of year because of high spearman corr)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b198720e-b5a0-46ef-8055-4ac33b852410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.5 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d75d2e08-9cae-4186-b329-d389b4ca8a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- We implement feature engineering as an sklearn transformer (`CarFeatureEngineer`) **inside the pipeline**.\n",
    "  - This makes the process **CV-safe / leakage-free**: all fold-specific statistics (e.g., model frequency, mean ages) are learned only on the training fold in `fit()` and applied to the validation fold in `transform()`.\n",
    "- Important design notes: Interaction features use `(age + 1)` to avoid division by zero for cars in the reference year.\n",
    "- We engineer features with two goals:\n",
    "  1. **Inject domain structure** (age, usage intensity, efficiency, “big engine + old car” effects).\n",
    "  2. **Create stronger signals for models** by expressing ratios and interactions that are difficult to learn reliably from raw variables.\n",
    "\n",
    "**Input columns** (after cleaning + imputation):\n",
    "- Numeric: `year`, `mileage`, `tax`, `mpg`, `engineSize`, `previousOwners`\n",
    "- Categorical: `brand`, `model`, `transmission`, `fuelType`\n",
    "- Boolean: `hasDamage`\n",
    "\n",
    "---\n",
    "\n",
    "| **New Feature** | **Calculation** | **Nature** | **Reasoning** |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| `age` | `ref_year - year` | Base | Captures depreciation; turns a calendar value into a meaningful pricing variable. |\n",
    "| `mpg_x_engine` | `mpg * engineSize` | Interaction (product) | Joint signal for “performance vs efficiency” patterns (high engine + low mpg vs small engine + high mpg). |\n",
    "| `engine_x_age` | `engineSize * (age + 1)` | Interaction (product) | Differentiates large engines in older cars vs newer cars; helps model capture age-dependent valuation of engine size. |\n",
    "| `mpg_x_age` | `mpg * (age + 1)` | Interaction (product) | Captures age-dependent fuel-efficiency patterns (e.g., older fleets / technology differences) that can correlate with price. |\n",
    "| `tax_x_age` | `tax * (age + 1)` | Interaction (product) | Models that tax effects can differ by car age (policy/regime + car segment composition). |\n",
    "| `miles_per_year` | `mileage / (age + 1)` | Interaction (ratio) | Normalizes mileage by lifetime: 60k miles on a 3-year car is very different from 60k on a 10-year car; reduces collinearity between `mileage` and `age`. |\n",
    "| `tax_per_mpg` | `tax / mpg` | Interaction (ratio) | “Cost pressure” proxy: high tax relative to efficiency can reflect segment / running cost patterns. |\n",
    "| `engine_per_mpg` | `engineSize / mpg` | Interaction (ratio) | Performance-style signal: high engine with low mpg tends to indicate sporty/luxury configurations. |\n",
    "| `brand_fuel` | `brand + \"_\" + fuelType` | Interaction (categorical) | Creates configuration groups for target encoding (e.g., Diesel BMW differs from Petrol BMW). |\n",
    "| `brand_trans` | `brand + \"_\" + transmission` | Interaction (categorical) | Creates configuration groups for target encoding (e.g., Automatic Mercedes vs Manual Mercedes). |\n",
    "| `model_freq` | `P(model)` from training fold | Popularity | Approximates market supply/demand stability: common models have more stable pricing; learned CV-safe in `fit()`. |\n",
    "| `age_rel_brand` | `age - mean_age(brand)` | Relative / group-stat | Measures whether a car is newer/older than typical within its brand (brand-relative positioning). |\n",
    "| `age_rel_model` | `age - mean_age(model)` | Relative / group-stat | Measures whether a car is newer/older than typical within its model (model-relative positioning). |\n",
    "| `engine_rel_model` | `engineSize / mean_engineSize(model)` | Relative / group-stat | Captures whether a car is under-/over-engined relative to its model’s typical configuration. |\n",
    "\n",
    "---\n",
    "\n",
    "Legend (feature “Nature”)\n",
    "\n",
    "- **Base Features**: derived from a single original variable (e.g. `age` from `year`)\n",
    "- **Interaction Features**: combine multiple variables to capture non-additive effects\n",
    "  - products (“amplifiers”) and ratios (“normalizers”)\n",
    "- **Popularity Features**: learned from the training fold distribution (e.g. model frequency)\n",
    "- **Relative / Group-stat Features**: compare a car to typical peers within `brand` or `model`\n",
    "  - learned in `fit()` and applied in `transform()` to avoid leakage\n",
    "\n",
    "---\n",
    "\n",
    "Relation to encoding (Target Encoded Features)\n",
    "\n",
    "We also create categorical “group keys” (`brand_fuel`, `brand_trans`) specifically so that our later target-encoding step inside the preprocessing pipeline can learn stable, configuration-specific signals.  \n",
    "This encoding is handled **after** feature engineering and is **CV-safe** because it is part of the pipeline.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a40f2b62-88b2-4530-92e3-86423cb38ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our findings:**\n",
    "The engineered features are main drivers for performance improvement. In addition, to the improved MAE, the following findings support this insight:\n",
    "- The correlation of the engineered features with the target show that we engineered meaningful features (e.g. miles_per_year: spearman corr of TODO, while miles and age only have Y and Z). This is visible through using y-data-profiling in the debugging pipeline.\n",
    "- The feature importance of the engineered features in the final model (e.g. mpg_x_age: TODO)\n",
    "- All Feature Importances and therefore also the impact of the engineered features in analyzed in detail in the Open End Section\n",
    "\n",
    "| Feature | Impact |\n",
    "| :--- | :--- |\n",
    "| age | ... |\n",
    "\n",
    "\n",
    "**Consequences/Interpretation:**\n",
    "\n",
    "Some relationships are better captured through engineered features than raw original features. For example miles_per_year captures the actual usage of the car.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9425a7c0-ec96-4458-a0da-8ef65acb3f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.6 Encoding, Transforming and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d59f995-10c7-4391-a5ce-759858b69889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- We combine the different treatments for different natures of features in the ColumnTransformer\n",
    "    - Numerics vs. Booleans vs. Categoricals\n",
    "- We have one `baseline pipe` and one `optimized pipe` to compare basic preprocessing to optimized preprocessing\n",
    "    - The baseline pipe does the bare minimum for the algorithms to work cleanly\n",
    "    - The optimized pipe was adjusted iteratively through multiple experiments and trials during the process to optimize model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4385f8f4-0a8b-4b53-8350-52ee453af34a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Optimized Pipe:**\n",
    "- `Log-Transforming` using [FunctionTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) for skewed Numerics identified in the EDA\n",
    "- `RobustScaler` ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)) for Numerics because it performed better than [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)     \n",
    "    -> Scaling only on training data to avoid data leakage and then scale val and later test set with the fitted scaler of the training set     \n",
    "- `Encoding` for Categoricals:\n",
    "    - Low cardinality: [OHE](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) because of optimized performance on best models. We don't use drop='first' let the FS handle it (better performance for our best tree-based models).\n",
    "    - High Cardinality: [Median TE](https://contrib.scikit-learn.org/category_encoders/quantile.html) and [Mean TE](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html) for optimized performance on best models\n",
    " \n",
    "| **Feature** | Nature | Transformation | Encoding | Scaling |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| age | Numerical | - | - | Robust |\n",
    "| tax | Numerical | - | - | Robust |\n",
    "| mpg | Numerical | - | - | Robust |\n",
    "| engineSize | Numerical | - | - | Robust |\n",
    "| previousOwners | Numerical | - | - | Robust |\n",
    "| mpg_x_engine | Numerical | - | - | Robust |\n",
    "| engine_x_age | Numerical | - | - | Robust |\n",
    "| mpg_x_age | Numerical | - | - | Robust |\n",
    "| tax_x_age | Numerical | - | - | Robust |\n",
    "| engine_per_mpg | Numerical | - | - | Robust |\n",
    "| tax_per_mpg | Numerical | - | - | Robust |\n",
    "| model_freq | Numerical | - | - | Robust |\n",
    "| age_rel_brand | Numerical | - | - | Robust |\n",
    "| age_rel_model | Numerical | - | - | Robust |\n",
    "| engine_rel_model | Numerical | - | - | Robust |\n",
    "| mileage | Numerical | log1p | - | Robust |\n",
    "| miles_per_year | Numerical | log1p | - | Robust |\n",
    "| hasDamage | Boolean | - | - | - |\n",
    "| transmission | Categorical | - | OHE | - |\n",
    "| fuelType | Categorical | - | OHE | - |\n",
    "| brand | Categorical | - | Mean TE **and** Median TE | Robust |\n",
    "| model | Categorical | - | Mean TE **and** Median TE | Robust |\n",
    "| brand_fuel | Categorical | - | Mean TE **and** Median TE | Robust |\n",
    "| brand_trans | Categorical | - | Mean TE **and** Median TE | Robust |\n",
    " \n",
    " \n",
    " \n",
    "All operations are combined in a [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) which applies the different steps to the columns of the data in one unified pipeline (reproducible and prevents leakage)    \n",
    "  -> outputs a combined feature matrix\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original features\n",
    "orig_numeric_features = [\"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\", \"previousOwners\"]\n",
    "orig_boolean = [\"hasDamage\"]\n",
    "orig_categorical_features = [\"brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "\n",
    "# Original and Engineered Features\n",
    "numeric_features = [\n",
    "    \"age\", \"tax\", \"mpg\", \"engineSize\", \"previousOwners\",        \n",
    "    \"mpg_x_engine\", \"engine_x_age\", \"mpg_x_age\", \"tax_x_age\",   \n",
    "    \"engine_per_mpg\", \"tax_per_mpg\",                            \n",
    "    \"model_freq\",\n",
    "    \"age_rel_brand\", \"age_rel_model\", \"engine_rel_model\"\n",
    "]\n",
    "numeric_features_for_log = [\"mileage\", \"miles_per_year\"]\n",
    "boolean_features = [\"hasDamage\"]\n",
    "categorical_features_ohe = [\"transmission\", \"fuelType\"]\n",
    "categorical_features_te_mean = [\"brand\", \"model\", \"brand_fuel\", \"brand_trans\"]\n",
    "categorical_features_te_median = [\"brand\", \"model\", \"brand_fuel\", \"brand_trans\"]\n",
    "unused_columns = [\"year\"] # replaced by age (see Section on Feature Selection for details)\n",
    "\n",
    "all_feature_names_before_encoding = set(numeric_features + numeric_features_for_log + boolean_features + categorical_features_ohe + categorical_features_te_median)\n",
    "print(len(all_feature_names_before_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fc19882e-d4db-4c22-a2d6-7bb290b42eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enc_transf_scale = ColumnTransformer([\n",
    "    (\"log\", Pipeline([\n",
    "        (\"log\", FunctionTransformer(np.log1p, validate=False, feature_names_out=\"one-to-one\")),  # log1p handles zeros safely\n",
    "        (\"scaler\", RobustScaler()),\n",
    "    ]), numeric_features_for_log),\n",
    "\n",
    "    (\"num\", RobustScaler(), numeric_features),\n",
    "\n",
    "    (\"boolean\", \"passthrough\", boolean_features),\n",
    "\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features_ohe), # Use sparse_output=False to get dense array back (e.g. necessary for hgb)\n",
    "\n",
    "    (\"mean_te\", Pipeline([ \n",
    "        (\"encoder\", TargetEncoder(target_type='continuous', cv=5, smooth='auto', random_state=rs)),\n",
    "        (\"scaler\", RobustScaler()),\n",
    "    ]), categorical_features_te_mean),\n",
    "\n",
    "    # Smoothing (m) mitigates but doesnt eliminate leakage, so we use nested cv to work similar to the sklearn TE\n",
    "    (\"median_te\", Pipeline(steps=[\n",
    "        ('median_encoder', SetOutputCompatibleWrapper(NestedCVWrapper(QuantileEncoder(quantile=0.5, m=10.0), cv=cv, random_state=rs))), # not specifying the cols means it encodes all columns\n",
    "        ('scaler', RobustScaler()),\n",
    "    ]), categorical_features_te_median)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0f4406-27be-4204-bd85-653c8d5ac173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.7 Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a716cfa0-d1e3-4978-83f2-183c1a346b94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Our approach:**\n",
    "- The [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) combines all steps into the preprocessing pipe (see table below)\n",
    "- Through calling the pipeline for data preparation, we ensure that the data is preprocessed independently for each training fold    \n",
    "-> prevent leakage while filling missing values, scaling, encoding, etc.\n",
    "\n",
    "##### Summary of the pipelines before modeling: Baseline vs Optimized (including outliers)\n",
    "\n",
    "| | **Baseline** | **Optimized** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Data cleaning** | - | `CarDataCleaner` (Section 2.2) |\n",
    "| **Outlier handling** | - | - |\n",
    "| **Imputation** | SimpleImputer median/mode <br>(simplicity; median more robust than mean)</br> | `IndividualHierarchyImputer` (Section 2.4) |\n",
    "| **Feature engineering** | - | `CarFeatureEngineer` (Section 2.5) |\n",
    "| **Transformation** | - | Log-Transform selected skewed numerics (Section 2.6) |\n",
    "| **Scaling** | StandardScaler <br>(popularity)</br>| RobustScaler (Section 2.6) |\n",
    "| **Encoding** | OneHotEncoder <br>(simplicity, most straight-forward)</br>| OneHotEncoder + Target Encoding (Section 2.6) |\n",
    "| **Feature selection** | - | VT + `Majority voting` (added in Section 3.2) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_orig = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), orig_numeric_features),\n",
    "    (\"bool\", SimpleImputer(strategy=\"most_frequent\"), orig_boolean),\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)) # dont use drop='first' for tree models, use sparse_output=False to get dense array back (e.g. necessary for hgb)\n",
    "    ]), orig_categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6949511c-a9e8-44f6-b9d4-a4d2950fff69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### The entire data preparation pipeline is build incorporating the enc_transf_scale defined in Section 2.6\n",
    "preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", enc_transf_scale),\n",
    "    # [Feature Selection: see Section 3.2 for details]\n",
    "    # [Modeling: see Section 3.3 for details]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "225688cc-6157-4613-bf28-9e430473a3a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### III. Regression Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) Model assessment strategy and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6297f034-638d-4db6-a681-597f20036432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definition of Function for Model Assessment"
    }
   },
   "source": [
    "**Performance Metrics**    \n",
    "Comparison on train and val set to identify potential overfitting:\n",
    "- **MAE** (Mean Absolute Error):     \n",
    "    Used as primary metric because it is the metric used for evaluating in the Kaggle competition. It is easy to interpret in pounds.\n",
    "\n",
    "- **MAE Std** (Standard Deviation):    \n",
    "    Used to asses the variance of MAE across folds. For the secondary metrics the std will not be regarded as one std is enough to get a first understanding.\n",
    "\n",
    "- **RMSE** (Root Mean Squared Error):    \n",
    "    Used to identify large prediction errors which helps us understand model weaknesses (sensitive to outliers)\n",
    "\n",
    "- **R-squared**:     \n",
    "    Used to compare how well the models explain the underlying data compared to the mean (R-squared=0). It serves as an additional information compared to the two \"error-metrics\" MAE and RMSE.\n",
    "\n",
    "=> All comparisons are based on **identical cross-validation splits** (same seed/folds). Reported values are fold-averaged metrics on train and validation.\n",
    "\n",
    "**Structured approach to identify production model**\n",
    "1. Run Baseline Models with default parameters to get a first glance of their performance\n",
    "2. Compare and discuss model performance\n",
    "3. Select top candidates for further optimization\n",
    "4. Run Hyperparameter tuning on top candidates (Section 5)\n",
    "5. Select best model and train it with best hyperparameters on minimizing the absolut_error (Section 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2) Feature Selection Strategy and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategy:**     \n",
    "\n",
    "We apply an automatic feature selection approach in addition to the previously removed features (data cleaning, feature engineering)\n",
    "- year: dropped because replaced by derived feature 'age'\n",
    "- paintQuality: dropped because filled by mechanic so not available for our predictions in production  as the car prediction skips the mechanic\n",
    "\n",
    "The goal is to create a very robust feature selection approach that finds features that are most likely actually irrelevant/redundant and therefore generate noise in the model that might lead to overfitting.     \n",
    "To achieve that goal, we apply **two steps** inside the feature selection:\n",
    "1) `Variance Threshold` (Filter) to filter constant variables ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html))    \n",
    "(If using a different value than threshold 0, VarianceThreshold has to be applied before scaling, because e.g. standard scaling leads to std=1)\n",
    "2) Majority voter:\n",
    "    - `Spearman` handles the clean, obvious trends and cleans up redundancy ([docu](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html))\n",
    "    - `MI` catches more complex relations that Spearman misses ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html))\n",
    "    - `RF feature importance` to account for importance of the features ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#selectfrommodel))\n",
    "\n",
    "\n",
    "The different voters capture different aspects:\n",
    "| **Voter** | **Nature** | **Role & Responsibility** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Spearman Voter** <br> *(SpearmanRedundancySelector)* | Filter | **Linear/Monotonic**<br>Captures obvious, strong relationships (e.g., \"Newer cars are expensive\"). Also handles Redundancy by filtering out features that are exact duplicates of better ones (Maximum Relevance, Minimum Redundancy (mRMR)-style pruning).|\n",
    "| **MI Voter** <br> *(MutualInfoThresholdSelector)* | Filter | **Non-Linear**<br>Captures complex \"physics\" and non-monotonic patterns that correlation misses. |\n",
    "| **RF Voter** <br> *(SelectFromModel)* | Embedded | **Interactions**<br>Captures features that are only important in combination with others. |\n",
    "\n",
    "==> The feature selection is performed inside the pipelines cross-validation and consistent across all models, ensuring no data leakage and consistent feature selection logic.\n",
    "\n",
    "<u>Place in the pipe:</u> The Feature Selection is placed after the scaling to have the features on one scale (just like in the lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Pipeline including the fs_pipe\n",
    "fs_pipe = Pipeline([\n",
    "        (\"vt\", VarianceThreshold(threshold=0.0)),\n",
    "        ('selector', MajorityVoteSelectorTransformer(\n",
    "            selectors=[\n",
    "                SpearmanRelevancyRedundancySelector(relevance_threshold=0.05, redundancy_threshold=0.95), # If we set redundancy threshold to 1.01, this becomes similar to just relevance filtering,\n",
    "                MutualInfoThresholdSelector(threshold=0.01, n_neighbors=10), # Increasing n_neighbors makes the estimation more stable but computationally slower\n",
    "                SelectFromModel(RandomForestRegressor(n_estimators=100, max_depth=8, n_jobs=-1, random_state=rs), # max_depths not too low (miss interactions) and not too high (selecting noise -> overfitting)\n",
    "                                threshold='0.001*mean'), # threshold relative because it sums to 1 and if we have many features, many features will have a low importance but are still important],\n",
    "                # [Unused] RFE unused because of high computational cost\n",
    "                # rfecv_rf = RFECV(estimator = RandomForestRegressor(n_jobs=-1, max_depth=50), step=1, random_state=rs, cv=cv, scoring='neg_mean_absolute_error', min_features_to_select=5)\n",
    "            ],\n",
    "            min_votes=2))\n",
    "        ])\n",
    "\n",
    "\n",
    "# The entire data preparation pipeline is build incorporating the enc_transf_scale and fs-pipe defined in Section 2.6 and 3.2 respectively\n",
    "preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", enc_transf_scale),\n",
    "    (\"fs\", fs_pipe),\n",
    "])\n",
    "\n",
    "# Save preprocessor for reuse in DL experiments\n",
    "with open('preprocessor_pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor_pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the votes of each contributor, resulting in the final decision whether to keep the feature or not. Be aware that the constant feature `boolean__hasDamage` has already bin pre-filtered by the VT before the majority voter.\n",
    "\n",
    "**Argumentation** why we keep the certain features:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Debug pipeline to visualize feature selection agreement\n",
    "enc_transf_scale.set_output(transform=\"pandas\")\n",
    "fs_pipe.set_output(transform=\"pandas\")\n",
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020, verbose=False)),\n",
    "    (\"ct\", (enc_transf_scale)),\n",
    "    (\"fs\", (fs_pipe)),\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data of the debug preprocesor to just visualize the result of a final model using this FS\n",
    "# -> The insights from here are not used for model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)\n",
    "\n",
    "# Feed the feed names after VT because VT is applied before the majority voting to remove constant features\n",
    "feature_names_after_vt = debug_preprocessor_pipe.named_steps['fs'].named_steps['vt'].get_feature_names_out()\n",
    "plot_selector_agreement(\n",
    "    majority_selector = debug_preprocessor_pipe.named_steps['fs'].named_steps['selector'], \n",
    "    feature_names = feature_names_after_vt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our findings:**\n",
    "- While trees are comparatively robust to unnecessary features, applying the feature selection pipeline improves the performance\n",
    "- Using feature selection compared to not using feature selection on the exact same pipeline improved the performance by roughly 2 MAE on the best baseline models (ET and RF) while the score of linear regression stays the same\n",
    "\n",
    "**Consequences/Interpretation:**\n",
    "- The similar performance with or without feature selection (small Ablation Study) is due to the few selected features to remove (see Section 8.1.7)\n",
    "- It shows that most features are important for the models while the feature selector detects useless and noisy features reliably (e.g. hasDamage and the OHE features that were created without dropping the first dummy: cat__transmission_Other and cat__fuelType_Other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0dc6124-f736-46fb-9f73-895b930faae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.3) Baseline Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Log-transforming the target (price)` using [TransformedTargetRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html) because EDA showed that it is heavily right-skewed. The model predicts the log-price and automatically convert it back to pounds at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc50f05c-4c02-4ff6-9a48-319af5baecfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3.3.1) Setup Default Models (Original vs. Optimized Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fair comparison:**    \n",
    "- We use **default parameters** to get a first result of models potential to decide on which ones to use for further optimizing (hyperparameter tuning). - Only use the same random_state for reproducibility and n_jobs to speed up computations.    \n",
    "- Use the rule of thumb provided in the lab for the MLP ()\n",
    "- Baseline: DummyRegressor using the median price as prediction\n",
    "\n",
    "The **log-transform** of the target is performed here because it is the most straightforwared implementation using the TransformedTargetRegressor ([docu](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html)). It handles transformation and afterwards uses the inverse automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Median ###\n",
    "baseline_median_pipe_orig = create_model_pipe(preprocessor_orig, DummyRegressor(strategy=\"median\")) # Preprocessing does not matter for median\n",
    "\n",
    "### Linear Models ###\n",
    "linear_reg_default = LinearRegression()\n",
    "linear_reg_default = TransformedTargetRegressor(\n",
    "    regressor=linear_reg_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "linear_reg_pipe_orig = create_model_pipe(preprocessor_orig, linear_reg_default)\n",
    "linear_reg_pipe_adjusted = create_model_pipe(preprocessor_pipe, linear_reg_default)\n",
    "\n",
    "\n",
    "### Instance-Based ###\n",
    "knn_default = KNeighborsRegressor(n_jobs=-3)\n",
    "knn_default = TransformedTargetRegressor(\n",
    "    regressor=knn_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "knn_pipe_orig = create_model_pipe(preprocessor_orig, knn_default)\n",
    "knn_pipe_adjusted = create_model_pipe(preprocessor_pipe, knn_default)\n",
    "# Long Duration (~4min)\n",
    "# => Better performance than linear models but still worse than tree-based models -> not further optimized\n",
    "\n",
    "\n",
    "### Neural Networks ###\n",
    "# The number of hidden neurons should be between the size of the input layer (~10 for orig, ~25 for optimized) and the size of the output layer (1)\n",
    "mlp_default = MLPRegressor(hidden_layer_sizes=(9,) , random_state=rs)\n",
    "mlp_default = TransformedTargetRegressor(\n",
    "    regressor=mlp_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "mlp_pipe_orig = create_model_pipe(preprocessor_orig, mlp_default)\n",
    "mlp_pipe_adjusted = create_model_pipe(preprocessor_pipe, mlp_default)\n",
    "# Long Duration (~4min)\n",
    "# => Worse performance than KNN and tree-based models (notably, orig better than preprocessed)\n",
    "\n",
    "\n",
    "### Tree-Based Models ###\n",
    "hgb_default = HistGradientBoostingRegressor(random_state=rs, loss='squared_error')\n",
    "hgb_default = TransformedTargetRegressor(\n",
    "    regressor=hgb_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "hgb_pipe_orig = create_model_pipe(preprocessor_orig, hgb_default)\n",
    "hgb_pipe_adjusted = create_model_pipe(preprocessor_pipe, hgb_default)\n",
    "# Long Duration (~1mins)\n",
    "\n",
    "\n",
    "rf_default = RandomForestRegressor(random_state=rs, n_jobs=-1, criterion='squared_error')\n",
    "rf_default = TransformedTargetRegressor(\n",
    "    regressor=rf_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "rf_pipe_orig = create_model_pipe(preprocessor_orig, rf_default)\n",
    "rf_pipe_adjusted = create_model_pipe(preprocessor_pipe, rf_default)\n",
    "# Long Duration (~5mins)\n",
    "# Good performance -> further hyperparameter tuning\n",
    "\n",
    "et_default = ExtraTreesRegressor(random_state=rs, n_jobs=-1, criterion='squared_error')\n",
    "et_default = TransformedTargetRegressor(\n",
    "    regressor=et_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "et_pipe_orig = create_model_pipe(preprocessor_orig, et_default)\n",
    "et_pipe_adjusted = create_model_pipe(preprocessor_pipe, et_default)\n",
    "# Long Duration (~7mins)\n",
    "# Good performance -> further hyperparameter tuning\n",
    "\n",
    "### Kernel-Based Models ###\n",
    "# SVR was tried but performed bad and then removed due to long running time\n",
    "svr_default = SVR()\n",
    "svr_default = TransformedTargetRegressor(\n",
    "    regressor=svr_default,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "svr_pipe_orig = create_model_pipe(preprocessor_orig, svr_default)\n",
    "svr_pipe_adjusted = create_model_pipe(preprocessor_pipe, svr_default)\n",
    "# Long Duration (~12mins)\n",
    "# => Much worse performance than other models -> not further optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a21d8b-3aeb-4155-b6db-17475e30c3b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3.3.2) Run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "80a5dc82-4f35-4d66-9e97-e249166949e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "default_models_orig = {\n",
    "    \"Baseline_Median_orig\": baseline_median_pipe_orig,\n",
    "    # \"LinearReg_orig\": linear_reg_pipe_orig,\n",
    "    # \"KNN_orig\": knn_pipe_orig,\n",
    "    # \"MLP_orig\": mlp_pipe_orig,\n",
    "    # \"HGB_orig\": hgb_pipe_orig,\n",
    "    # \"RF_orig\": rf_pipe_orig,\n",
    "    # \"ET_orig\": et_pipe_orig,\n",
    "    # \"SVR_orig\": svr_pipe_orig, # SVR was tried but performed bad and then removed here due to long running time\n",
    "}\n",
    "\n",
    "default_orig_models_results_df = get_cv_results(default_models_orig, X_train, y_train, cv=cv, rs=rs)\n",
    "display(default_orig_models_results_df)\n",
    "\n",
    "# Long Duration (~10mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_models = {\n",
    "    # \"LinearReg\": linear_reg_pipe_adjusted,\n",
    "    # \"KNN\": knn_pipe_adjusted,\n",
    "    # \"MLP\": mlp_pipe_adjusted,\n",
    "    # \"HGB\": hgb_pipe_adjusted,\n",
    "    \"RF\": rf_pipe_adjusted,\n",
    "    \"ET\": et_pipe_adjusted,\n",
    "    # \"SVR\": svr_pipe_adjusted, # SVR was tried but performed bad and then removed here due to long running time\n",
    "}\n",
    "\n",
    "default_models_results_df = get_cv_results(default_models, X_train, y_train, cv=cv, rs=rs)\n",
    "display(default_models_results_df)\n",
    "\n",
    "# Long Duration (~5mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.3) Baseline Model Comparison Discussion of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_results_df = pd.concat([default_orig_models_results_df, default_models_results_df])\n",
    "merged_model_results_df = merged_model_results_df.sort_values([\"preprocessing\", \"val_MAE\"])\n",
    "display(merged_model_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Comparative performance under optimized preprocessing**\n",
    "\n",
    "Under the optimized preprocessing pipeline, the ranking is dominated by **tree-based ensembles**:\n",
    " \n",
    "- **ExtraTrees (ET, optimized)** is best overall with **val MAE = 1234.50 ± 10.37**, **val RMSE = 2129.62**, **val R² = 0.9522**.  \n",
    "\n",
    "- **RandomForest (RF, optimized)** is a close second (**val MAE = 1236.17 ± 12.19**, **val RMSE = 2170.35**, **val R² = 0.9504**).\n",
    " \n",
    "Both outperform the next tier (HGB and KNN), and they do so with very small fold-to-fold variability, indicating that the gain is not driven by a subset of “lucky” splits.\n",
    " \n",
    "The remaining models behave as expected for this task:\n",
    "\n",
    "- **HGB/KNN** improve with preprocessing but remain worse than ET/RF, consistent with residual bias (HGB) or limited representational flexibility (KNN).\n",
    "\n",
    "- **Linear Regression** underperforms substantially, consistent with a pricing function that is not well-approximated by a global linear mapping in the engineered feature space.\n",
    "\n",
    "- **MLP (optimized)** is unstable: despite a moderate MAE, the **RMSE explodes** and **R² becomes strongly negative**, which is characteristic of rare but extremely large errors (tail instability), making it unsuitable in this configuration.\n",
    " \n",
    "---\n",
    " \n",
    "**2) Effect size of preprocessing (optimized vs original)**\n",
    "\n",
    "The optimized preprocessing delivers a large, consistent improvement for the models that can exploit nonlinear interactions:\n",
    " \n",
    "- **ET:** val MAE improves from **1442.82 → 1234.50** (Δ ≈ -208; ≈ **-14.4%**) and val R² increases from **0.9305 → 0.9522**.  \n",
    "\n",
    "- **RF:** val MAE improves from **1464.05 → 1236.17** (Δ ≈ -228; ≈ **-15.6%**) and val R² increases from **0.9276 → 0.9504**.  \n",
    "\n",
    "- **HGB/KNN** show similarly strong absolute MAE reductions, but remain below ET/RF in final validation performance.\n",
    " \n",
    "The baseline median predictor (MAE ≈ 6801, negative R²) provides a clear reference: the optimized ET/RF pipelines are not incremental improvements but a substantial step-change in predictive quality.\n",
    " \n",
    "An important nuance is that **Linear Regression becomes worse** under optimized preprocessing (MAE ≈ 2242 vs ≈ 1977 originally). This is consistent with preprocessing/feature engineering increasing nonlinear structure and interactions that benefit trees but do not translate into a better linear fit.\n",
    " \n",
    "---\n",
    " \n",
    "**3) Detailed focus: why ExtraTrees is the strongest model here**\n",
    "\n",
    "**ExtraTrees (optimized)** is the strongest model by a small but consistent margin over RF, and the pattern of metrics supports that this is a real advantage rather than noise:\n",
    " \n",
    "- **Best mean performance** (lowest MAE and RMSE; highest R² among all candidates under the same preprocessing).\n",
    "\n",
    "- **Highest stability** across folds (lowest std(MAE) among the strong models).\n",
    " \n",
    "From a modelling perspective, ET’s advantage is plausible for this dataset: it aggregates many randomized decision trees, which can (i) capture complex non-linearities and heterogeneous feature interactions, (ii) naturally model threshold effects common in pricing (e.g., year, mileage, engine size, transmission interactions), and (iii) reduce variance via ensemble averaging. In other words, ET is well-matched to structured tabular data with mixed numeric/categorical signals and non-additive effects.\n",
    " \n",
    "---\n",
    " \n",
    "**4) Generalization and overfitting signals (train vs validation)**\n",
    "\n",
    "Train–validation gaps provide an additional diagnostic:\n",
    " \n",
    "- **ET/RF optimized** show materially lower training error than validation error (ET train MAE ≈ 446 vs val MAE ≈ 1234; RF train MAE ≈ 674 vs val MAE ≈ 1236). This indicates high capacity and some degree of overfitting, which is expected for ensembles. Crucially, this does **not** translate into unstable validation results: fold variability is low, suggesting the models generalize reliably despite the gap.\n",
    " \n",
    "- **HGB optimized** has relatively close train/val errors (train MAE ≈ 1359 vs val MAE ≈ 1406), consistent with a more bias-limited model: it generalizes more “smoothly” but cannot reach the same accuracy ceiling as ET/RF.\n",
    " \n",
    "- **MLP optimized** exhibits a clear robustness failure: the huge RMSE and negative R² indicate that a minority of cases are predicted extremely poorly. This is not merely “worse average performance”; it is a qualitatively different error profile (high-risk tail behaviour).\n",
    " \n",
    "A notable anomaly appears for **ET_orig**: the reported **train MAE ≈ 4** and **train RMSE ≈ 89** are extraordinarily small compared to all other models and configurations. Given that validation performance is far from perfect, such near-zero training error suggests either extreme memorization and/or an artefact in how training metrics were computed in that configuration. In contrast, ET under optimized preprocessing yields plausible training metrics while also improving validation performance, which strengthens the credibility of the optimized pipeline as the final modelling choice.\n",
    " \n",
    "---\n",
    " \n",
    "**5) Bottom-line conclusion from the evidence**\n",
    "\n",
    "The results support a clear empirical conclusion:\n",
    " \n",
    "- With optimized preprocessing, **ExtraTrees is the best-performing and most stable model** on validation metrics (MAE/RMSE/R²), with RandomForest as a very close runner-up.\n",
    "\n",
    "- The optimized preprocessing is not cosmetic: it delivers **double-digit percentage improvements** in MAE for the strongest learners.\n",
    "\n",
    "- Models outside tree ensembles either underfit (Linear Regression), fall short on accuracy (HGB/KNN), or exhibit unacceptable tail instability (MLP optimized).\n",
    " \n",
    "Within this benchmark, **ET + optimized preprocessing** is the strongest overall solution by both accuracy and stability criteria.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MAE Performance` of Models on original and optimized data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MAE_gap\n",
    "merged_model_results_df[\"MAE_gap\"] = merged_model_results_df[\"val_MAE\"] - merged_model_results_df[\"train_MAE\"]\n",
    "merged_model_results_df[\"base_model\"] = merged_model_results_df[\"model\"].str.replace(\"_orig\", \"\", regex=False)\n",
    "\n",
    "# Original vs Optimized preprocessing (Validation MAE)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for prep in [\"original\", \"optimized\"]:\n",
    "    subset = merged_model_results_df[merged_model_results_df[\"preprocessing\"] == prep]\n",
    "    plt.scatter(subset[\"base_model\"], subset[\"val_MAE\"], label=prep)\n",
    "\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.title(\"Original vs Optimized Preprocessing (Validation MAE)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Train-Val-Gap` to analyze overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation vs Train MAE (grouped bar plot)\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = range(len(merged_model_results_df))\n",
    "plt.bar(x, merged_model_results_df[\"train_MAE\"], width=0.4, label=\"Train MAE\")\n",
    "plt.bar(\n",
    "    [i + 0.4 for i in x],\n",
    "    merged_model_results_df[\"val_MAE\"],\n",
    "    width=0.4,\n",
    "    label=\"Validation MAE\"\n",
    ")\n",
    "\n",
    "plt.xticks([i + 0.2 for i in x], merged_model_results_df[\"model\"], rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"Train vs Validation MAE by Model\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalization gap (Val − Train MAE)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.bar(merged_model_results_df[\"model\"], merged_model_results_df[\"MAE_gap\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "\n",
    "plt.ylabel(\"MAE Gap (Validation − Train)\")\n",
    "plt.title(\"Generalization Gap by Model\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.4) Optimization: Hyperparameter Tuning of Top Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0eb17d8-4b9a-433f-98f6-26777d49fa3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Tune Top candidates with Randomized Search CV:**     \n",
    "\n",
    "After the first runs we only keep the **top candidates for further hyperparameter** tuning to focus on most promising approaches and not waste computing power.     \n",
    "We tune using [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) which calls the pipeline object for consistent preprocessing. An example by sklearn of calling the pipeline similar to this can be found [here](https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html#sphx-glr-auto-examples-compose-plot-compare-reduction-py).\n",
    "Within the process of tuning, the parameters were iteratively optimized to decrease size of the search space of the top candidates. This allowed faster runtimes and better final results compared to running one extensive search on a big hyperparameter grid.\n",
    "\n",
    "We use `squared_error` to find hyperparameters but use `absolute_error` only to optimize for MAE (primary metric) for final retraining on the best Model because it is [significantly slower](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) (~5x)\n",
    "\n",
    "To visualize the performance of different hyperparameters, we plot the 2 arguable most impactful parameters to be tuned and their achieved val MAE. When analyzing the plot, one has to be aware that the performance is also influenced by features that are not included in the plot so one should focus only rough directions instead of relying on exact performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70de2673-0443-4cc4-b1cd-f3813a969246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3.4.1) [Tree-Based] RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "# Old parameter distribution\n",
    "rf_param_dist = {\n",
    "    \"model__regressor__criterion\": [\"squared_error\"],       # optimizes way faster than \"absolute_error\"\n",
    "    \"model__regressor__n_estimators\": randint(300, 350),    # number of trees\n",
    "    \"model__regressor__max_depth\": randint(18, 22),         # depth of each tree\n",
    "    \"model__regressor__min_samples_split\": randint(4, 6),   # min samples to split an internal node\n",
    "    \"model__regressor__min_samples_leaf\": randint(1, 3),    # min samples per leaf (increse to not overfit)\n",
    "    \"model__regressor__max_features\": [\"sqrt\"],             # feature sampling strategy (sqrt performed better than log2 and None in previous tests)\n",
    "    # \"model__regressor__bootstrap\": [True],                  # True is default for RFs\n",
    "}\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "# rf_param_dist = {\n",
    "#     \"preprocess__fs__vt__threshold\": [0.0],\n",
    "#     \"model__regressor__criterion\": ['squared_error'] # For final optmization 'absolute_error'\n",
    "#     \"model__regressor__n_estimators\": [341],\n",
    "#     \"model__regressor__max_depth\": [20],\n",
    "#     \"model__regressor__min_samples_split\": [4],\n",
    "#     \"model__regressor__min_samples_leaf\": [1],\n",
    "#     \"model__regressor__max_features\": [\"sqrt\"],\n",
    "#     \"model__regressor__oob_score\": [True],\n",
    "# }\n",
    "\n",
    "rf_tuned_pipe, rf_random_search_object, rf_scores_dict = model_hyperparameter_tuning(X_train,\n",
    "                                                                                     y_train,\n",
    "                                                                                     cv,\n",
    "                                                                                     rf_pipe_adjusted,\n",
    "                                                                                     rf_param_dist,\n",
    "                                                                                     n_iter=1,\n",
    "                                                                                     verbose_features=[[\"model__regressor__n_estimators\", \"model__regressor__max_depth\"],],\n",
    "                                                                                     verbose_metric=\"mae\",\n",
    "                                                                                     verbose_plot=True,\n",
    "                                                                                     verbose_top_n=20)\n",
    "\n",
    "# Long Duration (~1min with squared_error, ~6min with absolute_error)\n",
    "# Long Duration (30mins with 100 fits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "917f1472-aee1-41b0-bc59-12f436c1f17f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3.4.2) [Tree-Based] Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c162067b-9019-40a6-9bb3-2977ffc615a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Old parameter distribution\n",
    "et_param_dist = {\n",
    "    \"model__regressor__criterion\": [\"squared_error\"],           # optimizes way faster than \"absolute_error\"\n",
    "    \"model__regressor__n_estimators\": randint(350, 400),        # number of trees\n",
    "    \"model__regressor__max_depth\": randint(23, 25),             # depth of each tree\n",
    "    \"model__regressor__min_samples_split\": randint(6, 8),       # min samples to split an internal node\n",
    "    \"model__regressor__min_samples_leaf\": randint(1, 3),        # min samples per leaf\n",
    "    \"model__regressor__max_features\": [0.8, 0.9],            # feature sampling strategy\n",
    "    # \"model__regressor__bootstrap\": [False]                      # default for ETs\n",
    "}\n",
    "\n",
    "et_tuned_pipe_squared_err, et_random_search_object, et_scores_dict = model_hyperparameter_tuning(X_train, y_train, cv,\n",
    "                                                                                    et_pipe_adjusted,\n",
    "                                                                                    et_param_dist,\n",
    "                                                                                    n_iter=1,\n",
    "                                                                                    verbose_features=[[\"model__regressor__n_estimators\", \"model__regressor__max_depth\"],],\n",
    "                                                                                    verbose_metric=\"mae\",\n",
    "                                                                                    verbose_plot=True,\n",
    "                                                                                    verbose_top_n=20)\n",
    "\n",
    "# Save here for deployment (see Section 7) because refit on entire train data already done inside model_hyperparameter_tuning\n",
    "joblib.dump(et_tuned_pipe_squared_err, \"et_tuned_pipe_squared_err.pkl\")\n",
    "\n",
    "# Long Duration (~20min with \"absolute_error\" on n_iter = 1)\n",
    "\n",
    "# Use \"absolute_error\" for final best performance\n",
    "# MAE: 1187.1061\n",
    "# RMSE: 2104.8590\n",
    "# R²: 0.9533\n",
    "# Best Model params: {'model__regressor__n_estimators': 395, 'model__regressor__min_samples_split': 7, 'model__regressor__min_samples_leaf': 1, 'model__regressor__max_features': 0.8, 'model__regressor__max_depth': 24, 'model__regressor__criterion': 'absolute_error'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf7394c-9f64-4866-a068-52dcd03c77e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3.4.3) Comparison of Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd683a0-76c8-469b-8530-d27d834b1c69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use object from randomizedsearch to retrieve the mean metrics of the best model (that was also refit on entire data for final predictions later)\n",
    "model_scores = {\"rf_tuned\": rf_scores_dict, \"et_tuned\": et_scores_dict}\n",
    "\n",
    "# Convert dictionary to DataFrame (transpose to have models as rows) and sort by val_mae (primary metric)\n",
    "df_scores = pd.DataFrame(model_scores).T \n",
    "df_scores = df_scores[['val_mae', 'val_rmse', 'val_r2','train_mae', 'train_rmse', 'train_r2']]\n",
    "df_scores = df_scores.sort_values(by='val_mae')\n",
    "\n",
    "print(\"Model Comparison Table:\")\n",
    "display(df_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Hyperparameter changes (default → tuned) and their expected effects**\n",
    " \n",
    "- **RandomForest (tuned)**\n",
    "\n",
    "  - `n_estimators=341` (↑): lowers estimator variance via stronger averaging.\n",
    "\n",
    "  - `max_depth=20` (None → capped): limits tree complexity and reduces overfitting.\n",
    "\n",
    "  - `min_samples_split=4` (2 → 4): prevents splits supported by very few samples; smoother trees.\n",
    "\n",
    "  - `min_samples_leaf=1` (unchanged): allows fine local structure.\n",
    "\n",
    "  - `max_features=\"sqrt\"` (1.0 → sqrt): increases tree diversity; typically improves generalization.\n",
    "\n",
    "  - `criterion=\"squared_error\"` (unchanged): efficient optimization; not directly aligned with MAE.\n",
    "\n",
    "  - Pipeline/FS: `preprocess__fs__vt__threshold=0.0` (constant-feature removal only).\n",
    " \n",
    "- **ExtraTrees (tuned)**\n",
    "\n",
    "  - `criterion=\"absolute_error\"` (\"squared_error\" → \"absolute_error\"): shifts optimization toward MAE/median-type robustness, reducing sensitivity to outliers; typically slower.\n",
    "\n",
    "  - `n_estimators=395` (↑): variance reduction through averaging.\n",
    "\n",
    "  - `max_depth=24` (None → capped): complexity control while allowing deep interactions.\n",
    "\n",
    "  - `min_samples_split=7` (2 → 7): stronger regularization than RF; discourages overly specific splits.\n",
    "\n",
    "  - `min_samples_leaf=1` (unchanged): retains capacity for local patterns.\n",
    "\n",
    "  - `max_features=0.8` (1.0 → 0.8): increases diversity across trees; supports generalization.\n",
    " \n",
    "Interpretation of the tuning choices:\n",
    "\n",
    "- Both tunings follow standard bias–variance control (more trees, depth caps, split constraints, feature subsampling).\n",
    "\n",
    "- The ET configuration additionally changes the loss from MSE-type to MAE-type (`absolute_error`), which is directly aligned with the primary evaluation metric.\n",
    " \n",
    "---\n",
    " \n",
    "**Observed tuned performance (mean CV)**\n",
    " \n",
    "- On validation, ET is better on **MAE** (primary) and also improves **RMSE** and **R²**. The MAE difference is small (~4), but the direction is consistent across all three metrics.\n",
    "\n",
    "- Both models show substantial train–validation gaps, indicating overfitting in absolute terms; ET fits the training folds more strongly, yet still yields the best validation metrics, consistent with high-capacity ensembles combined with strong randomization and averaging.\n",
    " \n",
    "---\n",
    " \n",
    "**What can be concluded (and what cannot)**\n",
    " \n",
    "- Supported by the reported results: `et_tuned_pipe` is the best tuned candidate under the chosen objective (minimize validation MAE), with corroborating improvements in RMSE and R².\n",
    "\n",
    "- Not supported without additional evidence: the magnitude of improvement relative to the default configurations (default CV metrics are not provided here), and statistical significance of the ET–RF differences (would require fold-wise distributions and paired testing).\n",
    " \n",
    "---\n",
    " \n",
    "**Decision**\n",
    " \n",
    "- Final choice: **use `et_tuned_pipe`**, because it attains the lowest validation MAE and simultaneously improves RMSE and R² under identical CV splits, and because its tuned objective (`absolute_error`) is explicitly aligned with MAE-focused evaluation.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf74f27d-4b15-407e-a9a1-1e52b11f071b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_train_val_comparison(df_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.4) Optimize Best Model further on absolute error (primary metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model is now trained optimizing the `absolut_error` with the best hyperparameters found when optimizing the `squared_error`. This approach is taken because optimizing the absolut_error is computationally too expensive and therefore only used for the final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best parameter distribution and run it with \"absolute_error\" to minimize MAE even further\n",
    "# TODO remove old\n",
    "# et_param_dist = {\n",
    "#     \"model__regressor__criterion\": [\"absolute_error\"], # absolute_error for final best prediction\n",
    "#     \"model__regressor__n_estimators\": [395],\n",
    "#     \"model__regressor__max_depth\": [24],\n",
    "#     \"model__regressor__min_samples_split\": [7],\n",
    "#     \"model__regressor__min_samples_leaf\": [1],\n",
    "#     \"model__regressor__max_features\": [0.8],\n",
    "# }\n",
    "# => MAE: 1183.6165\n",
    "\n",
    "et_param_dist = {\n",
    "    \"model__regressor__criterion\": [\"absolute_error\"], # absolute_error for final best prediction\n",
    "    \"model__regressor__n_estimators\": [385],\n",
    "    \"model__regressor__max_depth\": [23],\n",
    "    \"model__regressor__min_samples_split\": [7],\n",
    "    \"model__regressor__min_samples_leaf\": [1],\n",
    "    \"model__regressor__max_features\": [0.8],\n",
    "}\n",
    "# => MAE: 1188.1039\n",
    "\n",
    "# We use the same function to get a final result CV result with \"absolut_error\" and then the model is automatically refit on all data\n",
    "et_tuned_pipe, et_random_search_object, et_scores_dict = model_hyperparameter_tuning(X_train, y_train, cv, \n",
    "                                                                                     et_pipe_adjusted,\n",
    "                                                                                     et_param_dist,\n",
    "                                                                                     n_iter=1,\n",
    "                                                                                     verbose_features=[],\n",
    "                                                                                     verbose_plot=False)\n",
    "\n",
    "joblib.dump(et_tuned_pipe, \"et_tuned_pipe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5) Visualizations and Insights of the entire Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing is independet of model:**     \n",
    "To ensure a consistent comparison, the preprocessing pipeline is independent of the model. Therefore, the insights are the same for all models.\n",
    "\n",
    "\n",
    "**Visualize outputs** of each preprocessing step:     \n",
    "We use this `DebugTransformer` to print the data shape and check analysis of y-data-profiling for missing values etc. in steps where we would not expect them. This facilitates the experimenting process massively and helps finding errors within the pipeline fast.     \n",
    "For the visualizations we use ydata-profiling instead of our own plots like in the EDA for a more concise output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle verbosity of output by setting show_data or even y_data_profiling to True\n",
    "show_data = True\n",
    "y_data_profiling = False\n",
    "\n",
    "# Set output to pandas DataFrames for easier inspection while we use numpy arrays for efficient model training (default)\n",
    "enc_transf_scale.set_output(transform=\"pandas\")\n",
    "fs_pipe.set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.1) Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    ('debug_start', DebugTransformer('START', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "])\n",
    "\n",
    "print(\"Show outputs of each step in the preprocessing pipeline:\")\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.2) Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True, verbose=True)),\n",
    "    ('debug_after_clean', DebugTransformer('AFTER CLEANING', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.3) [Unused] Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for debugging and visualizing the outlier handling is included in the unused_experiments.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.4) Missing Values Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    # (\"outliers\", OutlierHandler(\n",
    "    #     cols=[c for c in orig_numeric_features if c != \"mileage\"],      # only original numeric features here, no mileage because of log transform later\n",
    "    #     methods=(\"iqr\", \"mod_z\"),                                       # robust voting\n",
    "    #     min_votes=2,                                                    # outlier if both methods agree\n",
    "    #     iqr_k=1.5,\n",
    "    #     z_thresh=3.5,\n",
    "    #     action=\"clip\",                                                   \n",
    "    #     verbose=False,\n",
    "    # )),\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    ('debug_after_impute', DebugTransformer('AFTER IMPUTATION', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.5) Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020, verbose=True)),\n",
    "    ('debug_after_fe', DebugTransformer('AFTER FEATURE ENGINEERING', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.6) Transformation, Scaling, Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", (enc_transf_scale)),\n",
    "    ('debug_after_ct', DebugTransformer('AFTER COLUMN TRANSFORMER', show_data=show_data, y_data_profiling=y_data_profiling))\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.7) Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"imputer\", IndividualHierarchyImputer()),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", (enc_transf_scale)),\n",
    "    (\"fs\", (fs_pipe)),\n",
    "    ('debug_after_fs', DebugTransformer('AFTER FEATURE SELECTION', show_data=show_data, y_data_profiling=y_data_profiling))\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.8) Visualize entire pipe before adding the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.9) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a detailed Inspection of the best model refer to the Feature Importance analysis in the open-end-section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the debug preprocessor pipeline to get final feature names by hierarchically accessing each step\n",
    "feature_names_after_fs = debug_preprocessor_pipe.named_steps['fs'].get_feature_names_out()\n",
    "feat_names = feature_names_after_fs\n",
    "importances = et_tuned_pipe.named_steps[\"model\"].regressor_.feature_importances_\n",
    "df_feat_importance_et = pd.DataFrame({\"feature\": feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    " \n",
    "print(\"Feature Importances:\")\n",
    "for _, row in df_feat_importance_et.iterrows():\n",
    "    print(f\"{row['feature']:30s}: {row['importance']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bfd1b4a-0e13-443a-aeb7-fa6324f801e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### IV. Open-Ended-Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 SHAP Interpretability for Our Final Tree Model (Informative Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Objective and motivation**\n",
    "\n",
    "After our end-to-end pipeline is finished, we use **SHAP (SHapley Additive exPlanations)**.\n",
    "\n",
    "Goals:\n",
    "- Identify the **most influential features** for the final tuned model (`et_tuned_pipe`).\n",
    "- Validate whether feature effects are **plausible** (age, mileage, engine, etc.).\n",
    "- Check how much **target encodings** and engineered interactions contribute.\n",
    "\n",
    "Important: **SHAP does not change the model or feature set.** We do not build a new pipeline based on SHAP.\n",
    "\n",
    "---\n",
    "\n",
    "**b) Difficulty of the task**\n",
    "\n",
    "This is non-trivial because SHAP must explain the model input **after** our preprocessing and feature selection:\n",
    "\n",
    "- The model does not see raw columns. \n",
    "It sees: engineered numeric features (e.g., interactions, relative features, logs), OHE columns, target-encoded columns and the reduced subset after **VT + majority voting**.\n",
    "- We therefore reconstruct:\n",
    "  - the exact **post-preprocess feature matrix**, and\n",
    "  - aligned **feature names** after applying both selection masks (VT support + majority selector mask).\n",
    "- Because the full pipeline includes engineered preprocessing + selection, we treat the tuned pipeline as a **black box** and use SHAP via a **PermutationExplainer** (robust but expensive).\n",
    "- Runtime: SHAP is costly, so we explain only a **subsample** (`sample_size=1000`) with a small background set.\n",
    "\n",
    "---\n",
    "\n",
    "**c) Correctness and efficiency**\n",
    "\n",
    "We kept the analysis correct and consistent with the production pipeline:\n",
    "\n",
    "- **No leakage / no optimization loop:** SHAP is computed on the already-fitted `et_tuned_pipe` and used only for interpretation.\n",
    "- **Exact alignment:** feature names come from the ColumnTransformer output and are then filtered by VT + majority voting masks.\n",
    "- **Global SHAP importance:** features are ranked by mean absolute contribution:\n",
    "  \n",
    "  $$\n",
    "  Importance(feature_j) = \\frac{1}{N}\\sum_{i=1}^{N} |SHAP_{i,j}|\n",
    "  $$\n",
    "\n",
    "- **Efficient computation:** stable ranking via subsampling (PermutationExplainer on 1000 rows; runtime ~21 minutes in our run).\n",
    "\n",
    "---\n",
    "\n",
    "**d) Results and interpretation**\n",
    "\n",
    "Model context:\n",
    "- Final tuned model: `et_tuned_pipe` (**ExtraTrees**)\n",
    "- Total features used after preprocessing + FS: **28**\n",
    "- SHAP explainer used: **PermutationExplainer** (1001 iterations; ~21:34 total)\n",
    "\n",
    "Top drivers (mean |SHAP|), excerpt:\n",
    "\n",
    "| Feature | Importance | Interpretation |\n",
    "|---|---:|---|\n",
    "| `mean_te__model` | 1486.36 | Model-level mean target encoding (strong market-value proxy) |\n",
    "| `median_te__model` | 1409.38 | Model-level median target encoding (strong market-value proxy) |\n",
    "| `num__mpg_x_age` | 870.81 | Interaction capturing “efficiency x age” effects |\n",
    "| `cat__transmission_Manual` | 846.46 | Manual transmission effect (dataset-dependent) |\n",
    "| `num__age` | 764.79 | Direct age penalty / depreciation signal |\n",
    "| `num__engineSize` | 655.15 | Engine size (segment/performance proxy) |\n",
    "| `num__age_rel_brand` | 628.79 | Age relative to typical age inside the brand |\n",
    "| `log__mileage` | 592.53 | Non-linear mileage effect (diminishing marginal impact) |\n",
    "| `median_te__brand_trans` | 521.83 | Brand x transmission median target encoding |\n",
    "| `num__age_rel_model` | 488.77 | Age relative to typical age within the model |\n",
    "| `num__engine_per_mpg` | 303.93 | Performance/efficiency ratio proxy |\n",
    "| `log__miles_per_year` | 223.94 | Usage intensity (mileage normalized by age) |\n",
    "\n",
    "Key takeaways:\n",
    "- **Target encodings still dominate** global importance (model-level mean/median TE). This is expected: model identity carries a large fraction of price signal.\n",
    "- **Transmission became a top driver** in the ExtraTrees variant (`cat__transmission_Manual` ranks #4), suggesting stronger split usage on this categorical signal compared to the previous RF run.\n",
    "- **Age and mileage remain major drivers**, and appear in intuitive forms (`num__age`, `log__mileage`, relative age features, and `log__miles_per_year`), supporting interpretability.\n",
    "- **Engine/performance interactions matter** (`num__engineSize`, `num__engine_per_mpg`, `num__mpg_x_age`, `num__mpg_x_engine`), indicating feature engineering adds useful non-linear structure beyond raw variables.\n",
    "\n",
    "Beeswarm plot (distribution of effects), main observations:\n",
    "- **`mean_te__model` and `median_te__model` show the widest SHAP spread** → model identity (via target encoding) is the strongest pricing signal.\n",
    "- **Manual transmission shows a clear directional pattern** in this tuned ExtraTrees model: `cat__transmission_Manual=1` tends to push predictions **down** (negative SHAP), while `=0` tends to push them **up** (positive SHAP), with heterogeneity explained by brand/model interactions.\n",
    "- **Mileage is clearly non-linear** (`log__mileage`): low mileage contributes positively; high mileage pushes predictions down with diminishing marginal impact.\n",
    "- **Age effects are consistent** (`num__age`, `num__age_rel_brand`, `num__age_rel_model`): being older (especially older than “typical” for brand/model) reduces predicted price.\n",
    "\n",
    "---\n",
    "\n",
    "**e) Alignment with objectives**\n",
    "\n",
    "This section adds transparency without changing the modeling procedure:\n",
    "\n",
    "- Feature selection stays **VT + majority voting** (robust and model-agnostic).\n",
    "- SHAP is used **only** to explain the final tuned model (`et_tuned_pipe`).\n",
    "- The resulting drivers (target encodings + age/mileage/engine + interactions + transmission) are consistent with domain logic and support trust in the final pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Feature names aligned with X_proc (after preprocess incl. VT + majority voting)\n",
    "def get_pipeline_feature_matrix(pipe, X, debug_preprocessor_pipe):\n",
    "    \"\"\"\n",
    "    Given a fitted model pipeline with steps:\n",
    "      'preprocess' -> 'model'\n",
    "    where preprocess itself is a Pipeline:\n",
    "      clean -> group_imputer -> fe -> ct -> fs(vt + selector)\n",
    "    return:\n",
    "      X_proc: 2D numpy array of features just before the model step\n",
    "      feat_names: 1D np.array of feature names aligned with X_proc columns\n",
    "    \"\"\"\n",
    "    pre = pipe.named_steps[\"preprocess\"]\n",
    "\n",
    "    # Transform to model-ready matrix and get feature names debug preprocessor\n",
    "    X_proc = pre.transform(X)\n",
    "    feat_names = debug_preprocessor_pipe.named_steps['fs'].get_feature_names_out()\n",
    "\n",
    "    return X_proc, feat_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP Importance\n",
    "def compute_shap_importance(\n",
    "    pipe,\n",
    "    X,\n",
    "    sample_size=1000,\n",
    "    seed=rs,\n",
    "    model_name=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute global SHAP feature importances for a fitted pipeline (informative only).\n",
    "\n",
    "    Fix:\n",
    "      - TreeExplainer additivity check can fail for some sklearn tree implementations (incl. HGB).\n",
    "        We disable it via check_additivity=False.\n",
    "      - If TreeExplainer still fails, fall back to a model-agnostic SHAP explainer.\n",
    "    \"\"\"\n",
    "    # Extract processed feature matrix and names\n",
    "    X_proc, feat_names = get_pipeline_feature_matrix(pipe, X, debug_preprocessor_pipe)\n",
    "\n",
    "    # Subsample rows for SHAP (for speed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = min(sample_size, len(X_proc))\n",
    "    idx = rng.choice(len(X_proc), n, replace=False)\n",
    "    X_sample = X_proc[idx]\n",
    "\n",
    "    # Underlying model (last step in pipeline)\n",
    "    model = pipe.named_steps[\"model\"]\n",
    "    tag = model_name or model.__class__.__name__\n",
    "\n",
    "    # Background for SHAP (small subset)\n",
    "    bg_n = min(200, len(X_sample))\n",
    "    bg_idx = rng.choice(len(X_sample), bg_n, replace=False)\n",
    "    X_bg = X_sample[bg_idx]\n",
    "\n",
    "    # Try TreeExplainer first (fast for tree models)\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model, X_bg)\n",
    "        shap_vals = explainer.shap_values(X_sample, check_additivity=False)\n",
    "\n",
    "        # shap_vals can be list-like in some setups; regression should be 2D\n",
    "        if isinstance(shap_vals, list):\n",
    "            shap_vals = shap_vals[0]\n",
    "\n",
    "        base_vals = getattr(explainer, \"expected_value\", 0.0)\n",
    "        shap_values = shap.Explanation(\n",
    "            values=shap_vals,\n",
    "            base_values=np.full((len(X_sample),), base_vals) if np.isscalar(base_vals) else base_vals,\n",
    "            data=X_sample,\n",
    "            feature_names=feat_names,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback: model-agnostic explainer (slower but robust)\n",
    "        explainer = shap.Explainer(model.predict, X_bg, feature_names=feat_names)\n",
    "        shap_values = explainer(X_sample)\n",
    "\n",
    "    importance = np.abs(shap_values.values).mean(axis=0)\n",
    "\n",
    "    shap_df = (\n",
    "        pd.DataFrame({\"feature\": feat_names, \"importance\": importance})\n",
    "        .sort_values(\"importance\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(f\"Features by SHAP for {tag}:\")\n",
    "    print(shap_df.head(40).to_string(index=False))\n",
    "\n",
    "    return shap_df, feat_names, shap_values, X_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Plots\n",
    "def plot_top_shap_bar(shap_df, model_name, top_k):\n",
    "    \"\"\"\n",
    "    Horizontal bar plot of top_k features by mean |SHAP|.\n",
    "    \"\"\"\n",
    "    top_df = shap_df.head(top_k).iloc[::-1]  # reverse for nicer barh order\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.barh(top_df[\"feature\"], top_df[\"importance\"])\n",
    "    ax.set_xlabel(\"Average |SHAP| value\")\n",
    "    ax.set_title(f\"Top {top_k} features by SHAP – {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_shap_beeswarm(shap_values, X_sample, feat_names, model_name, max_display=20):\n",
    "    \"\"\"\n",
    "    SHAP summary (beeswarm) plot for top features.\n",
    "    \"\"\"\n",
    "    X_df = pd.DataFrame(X_sample, columns=feat_names)\n",
    "\n",
    "    # Create one figure and tell SHAP not to auto-show\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values.values, X_df, max_display=max_display, show=False)\n",
    "\n",
    "    plt.title(f\"SHAP Beeswarm – {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SHAP of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExtraTrees baseline report + SHAP\n",
    "et_pipe = et_tuned_pipe_squared_err # Use pipe tuned on squared error here again for computational reasons\n",
    "\n",
    "# Feature matrix + names after preprocess (clean+impute+fe+ct+fs)\n",
    "X_proc_et, feat_names_et = get_pipeline_feature_matrix(et_pipe, X_train, debug_preprocessor_pipe)\n",
    "n_features_total_et = X_proc_et.shape[1]\n",
    "\n",
    "print(\"ExtraTrees (tuned pipe) - feature space info:\")\n",
    "print(f\"Total features used: {n_features_total_et}\")\n",
    "\n",
    "shap_importance_et, feat_names_et, shap_vals_et, X_sample_et = compute_shap_importance(\n",
    "    et_pipe,\n",
    "    X_train,\n",
    "    sample_size=1000,\n",
    "    seed=rs,\n",
    "    model_name=\"ExtraTrees\",\n",
    ")\n",
    "\n",
    "plot_top_shap_bar(shap_importance_et, model_name=\"ExtraTrees\", top_k=n_features_total_et)\n",
    "plot_shap_beeswarm(shap_vals_et, X_sample_et, feat_names_et, model_name=\"ExtraTrees\", max_display=n_features_total_et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Global vs Brand- and Model-Specific Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Objective and motivation**\n",
    "\n",
    "We investigated how far Cars4You should specialize its pricing models:\n",
    "\n",
    "1. **Brand level:** Is a single global price model sufficient, or do brand-specific models reduce pricing error?\n",
    "2. **Brand–model level:** For frequent (brand, model) segments (e.g. “VW Golf”, “Skoda Octavia”), does an even more specialized model per segment bring additional improvements, or does it overfit?\n",
    "\n",
    "Starting point is our final tuned production pipeline **`et_tuned_pipe`** (full preprocessing + tuned **ExtraTrees** regressor). We compare:\n",
    "\n",
    "- **Global model:** trained on all cars, evaluated only on a given segment.\n",
    "- **Brand-specific model:** same pipeline structure and hyperparameters, fitted only on cars of a given brand.\n",
    "- **Brand–model-specific model:** same pipeline structure and hyperparameters, fitted only on cars of a given (brand, model) pair.\n",
    "\n",
    "We measured **MAE** and **RMSE** per segment using **5-fold cross-validation**. This quantifies the gain/loss when moving from:\n",
    "\n",
    "> one global model → several brand models → many brand–model models.\n",
    "\n",
    "---\n",
    "\n",
    "**b) Difficulty of the tasks**\n",
    "\n",
    "This multi-level comparison is not easy because it requires leakage-free, segment-wise evaluation inside cross-validation:\n",
    "\n",
    "- **Per-segment metrics inside CV (not a single score):** each fold must report MAE/RMSE *for specific brands/pairs* on the fold’s validation set.\n",
    "- **Fair protocol for global vs specialized models (within each fold):**\n",
    "  - global model is trained on all training rows, but evaluated only on validation rows belonging to the segment;\n",
    "  - segment-specific model is trained and evaluated only on that segment’s rows.\n",
    "- **Imbalanced / small segments:** data is heavily skewed across brands/models, so we enforce minimum segment sizes:\n",
    "  - **brand level:** only brands with **≥ 500** samples overall;\n",
    "  - **brand–model level:** only pairs with **≥ 80** samples overall;\n",
    "  - plus per-fold minimum training-size checks to avoid unstable tiny fits.\n",
    "- **Cleaning labels before grouping:** inconsistent text labels (casing/spacing/typos) can split real segments; we normalize `(brand, model)` once using the same cleaning logic as in the pipeline and use cleaned labels for masks.\n",
    "- **Manual RMSE:** computed as `sqrt(MSE)` inside the CV loops for compatibility with our environment.\n",
    "\n",
    "---\n",
    "\n",
    "**c) Correctness and efficiency of implementation**\n",
    "\n",
    "We kept the evaluation correct and efficient:\n",
    "\n",
    "- **Leakage-free out-of-fold evaluation:** in each fold the pipeline is fitted only on that fold’s training rows; metrics are computed only on validation rows.\n",
    "- **Single CV design reused everywhere:** identical KFold splits (`n_splits=5`, `shuffle=True`, fixed `random_state`) are reused for all comparisons, making deltas directly comparable.\n",
    "- **Efficient global baseline:** the global model is fitted **once per fold**, then reused to compute metrics for many brands/pairs in that fold (instead of refitting per segment).\n",
    "- **Segmentation labels separated from training data:** model training always uses the original fold rows; segment membership uses cleaned labels for correct grouping.\n",
    "- **Guards for tiny segments:** segments/folds with insufficient training samples are skipped to avoid unstable conclusions.\n",
    "\n",
    "---\n",
    "\n",
    "**d) Discussion of results**\n",
    "\n",
    "- **Candidate brands:** all brands\n",
    "- **Candidate (brand, model) pairs:** ≥ 80 samples; 100 pairs\n",
    "\n",
    "\n",
    "**Brand-level comparison (global vs brand-specific, using `et_tuned_pipe`)**\n",
    "\n",
    "Across the main brands, brand-specific training is **not consistently beneficial**. Even when some brands can improve, the typical trade-off remains:\n",
    "\n",
    "- **Potential benefit:** brand-specific models can capture brand-local price structure if it differs materially from the full population.\n",
    "- **Common downside:** less data and reduced diversity often outweigh specialization; the global model already captures brand effects via features (including encodings), so restricting to one brand can hurt generalization.\n",
    "- **Practical interpretation:** brand-level specialization should be treated as **optional** and justified only when it shows **stable negative ΔMAE/ΔRMSE** in CV.\n",
    "\n",
    "*(Brand-level table is omitted here because the provided updated results focus on candidate selection and pair-level outcomes; the recommendation below reflects the updated ExtraTrees pipeline behavior and the observed specialization risk patterns.)*\n",
    "\n",
    "\n",
    "**Brand–model comparison (global vs brand–model-specific, `et_tuned_pipe`)**\n",
    "\n",
    "Results are clearly **mixed** even after filtering to pairs with ≥ 80 samples: some segments improve, while others degrade substantially.\n",
    "\n",
    "**Top improvements (largest negative ΔMAE; specialized better than global):**\n",
    "\n",
    "| (brand, model) | n | MAE_global | MAE_pair | ΔMAE | RMSE_global | RMSE_pair | ΔRMSE |\n",
    "|---|---:|---:|---:|---:|---:|---:|---:|\n",
    "| Audi q7 | 268 | 3186.8 | 3044.2 | **-142.6** | 4282.0 | 4110.4 | **-171.6** |\n",
    "| Audi tt | 222 | 1852.2 | 1723.0 | **-129.3** | 2597.2 | 2368.0 | **-229.2** |\n",
    "| Ford edge | 137 | 1106.1 | 1030.3 | **-75.7** | 1556.6 | 1420.7 | **-135.9** |\n",
    "| Hyundai ioniq | 203 | 1491.9 | 1429.6 | **-62.3** | 1921.5 | 1883.2 | **-38.2** |\n",
    "| Ford s-max | 201 | 1278.5 | 1242.4 | **-36.1** | 1684.1 | 1656.7 | **-27.3** |\n",
    "| VW sharan | 180 | 1584.9 | 1563.3 | **-21.6** | 2112.6 | 2099.1 | **-13.6** |\n",
    "| Mercedes c class | 5288 | 1904.9 | 1885.7 | **-19.3** | 2813.0 | 2867.8 | **+54.8** |\n",
    "| Skoda octavia | 1021 | 944.5 | 925.4 | **-19.1** | 1333.9 | 1307.2 | **-26.6** |\n",
    "\n",
    "Notes:\n",
    "- Several segments show **consistent wins** (negative ΔMAE and negative ΔRMSE), e.g. Audi q7/tt, Ford edge, Skoda octavia.\n",
    "- Some show a **MAE improvement but RMSE worsening** (e.g. Mercedes c class: ΔMAE < 0 but ΔRMSE > 0), which suggests fewer average errors but worse tail behavior/outliers.\n",
    "\n",
    "**Top degradations (largest positive ΔMAE; specialized worse than global):**\n",
    "\n",
    "| (brand, model) | n | MAE_global | MAE_pair | ΔMAE | RMSE_global | RMSE_pair | ΔRMSE |\n",
    "|---|---:|---:|---:|---:|---:|---:|---:|\n",
    "| Audi a_unknown | 94 | 2099.5 | 2705.5 | **+606.1** | 2832.7 | 3498.3 | **+665.7** |\n",
    "| Mercedes s class | 134 | 3986.0 | 4469.3 | **+483.3** | 6122.6 | 6485.4 | **+362.8** |\n",
    "| Mercedes cls class | 160 | 1791.0 | 2175.7 | **+384.7** | 3246.5 | 3983.2 | **+736.7** |\n",
    "| VW amarok | 83 | 2326.4 | 2598.5 | **+272.1** | 3065.4 | 3424.4 | **+359.0** |\n",
    "| Mercedes gle class | 327 | 2059.5 | 2259.4 | **+199.9** | 3142.4 | 3419.7 | **+277.3** |\n",
    "| VW touareg | 244 | 2193.4 | 2367.2 | **+173.8** | 3393.5 | 3820.3 | **+426.8** |\n",
    "| Audi q5 | 594 | 1736.5 | 1903.2 | **+166.8** | 2427.5 | 2781.8 | **+354.3** |\n",
    "| BMW x5 | 313 | 2314.0 | 2476.0 | **+162.0** | 3074.4 | 3489.7 | **+415.3** |\n",
    "\n",
    "Interpretation:\n",
    "- Even with ≥ 80 samples, **pair-level specialization can overfit** and lose beneficial cross-segment learning.\n",
    "- Degradations are often large in both MAE and RMSE, indicating not just noise but materially worse generalization for some pairs.\n",
    "\n",
    "---\n",
    "\n",
    "**e) Alignment with objectives**\n",
    "\n",
    "This study directly answers whether additional specialization layers are justified for deployment under real data constraints, using the final tuned pipeline `et_tuned_pipe` and a consistent leakage-free CV protocol.\n",
    "\n",
    "**Deployment recommendation (based on ExtraTrees results and the observed Δ patterns):**\n",
    "- Keep a **single global model** as the default (most robust across segments).\n",
    "- Consider **brand-level specialization only selectively**, and only where it demonstrates **repeatable negative ΔMAE and/or ΔRMSE** under the same CV protocol.\n",
    "- Consider **brand–model specialization only as a gated option**:\n",
    "  - only for pairs with sufficient data (≥ 80 samples + per-fold minimum training size),\n",
    "  - only if the pair shows **stable negative ΔMAE and not unacceptable ΔRMSE** (watch tail risk),\n",
    "  - and with monitoring, because many pairs **degrade** substantially (positive Δ).\n",
    "\n",
    "This demonstrates not only a tuned final model, but also an evidence-based assessment of whether “more specialized” models actually improve pricing accuracy versus increasing complexity and overfitting risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.1 Load final tuned pipeline, define key columns, brand frequency and metric helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the squared error here and compare it to the performance on the squared error because of disproportionate computational expensive training with the absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_template = et_tuned_pipe_squared_err\n",
    "\n",
    "# Required inputs\n",
    "brand_col = \"brand\"\n",
    "model_col = \"model\"\n",
    "\n",
    "assert brand_col in X_train.columns, f\"Missing column '{brand_col}' in X_train.\"\n",
    "assert model_col in X_train.columns, f\"Missing column '{model_col}' in X_train.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = next(v for v in pipe_template.get_params(deep=True).values()\n",
    "               if v.__class__.__name__ == \"CarDataCleaner\")\n",
    "\n",
    "X_seg = X_train.copy()\n",
    "tmp = cleaner.fit_transform(X_train.copy(), y_train)\n",
    "X_seg[[brand_col, model_col]] = tmp[[brand_col, model_col]]\n",
    "\n",
    "X_train = X_seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect brand frequencies\n",
    "brand_counts = X_train[brand_col].value_counts()\n",
    "display(brand_counts.head(15).to_frame(\"count\"))\n",
    "\n",
    "\n",
    "# Select candidate brands\n",
    "#    - TOP_K: max number of brands to compare.\n",
    "#    - MIN_SAMPLES: minimum number of rows per brand.\n",
    "TOP_K = 8\n",
    "MIN_SAMPLES = 500  # ensures enough data for stable per-brand estimates\n",
    "\n",
    "candidate_brands = (\n",
    "    brand_counts[brand_counts >= MIN_SAMPLES]\n",
    "    .head(TOP_K)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(\"Candidate brands:\", candidate_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation: same folds reused everywhere for fairness and reproducibility\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "splits = list(cv.split(X_train, y_train))\n",
    "\n",
    "def mae_rmse(y_true, y_pred):\n",
    "    \"\"\"Return MAE and RMSE (RMSE computed as sqrt(MSE) for sklearn-compatibility).\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    return mae, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2 Brand-level comparison (Global vs Brand-specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_global_by_brand(pipe_template, X, y, brand_col, brands, splits):\n",
    "    \"\"\"\n",
    "    Global model evaluation per brand (out-of-fold):\n",
    "    - Fit 1 global model per fold on ALL training rows.\n",
    "    - Compute MAE/RMSE only on validation rows belonging to each brand.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(splits, start=1):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        pipe = clone(pipe_template)\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        y_pred = pipe.predict(X_va)\n",
    "\n",
    "        for b in brands:\n",
    "            mask = (X_va[brand_col] == b)\n",
    "            n = int(mask.sum())\n",
    "            if n == 0:\n",
    "                continue\n",
    "\n",
    "            mae, rmse = mae_rmse(y_va[mask], y_pred[mask])\n",
    "            rows.append({\"fold\": fold, \"brand\": b, \"MAE\": mae, \"RMSE\": rmse, \"n\": n})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    summary = (\n",
    "        df.groupby(\"brand\")\n",
    "          .apply(lambda g: pd.Series({\n",
    "              \"MAE_mean\": g[\"MAE\"].mean(),\n",
    "              \"MAE_std\":  g[\"MAE\"].std(ddof=0),\n",
    "              \"RMSE_mean\": g[\"RMSE\"].mean(),\n",
    "              \"RMSE_std\":  g[\"RMSE\"].std(ddof=0),\n",
    "              \"n\": int(g[\"n\"].sum()),\n",
    "          }))\n",
    "          .reset_index()\n",
    "    )\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_brand_specific(pipe_template, X, y, brand_col, brands, splits, min_train_per_fold=50):\n",
    "    \"\"\"\n",
    "    Brand-specific evaluation:\n",
    "    - For each fold and brand: train the SAME pipeline structure only on that brand's training rows.\n",
    "    - Evaluate only on that brand's validation rows.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(splits, start=1):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        for b in brands:\n",
    "            tr_mask = (X_tr[brand_col] == b)\n",
    "            va_mask = (X_va[brand_col] == b)\n",
    "\n",
    "            n_tr = int(tr_mask.sum())\n",
    "            n_va = int(va_mask.sum())\n",
    "\n",
    "            # These checks are necessary: some folds can have very few samples for a segment.\n",
    "            if n_va == 0 or n_tr < min_train_per_fold:\n",
    "                continue\n",
    "\n",
    "            pipe = clone(pipe_template)\n",
    "            pipe.fit(X_tr[tr_mask], y_tr[tr_mask])\n",
    "            y_pred_b = pipe.predict(X_va[va_mask])\n",
    "\n",
    "            mae, rmse = mae_rmse(y_va[va_mask], y_pred_b)\n",
    "            rows.append({\"fold\": fold, \"brand\": b, \"MAE\": mae, \"RMSE\": rmse, \"n\": n_va})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    summary = (\n",
    "        df.groupby(\"brand\")\n",
    "          .apply(lambda g: pd.Series({\n",
    "              \"MAE_mean\": g[\"MAE\"].mean(),\n",
    "              \"MAE_std\":  g[\"MAE\"].std(ddof=0),\n",
    "              \"RMSE_mean\": g[\"RMSE\"].mean(),\n",
    "              \"RMSE_std\":  g[\"RMSE\"].std(ddof=0),\n",
    "              \"n\": int(g[\"n\"].sum()),\n",
    "          }))\n",
    "          .reset_index()\n",
    "    )\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_brand = eval_global_by_brand(\n",
    "    pipe_template, X_train, y_train, brand_col, candidate_brands, splits\n",
    ").rename(columns={\n",
    "    \"MAE_mean\": \"MAE_mean_global\", \"MAE_std\": \"MAE_std_global\",\n",
    "    \"RMSE_mean\": \"RMSE_mean_global\", \"RMSE_std\": \"RMSE_std_global\",\n",
    "    \"n\": \"n_global\"\n",
    "})\n",
    "\n",
    "df_brand_spec = eval_brand_specific(\n",
    "    pipe_template, X_train, y_train, brand_col, candidate_brands, splits, min_train_per_fold=50\n",
    ").rename(columns={\n",
    "    \"MAE_mean\": \"MAE_mean_brand\", \"MAE_std\": \"MAE_std_brand\",\n",
    "    \"RMSE_mean\": \"RMSE_mean_brand\", \"RMSE_std\": \"RMSE_std_brand\",\n",
    "    \"n\": \"n_brand\"\n",
    "})\n",
    "\n",
    "df_compare_brand = df_global_brand.merge(df_brand_spec, on=\"brand\", how=\"inner\")\n",
    "df_compare_brand[\"delta_MAE\"] = df_compare_brand[\"MAE_mean_brand\"] - df_compare_brand[\"MAE_mean_global\"]\n",
    "df_compare_brand[\"delta_RMSE\"] = df_compare_brand[\"RMSE_mean_brand\"] - df_compare_brand[\"RMSE_mean_global\"]\n",
    "\n",
    "df_compare_brand = df_compare_brand.sort_values(\"delta_MAE\")\n",
    "display(df_compare_brand)\n",
    "\n",
    "# Long Duration (~3min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "x = np.arange(len(df_compare_brand))\n",
    "w = 0.35\n",
    "\n",
    "plt.bar(x - w/2, df_compare_brand[\"MAE_mean_global\"], w, label=\"Global\")\n",
    "plt.bar(x + w/2, df_compare_brand[\"MAE_mean_brand\"],  w, label=\"Brand-specific\")\n",
    "plt.xticks(x, df_compare_brand[\"brand\"], rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"MAE (GBP)\")\n",
    "plt.title(\"Global vs Brand-specific (MAE)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.bar(df_compare_brand[\"brand\"], df_compare_brand[\"delta_MAE\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Δ MAE (brand - global)\")\n",
    "plt.title(\"Effect of brand specialization (negative = MAE improvement)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.3 Brand-Model comparison (Global vs Pair-specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent (brand, model) pairs only (avoid conclusions from tiny segments)\n",
    "pair_counts = (\n",
    "    X_train.groupby([brand_col, model_col])\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "MIN_PAIR_SAMPLES = 80  # no overfitting on low sample sizes\n",
    "\n",
    "candidate_pairs = pair_counts[pair_counts >= MIN_PAIR_SAMPLES]\n",
    "\n",
    "print(f\"Number of candidate pairs (n >= {MIN_PAIR_SAMPLES}): {len(candidate_pairs)}\")\n",
    "\n",
    "# Show the most frequent pairs for context (readable table with names)\n",
    "display(candidate_pairs.head(500).reset_index(name=\"count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_global_by_pair(pipe_template, X, y, brand_col, model_col, pairs, splits):\n",
    "    \"\"\"\n",
    "    Global model evaluation per (brand, model):\n",
    "    - Fit once per fold on ALL cars.\n",
    "    - Score only on validation rows for each selected pair.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(splits, start=1):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        pipe = clone(pipe_template)\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        y_pred = pipe.predict(X_va)\n",
    "\n",
    "        for (b, m) in pairs:\n",
    "            mask = (X_va[brand_col] == b) & (X_va[model_col] == m)\n",
    "            n = int(mask.sum())\n",
    "            if n == 0:\n",
    "                continue\n",
    "\n",
    "            mae, rmse = mae_rmse(y_va[mask], y_pred[mask])\n",
    "            rows.append({\"fold\": fold, \"brand\": b, \"model\": m, \"MAE\": mae, \"RMSE\": rmse, \"n\": n})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    summary = (\n",
    "        df.groupby([\"brand\", \"model\"])\n",
    "          .apply(lambda g: pd.Series({\n",
    "              \"MAE_mean\": g[\"MAE\"].mean(),\n",
    "              \"MAE_std\":  g[\"MAE\"].std(ddof=0),\n",
    "              \"RMSE_mean\": g[\"RMSE\"].mean(),\n",
    "              \"RMSE_std\":  g[\"RMSE\"].std(ddof=0),\n",
    "              \"n\": int(g[\"n\"].sum()),\n",
    "          }))\n",
    "          .reset_index()\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "\n",
    "def eval_pair_specific(pipe_template, X, y, brand_col, model_col, pairs, splits, min_train_per_fold=40):\n",
    "    \"\"\"\n",
    "    Pair-specific models:\n",
    "    - For each fold and (brand, model): fit the pipeline only on that segment's training rows.\n",
    "    - Evaluate only on that segment's validation rows.\n",
    "\n",
    "    The min_train_per_fold guard is necessary because some folds can have too few samples\n",
    "    even if the pair is frequent overall.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(splits, start=1):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        for (b, m) in pairs:\n",
    "            tr_mask = (X_tr[brand_col] == b) & (X_tr[model_col] == m)\n",
    "            va_mask = (X_va[brand_col] == b) & (X_va[model_col] == m)\n",
    "\n",
    "            n_tr = int(tr_mask.sum())\n",
    "            n_va = int(va_mask.sum())\n",
    "\n",
    "            if n_va == 0 or n_tr < min_train_per_fold:\n",
    "                continue\n",
    "\n",
    "            pipe = clone(pipe_template)\n",
    "            pipe.fit(X_tr[tr_mask], y_tr[tr_mask])\n",
    "            y_pred = pipe.predict(X_va[va_mask])\n",
    "\n",
    "            mae, rmse = mae_rmse(y_va[va_mask], y_pred)\n",
    "            rows.append({\"fold\": fold, \"brand\": b, \"model\": m, \"MAE\": mae, \"RMSE\": rmse, \"n\": n_va})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    summary = (\n",
    "        df.groupby([\"brand\", \"model\"])\n",
    "          .apply(lambda g: pd.Series({\n",
    "              \"MAE_mean\": g[\"MAE\"].mean(),\n",
    "              \"MAE_std\":  g[\"MAE\"].std(ddof=0),\n",
    "              \"RMSE_mean\": g[\"RMSE\"].mean(),\n",
    "              \"RMSE_std\":  g[\"RMSE\"].std(ddof=0),\n",
    "              \"n\": int(g[\"n\"].sum()),\n",
    "          }))\n",
    "          .reset_index()\n",
    "    )\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_list = list(candidate_pairs.index)  # list of (brand, model) tuples\n",
    "\n",
    "df_global_pair = eval_global_by_pair(\n",
    "    pipe_template, X_train, y_train, brand_col, model_col, pairs_list, splits\n",
    ").rename(columns={\n",
    "    \"MAE_mean\": \"MAE_mean_global\", \"MAE_std\": \"MAE_std_global\",\n",
    "    \"RMSE_mean\": \"RMSE_mean_global\", \"RMSE_std\": \"RMSE_std_global\",\n",
    "    \"n\": \"n_global\"\n",
    "})\n",
    "\n",
    "df_pair_spec = eval_pair_specific(\n",
    "    pipe_template, X_train, y_train, brand_col, model_col, pairs_list, splits, min_train_per_fold=40\n",
    ").rename(columns={\n",
    "    \"MAE_mean\": \"MAE_mean_pair\", \"MAE_std\": \"MAE_std_pair\",\n",
    "    \"RMSE_mean\": \"RMSE_mean_pair\", \"RMSE_std\": \"RMSE_std_pair\",\n",
    "    \"n\": \"n_pair\"\n",
    "})\n",
    "\n",
    "df_compare_pair = df_global_pair.merge(df_pair_spec, on=[\"brand\", \"model\"], how=\"inner\")\n",
    "df_compare_pair[\"delta_MAE\"] = df_compare_pair[\"MAE_mean_pair\"] - df_compare_pair[\"MAE_mean_global\"]\n",
    "df_compare_pair[\"delta_RMSE\"] = df_compare_pair[\"RMSE_mean_pair\"] - df_compare_pair[\"RMSE_mean_global\"]\n",
    "\n",
    "df_compare_pair = df_compare_pair.sort_values(\"delta_MAE\")\n",
    "\n",
    "# Full results (all frequent pairs) are here:\n",
    "display(df_compare_pair)\n",
    "\n",
    "# For the report: show the most improved + most harmed (readable subset)\n",
    "display_cols = [\"brand\", \"model\", \"n_global\", \"MAE_mean_global\", \"MAE_mean_pair\", \"delta_MAE\",\n",
    "                \"RMSE_mean_global\", \"RMSE_mean_pair\", \"delta_RMSE\"]\n",
    "\n",
    "print(\"Top 15 improvements (most negative ΔMAE):\")\n",
    "display(df_compare_pair[display_cols].head(15).round(1))\n",
    "\n",
    "print(\"Top 15 degradations (most positive ΔMAE):\")\n",
    "display(df_compare_pair[display_cols].tail(15).round(1))\n",
    "\n",
    "# Plots (same as before): ΔMAE bar plot for stable segments + scatter vs size\n",
    "MIN_PLOT_SAMPLES = 100\n",
    "df_plot = df_compare_pair[df_compare_pair[\"n_global\"] >= MIN_PLOT_SAMPLES].copy()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "x = np.arange(len(df_plot))\n",
    "plt.bar(x, df_plot[\"delta_MAE\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xticks(\n",
    "    x,\n",
    "    [f\"{b} {m}\" for b, m in zip(df_plot[\"brand\"], df_plot[\"model\"])],\n",
    "    rotation=90, ha=\"right\"\n",
    ")\n",
    "plt.ylabel(\"Δ MAE (pair - global)\")\n",
    "plt.title(\"Effect of (brand, model) specialization (negative = improvement)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(df_compare_pair[\"n_global\"], df_compare_pair[\"delta_MAE\"])\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xlabel(\"Number of samples per (brand, model) (out-of-fold counted)\")\n",
    "plt.ylabel(\"Δ MAE (pair - global)\")\n",
    "plt.title(\"ΔMAE vs segment size\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Long Duration (~5min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Price Predictor Web Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ML Cars 4 You` Web Application is implemented in `app.py` and hosted live: \n",
    "\n",
    "👉 **[Click here to open the ML Cars 4 You App](https://ml-cars-4-you.streamlit.app)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### a.) Objective and Motivation\n",
    "\n",
    "**Project Goal**  \n",
    "\n",
    "The objective of this web application is to provide an accessible, user-friendly interface for predicting used car prices based on our best machine learning model. This deployment serves as the practical implementation component of the Open End Section, transforming our trained model into a production-ready tool.\n",
    "\n",
    "**Motivation**\n",
    "- **Accessibility**: Make the ML model accessible to non-technical users through an intuitive web interface\n",
    "- **User Experience**: Provide instant price predictions with intervals to help users make informed decisions\n",
    "\n",
    "**Target Users**\n",
    "- Used car buyers seeking fair price estimates\n",
    "- Car dealers looking for market-competitive pricing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.) Difficulty of Tasks\n",
    "\n",
    "1. Model Size Management (High Difficulty)\n",
    "- **Challenge**: The trained model file (`rf_tuned_pipe.pkl`) was 900 mb, far exceeding GitHub's 100 MB limit\n",
    "- **Solution**: Implemented Git Large File Storage (LFS) to handle the massive model file\n",
    "- **Complexity**: Required understanding of Git LFS, bandwidth limitations, and deployment constraints\n",
    "\n",
    "2. Dynamic Feature Engineering (Medium Difficulty)\n",
    "- **Challenge**: The model uses custom preprocessing functions from `pipeline_functions.py`\n",
    "- **Solution**: Ensured all custom transformers and functions are properly included in the repository\n",
    "- **Complexity**: Maintaining consistency between training pipeline and deployment environment\n",
    "\n",
    "3. Brand-Model Dependency Mapping (Medium-Low Difficulty)\n",
    "- **Challenge**: Creating a user-friendly interface where model selection dynamically updates based on brand choice\n",
    "- **Solution**: Implemented a nested dictionary structure with 9 brands and 114 models\n",
    "- **Complexity**: Balancing comprehensive coverage with UI simplicity\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.) Correctness and Efficiency of Implementation\n",
    "\n",
    "**Accurate Prediction Pipeline**\n",
    "- The `load_model()` function correctly loads the sklearn pipeline\n",
    "- Input data is formatted as a pandas DataFrame with exact feature names and types expected by the model\n",
    "\n",
    "**Input Validation**\n",
    "- Realistic min/max values based on dataset ranges (e.g., year: 1970-2020)\n",
    "- Step increments appropriate for each input type\n",
    "- Default values represent typical vehicles\n",
    "\n",
    "**Performance Optimizations**\n",
    "- `@st.cache_resource` decorator caches the model in memory (loads only once)\n",
    "- Single prediction call per button press (no redundant computations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### d.) Discussion of Results\n",
    "\n",
    "This web application successfully bridges the gap between ML model development and real-world deployment. Despite technical challenges, the implementation achieves its core objectives:\n",
    "\n",
    "- **Accessible**: Anyone can use the tool without ML knowledge  \n",
    "- **Accurate**: MAE of £1,200 provides actionable price estimates  \n",
    "- **Efficient**: Cached model and optimized code ensure fast predictions  \n",
    "\n",
    "The deployment demonstrates practical ML engineering skills including version control (Git LFS), dependency management, cloud deployment (Streamlit Cloud), and user-centric design.  \n",
    "\n",
    "\n",
    "However, this tool has important limitations:\n",
    "\n",
    "- **Brand Bias**: Performance is strongest on well-represented brands while predictions for underrepresented manufacturers may be less reliable\n",
    "- **Temporal Relevance**: The model was trained on data up to 2020 and may not fully capture recent market dynamics such as semiconductor shortages or accelerated EV adoption\n",
    "- **Edge Cases**: Predictions for rare models, unusual configurations, or vehicles at price extremes should be treated with additional caution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Deployment and Prediction on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the notebook includes the deployment to generate reliable predictions for new data. The pipeline is stored in the `et_tuned_pipe.pkl` file and the final output of the predicted test data is stored in `Group05_Version20.csv` (selected as best on Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(et_tuned_pipe, \"et_tuned_pipe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test(model_pipeline, model_name):\n",
    "    # Load best model from Joblib and predict on validation set to verify\n",
    "    pipe_best = joblib.load(model_pipeline)\n",
    "    \n",
    "    # Predict on test set\n",
    "    df_cars_test['price'] = pipe_best.predict(df_cars_test)\n",
    "    df_cars_test[['carID', 'price']].to_csv(f'Group05_Version20.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_test(\"et_tuned_pipe.pkl\", \"ET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c cars4you -f Group05_Version05.csv -m \"Message\" # Uncomment to submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submissions -c cars4you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**     \n",
    "The test score is close to validation score so our model should generalize well, assuming that the data in validation and test is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Discussion and Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree-based models performed best:**     \n",
    "Regarding the baseline models we used in Section 5, it becomes clear that the tree-based models outperform the other models. This is probably due to ...\n",
    "\n",
    "**Constraints:**     \n",
    "However, the nature of tree-based models constraints the predictions to never be lower than the lowest or higher than the highest price in the train set. This is because tree-based models return the average price of the cars in the leaf node.\n",
    "\n",
    "**Outlook:**     \n",
    "A potential solution of this could be a **Stacking Regressor** that combines a tree-based model with another model that is better in extrapolation (e.g. RF + Linear Regression). A final Meta-Learner (also Linear) can combine their predictions. If the RF predicts \"Max Value\" but the Linear Model predicts \"Higher Value,\" the Meta-Learner could follow the Linear trend upward.    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "group05_main_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
