{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a4f9927-9e22-467f-8519-76f417559090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Homework Group 49 - Car Price Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f121b157-912d-4aae-8c5e-effc9abfb30d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Table of content\n",
    "\n",
    "* [0. Importing required libraries](#0-importing-required-libraries)\n",
    "\n",
    "* [1. Loading dataset](#1-loading-dataset)\n",
    "    * [1.1 Loading dataset](#11-loading-dataset)\n",
    "    * [1.2 Streamline column names](#12-streamline-column-names)\n",
    "\n",
    "* [2. Implementing model pipeline components](#2-implementing-model-pipeline-components)\n",
    "    * [2.1 Cleaning and Imputation](#21-cleaning-and-imputation)\n",
    "    * [2.2 Feature Engineering](#22-feature-engineering)\n",
    "    * [2.3 Feature Definition and Scoring Configuration](#23-feature-definition-and-scoring-configuration)\n",
    "\n",
    "* [3. Feature Selection](#3-feature-selection)\n",
    "    * [3.0 Create Baseline Pipeline](#30-create-baseline-pipeline)\n",
    "    * [3.1 Filter Methods](#31-filter-methods)\n",
    "        * [3.1.1 Variance Thresholding](#311-variance-thresholding)\n",
    "        * [3.1.2 Correlations](#312-correlations)\n",
    "        * [3.1.3 F_Regression and Mutual Information](#313-f_regression-and-mutual-information)\n",
    "    * [3.2 Wrapper Methods](#32-wrapper-methods)\n",
    "    * [3.3 Comparing feature selection results](#33-comparing-feature-selection-results)\n",
    "\n",
    "* [4. Model training](#4-model-training)\n",
    "\n",
    "* [5. Tuning Hyperparameters](#5-tuning-hyperparameters)\n",
    "\n",
    "* [6. Training and utilizing final model](#6-training-and-utilizing-final-model)\n",
    "    * [6.1 Training final model](#61-training-final-model)\n",
    "    * [6.2 Making final predictions](#62-making-final-predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab863181-63c4-4c6f-8aed-9cc51e7840a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 0. Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de753579-65b6-4cfa-a462-c91da86a9fc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.model_selection import cross_validate, KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression, mutual_info_regression \n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24fb3b4b-6e79-42e3-8d5d-c2fc75e331d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a05d5389-dc4e-45f2-b2f8-703761efb668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de267b02-eec4-4576-9ccc-c28281e1016d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.1 Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "767cde77-8e53-4593-a7d4-4834e976b0d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"train.csv\")\n",
    "test_raw = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fcebff5-8df6-41e8-a901-a7170cf32cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.2 Streamline column names\n",
    "For better assessment, we standardise the column names of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "75aa6267-49b2-46ac-b8b7-4b124f257cbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_raw = train_raw.rename(\n",
    "    columns={\n",
    "        \"carID\": \"car_id\",\n",
    "        \"Brand\": \"brand\",\n",
    "        \"model\": \"model\",\n",
    "        \"year\": \"year\",\n",
    "        \"price\": \"price\",\n",
    "        \"transmission\": \"transmission\",\n",
    "        \"mileage\": \"mileage\",\n",
    "        \"fuelType\": \"fuel_type\",\n",
    "        \"tax\": \"tax\",\n",
    "        \"mpg\": \"mpg\",\n",
    "        \"engineSize\": \"engine_size\",\n",
    "        \"paintQuality%\": \"paint_quality\",\n",
    "        \"previousOwners\": \"previous_owners\",\n",
    "        \"hasDamage\": \"has_damage\",\n",
    "    }\n",
    ")\n",
    "\n",
    "test_raw = test_raw.rename(\n",
    "    columns={\n",
    "        \"carID\": \"car_id\",\n",
    "        \"Brand\": \"brand\",\n",
    "        \"model\": \"model\",\n",
    "        \"year\": \"year\",\n",
    "        \"price\": \"price\",\n",
    "        \"transmission\": \"transmission\",\n",
    "        \"mileage\": \"mileage\",\n",
    "        \"fuelType\": \"fuel_type\",\n",
    "        \"tax\": \"tax\",\n",
    "        \"mpg\": \"mpg\",\n",
    "        \"engineSize\": \"engine_size\",\n",
    "        \"paintQuality%\": \"paint_quality\",\n",
    "        \"previousOwners\": \"previous_owners\",\n",
    "        \"hasDamage\": \"has_damage\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96025390-6d7b-40df-8465-673d99fd3551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddcf71d6-9ecd-4899-ab74-1353906088f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Implementing model pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4d286c7-31bb-4472-8dc2-67bf40c8c06f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this part, we implemented custom-made pipeline classes. We rely on custom pipeline components because several preprocessing steps are highly domain-specific and must integrate with the pipeline paradigm propossed by sklearn. \n",
    "The pipeline classes are:\n",
    "- CleanerTransformer()\n",
    "- GroupImputer()\n",
    "- FeatureEngineer()\n",
    "- TargetMeanEncoder()\n",
    "\n",
    "By incorporating the pipeline steps, we can ensure that no data leakage will happen during Cross-validation steps or tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6ef5a40-f5f0-4d31-9a6a-8d1894b09fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.1 Cleaning and Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c73bef6c-0fe6-4f2b-9aff-564593d3cafd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Cleaner Transformer**\n",
    "\n",
    "CleanerTransformer standardizes raw car data and removes any logically impossible values before any modeling or imputation. It applies deterministic string normalization and plausibility checks.\n",
    "\n",
    "Purpose: The goal is to make semantically identical cars comparable across records and to remove values that cannot be real (for example negative mileage). This avoids false values in the columns such as misspelled fuel types and avoids unrealistic numeric values leaking into downstream steps.\n",
    "\n",
    "Steps: \n",
    "1. Text normalization: Columns 'brand', 'model', 'transmission', 'fuel_type' are lowercased and stripped of extra whitespace to enable consistent matching.\n",
    "2. String alignment / mapping:\n",
    "    - 'fuel_type' is mapped using a lookup. Common typos are fixed. Values like \"other\", \"unknown\", and \"electric\" are set to NaN because they are either non-informative catch-alls or rare singletons that would create unstable categories.\n",
    "    - 'transmission' is mapped using a lookup. Misspellings are corrected. \"unknown\" and \"other\" are set to NaN.\n",
    "    - 'model' is mapped to a standardized canonical model name (fuzzy model_to_clean).\n",
    "    - 'brand' is recomputed from the cleaned model (model_to_brand_clean) to fix cars that were assigned to the wrong brand.\n",
    "3. Plausibility and logical checks on numerical columns:\n",
    "    - 'year': values > 2020 are set to NaN because the dataset was collected in 2020 so future cars cannot exist. Values with decimals are set to NaN since model years are discrete.\n",
    "    - 'mileage': negative mileage is set to NaN. Mileage entries with decimals are set to NaN because mileage is expected as an integer reading and only a minority of rows use floats.\n",
    "    - 'tax': tax must be non-negative integer. Negative values and decimal values are set to NaN.\n",
    "    - 'mpg': negative mpg is set to NaN. Values with many decimal places are set to NaN.\n",
    "    - 'engine_size': values < 1 liter are set to NaN as implausible for this dataset. Values with many decimal places are set to NaN.\n",
    "    - 'paint_quality': must be a percentage from 0 to 100. Values > 100 or non-integer style decimals are set to NaN.\n",
    "    - 'previous_owners': negative or decimal counts are set to NaN because owner count must be a non-negative integer.\n",
    "4. Column removal: Has damage is dropped because it always has 0 values or NaNs.\n",
    "\n",
    "Notice: After this, a more values have been set to NaN. The later Imputer will handle these cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6502dd1a-fcf2-408e-a192-121be2bd5c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CleanerTransformer(BaseEstimator, TransformerMixin): \n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the CleanerTransformer.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None): \n",
    "        \"\"\"Fit method - does nothing as no fitting is required.\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X): \n",
    "        \"\"\"\n",
    "            This function cleans and standardizes the input DataFrame X.\n",
    "            It performs the following operations:\n",
    "            - Standardizes brand names using a predefined mapping.\n",
    "            - Standardizes model names based on brand-specific mappings.\n",
    "            - Standardizes transmission types.\n",
    "            - Standardizes fuel types.\n",
    "            - Cleans numerical columns by removing invalid or out-of-range values.\n",
    "\n",
    "            Args:\n",
    "                X (pd.DataFrame): Input DataFrame containing car data.\n",
    "            \n",
    "            Returns: \n",
    "                pd.DataFrame: Cleaned and standardized DataFrame.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        df = X.copy()\n",
    "\n",
    "        # Transmission\n",
    "        transmission_alignment = {\n",
    "            # Semi-Automatic\n",
    "            'semi-auto': 'semi-automatic',\n",
    "            'semi-aut': 'semi-automatic',\n",
    "            'emi-auto': 'semi-automatic',\n",
    "            'emi-aut': 'semi-automatic',\n",
    "            # Automatic\n",
    "            'automatic': 'automatic',\n",
    "            'automati': 'automatic',\n",
    "            'utomatic': 'automatic',\n",
    "            'utomati': 'automatic',\n",
    "            # Manual\n",
    "            'manual': 'manual',\n",
    "            'manua': 'manual',\n",
    "            'anual': 'manual',\n",
    "            'anua': 'manual',\n",
    "            # Unknown\n",
    "            'unknown': np.nan,\n",
    "            'unknow': np.nan,\n",
    "            'nknown': np.nan,\n",
    "            'nknow': np.nan,\n",
    "            # Other\n",
    "            'other': np.nan,\n",
    "            # Missing values\n",
    "            np.nan: np.nan\n",
    "        }\n",
    "\n",
    "        # Model\n",
    "        model_mapping = {\n",
    "                    'audi': {\n",
    "                        'a': 'a series (unspecified)',\n",
    "                        'a1': 'a1',\n",
    "                        'a2': 'a2',\n",
    "                        'a3': 'a3',\n",
    "                        'a4': 'a4',\n",
    "                        'a5': 'a5',\n",
    "                        'a6': 'a6',\n",
    "                        'a7': 'a7',\n",
    "                        'a8': 'a8',\n",
    "                        'q': 'q series (unspecified)',\n",
    "                        'q2': 'q2',\n",
    "                        'q3': 'q3',\n",
    "                        'q5': 'q5',\n",
    "                        'q7': 'q7',\n",
    "                        'q8': 'q8',\n",
    "                        'r8': 'r8',\n",
    "                        'rs': 'rs series (unspecified)',\n",
    "                        'rs3': 'rs3',\n",
    "                        'rs4': 'rs4',\n",
    "                        'rs5': 'rs5',\n",
    "                        'rs6': 'rs6',\n",
    "                        'rs7': 'rs7',\n",
    "                        's3': 's3',\n",
    "                        's4': 's4',\n",
    "                        's5': 's5',\n",
    "                        's8': 's8',\n",
    "                        'sq5': 'sq5',\n",
    "                        'sq7': 'sq7',\n",
    "                        't': 'tt',\n",
    "                        'tt': 'tt'\n",
    "                    },\n",
    "                    'bmw': {\n",
    "                        'i': 'i (unspecified)',\n",
    "                        '1 serie': '1 series',\n",
    "                        '1 series': '1 series',\n",
    "                        '2 serie': '2 series',\n",
    "                        '2 series': '2 series',\n",
    "                        '3 serie': '3 series',\n",
    "                        '3 series': '3 series',\n",
    "                        '4 serie': '4 series',\n",
    "                        '4 series': '4 series',\n",
    "                        '5 serie': '5 series',\n",
    "                        '5 series': '5 series',\n",
    "                        '6 serie': '6 series',\n",
    "                        '6 series': '6 series',\n",
    "                        '7 serie': '7 series',\n",
    "                        '7 series': '7 series',\n",
    "                        '8 serie': '8 series',\n",
    "                        '8 series': '8 series',\n",
    "                        'i3': 'i3',\n",
    "                        'i8': 'i8',\n",
    "                        'm': 'm series (unspecified)',\n",
    "                        'm2': 'm2',\n",
    "                        'm3': 'm3',\n",
    "                        'm4': 'm4',\n",
    "                        'm5': 'm5',\n",
    "                        'm6': 'm6',\n",
    "                        'x': 'x series (unspecified)',\n",
    "                        'x1': 'x1',\n",
    "                        'x2': 'x2',\n",
    "                        'x3': 'x3',\n",
    "                        'x4': 'x4',\n",
    "                        'x5': 'x5',\n",
    "                        'x6': 'x6',\n",
    "                        'x7': 'x7',\n",
    "                        'z': 'z series (unspecified)',\n",
    "                        'z3': 'z3',\n",
    "                        'z4': 'z4'\n",
    "                    },\n",
    "                    'ford': {\n",
    "                        'amica': np.nan,\n",
    "                        'b-ma': 'b-max',\n",
    "                        'b-max': 'b-max',\n",
    "                        'c-ma': 'c-max',\n",
    "                        'c-max': 'c-max',\n",
    "                        'ecospor': 'ecosport',\n",
    "                        'ecosport': 'ecosport',\n",
    "                        'edg': 'edge',\n",
    "                        'edge': 'edge',\n",
    "                        'escort': 'escort',\n",
    "                        'fiest': 'fiesta',\n",
    "                        'fiesta': 'fiesta',\n",
    "                        'focu': 'focus',\n",
    "                        'focus': 'focus',\n",
    "                        'fusion': 'fusion',\n",
    "                        'galax': 'galaxy',\n",
    "                        'galaxy': 'galaxy',\n",
    "                        'grand c-ma': 'grand c-max',\n",
    "                        'grand c-max': 'grand c-max',\n",
    "                        'grand tourneo connec': 'grand tourneo connect',\n",
    "                        'grand tourneo connect': 'grand tourneo connect',\n",
    "                        'k': 'ka',\n",
    "                        'ka': 'ka',\n",
    "                        'ka+': 'ka+',\n",
    "                        'kug': 'kuga',\n",
    "                        'kuga': 'kuga',\n",
    "                        'monde': 'mondeo',\n",
    "                        'mondeo': 'mondeo',\n",
    "                        'mustang': 'mustang',\n",
    "                        'puma': 'puma',\n",
    "                        'pum': 'puma',\n",
    "                        'ranger': 'ranger',\n",
    "                        's-ma': 's-max',\n",
    "                        's-max': 's-max',\n",
    "                        'streetka': 'streetka',\n",
    "                        'tourneo connect': 'tourneo connect',\n",
    "                        'tourneo custo': 'tourneo custom',\n",
    "                        'tourneo custom': 'tourneo custom',\n",
    "                        'turneo custom': 'tourneo custom',\n",
    "                        'transit tourneo': np.nan\n",
    "                    },\n",
    "                    'hyundai': {\n",
    "                        'accent': 'accent',\n",
    "                        'getz': 'getz',\n",
    "                        'i1': 'i10',\n",
    "                        'i10': 'i10',\n",
    "                        'i2': 'i20',\n",
    "                        'i20': 'i20',\n",
    "                        'i3': 'i30',\n",
    "                        'i4': np.nan,\n",
    "                        'i30': 'i30',\n",
    "                        'i40': 'i40',\n",
    "                        'i80': 'i800',\n",
    "                        'i800': 'i800',\n",
    "                        'ioni': 'ioniq',\n",
    "                        'ioniq': 'ioniq',\n",
    "                        'ix2': 'ix20',\n",
    "                        'ix20': 'ix20',\n",
    "                        'ix35': 'ix35',\n",
    "                        'kon': 'kona',\n",
    "                        'kona': 'kona',\n",
    "                        'santa f': 'santa fe',\n",
    "                        'santa fe': 'santa fe',\n",
    "                        'terracan': 'terracan',\n",
    "                        'tucso': 'tucson',\n",
    "                        'tucson': 'tucson',\n",
    "                        'veloste': 'veloster'\n",
    "                    },\n",
    "                    'mercedes': {\n",
    "                        '180': np.nan,\n",
    "                        '200': '200',\n",
    "                        '220': '220',\n",
    "                        '230': '230',\n",
    "                        'a clas': 'a class',\n",
    "                        'a class': 'a class',\n",
    "                        'b clas': 'b class',\n",
    "                        'b class': 'b class',\n",
    "                        'c clas': 'c class',\n",
    "                        'c class': 'c class',\n",
    "                        'cl clas': 'cl class',\n",
    "                        'cl class': 'cl class',\n",
    "                        'cla class': 'cla class',\n",
    "                        'cla clas': 'cla class',\n",
    "                        'clc class': 'clc class',\n",
    "                        'clk': 'clk',\n",
    "                        'cls clas': 'cls class',\n",
    "                        'cls class': 'cls class',\n",
    "                        'e clas': 'e class',\n",
    "                        'e class': 'e class',\n",
    "                        'g class': 'g class',\n",
    "                        'g clas': 'g class',\n",
    "                        'gl class': 'gl class',\n",
    "                        'gl clas': 'gl class',\n",
    "                        'gla clas': 'gla class',\n",
    "                        'gla class': 'gla class',\n",
    "                        'glb class': 'glb class',\n",
    "                        'glc clas': 'glc class',\n",
    "                        'glc class': 'glc class',\n",
    "                        'gle clas': 'gle class',\n",
    "                        'gle class': 'gle class',\n",
    "                        'gls clas': 'gls class',\n",
    "                        'gls class': 'gls class',\n",
    "                        'm clas': 'm class',\n",
    "                        'm class': 'm class',\n",
    "                        'r class': np.nan,\n",
    "                        's clas': 's class',\n",
    "                        's class': 's class',\n",
    "                        'sl': 'sl class',\n",
    "                        'sl clas': 'sl class',\n",
    "                        'sl class': 'sl class',\n",
    "                        'slk': 'slk',\n",
    "                        'v clas': 'v class',\n",
    "                        'v class': 'v class',\n",
    "                        'x-clas': 'x class',\n",
    "                        'x-class': 'x class'\n",
    "                    },\n",
    "                    'opel': {\n",
    "                        'ada': 'adam',\n",
    "                        'adam': 'adam',\n",
    "                        'agila': 'agila',\n",
    "                        'ampera': 'ampera',\n",
    "                        'antara': 'antara',\n",
    "                        'astr': 'astra',\n",
    "                        'astra': 'astra',\n",
    "                        'cascada': 'cascada',\n",
    "                        'combo lif': 'combo life',\n",
    "                        'combo life': 'combo life',\n",
    "                        'cors': 'corsa',\n",
    "                        'corsa': 'corsa',\n",
    "                        'crossland': 'crossland',\n",
    "                        'crossland x': 'crossland x',\n",
    "                        'grandland': 'grandland x',\n",
    "                        'grandland x': 'grandland x',\n",
    "                        'gtc': 'gtc',\n",
    "                        'insigni': 'insignia',\n",
    "                        'insignia': 'insignia',\n",
    "                        'meriv': 'meriva',\n",
    "                        'meriva': 'meriva',\n",
    "                        'mokk': 'mokka',\n",
    "                        'mokka': 'mokka',\n",
    "                        'mokka x': 'mokka x',\n",
    "                        'tigra': 'tigra',\n",
    "                        'vectra': 'vectra',\n",
    "                        'viv': 'viva',\n",
    "                        'viva': 'viva',\n",
    "                        'vivaro': 'vivaro',\n",
    "                        'zafir': 'zafira',\n",
    "                        'zafira': 'zafira',\n",
    "                        'zafira toure': 'zafira tourer',\n",
    "                        'zafira tourer': 'zafira tourer',\n",
    "                        'kadjar': np.nan\n",
    "                    },\n",
    "                    'skoda': {\n",
    "                        'citig': 'citigo',\n",
    "                        'citigo': 'citigo',\n",
    "                        'fabi': 'fabia',\n",
    "                        'fabia': 'fabia',\n",
    "                        'kami': 'kamiq',\n",
    "                        'kamiq': 'kamiq',\n",
    "                        'karo': 'karoq',\n",
    "                        'karoq': 'karoq',\n",
    "                        'kodia': 'kodiaq',\n",
    "                        'kodiaq': 'kodiaq',\n",
    "                        'octavi': 'octavia',\n",
    "                        'octavia': 'octavia',\n",
    "                        'rapi': 'rapid',\n",
    "                        'rapid': 'rapid',\n",
    "                        'roomste': 'roomster',\n",
    "                        'roomster': 'roomster',\n",
    "                        'scal': 'scala',\n",
    "                        'scala': 'scala',\n",
    "                        'super': 'superb',\n",
    "                        'superb': 'superb',\n",
    "                        'yet': 'yeti',\n",
    "                        'yeti': 'yeti',\n",
    "                        'yeti outdoo': 'yeti outdoor',\n",
    "                        'yeti outdoor': 'yeti outdoor'\n",
    "                    },\n",
    "                    'toyota': {\n",
    "                        'auri': 'auris',\n",
    "                        'auris': 'auris',\n",
    "                        'avensis': 'avensis',\n",
    "                        'ayg': 'aygo',\n",
    "                        'aygo': 'aygo',\n",
    "                        'c-h': 'c-hr',\n",
    "                        'c-hr': 'c-hr',\n",
    "                        'camry': 'camry',\n",
    "                        'cam': 'camry',\n",
    "                        'camr': 'camry',\n",
    "                        'coroll': 'corolla',\n",
    "                        'corolla': 'corolla',\n",
    "                        'gt86': 'gt86',\n",
    "                        'hilu': 'hilux',\n",
    "                        'hilux': 'hilux',\n",
    "                        'iq': 'iq',\n",
    "                        'land cruise': 'land cruiser',\n",
    "                        'land cruiser': 'land cruiser',\n",
    "                        'prius': 'prius',\n",
    "                        'proace verso': 'proace verso',\n",
    "                        'rav': 'rav4',\n",
    "                        'rav4': 'rav4',\n",
    "                        'supra': 'supra',\n",
    "                        'urban cruise': 'urban cruiser',\n",
    "                        'urban cruiser': 'urban cruiser',\n",
    "                        'vers': 'verso',\n",
    "                        'verso': 'verso',\n",
    "                        'verso-s': 'verso-s',\n",
    "                        'yari': 'yaris',\n",
    "                        'yaris': 'yaris'\n",
    "                    },\n",
    "                    'volkswagen': {\n",
    "                        'amaro': 'amarok',\n",
    "                        'amarok': 'amarok',\n",
    "                        'arteo': 'arteon',\n",
    "                        'arteon': 'arteon',\n",
    "                        'beetl': 'beetle',\n",
    "                        'beetle': 'beetle',\n",
    "                        'caddy': 'caddy',\n",
    "                        'caddy life': 'caddy life',\n",
    "                        'caddy maxi': 'caddy maxi',\n",
    "                        'caddy maxi lif': 'caddy maxi life',\n",
    "                        'caddy maxi life': 'caddy maxi life',\n",
    "                        'california': 'california',\n",
    "                        'californi': 'california',\n",
    "                        'caravell': 'caravelle',\n",
    "                        'caravelle': 'caravelle',\n",
    "                        'cc': 'cc',\n",
    "                        'eos': 'eos',\n",
    "                        'fox': 'fox',\n",
    "                        'gol': 'golf',\n",
    "                        'golf': 'golf',\n",
    "                        'golf s': 'golf sv',\n",
    "                        'golf sv': 'golf sv',\n",
    "                        'jetta': 'jetta',\n",
    "                        'passa': 'passat',\n",
    "                        'passat': 'passat',\n",
    "                        'pol': 'polo',\n",
    "                        'polo': 'polo',\n",
    "                        'scirocc': 'scirocco',\n",
    "                        'scirocco': 'scirocco',\n",
    "                        'shara': 'sharan',\n",
    "                        'sharan': 'sharan',\n",
    "                        'shuttle': 'shuttle',\n",
    "                        't-cros': 't-cross',\n",
    "                        't-cross': 't-cross',\n",
    "                        't-ro': 't-roc',\n",
    "                        't-roc': 't-roc',\n",
    "                        'tigua': 'tiguan',\n",
    "                        'tiguan': 'tiguan',\n",
    "                        'tiguan allspac': 'tiguan allspace',\n",
    "                        'tiguan allspace': 'tiguan allspace',\n",
    "                        'touare': 'touareg',\n",
    "                        'touareg': 'touareg',\n",
    "                        'toura': 'touran',\n",
    "                        'touran': 'touran',\n",
    "                        'u': 'up',\n",
    "                        'up': 'up'\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Fuel_type\n",
    "        alignment_fuel = {\n",
    "            # Petrol\n",
    "            'petrol': 'petrol',\n",
    "            'petro': 'petrol',\n",
    "            'etrol': 'petrol',\n",
    "            'etro': 'petrol',\n",
    "\n",
    "            # Diesel\n",
    "            'diesel': 'diesel',\n",
    "            'diese': 'diesel',\n",
    "            'iesel': 'diesel',\n",
    "            'iese': 'diesel',\n",
    "\n",
    "            # Hybrid\n",
    "            'hybrid': 'hybrid',\n",
    "            'hybri': 'hybrid',\n",
    "            'ybrid': 'hybrid',\n",
    "            'ybri': 'hybrid',\n",
    "\n",
    "            # Electric\n",
    "            'electric': np.nan,\n",
    "\n",
    "            # Other\n",
    "            'other': np.nan,\n",
    "            'othe': np.nan,\n",
    "            'ther': np.nan,\n",
    "\n",
    "            # Missing values\n",
    "            np.nan: np.nan\n",
    "        }\n",
    "\n",
    "        for col in ['brand', 'model', 'transmission', 'fuel_type']:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "\n",
    "        df['fuel_type'] = df['fuel_type'].replace(alignment_fuel)\n",
    "        df['transmission'] = df['transmission'].replace(transmission_alignment)\n",
    "\n",
    "        model_to_brand = {}\n",
    "        model_to_clean = {}\n",
    "\n",
    "        for brand, models in model_mapping.items():\n",
    "            for key, value in models.items():\n",
    "                model_to_brand[key] = brand\n",
    "                model_to_clean[key] = value\n",
    "\n",
    "        df['model'] = df['model'].map(model_to_clean)\n",
    "\n",
    "        model_to_brand_clean = {}\n",
    "        for brand, model in model_mapping.items():\n",
    "            for key, value in model.items():\n",
    "                if pd.notna(value):\n",
    "                    model_to_brand_clean[value] = brand\n",
    "\n",
    "        df['brand'] = df['model'].map(model_to_brand_clean)\n",
    "\n",
    "\n",
    "        ############################################################################################################################################\n",
    "\n",
    "        # year\n",
    "        df.loc[df[\"year\"] > 2020, \"year\"] = np.nan\n",
    "        df.loc[df[\"year\"].astype(str).str.rstrip(\"0\").str.contains(r\"\\.\\d+\"), \"year\"] = (np.nan)\n",
    "\n",
    "        # mileage\n",
    "        df.loc[df[\"mileage\"] < 0, \"mileage\"] = np.nan\n",
    "\n",
    "        df.loc[df[\"mileage\"].astype(str).str.rstrip(\"0\").str.contains(r\"\\.\\d+\"), \"mileage\"] = np.nan\n",
    "\n",
    "        # tax\n",
    "        df.loc[df[\"tax\"] < 0, \"tax\"] = np.nan\n",
    "        df.loc[df[\"tax\"].astype(str).str.rstrip(\"0\").str.contains(r\"\\.\\d+\"), \"tax\"] = np.nan\n",
    "\n",
    "        # mpg\n",
    "        df.loc[df[\"mpg\"] < 0, \"mpg\"] = np.nan\n",
    "        df.loc[df[\"mpg\"].astype(str).str.rstrip(\"0\").str.contains(r\"\\.\\d{2,}\"), \"mpg\"] = (np.nan)\n",
    "\n",
    "        # engine size\n",
    "        df.loc[df[\"engine_size\"] < 1, \"engine_size\"] = np.nan\n",
    "        df.loc[df[\"engine_size\"].astype(str).str.rstrip(\"0\").str.contains(r\"\\.\\d{2,}\"), \"engine_size\", ] = np.nan\n",
    "\n",
    "        # paint quality\n",
    "        df.loc[df[\"paint_quality\"] > 100, \"paint_quality\"] = np.nan\n",
    "        df.loc[df[\"paint_quality\"].astype(str).str.rstrip(\"0\").str.contains(r\"\\.\\d+\"), \"paint_quality\", ] = np.nan\n",
    "\n",
    "        # Previous owners\n",
    "        df.loc[df[\"previous_owners\"] < 0, \"previous_owners\"] = np.nan  \n",
    "        df.loc[df[\"previous_owners\"].astype(str).str.rstrip(\"0\").str.contains(r\"\\.\\d+\"), \"previous_owners\", ] = np.nan\n",
    "\n",
    "        # has_damage\n",
    "        df.drop(columns=[\"has_damage\"], inplace=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3334fa5-83b8-47f0-8bf5-90d67ae56c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**GroupImputer**\n",
    "\n",
    "The GroupImputer class imputes missing categorical and numerical values using subgroup based statisics instead of global ones. It preserves within-group consistency such as brand–model relationships and avoids loss of rows due to missing data.\n",
    "\n",
    "Purpose: The transformer fills NaN values in both categorical and numerical features by using learned medians or modes within logical subgroups. This creates more realistic imputations that respect the internal structure of the dataset. It is applied after CleanerTransformer to ensure standardized group identifiers.\n",
    "\n",
    "Steps: \n",
    "1. Fit Phase:\n",
    "    - For each target column in cat_specs and num_specs, compute the lookup tables based in hierarchical groupings.\n",
    "    - Categorical variables use the mode (most frequent value) per group.\n",
    "    - Numerical variables use the median per group.\n",
    "    - Global fallback statistics (overall mode or median) are stored per column. \n",
    "2. Transform Phase:\n",
    "    - Sequentially apply lookup tables from most specific to most general grouping.\n",
    "    - For each missing value, search for matches in the hierarchy.\n",
    "    - If no match found, use the global fallback.\n",
    "    - Columns defined in int_cols are converted back to integer type.\n",
    "\n",
    "Data Leakage Prevention: Looup values and fallback values are learned during fitting on the training fold (data without the NaN rows). This is then applied to non Nan rows, preventing data leakage.\n",
    "\n",
    "Notice: Must run after CleanerTransformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eeef6ca-98f9-4852-a731-c6f69c946f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class GroupImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cat_specs = [], num_specs = [], int_cols = None):\n",
    "        # cat_specs: Dictionary mapping categorical columns to their grouping hierarchies\n",
    "        # num_specs: Dictionary mapping numerical columns to their grouping hierarchies\n",
    "        # int_cols: List of columns that should be treated as integers after imputation\n",
    "        # tables_: Learned lookup tables that is filled during fit\n",
    "        # fallbacks_: Global statistics for each column that is filled during fit\n",
    "        self.cat_specs = cat_specs\n",
    "        self.num_specs = num_specs\n",
    "        self.int_cols = int_cols\n",
    "        self.tables_ = {}\n",
    "        self.fallbacks_ = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _series_mode(s):\n",
    "        \"\"\"\n",
    "        Helper function to compute the mode (most frequent non-NA value) of a pandas Series.\n",
    "\n",
    "        Args:\n",
    "            s (pandas.Series): Input series for which the mode should be calculated.\n",
    "\n",
    "        Return: \n",
    "            scalar or numpy.nan: The most frequent non-missing value in the series. If multiple modes exist, the first one is returned. \n",
    "            If the series is empty or contains only missing values, return numpy.nan.\n",
    "        \"\"\"\n",
    "        \n",
    "        mode = s.mode(dropna=True)\n",
    "        \n",
    "        if mode.empty:\n",
    "            return np.nan\n",
    "        else:  \n",
    "            return mode.iloc[0]\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _reduce_by_groups(df, target, groups, reducer):\n",
    "        \"\"\"\n",
    "        Creates lookup tables for imputation by aggregating values across different grouping combinations.\n",
    "        \n",
    "        The function iterates through various grouping column combinations and computes an aggregated\n",
    "        value (mode or median) of the target variable for each group. Only rows without missing values\n",
    "        in both the grouping columns and the target variable are used for aggregation.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing the data\n",
    "            target: Name of the column for which values should be aggregated\n",
    "            groups: List of column combinations to use for grouping\n",
    "            reducer: Aggregation method ('mode' for categorical, 'median' for numerical data)\n",
    "        \n",
    "        Returns:\n",
    "            List of tuples, where each tuple contains:\n",
    "            - List of grouping columns used\n",
    "            - DataFrame with aggregated values as lookup table\n",
    "        \"\"\"\n",
    "        if target not in df.columns:\n",
    "            return []\n",
    "        \n",
    "        out = []\n",
    "        \n",
    "        for cols in groups:\n",
    "            grouping_cols = []\n",
    "            \n",
    "            for c in cols:\n",
    "                if c in df.columns and c != target:\n",
    "                    grouping_cols.append(c)\n",
    "            \n",
    "            if len(grouping_cols) == 0:\n",
    "                continue\n",
    "            \n",
    "            columns_to_check = grouping_cols + [target]\n",
    "            valid_rows = df.dropna(subset=columns_to_check)\n",
    "            \n",
    "            if valid_rows.empty:\n",
    "                continue\n",
    "            \n",
    "            if reducer == \"mode\":\n",
    "                agg = valid_rows.groupby(grouping_cols)[target].agg(GroupImputer._series_mode)\n",
    "            else:\n",
    "                agg = valid_rows.groupby(grouping_cols)[target].median()\n",
    "            \n",
    "            agg = agg.dropna()\n",
    "            \n",
    "            if isinstance(agg, pd.Series):\n",
    "                agg = agg.to_frame(name=target)\n",
    "            \n",
    "            if agg.empty:\n",
    "                continue\n",
    "            \n",
    "            out.append((grouping_cols, agg.reset_index()))\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _apply_tables_inplace(df, target, tables, fallback):\n",
    "        \"\"\"\n",
    "        Applies lookup tables to impute missing values in the target column in-place.\n",
    "        \n",
    "        The function iterates through the provided lookup tables and fills missing values \n",
    "        in the target column by merging rows with matching grouping columns. If multiple \n",
    "        tables are provided, they are applied sequentially until no more missing values \n",
    "        can be filled. Finally, any remaining missing values are replaced with the fallback value.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to be modified\n",
    "            target: Name of the column for which missing values should be imputed\n",
    "            tables: List of tuples containing grouping columns and lookup tables\n",
    "            fallback: Default value to fill remaining missing values after table application\n",
    "        \n",
    "        Returns:\n",
    "            None (DataFrame is modified in-place)\n",
    "        \"\"\"\n",
    "\n",
    "        if target not in df.columns:\n",
    "            return\n",
    "        \n",
    "        for grouping_cols, lookup_table in tables:\n",
    "            \n",
    "            missing_target_mask = df[target].isna()\n",
    "            \n",
    "            if not missing_target_mask.any():\n",
    "                break\n",
    "            \n",
    "            eligible_rows = missing_target_mask\n",
    "            \n",
    "            for col in grouping_cols:\n",
    "                if col not in df.columns:\n",
    "                    eligible_rows = eligible_rows & False\n",
    "                else:\n",
    "                    eligible_rows = eligible_rows & df[col].notna()\n",
    "            \n",
    "            if not eligible_rows.any():\n",
    "                continue\n",
    "            \n",
    "            rows_to_fill = df.loc[eligible_rows, grouping_cols]\n",
    "            \n",
    "            merged_data = rows_to_fill.merge(lookup_table, how=\"left\", on=grouping_cols)\n",
    "            merged_data.index = rows_to_fill.index\n",
    "            \n",
    "            successfully_filled = merged_data.index[merged_data[target].notna()]\n",
    "            \n",
    "            if len(successfully_filled) == 0:\n",
    "                continue\n",
    "            \n",
    "            df.loc[successfully_filled, target] = merged_data.loc[successfully_filled, target].values\n",
    "        \n",
    "        if fallback is not None:\n",
    "            df[target] = df[target].fillna(fallback)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn group-based imputation tables and fallback values from the given DataFrame.\n",
    "\n",
    "        The method builds lookup tables for categorical and numerical columns. The columns are later specified in\n",
    "        `cat_specs` and `num_specs`. Each table stores aggregated values (either mode or median)\n",
    "        of the target column for various grouping combinations. These tables are later\n",
    "        used by `transform` to impute missing values.\n",
    "        \n",
    "        Args: \n",
    "            X: Input DataFrame containing the data to learn from.\n",
    "            y: Not used but necessary for sklearns TransformerMixMin.\n",
    "        \n",
    "        Returns: \n",
    "            self: Instance of fitted GroupImputer.\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.copy()\n",
    "        self.tables_ = {}\n",
    "        self.fallbacks_ = {}\n",
    "\n",
    "        def _fit_block(specs, reducer, fb_func):\n",
    "            for tgt, groups in specs.items():\n",
    "                tables = self._reduce_by_groups(X, tgt, groups, reducer)\n",
    "                if tgt in X.columns:\n",
    "                    fb = fb_func(X[tgt])\n",
    "                else:\n",
    "                    fb = None\n",
    "                self.tables_[tgt] = tables\n",
    "                self.fallbacks_[tgt] = fb\n",
    "\n",
    "        _fit_block(self.cat_specs, \"mode\", self._series_mode)\n",
    "        _fit_block(self.num_specs, \"median\", lambda s: s.median() if s.notna().any() else None)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Impute missing values in a DataFrame using group-based lookup tables and fallback values.\n",
    "\n",
    "        This method applies the lookup tables and global fallback values learned during `fit()`\n",
    "        to fill missing entries in each target column defined in `cat_specs` and `num_specs`.\n",
    "        For each column, the method sequentially uses the stored lookup tables—from the most\n",
    "        specific to the most general grouping—to infer missing values. If no match is found in\n",
    "        any table, a global fallback value is used.\n",
    "\n",
    "        Example: \n",
    "            Hierarchical order: \n",
    "            [\"transmission\", \"fuel_type\", \"tax\", \"year\", \"engine_size\"],\n",
    "            [\"year\", \"engine_size\", \"tax\"],                             \n",
    "            [\"engine_size\", \"tax\"],\n",
    "            [\"engine_size\"],\n",
    "            [\"tax\"],\n",
    "            [\"year\"],\n",
    "            \n",
    "            Filling the brand and model: Search for cars with identical transmission, fuel type, tax, year, and engine size -> fill with most frequent value in this group\n",
    "            If no match found, search for cars with identical year, engine size, and tax -> fill with most frequent value in this group\n",
    "            ...\n",
    "            \n",
    "\n",
    "        Args: \n",
    "            X: Input data containing missing values to impute.\n",
    "\n",
    "        Returns: \n",
    "            df: Input DataFrame with missing values imputed.\n",
    "        \"\"\"\n",
    "        \n",
    "        df = X.copy()\n",
    "        for tgt, tables in self.tables_.items():\n",
    "            fb = self.fallbacks_.get(tgt, None)\n",
    "            self._apply_tables_inplace(df, tgt, tables, fb)\n",
    "\n",
    "        if self.int_cols is not None:\n",
    "            for c in self.int_cols:\n",
    "                if c in df.columns:\n",
    "                    df[c] = pd.to_numeric(df[c], errors=\"coerce\").round().astype(\"Int64\")\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36deea3a-0275-4664-9839-7993edf3e74f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Define groups for imputation**\n",
    "\n",
    "When filling in missing values, we group similar observations so that imputed values come from comparable data points instead of the whole dataset.\n",
    "\n",
    "For example consider the car Audi Q7. One version might be manual, diesel, from 2010, with small engine and lower tax. Another Audi Q7 might be automatic, petrol, from 2019, with a larger engine and higher tax. We would expect the second car to be more expensive than the first. To account for this, we use group imputings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6ebffef-a359-4d4f-9c4b-0d7892355425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BRAND_MODEL_GROUPS = [\n",
    "    [\"transmission\", \"fuel_type\", \"tax\", \"year\", \"engine_size\"],\n",
    "    [\"year\", \"engine_size\", \"tax\"],\n",
    "    [\"engine_size\", \"tax\"],\n",
    "    [\"engine_size\"],\n",
    "    [\"tax\"],\n",
    "    [\"year\"],\n",
    "]\n",
    "\n",
    "TRANS_FUEL_GROUPS = [\n",
    "    [\"brand\", \"model\", \"engine_size\", \"tax\", \"year\"],\n",
    "    [\"brand\", \"model\", \"engine_size\", \"tax\"],\n",
    "    [\"brand\", \"model\", \"engine_size\"],\n",
    "    [\"brand\", \"model\"],\n",
    "    [\"brand\"],\n",
    "]\n",
    "\n",
    "NUMERIC_GROUPS = [\n",
    "    [\"brand\", \"model\", \"transmission\", \"fuel_type\", \"tax\", \"year\", \"engine_size\"],\n",
    "    [\"brand\", \"model\", \"transmission\", \"fuel_type\", \"year\", \"engine_size\"],\n",
    "    [\"brand\", \"model\", \"fuel_type\", \"year\", \"engine_size\"],\n",
    "    [\"brand\", \"model\", \"engine_size\", \"year\"],\n",
    "    [\"brand\", \"model\", \"engine_size\"],\n",
    "    [\"brand\", \"model\"],\n",
    "    [\"brand\"],\n",
    "]\n",
    "\n",
    "NUMERIC_COLUMNS = [\n",
    "    \"mpg\",\n",
    "    \"tax\",\n",
    "    \"previous_owners\",\n",
    "    \"engine_size\",\n",
    "    \"paint_quality\",\n",
    "    \"mileage\",\n",
    "    \"year\",\n",
    "]\n",
    "\n",
    "cat_specs = {\n",
    "    \"brand\": BRAND_MODEL_GROUPS,\n",
    "    \"model\": BRAND_MODEL_GROUPS,\n",
    "    \"transmission\": TRANS_FUEL_GROUPS,\n",
    "    \"fuel_type\": TRANS_FUEL_GROUPS\n",
    "}\n",
    "\n",
    "num_specs = {col: NUMERIC_GROUPS for col in NUMERIC_COLUMNS}\n",
    "\n",
    "int_cols_for_imputer = [\"year\", \"previous_owners\", \"tax\", \"paint_quality\", \"mileage\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29b99f4b-e1a4-48b7-ad7b-3e5e4e977d88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "614c2326-763e-4750-bcd2-b289985e1432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**FeatureEngineer**\n",
    "\n",
    "The FeatureEngineer class creates additionally derived features that enhance the model interpretability and predicitve power. It converts existing raw variables into more meaningful representations of car characteristics and relationships.\n",
    "\n",
    "Purpose: The class automatically generates engineered features inside the pipeline to maintain strict isolation.\n",
    "\n",
    "Steps: \n",
    "1. Store reference year(2020) during fitting.\n",
    "2. Derive new features during transformation:\n",
    "    - vehicle_age: This feature is used to quantify the depreciation of a car. As the dataset is from 2020, the age of the vehicle is measured from this point on. We hope that this feature represents the fact that newer vehicles are potentially higher priced than older ones. \n",
    "    - mileage_per_year: This feature is used to normalize the total mileage by vehicle age to reveal annual usage intensity. \n",
    "    - tax_per_engine: This feature quantifies the tax burden relative to the engine size. Effectively, the taxation efficiency of the vehicle is captured. We hope, that the model can identify vehicle that might be expensive to own and operate, as this can affect their market value.\n",
    "    - mpg_per_engine: This feature measures the fuel efficiency normalized by engine size. It helps distinguishing between large engines that are relative efficient and smaller engines that underperform. \n",
    "    - log_mileage: This feature applies a logarithmic transformation to the mileage, compressing the scale and reducing the impact of extreme outlier. \n",
    "    - log_tax: This feature applies a logarithmic transformation to the tax, compressing the scale and reducing the impact of extreme outlier.\n",
    "    - efficiency_index: This feature combines mpg and engine size to create a composite measure of overall engine efficiency. It helps to identify cars that deliver high performance with lower fuel consumption.\n",
    "    - is_premium_brand: This feature binary encodes the brands (audi, bmw, mercedes). It helps identify brands that are typically luxurious and more expensive.\n",
    "\n",
    "Notice: The constant 0.00001 avoids division by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b9af7097-3a7c-4791-a809-11be17ca9dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method - only the reference year is stored.\n",
    "        \"\"\" \n",
    "        self.ref_year_ = 2020\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df): \n",
    "        \"\"\"\n",
    "        This function creates new features based on the existing columns\n",
    "        \n",
    "        Args: \n",
    "            X: Input DataFrame\n",
    "        \n",
    "        Returns: \n",
    "            df: DataFrame with new features added.    \n",
    "        \"\"\"\n",
    "        \n",
    "        df = df.copy()\n",
    "\n",
    "        df_year = df['year']\n",
    "        df['vehicle_age'] = (self.ref_year_ - df_year).clip(lower=0)\n",
    "        df['mileage_per_year'] = df['mileage'] / (df['vehicle_age']+0.00001)\n",
    "        df['tax_per_engine'] = df['tax'] / (df['engine_size']+0.00001)\n",
    "        df['mpg_per_engine'] = df['mpg'] / (df['engine_size']+0.00001)\n",
    "        df['log_mileage'] = np.log1p(df['mileage'])\n",
    "        df['log_tax'] = np.log1p(df['tax'])\n",
    "        df['efficiency_index'] = df['engine_size'] / (df['mpg']+0.00001)\n",
    "        \n",
    "        premium_brands = ['audi', 'bmw', 'mercedes']\n",
    "        df['is_premium_brand'] = df['brand'].isin(premium_brands).astype(int)\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb32e99d-72c3-4c5a-baa4-05ad9d89fb8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**TargetMeanEncoder**\n",
    "\n",
    "The TargetMeanEncoder class replaces categorical values with the mean of the target variable (price) for each category. It handles high-cardinality categorical features efficiently by encoding them into single numerical columns instead of many one-hot columns.\n",
    "\n",
    "Purpose: This transformer allows the model to capture relationships between category levels and the target variable, improving predictive strength while avoiding dimensionality explosion. It is particularly effective for columns such as brand or model that contain many unique values.\n",
    "\n",
    "Steps: \n",
    "1. Fit Phase: For each column, compute the mean of the target grouped by category and store them as fallback.\n",
    "2. Fit-transform Phase (out-of-fold encoding):\n",
    "    - Split the data into K folds using cross-validation\n",
    "    - For each fold, compute category means using only the training data and apply them to the validation data.\n",
    "    - Combine the fold ouputs into one out-of-fold dataset.\n",
    "    - Refit on the full dataset to store final category mappings and global means.\n",
    "3. Transform Phase:\n",
    "    - Replace each category with its stored mean from trainig.\n",
    "    - Replace unseen categories with global mean.\n",
    "\n",
    "Data Leakage Prevention: Out-of-fold encoding prevents leakage by ensuring target means for validation rows are computed only for training rows in each fold. The model never learns its own target during encoding. When applied to test data, mappings are fixed and use only statistics learned from training. \n",
    "\n",
    "Notice: Fit() should only be called through fit_transform(); never run fit() alone. The global mean is only stored for inference on unseen data and does not cause leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b299b2e-dafb-4bf4-9173-6129ae944d3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TargetMeanEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, cols, n_splits=5, random_state=0):\n",
    "        \"\"\"\n",
    "        Initialization and configure the encoder by specify categorical columns to encode and cross validation parameters \n",
    "    \n",
    "        Args: \n",
    "            cols: List of categorical columns to be target mean encoded\n",
    "            n_splits: Number of splits for K-Fold cross validation (default=5)\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.cols = cols\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes target mean mappings for the specified categorical columns.\n",
    "\n",
    "        This method trains the encoder by calculating the mean of the target variable\n",
    "        for each category in the given columns. The computed mappings are stored\n",
    "        for later use during transformation.\n",
    "\n",
    "        Args:\n",
    "            X: Input data containing the categorical columns to be encoded.\n",
    "            y: Target variable used to compute mean values per category.\n",
    "\n",
    "        Returns:\n",
    "            self: Fitted TargetMeanEncoder instance.\n",
    "        \"\"\"\n",
    "        X = pd.DataFrame(X).reset_index(drop=True)\n",
    "        y = pd.Series(y).reset_index(drop=True)\n",
    "        self.col_maps = {}\n",
    "        self.global_means = {}\n",
    "        for column in self.cols:\n",
    "            if column in X.columns:\n",
    "                grp = y.groupby(X[column]).mean()\n",
    "                self.col_maps[column] = grp\n",
    "                self.global_means[column] = float(y.mean())\n",
    "        self.feature_names_out = [f\"{column}_tgt\" for column in self.cols if column in X.columns]\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes out-of-fold target mean encodings for the specified categorical columns and returns the transformed DataFrame.\n",
    "        During each fold of K-Fold cross-validation, mean target values are computed using only the training portion, then applied to the validation portion to prevent target leakage.\n",
    "        After all folds are processed, the encoder is fitted on the full dataset, and missing values are replaced with the global mean of the target.\n",
    "        The output includes the original features and new encoded columns with the suffix `_tgt`.\n",
    "\n",
    "        Args: \n",
    "            X: Input data containing the categorical columns to be encoded.\n",
    "            y: Target variable used to compute mean values per category.\n",
    "\n",
    "        Returns: \n",
    "            Z: Transformed DataFrame with original features and new target mean encoded columns.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Making sure X and y are indeed DataFrames and Series with correct indicies\n",
    "        X = pd.DataFrame(X).reset_index(drop=True)\n",
    "        y = pd.Series(y).reset_index(drop=True)\n",
    "        \n",
    "        # Initialize DataFrame that will hold out-of-fold encodings later\n",
    "        # Initially, all values are set to NaN\n",
    "        oof = pd.DataFrame(index=X.index)\n",
    "        for column in self.cols:\n",
    "                oof[column] = np.nan\n",
    "        \n",
    "        # Create k-Fold cross-validator\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        # Extract train and validation indices for each fold\n",
    "        for train, validation in kf.split(X):\n",
    "\n",
    "            # Extract training and validation sets\n",
    "            Xtr, ytr = X.loc[train], y.loc[train]\n",
    "\n",
    "            # For each categorical column, compute mean target value per category (only based on ytr and therefore train data)\n",
    "            for column in self.cols:\n",
    "                    # Calculate the means\n",
    "                    means = ytr.groupby(Xtr[column]).mean()\n",
    "\n",
    "                    # Apply the computed means to the validation set \n",
    "                    oof.loc[validation, column] = X.loc[validation, column].map(means)\n",
    "        \n",
    "        # Fit the encoder on the full dataset after cross-validation\n",
    "        # This stores the final mappings and global means for use in transform()\n",
    "        self.fit(X, y)\n",
    "        \n",
    "        # Create a copy of X and add the out-of-fold encodings\n",
    "        Z = X.copy()\n",
    "        for column in self.cols:\n",
    "                \n",
    "                # Replace any NaN (and therefore unseen category) with the global mean of y (Fallback)\n",
    "                # Store final columns with suffix '_tgt'\n",
    "                val = oof[column].fillna(self.global_means[column])\n",
    "                Z[f\"{column}_tgt\"] = val.values\n",
    "        \n",
    "        return Z\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforms the given data using target mean mappings learned during fit.\n",
    "        Categories that have not been seen during training are replaced with the global mean of the target variable.\n",
    "\n",
    "        Args: \n",
    "            X: Input data containing the categorical columns to be encoded.\n",
    "\n",
    "        Returns: \n",
    "            Z: Transformed DataFrame with original features and new target mean encoded columns.\n",
    "        \"\"\"\n",
    "\n",
    "        X = pd.DataFrame(X).reset_index(drop=True)\n",
    "        Z = X.copy()\n",
    "        for column in self.cols:\n",
    "                Z[f\"{column}_tgt\"] = X[column].map(self.col_maps[column]).fillna(self.global_means[column]).values\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ff2dcd2-3669-41a4-8c87-3effd4c482a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.3 Feature Definition and Scoring Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be2e534b-b13a-4180-93f0-6f926b47e0fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define columns that should be target encoded \n",
    "target_encoding_cols = [\"brand\", \"model\", \"is_premium_brand\"]\n",
    "\n",
    "# Define numeric\n",
    "numeric_features = [\n",
    "    \"year\", \"mileage\", \"tax\", \"mpg\", \"engine_size\", \"paint_quality\", \n",
    "    \"previous_owners\", \"vehicle_age\", \"mileage_per_year\", \"tax_per_engine\", \n",
    "    \"mpg_per_engine\", \"log_mileage\", \"log_tax\", \"efficiency_index\"\n",
    "]\n",
    "\n",
    "# Add the target encoded columns to numeric stack\n",
    "numeric_features = numeric_features + [f\"{col}_tgt\" for col in target_encoding_cols]\n",
    "\n",
    "# Define categorical features\n",
    "categorical_features = [\"transmission\", \"fuel_type\"]\n",
    "\n",
    "# Split data into X and y\n",
    "X = train_raw.drop(\"price\", axis=1)\n",
    "y = train_raw[\"price\"]\n",
    "X_test = test_raw\n",
    "\n",
    "# Define the scoring metrics that should be used\n",
    "scoring = {\n",
    "    \"mae\": \"neg_mean_absolute_error\",\n",
    "    \"rmse\": \"neg_root_mean_squared_error\",\n",
    "    \"r2\": \"r2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8090c45-9465-47fb-82fa-f41c7b1e0ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65235cfd-4161-4de8-99e7-5fe9d4847609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f160b09-dde8-4fa2-a8ca-06068c1c5d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this section, we will apply several feature selection methods in order to find out, if a selection of features is advantegous for the decision of the model. \n",
    "\n",
    "The following techniques are used and compared: \n",
    "- Filter methods\n",
    "    - Variance Thresholding\n",
    "    - Correlations (feature to target and feature to feature)\n",
    "    - F Regression and Mutual Information test\n",
    "- Wrapper methods\n",
    "    - Recursive Feature Selection with Cross Validation based on RandomForestRegressor feature importance\n",
    "\n",
    "In the end, the selected features of all methods are compared to choose and discuss best cross-method feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f33106f8-08e9-4ea1-a08c-04fbd7e434c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.0 Create Baseline Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b3e3977-06b7-4733-9048-a1811aba97bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To be able to apply the feature selection methods accordingly we need a base dataset. This is reached by applying the preprocessing steps previously implemented.\n",
    "\n",
    "We use the RobustScaler for numerical features and OneHotEncoder for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25ed38b6-081f-4e43-85ec-0ea3a97c6b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize preprocessor transformer, which scales numeric features and one-hot encodes categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(), numeric_features),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), categorical_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Initialize pipeline used for feature selection later\n",
    "base_feature_selection_pipe = Pipeline(steps=[\n",
    "    ('cleaner', CleanerTransformer()),\n",
    "    ('imputer', GroupImputer(cat_specs=cat_specs, num_specs=num_specs, int_cols=int_cols_for_imputer)),\n",
    "    ('feature_engineer', FeatureEngineer()),\n",
    "    ('target_encoder', TargetMeanEncoder(cols=target_encoding_cols, n_splits=5, random_state=0)),\n",
    "    ('preprocessor', preprocessor),\n",
    "], verbose=True)\n",
    "\n",
    "# Fitting the pipeline to the training data and transforming it\n",
    "X_processed = base_feature_selection_pipe.fit_transform(X, y)\n",
    "\n",
    "# Extracting feature names after preprocessing\n",
    "feature_names = base_feature_selection_pipe.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "if hasattr(X_processed, \"toarray\"):\n",
    "    X_processed = X_processed.toarray()\n",
    "\n",
    "X_processed_df = pd.DataFrame(X_processed, columns=feature_names)\n",
    "\n",
    "print(f'Number of initial features: {len(X_processed_df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2dac2ba-7f1b-4c73-abed-5d6ebcce9c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.1 Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c589da7-bae8-4157-8515-5438b0aa2013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.1.1 Variance Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "607d4299-87de-4826-9a96-6d07d62c90c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Variance Thresholding identifies features with variance below a specified threshold. We assume that features with near-uero variance potentially carry little informative benefits since their values barely change across observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d81b6c86-094b-4bf4-b42d-0b354cfa64c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Constructing a DataFrame and calculating variances \n",
    "variances = pd.DataFrame({\n",
    "    'Feature': X_processed_df.columns,\n",
    "    'Variance': X_processed_df.var().values\n",
    "}).sort_values(by='Variance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "display(variances.style.format(precision=4, thousands=\".\", decimal=\",\"))\n",
    "\n",
    "vt = VarianceThreshold(threshold=0.01)\n",
    "vt.fit(X_processed_df)\n",
    "\n",
    "# Features to keep are those with variance above the threshold. Threshold is choosen \"logically\", but is subject to discussion\n",
    "features_to_keep_vt = X_processed_df.columns[vt.get_support()]\n",
    "features_to_eliminate_vt = X_processed_df.columns[~vt.get_support()]\n",
    "\n",
    "print(f'Features to keep: {list(features_to_keep_vt)}')\n",
    "print(f'Features to eliminate: {list(features_to_eliminate_vt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "629c0568-fb1c-4fbb-a8ec-716654d3a06e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Results**\n",
    "The Variance threshold identifies, that all features have a high enough variance and should be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f918c62-f08a-4780-995b-09f3f9d6da0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.1.2 Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de9f09f1-406c-473b-8c69-036278042253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this section, we will analyze correlations. We splitted this analysis into two parts: feature-to-target correlation and feature-to-feature correlation. This is done to analyze the correlations between each feature and the target variable as well as the correlations between all features itself. \n",
    "\n",
    "By doing this, we can identify 1. which variables correlate with the target and therefore have potentially high impact (relevancy) and 2. scan for features that are similar to each other (redundancy). \n",
    "\n",
    "We chose to focus on the Spearman correlation coefficient because it can capture non-linear relationships, which are predominant in our data according to the exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6c9ee125-17a4-43e6-9fee-e627bf36ebd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a deep copy and adding the target variable back again to feature dataset to analyse feature-target correlations\n",
    "X_with_target = X_processed_df.copy()\n",
    "X_with_target['price'] = y.values\n",
    "\n",
    "# Calculating spearman correlation between features and target variable. Absolute values are taken to consider both positive and negative correlations.\n",
    "corr_spearman_features_target = X_with_target.corr(method='spearman')['price'].abs().reset_index()\n",
    "corr_spearman_features_target = corr_spearman_features_target.rename(columns={'index': 'Feature', 'price': 'Corr'})\n",
    "corr_spearman_features_target = corr_spearman_features_target.sort_values(by='Corr', ascending=False)\n",
    "\n",
    "# Selecting features with correlation >= 0.1 to target variable. Thresholds are choosen \"logically\", but are subject of discussion\n",
    "features_to_keep_corr = corr_spearman_features_target[corr_spearman_features_target['Corr'] >= 0.1]['Feature'].values\n",
    "features_to_eliminate_corr = corr_spearman_features_target[corr_spearman_features_target['Corr'] < 0.1]['Feature'].values\n",
    "\n",
    "print(f'Correlation matrix features-target (Spearman):')\n",
    "display(corr_spearman_features_target.style.format(precision=4, thousands=\".\", decimal=\",\").background_gradient(axis=0))\n",
    "\n",
    "print(f'Features to keep: {list(features_to_keep_corr)}')\n",
    "\n",
    "# Calculating spearman correlation matrix between features to identify highly correlated features\n",
    "corr_matrix_spearman_feature_feature = X_processed_df.corr(method='spearman').abs()\n",
    "\n",
    "print(f'\\n Correlation matrix features-features (Spearman):')\n",
    "display(corr_matrix_spearman_feature_feature.style.format(precision=4, thousands=\".\", decimal=\",\").background_gradient(axis=0))\n",
    "\n",
    "# Calculating pairs of features with correlation > 0.8\n",
    "high_corr = []\n",
    "for i in range(len(corr_matrix_spearman_feature_feature.columns)):\n",
    "    for j in range(i):\n",
    "        if corr_matrix_spearman_feature_feature.iloc[i, j] > 0.8:\n",
    "            high_corr.append((corr_matrix_spearman_feature_feature.columns[i], corr_matrix_spearman_feature_feature.columns[j]))\n",
    "\n",
    "display(high_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "319d800d-24c5-47a1-bb04-956695e18361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Results**\n",
    "- The feature-target corrlation with a threshold of 0.1 eliminate num__tax_per_engine, num__mileage_per_year, num__paint_quality, and num__previous_owners.\n",
    "- Features that correlate highly with eachother are mainly mileage, year, and vehicle age, as well as one ohe pair. We initially expect these to have a higher correlation with eachother.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53f26f2a-7779-4f7a-a047-5ae3a09ddec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.1.3 F_Regression and Mutual Information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71a5ddd8-8113-47d6-8aa4-d72fe462b9ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this section, we are filtering the features based on the F_Regression and Mutual Information paradimgs. \n",
    "\n",
    "F_Regression: \n",
    "Tests the null hypothesis that a feature has zero linear relationship with the target. For each feature, it computes an F statistic from its correlation with price; higher F and smaller p-values indicate stronger linear explanatory power.\n",
    "\n",
    "Mutual Information: \n",
    "We are also calculating the mutual informativeness as the F_Regression is only able to capture linearity. In short, the Mutual Information measures, how much information a feature provides about the target. Therefore, a non-zero value hints that the feature reduces uncertainty about the target. \n",
    "\n",
    "Result interpretation: High p-value for the f test and low MI value indicate, that a feature has limited predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8b8d9252-98fd-4326-b345-fe6c06a4f1bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculating f scores for all features\n",
    "selector_f = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "selector_f.fit(X_processed_df, y)\n",
    "\n",
    "# Calculating mutual information for all features\n",
    "selector_mi = SelectKBest(score_func=mutual_info_regression, k=\"all\")\n",
    "selector_mi.fit(X_processed_df, y)\n",
    "\n",
    "# Create DataFrame to hold the results\n",
    "select_k_results = pd.DataFrame({\n",
    "    'Feature': X_processed_df.columns,\n",
    "    'F_Score': selector_f.scores_,\n",
    "    'P_Value': selector_f.pvalues_,\n",
    "    'MI_Score': selector_mi.scores_\n",
    "}).sort_values(by=['F_Score', 'MI_Score'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the results with styling configuration to highlight results\n",
    "display(select_k_results.style.format(precision=4, thousands=\".\", decimal=\",\").background_gradient(axis=0))\n",
    "\n",
    "# Determine features to keep based on p-value and mutual information score. Thresholds are choosen \"logically\", but are subject for discussions\n",
    "features_to_keep_f_mi = select_k_results[(select_k_results['P_Value'] < 0.05) | (select_k_results['MI_Score'] > 0.01)]['Feature'].values\n",
    "features_to_eliminate_f_mi = list(set(X_processed_df.columns) - set(features_to_keep_f_mi))\n",
    "print(f'Features to keep: {list(features_to_keep_f_mi)}')\n",
    "print(f'Features to eliminate: {features_to_eliminate_f_mi}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2172492b-dec9-493f-b253-24532ff3fb2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Results**\n",
    "The p-values for num__paint_quality and num__previous_owners are very high while the MI-Score is below 0.005, indicating they have limited predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f91bbfaa-0559-42ac-96dd-b77d94aedde3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.2 Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d62cf5-e746-460d-a063-db118db8e71d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wrapper Methods evaluate feature subsets by model performance. They wrap the learning algorithm and assess which features improve predictive accuracy.\n",
    "\n",
    "1. We apply the Recursive Feature Elemination (RFE) with five cross-validations using the RandomForestRegressor as the estimator. The initally prefared model is the Random Forest thats why we do the RFECV with that model. We are aware that this might bias the model evaluation to take the model that the feature selecetion was based on.\n",
    "2. After doing the inital feature selection and model evaluation we discover that the Random Forest is the best model for our current setup. So we do the feature selection again with the tuned hyperparameters. This should help improve the feature selection for our final Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "047b5526-f022-4ad6-b1b1-77d1b0b0314b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize preprocessing pipeline, which includes cleaning, imputation, feature engineering, target mean encoding, and scaling/encoding\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('cleaner', CleanerTransformer()),\n",
    "    ('imputer', GroupImputer(cat_specs=cat_specs, num_specs=num_specs, int_cols=int_cols_for_imputer)),\n",
    "    ('feature_engineer', FeatureEngineer()),\n",
    "    ('target_encoder', TargetMeanEncoder(cols=target_encoding_cols, n_splits=5, random_state=0)),\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", RobustScaler(), numeric_features),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Initialize models\n",
    "base_models = {\n",
    "    'Default RandomForestRegressor': RandomForestRegressor(random_state=0, n_jobs=-1),\n",
    "    'Tuned RandomForestRegressor': RandomForestRegressor(\n",
    "        n_estimators=360,\n",
    "        max_depth=22,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=0.4,\n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Initialize K-Fold cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Iterate over each model for RFE\n",
    "for model_name, base_model in base_models.items(): \n",
    "    print(f'Starting RFE for model: {model_name}')\n",
    "    \n",
    "    # Store results for analysis\n",
    "    all_results = []\n",
    "\n",
    "    # Perform Recursive Feature Elimination with Cross-Validation\n",
    "    # fold: Current fold number\n",
    "    # train_fold_idx: Indices for training data in the current fold\n",
    "    # val_fold_idx: Indices for validation data in the current fold\n",
    "    for fold, (train_fold_idx, val_fold_idx) in enumerate(cv.split(X, y)):\n",
    "        \n",
    "        # Split data into training and validation sets for the current fold\n",
    "        X_train_fold, X_val_fold = X.iloc[train_fold_idx], X.iloc[val_fold_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_fold_idx], y.iloc[val_fold_idx]\n",
    "\n",
    "        # Process the training and validation data\n",
    "        X_train_processed = preprocessing_pipeline.fit_transform(X_train_fold, y_train_fold)\n",
    "        X_val_processed = preprocessing_pipeline.transform(X_val_fold)\n",
    "\n",
    "        # Get processed feature names    \n",
    "        processed_feature_names = list(preprocessing_pipeline.named_steps['preprocessor'].get_feature_names_out())\n",
    "\n",
    "        # Convert processed data to DataFrames for easier feature manipulation\n",
    "        X_train_processed_df = pd.DataFrame(X_train_processed, columns=processed_feature_names).reset_index(drop=True)\n",
    "\n",
    "        # Convert validation data to DataFrame\n",
    "        X_val_processed_df = pd.DataFrame(X_val_processed, columns=processed_feature_names).reset_index(drop=True)\n",
    "\n",
    "        # Initialize features to keep; by start of a new fold, all features are kept\n",
    "        features_to_keep = list(processed_feature_names)\n",
    "        \n",
    "        # Recursive Feature Elimination loop\n",
    "        for i in range(len(processed_feature_names)):\n",
    "            # Get the current number of features\n",
    "            num_features = len(features_to_keep)\n",
    "            \n",
    "            # Clone the base model to ensure a fresh model for each iteration\n",
    "            model = clone(base_model)\n",
    "\n",
    "            # Fit the model on the training data with the current set of features\n",
    "            model.fit(X_train_processed_df[features_to_keep], y_train_fold)\n",
    "            \n",
    "            # Predict on the validation set and compute MAE\n",
    "            y_pred = model.predict(X_val_processed_df[features_to_keep])\n",
    "            score = mean_absolute_error(y_val_fold, y_pred)\n",
    "            \n",
    "            print(f'Fold: {fold}, Features: {num_features}, MAE: {score}')\n",
    "\n",
    "            # If only one feature remains, record the result and exit the loop\n",
    "            if num_features == 1:\n",
    "                all_results.append({\n",
    "                    'Fold': fold,\n",
    "                    'Num_Features': num_features,\n",
    "                    'MAE': score,\n",
    "                    'Eliminated_Feature': None\n",
    "                })\n",
    "                break\n",
    "\n",
    "            # Identify the least important feature to eliminate\n",
    "            importances = model.feature_importances_\n",
    "            worst_feature_index = np.argmin(importances)\n",
    "            worst_feature_name = features_to_keep.pop(worst_feature_index)\n",
    "            \n",
    "            # Record the results of this iteration\n",
    "            all_results.append({\n",
    "                'Fold': fold,\n",
    "                'Num_Features': num_features,\n",
    "                'MAE': score,\n",
    "                'Eliminated_Feature': worst_feature_name\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(model_name)\n",
    "    sns.lineplot(data=results_df, x='Num_Features', y='MAE', ci='sd')\n",
    "\n",
    "    feature_analysis = results_df[results_df['Eliminated_Feature'].notna()].groupby('Eliminated_Feature').agg({'Num_Features': 'mean'}).reset_index()\n",
    "\n",
    "    feature_analysis.columns = ['Feature', 'Avg Elimination Position']\n",
    "\n",
    "    feature_analysis = feature_analysis.sort_values('Avg Elimination Position', ascending=False)\n",
    "\n",
    "    display(feature_analysis)\n",
    "\n",
    "\n",
    "    optimal_n = results_df.groupby('Num_Features')['MAE'].mean().idxmin()\n",
    "\n",
    "    final_features = set(processed_feature_names)\n",
    "\n",
    "    for fold in results_df['Fold'].unique():\n",
    "        eliminated = set(results_df[(results_df['Fold'] == fold) & (results_df['Num_Features'] >= optimal_n)]['Eliminated_Feature'].dropna())\n",
    "        final_features = final_features - eliminated\n",
    "\n",
    "    features_to_keep_rfe = sorted(final_features)\n",
    "    features_to_eliminate_rfe = sorted(set(processed_feature_names) - final_features)\n",
    "    print(f\"Final selected features for {model_name} (based on lowest average MAE):{features_to_keep_rfe}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35d8fa22-3783-47cc-a6a5-64e03f3b2f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Results**\n",
    "1. Iteration: Default Random Forest\n",
    "\t-  Using the default hyperparameters for the Random Forest in RFECV, no feature should be eliminated.\n",
    "2. Iteration: Tuned Random Forest\n",
    "\t- Using the Tuned hyperparameters for the Random Forest in RFECV, only 12 features should be kept.\n",
    "\n",
    "**Interpretation**: The difference for the final model is recognizable (40 MAE improvement). It is clearly recognizable that adjusting the feature selection for an already tuned model improves the feature selection for that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc81f591-3a65-45da-9c65-6474a422e92d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.3 Comparing feature selection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e3633781-53b1-4c8d-9beb-9a5b8a36c135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vt_keep = features_to_keep_vt \n",
    "vt_elim = features_to_eliminate_vt\n",
    "\n",
    "corr_keep = features_to_keep_corr\n",
    "corr_elim = features_to_eliminate_corr\n",
    "\n",
    "fmi_keep = features_to_keep_f_mi\n",
    "fmi_elim = features_to_eliminate_f_mi\n",
    "\n",
    "rfecv_keep = features_to_keep_rfe\n",
    "rfecv_elim = features_to_eliminate_rfe\n",
    "\n",
    "all_features = list(X_processed_df.columns)\n",
    "\n",
    "results = []\n",
    "for feature in all_features: \n",
    "    results.append({'Methode': 'Variance Threshold', 'Feature': feature, 'Kept': feature in vt_keep})\n",
    "    results.append({'Methode': 'Correlation', 'Feature': feature, 'Kept': feature in corr_keep})\n",
    "    results.append({'Methode': 'F-Test/MI', 'Feature': feature, 'Kept': feature in fmi_keep})\n",
    "    results.append({'Methode': 'RFECV', 'Feature': feature, 'Kept': feature in rfecv_keep})\n",
    "\n",
    "comparison_df = pd.DataFrame(results).pivot(index='Feature', columns='Methode', values='Kept').reset_index()\n",
    "\n",
    "display(comparison_df.style.applymap(lambda x: 'background-color: green' if x else 'background-color: red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b86a6f82-f584-4a0a-8be7-f0c83c72980b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Results**\n",
    "By plotting the features to keep and features to eliminate for each feature selection method, one can see:\n",
    "- The RFECV with tuned parameters selects the most features to eliminate.\n",
    "- All other methods confirm at least a few of the eliminations. Only correlation identifies num__tax_per_engine as additional elimination.\n",
    "- Since RFECV used the Random Forest with the model used later for car price prediction, we only take this feature selection into account.\n",
    "\n",
    "The following features are eliminated: num__is_premium_brand_tgt, num__log_tax, num__mileage_per_year, num__mpg, num__paint_quality, num__previous_owners, num__tax, ohe__fuel_type_diesel, ohe__fuel_type_hybrid, ohe__fuel_type_petrol, ohe__transmission_automatic, ohe__transmission_semi-automatic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4e1ed38-0766-4350-b1c8-3558e5487c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88ddb0b7-45aa-43cd-8c97-173222dc9cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Overview**:\n",
    "We train and compare diverse regressors to select the most accurate model for final prediction. (Note: The Notebook favors the forests heavily, as the preprocessing does not include any normalization steps and scaling for the linear regressors) Preprocessing is varied to quantify the impact of cleaning, feature engineering, encoding, and feature selection.  \n",
    "\n",
    "**Pipelines**\n",
    "1. Pipeline: Clean + Impute + Encoding + Scaling -> Build a baseline model.\n",
    "2. Pipeline: Pipeline 1 + Feature Engineering -> Test FE impact on baseline model.\n",
    "3. Pipeline: Pipeline 2 + Feature Selection -> Test FS impact on model performance. The tuned Random Forest is used in RFE.\n",
    "\n",
    "**Models Evaluated**\n",
    "- Linear: LinearRegression, Ridge\n",
    "- Trees / Ensambles: DecisionTreeRegressor, RandomForestRegressor, HistGradientBoostingRegressor\n",
    "- Neural Network: MLPRegressor\n",
    "\n",
    "**Assessment protocol**\n",
    "We use a 5-fold CV strategy throughout the project. This is mainly done since the 'simple' holdout validation is to simplistic and allows to much variability. We try to reduce variability with the folds for a better generalization. For the k-fold parameters we decided to use k of 5, so that each fold contains 20% of the data (around 15k values), which is sufficiently large enough. The folds are created using the shuffle functionality to ensure that the data is randomly distributed across the folds and that there are no systematic biases.\n",
    "\n",
    "**Metrics**\n",
    "- MAE, RMSE, and R² are displayed. Sorted and evaluated for MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8efae644-77aa-4c03-8c7f-3ee59d8e764c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selecting the feature that we will use in the first base case. Here, only raw features without feature engineering or target encoding are used.\n",
    "numeric_features_pipe_1 = [\"year\", \"mileage\", \"tax\", \"mpg\", \"engine_size\", \"paint_quality\", \"previous_owners\"]\n",
    "categorical_features_pipe_1 = [\"brand\", \"model\", \"transmission\", \"fuel_type\"]\n",
    "\n",
    "# Constructing the first pipeline, which only includes data cleaning, imputation, and preprocessing\n",
    "pipe_1 = Pipeline(steps=[\n",
    "    ('cleaner', CleanerTransformer()),\n",
    "    ('imputer', GroupImputer(cat_specs=cat_specs, num_specs=num_specs, int_cols=int_cols_for_imputer)),\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", RobustScaler(), numeric_features_pipe_1),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features_pipe_1)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Selecting the features that will be used in the second pipeline, which includes the raw features plus feature engineering and target encoding\n",
    "numeric_features_pipe_2 = [\n",
    "    \"year\", \"mileage\", \"tax\", \"mpg\", \"engine_size\", \"paint_quality\", \n",
    "    \"previous_owners\", \"vehicle_age\", \"mileage_per_year\", \"tax_per_engine\", \n",
    "    \"mpg_per_engine\", \"log_mileage\", \"log_tax\", \"efficiency_index\",\n",
    "    \"brand_tgt\", \"model_tgt\", \"is_premium_brand_tgt\"\n",
    "]\n",
    "\n",
    "categorical_features_pipe_2 = [\"transmission\", \"fuel_type\"]\n",
    "\n",
    "# Constructing the second pipeline, which includes data cleaning, imputation, feature engineering, target encoding and preprocessing\n",
    "pipe_2 = Pipeline(steps=[\n",
    "    ('cleaner', CleanerTransformer()),\n",
    "    ('imputer', GroupImputer(cat_specs=cat_specs, num_specs=num_specs, int_cols=int_cols_for_imputer)),\n",
    "    ('feature_engineer', FeatureEngineer()),\n",
    "    ('target_encoder', TargetMeanEncoder(cols=target_encoding_cols, n_splits=5, random_state=0)),\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", RobustScaler(), numeric_features_pipe_2),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features_pipe_2)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Selecting the features that will be used in the third pipeline, which includes only the selected features after feature selection\n",
    "numeric_features_pipe_3 = [\n",
    "    \"brand_tgt\", \"efficiency_index\", \"engine_size\", \"log_mileage\", \"mileage\", \"model_tgt\", \"mpg_per_engine\", \"tax_per_engine\", \"vehicle_age\", \"year\"\n",
    "]\n",
    "\n",
    "categorical_features_pipe_3 = []\n",
    "\n",
    "# Constructing the third pipeline, which includes data cleaning, imputation, feature engineering, target encoding, preprocessing and feature selection\n",
    "pipe_3 = Pipeline(steps=[\n",
    "    ('cleaner', CleanerTransformer()),\n",
    "    ('imputer', GroupImputer(cat_specs=cat_specs, num_specs=num_specs, int_cols=int_cols_for_imputer)),\n",
    "    ('feature_engineer', FeatureEngineer()),\n",
    "    ('target_encoder', TargetMeanEncoder(cols=target_encoding_cols, n_splits=5, random_state=0)),\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", RobustScaler(), numeric_features_pipe_3),\n",
    "            (\"ohe\", Pipeline([\n",
    "                ('encoder', OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "\n",
    "                # Special case: We have identified in the feature selection phase, that only manual transmission cars are relevant. Therefore, we will only keep that category.\n",
    "                ('selector', ColumnTransformer([\n",
    "                    ('transmission_manual_only', 'passthrough', [1])\n",
    "                ], remainder='drop'))\n",
    "            ]), categorical_features_pipe_3)],\n",
    "        remainder=\"drop\"\n",
    "    )),\n",
    "])\n",
    "\n",
    "# Define the models which will be used in the comparison\n",
    "models = {\n",
    "    'LinearRegressor': LinearRegression(),\n",
    "    'RidgeRegressor': Ridge(random_state=0),\n",
    "\n",
    "    \n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(random_state=0),\n",
    "    # Remark: n_jobs is used to optimize CPU usage\n",
    "    'RandomForestRegressor': RandomForestRegressor(random_state=0, n_jobs=-1),\n",
    "    \n",
    "    'HistGradientBoostingRegressor': HistGradientBoostingRegressor(random_state=0),\n",
    "    'MLPRegressor': MLPRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "# Create dictionary of pipelines to iterate over\n",
    "pipelines_to_compare = {\n",
    "    'Pipeline 1 (Base)': pipe_1,\n",
    "    'Pipeline 2 (+ Feature Engineering)': pipe_2,\n",
    "    'Pipeline 3 (+ Feature Selection)': pipe_3\n",
    "}\n",
    "\n",
    "# Initialize empty list to store comparison results\n",
    "comparison_results = []\n",
    "\n",
    "# Create loop that will iterate over each pipeline\n",
    "for pipeline_name, pipeline in pipelines_to_compare.items():\n",
    "    print(f\"Pipeline: {pipeline_name}\")\n",
    "    \n",
    "    # Create loop that will iterate over each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n Training {model_name}\")\n",
    "        \n",
    "        # Add the current model to the pipeline\n",
    "        curr_model = [('model', model)]\n",
    "        full_pipeline = Pipeline(steps=pipeline.steps + curr_model)\n",
    "\n",
    "        # Initialize K-Fold cross-validation\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        \n",
    "        # Calculate the cross-validated scores\n",
    "        scores = cross_validate(\n",
    "            full_pipeline, \n",
    "            X, \n",
    "            y, \n",
    "            cv=cv, \n",
    "            scoring=scoring,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Extract and store results of training\n",
    "        # Remark: sklearn stores error metrics as negative values for maximization. Therefore, we have to negate them back\n",
    "        mean_mae = -scores['test_mae'].mean()\n",
    "        std_mae = scores['test_mae'].std()\n",
    "        mean_rmse = -scores['test_rmse'].mean()\n",
    "        std_rmse = scores['test_rmse'].std()\n",
    "        mean_r2 = scores['test_r2'].mean()\n",
    "        std_r2 = scores['test_r2'].std()\n",
    "        \n",
    "        comparison_results.append({\n",
    "            'Pipeline': pipeline_name,\n",
    "            'Model': model_name,\n",
    "            'MAE Mean': mean_mae,\n",
    "            'MAE Std': std_mae,\n",
    "            'RMSE Mean': mean_rmse,\n",
    "            'RMSE Std': std_rmse,\n",
    "            'R2 Mean': mean_r2,\n",
    "            'R2 Std': std_r2\n",
    "        })\n",
    "        \n",
    "        print(f\" MAE: {mean_mae} , Std: {std_mae}\")\n",
    "        print(f\" RMSE: {mean_rmse}, Std: {std_rmse}\")\n",
    "        print(f\" R^2: {mean_r2} , Std: {std_r2}\")\n",
    "\n",
    "# Create DataFrame from comparison results and display it\n",
    "results_df = pd.DataFrame(comparison_results)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9a31684-b1ee-4266-82ca-b60f49041639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Results**\n",
    "1. Pipeline:\n",
    "\t- Linear models show higher MAE/RMSE and lower R² than Trees/Ensambles. This indicates a non-linear interactions and structure in the data.\n",
    "\t- RandomForest has the lowest errors and the most stable folds. It is the primary candidate for refinement.\n",
    "2. Pipeline:\n",
    "\t- Feature Engineering helps HistGradientBoost across all metrics.\n",
    "    - They hurt the linear models and the decision tree.\n",
    "    - Random Forest shows a small MAE improvement and but slightly worse RMSE and R². \n",
    "3. Pipeline:\n",
    "\t- By using feature selection all models are getting worse. \n",
    "    - It will be interesting to see the later performance of the tuned Random Forest.\n",
    "\n",
    "Note: For this evaluation we expected the Random Forest to be the best model, since we did the feature selection explicitly for the Random Forest and no specific steps were done for linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faeb706b-668b-4599-8407-036ce35d8622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9cfd9e0-fd44-48d2-9935-2982a02d1c32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this section, we focus on the best-performing model from the previous analysis, the RandomForestRegressor, and tune its hyperparameters to further enhance its performance. To stabilise the results, we will rely on cross-validation in this section as well.\n",
    "\n",
    "Remark: Through multiple testing iterations, we identified a subset of optimal parameters. To demonstrate the parameter tuning process, this subset was defined as the parameter grid. Consequently, the tuning does not cover the entire parameter space due to computational constraints. Instead, we focus on the subset that performed well and refine the search within this space. To find the optimal configuration in this narrowed space, we use the more unefficient but more precise GridSearchCV instead of RandomizedSearchCV. However, during extensive testing in the future, more advanced/ efficienct optimization algorithms will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06dffd39-8135-490e-92f1-fccf9433968e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize preprocessor transformer, which scales numeric features and one-hot encodes categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(), numeric_features),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), categorical_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Initialize the full pipeline for hyperparameter tuning\n",
    "tuning_pipeline = Pipeline(steps=[\n",
    "    ('cleaner', CleanerTransformer()),\n",
    "    ('imputer', GroupImputer(cat_specs=cat_specs, num_specs=num_specs, int_cols=int_cols_for_imputer)),\n",
    "    ('feature_engineer', FeatureEngineer()),\n",
    "    ('target_encoder', TargetMeanEncoder(cols=target_encoding_cols, n_splits=5, random_state=0)),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=0, n_jobs=1)),\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning. This grid will be searched during GridSearchCV. \n",
    "# In total, 4*4*2*3 = 96 combinations will be tested. Each 96 combinations will be evaluated using 2-Fold cross-validation, resulting in 192 model fits.\n",
    "param_grid = {\n",
    "    'model__n_estimators': [300, 320, 340, 360],\n",
    "    'model__max_depth': [18, 22, 26, 30],\n",
    "    'model__min_samples_leaf': [1, 2],\n",
    "    'model__max_features': [0.2, 0.4, 0.6],\n",
    "}\n",
    "\n",
    "# Define GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=tuning_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=2,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data to find the best hyperparameters\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Output the best score and parameters found during hyperparameter tuning\n",
    "print(f\"Loest MAE: {-grid_search.best_score_}\")\n",
    "print(\"Best Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "# Create DataFrame to display all results from the grid search\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_df['mae'] = -results_df['mean_test_score']\n",
    "\n",
    "results = results_df[[\n",
    "    'param_model__n_estimators',\n",
    "    'param_model__max_depth',\n",
    "    'param_model__min_samples_leaf',\n",
    "    'param_model__max_features',\n",
    "    'mae',\n",
    "    'std_test_score',\n",
    "]].reset_index(drop=True)\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e44cf1b4-fc41-4692-9bfd-d11eec3abd6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Impact of parameter tuning: \n",
    "    - The tuning of the RandomForest does indeed improved the performance by a fair bit. \n",
    "    - The optimal parameters are: \n",
    "        - model__max_depth: 22\n",
    "        - model__max_features: 0.4\n",
    "        - model__min_samples_leaf: 1\n",
    "        - model__n_estimators: 360\n",
    "\n",
    "    - default parameters: \n",
    "        - n_estimators: 100\n",
    "        - min_samples_leaf: 1\n",
    "        - max_depth: None\n",
    "        - max_features: 1\n",
    "\n",
    "    - The relativ high number of estimators and the depth of the trees leads to the hypothesis, that the regression problem is non-trivial with non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d7fdef2-e559-48a1-bc04-4a868e992812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6. Training and utilizing final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bdc2dea-92a1-414a-844f-58a2e13fb054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6.1 Training final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bbc6344-9418-4cec-95ba-df1495ed6e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this final section, we will utilize all optimizations done before to initialize one final, tuned model. This model will be used to make the final predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd4cde03-5786-4d19-a992-910a5a832100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the columns that will be target encoded in the final model\n",
    "target_encoding_cols = [\"brand\", \"model\"]\n",
    "\n",
    "# Define the final numeric features that will be used in the model\n",
    "numeric_features_final = [\n",
    "    \"mpg\",\n",
    "    \"tax_per_engine\",\n",
    "    \"log_mileage\",\n",
    "    \"engine_size\",\n",
    "    \"mileage\",\n",
    "    \"vehicle_age\",\n",
    "    \"efficiency_index\",\n",
    "    \"year\",\n",
    "    \"mpg_per_engine\"\n",
    "]\n",
    " \n",
    "# Add the target encoded columns to numeric stack\n",
    "numeric_features_final = numeric_features_final + [f\"{col}_tgt\" for col in target_encoding_cols]\n",
    "\n",
    "# Define the final categorical features that will be used in the model\n",
    "categorical_features_final = [\"transmission\"]\n",
    "\n",
    "# Initialize the final model with the best hyperparameters found during tuning\n",
    "model = RandomForestRegressor(\n",
    "        n_estimators=360,\n",
    "        max_depth=22,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=0.4,\n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "results = {}\n",
    "fold_results = {}\n",
    "\n",
    "# Construct the final pipeline\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    ('cleaner', CleanerTransformer()),\n",
    "    ('imputer', GroupImputer(cat_specs=cat_specs, num_specs=num_specs, int_cols=int_cols_for_imputer)),\n",
    "    ('feature_engineer', FeatureEngineer()),\n",
    "    ('target_encoder', TargetMeanEncoder(cols=target_encoding_cols, n_splits=5, random_state=0)),\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", RobustScaler(), numeric_features_final),\n",
    "            (\"ohe\", Pipeline([\n",
    "                ('encoder', OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "                ('selector', ColumnTransformer([\n",
    "                    ('transmission_manual_only', 'passthrough', [1])\n",
    "                ], remainder='drop'))\n",
    "            ]), categorical_features_final)],\n",
    "        remainder=\"drop\"\n",
    "    )),\n",
    "    ('model', model),\n",
    "])\n",
    " \n",
    "# Initialize K-Fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# Calculate the cross-validated scores\n",
    "scores = cross_validate(\n",
    "    final_pipeline,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=1\n",
    ")\n",
    " \n",
    "# Extract and store results of training\n",
    "mean_mae = -scores['test_mae'].mean()\n",
    "std_mae = scores['test_mae'].std()\n",
    "mean_rmse = -scores['test_rmse'].mean()\n",
    "std_rmse = scores['test_rmse'].std()\n",
    "mean_r2 = scores['test_r2'].mean()\n",
    "std_r2 = scores['test_r2'].std()\n",
    " \n",
    "fold_results = pd.DataFrame({\n",
    "    'Fold': range(1, len(scores['test_mae']) + 1),\n",
    "    'MAE': -scores['test_mae'],\n",
    "    'RMSE': -scores['test_rmse'],\n",
    "    'R2': scores['test_r2']\n",
    "})\n",
    " \n",
    "results = {\n",
    "    'MAE Mean': mean_mae,\n",
    "    'MAE Std': std_mae,\n",
    "    'RMSE Mean': mean_rmse,\n",
    "    'RMSE Std': std_rmse,\n",
    "    'R2 Mean': mean_r2,\n",
    "    'R2 Std': std_r2\n",
    "}\n",
    "\n",
    "display(results)\n",
    "display(fold_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cb219a1-93c2-45f1-8257-0485f7abc682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6.2 Making final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84a55564-9d09-442f-ad09-fbcb5f5033c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fitting the final pipeline on the entire training data\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# Making predictions on the test data\n",
    "predictions = final_pipeline.predict(X_test)\n",
    "\n",
    "# Creating submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"carID\": X_test[\"car_id\"],\n",
    "    \"Price\": predictions\n",
    "})\n",
    "\n",
    "# Saving submission to CSV file\n",
    "submission.to_csv(\"Group49_Version96.csv\", index=False)\n",
    "\n",
    "# Displaying the first few rows of the submission DataFrame\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f29d29f-358d-454e-a578-b1868f2d9587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle score: 1194.33751 (Public score, \"Group49_Version96.csv\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Group_49_notebook_score_1194_3375",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
