{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19e6ba90-2bd1-4be4-b026-62fb52a45eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Missing Values (GroupImputer)\n",
    "################################################################################\n",
    "\n",
    "class GroupImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Hierarchical imputer for numeric + categorical features.\n",
    "\n",
    "    Idea:\n",
    "    ----\n",
    "    We have to  compute the median value for the train dataset and fill the missing values in train, validation and test set with the median from the train dataset.\n",
    "    For each row with a missing value, fill it using statistics from \"similar\" rows first, and only fall back to global statistics if needed.\n",
    "\n",
    "    Extra (age ↔ mileage consistency):\n",
    "    -------------------------------\n",
    "    We found that `mileage` and `age` are strongly correlated.\n",
    "    Therefore, before the group hierarchy, we add a lightweight bucket-based fallback:\n",
    "\n",
    "    - If `mileage` is missing and `age` is available:\n",
    "        fill with median mileage of the corresponding age bucket (learned on train fold in fit()).\n",
    "    - If `age` is missing and `mileage` is available:\n",
    "        fill with median age of the corresponding mileage bucket (learned on train fold in fit()).\n",
    "\n",
    "    Hierarchy for numeric columns (num_cols):\n",
    "        0) (special case) mileage from age-bucket median, age from mileage-bucket median\n",
    "        1) median per (group_cols[0], group_cols[1])    > we use brand, model\n",
    "        2) median per group_cols[0]                     > we use brand\n",
    "        3) global median across all rows\n",
    "\n",
    "    Hierarchy for categorical columns (cat_cols):\n",
    "        1) mode per (group_cols[0], group_cols[1])      > we use brand, model\n",
    "        2) mode per group_cols[0]                       > we use brand\n",
    "        3) global mode across all rows\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - `group_cols` are used only to define groups; they themselves are not imputed.\n",
    "    - `num_cols` and `cat_cols` can be given explicitly (lists of column names). If None, they are inferred from the dtypes in `fit`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        group_cols=(\"brand\", \"model\"),\n",
    "        num_cols=None,\n",
    "        cat_cols=None,\n",
    "        fallback=\"__MISSING__\",\n",
    "        verbose=False,\n",
    "        verbose_top_n=10,\n",
    "        # NEW: correlation-based bucket imputation (lean defaults)\n",
    "        age_col=\"age\",\n",
    "        mileage_col=\"mileage\",\n",
    "        n_bins_age=8,\n",
    "        n_bins_mileage=8,\n",
    "        min_bucket_samples=20,\n",
    "    ):\n",
    "        self.group_cols = group_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.fallback = fallback\n",
    "        self.verbose = verbose\n",
    "        self.verbose_top_n = verbose_top_n\n",
    "\n",
    "        self.age_col = age_col\n",
    "        self.mileage_col = mileage_col\n",
    "        self.n_bins_age = n_bins_age\n",
    "        self.n_bins_mileage = n_bins_mileage\n",
    "        self.min_bucket_samples = min_bucket_samples\n",
    "\n",
    "    # helpers\n",
    "    def _mode(self, s: pd.Series):\n",
    "        \"\"\"\n",
    "        Deterministic mode helper.\n",
    "\n",
    "        - Compute the most frequent non-null value.\n",
    "        - If multiple values tie, Series.mode() returns them in order, we take .iloc[0].\n",
    "        - If there is no valid mode (all NaN), return fallback token.\n",
    "        \"\"\"\n",
    "        m = s.mode(dropna=True)\n",
    "        if not m.empty:\n",
    "            return m.iloc[0]\n",
    "        return self.fallback\n",
    "\n",
    "    def _get_group_series(self, df: pd.DataFrame, col_name: str) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Get the FIRST physical column with the given label from df.\n",
    "\n",
    "        Reason\n",
    "        ------\n",
    "        - In some workflows, df.columns can contain duplicate labels\n",
    "          (e.g. \"brand\" appearing twice after some operations).\n",
    "        - df[\"brand\"] would then raise \"Grouper for 'brand' not 1-dimensional\".\n",
    "        - By using np.where(df.columns == col_name)[0] we get *positions* and\n",
    "          explicitly pick the first one.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError if no column with that name exists.\n",
    "        \"\"\"\n",
    "        matches = np.where(df.columns == col_name)[0]\n",
    "        if len(matches) == 0:\n",
    "            raise ValueError(f\"GroupImputer: grouping column '{col_name}' not found in data.\")\n",
    "        return df.iloc[:, matches[0]]\n",
    "\n",
    "    # NEW: robust quantile-based bin edges (works with out-of-range at transform time)\n",
    "    def _make_bin_edges(self, s: pd.Series, n_bins: int):\n",
    "        vals = pd.to_numeric(s, errors=\"coerce\").dropna().values\n",
    "        if vals.size < self.min_bucket_samples:\n",
    "            return None\n",
    "        qs = np.linspace(0.0, 1.0, int(n_bins) + 1)\n",
    "        edges = np.unique(np.quantile(vals, qs))\n",
    "        # need at least 2 distinct cut points (+/- inf wrapper makes >=3 total)\n",
    "        if edges.size < 2:\n",
    "            return None\n",
    "        inner = edges[1:-1]  # can be empty\n",
    "        return np.r_[-np.inf, inner, np.inf]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn the group-level and global statistics from the training data.\n",
    "\n",
    "        Steps\n",
    "        -----\n",
    "        1) Convert X to DataFrame and remember the original column order.\n",
    "        2) Resolve which columns are numeric/categorical to impute.\n",
    "        3) Build group keys (g0, g1) from group_cols (e.g. brand, model).\n",
    "        4) For numeric columns:\n",
    "            - compute global medians\n",
    "            - medians per g0 (e.g. per brand)\n",
    "            - medians per (g0, g1) (e.g. per brand+model)\n",
    "        5) For categorical columns:\n",
    "            - global modes\n",
    "            - modes per g0\n",
    "            - modes per (g0, g1)\n",
    "        6) (special case) learn bucket medians for mileage↔age fallback.\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.DataFrame(X).copy()\n",
    "        self.feature_names_in_ = df.columns.to_list()\n",
    "\n",
    "        # group_cols must contain at least one column name\n",
    "        if self.group_cols is None or len(self.group_cols) == 0:\n",
    "            raise ValueError(\"GroupImputer: at least one group column must be specified.\")\n",
    "\n",
    "        self.group_cols_ = list(self.group_cols)\n",
    "\n",
    "        # Determine numeric columns to impute (internal num_cols_)\n",
    "        if self.num_cols is None:\n",
    "            # If not specified: all numeric columns except the group columns\n",
    "            num_cols_all = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "            self.num_cols_ = [c for c in num_cols_all if c not in self.group_cols_]\n",
    "        else:\n",
    "            # If specified: keep only those that exist in df\n",
    "            self.num_cols_ = [c for c in self.num_cols if c in df.columns]\n",
    "\n",
    "        # Determine categorical columns to impute (internal cat_cols_)\n",
    "        if self.cat_cols is None:\n",
    "            # If not specified: all non-group, non-numeric columns\n",
    "            self.cat_cols_ = [c for c in df.columns if c not in self.group_cols_ + self.num_cols_]\n",
    "        else:\n",
    "            # If specified: keep only those that exist in df\n",
    "            self.cat_cols_ = [c for c in self.cat_cols if c in df.columns]\n",
    "\n",
    "        # Build group key series based on the current df\n",
    "        # g0 = first grouping column (e.g. brand)\n",
    "        g0 = self._get_group_series(df, self.group_cols_[0])\n",
    "\n",
    "        # g1 = second grouping column (e.g. model), optional\n",
    "        g1 = None\n",
    "        if len(self.group_cols_) > 1:\n",
    "            g1 = self._get_group_series(df, self.group_cols_[1])\n",
    "\n",
    "        # NEW: mileage <-> age bucket medians (learned only on train fold)\n",
    "        self.age_bin_edges_ = None\n",
    "        self.mileage_bin_edges_ = None\n",
    "        self.mileage_by_agebin_ = None\n",
    "        self.age_by_mileagebin_ = None\n",
    "\n",
    "        has_age = self.age_col in df.columns\n",
    "        has_mileage = self.mileage_col in df.columns\n",
    "\n",
    "        if has_age and has_mileage:\n",
    "            age_s = pd.to_numeric(df[self.age_col], errors=\"coerce\")\n",
    "            mil_s = pd.to_numeric(df[self.mileage_col], errors=\"coerce\")\n",
    "\n",
    "            # mileage from age buckets\n",
    "            self.age_bin_edges_ = self._make_bin_edges(age_s, self.n_bins_age)\n",
    "            if self.age_bin_edges_ is not None:\n",
    "                age_bins = pd.cut(age_s, self.age_bin_edges_, include_lowest=True)\n",
    "                self.mileage_by_agebin_ = mil_s.groupby(age_bins, dropna=True).median()\n",
    "\n",
    "            # age from mileage buckets\n",
    "            self.mileage_bin_edges_ = self._make_bin_edges(mil_s, self.n_bins_mileage)\n",
    "            if self.mileage_bin_edges_ is not None:\n",
    "                mil_bins = pd.cut(mil_s, self.mileage_bin_edges_, include_lowest=True)\n",
    "                self.age_by_mileagebin_ = age_s.groupby(mil_bins, dropna=True).median()\n",
    "\n",
    "        # numeric statistics\n",
    "        if self.num_cols_:\n",
    "            # Extract the numeric columns to impute\n",
    "            num_df = df[self.num_cols_].copy()\n",
    "\n",
    "            # 3) Global median per numeric column (fallback for any group with no stats)\n",
    "            self.num_global_ = num_df.median(numeric_only=True)\n",
    "\n",
    "            # 2) Median per first-level group (g0, e.g. brand)\n",
    "            num_first = num_df.copy()\n",
    "            num_first[\"_g0\"] = g0.values  # temporary group key column\n",
    "            self.num_first_ = num_first.groupby(\"_g0\", dropna=True).median(numeric_only=True)\n",
    "\n",
    "            # 1) Median per pair (g0, g1), e.g. (brand, model)\n",
    "            if g1 is not None:\n",
    "                num_pair = num_df.copy()\n",
    "                num_pair[\"_g0\"] = g0.values\n",
    "                num_pair[\"_g1\"] = g1.values\n",
    "                self.num_pair_ = num_pair.groupby([\"_g0\", \"_g1\"], dropna=True).median(numeric_only=True)\n",
    "            else:\n",
    "                self.num_pair_ = pd.DataFrame()\n",
    "        else:\n",
    "            self.num_global_ = pd.Series(dtype=\"float64\")\n",
    "            self.num_first_ = pd.DataFrame()\n",
    "            self.num_pair_ = pd.DataFrame()\n",
    "\n",
    "        # categorical statistics\n",
    "        if self.cat_cols_:\n",
    "            cat_df = df[self.cat_cols_].copy()\n",
    "\n",
    "            # 3) Global mode per categorical column\n",
    "            self.cat_global_ = pd.Series({c: self._mode(cat_df[c]) for c in self.cat_cols_}, dtype=\"object\")\n",
    "\n",
    "            # 2) Mode per first-level group (g0)\n",
    "            cat_first = cat_df.copy()\n",
    "            cat_first[\"_g0\"] = g0.values\n",
    "            self.cat_first_ = cat_first.groupby(\"_g0\", dropna=True).agg(lambda s: self._mode(s))\n",
    "\n",
    "            # 1) Mode per pair (g0, g1)\n",
    "            if g1 is not None:\n",
    "                cat_pair = cat_df.copy()\n",
    "                cat_pair[\"_g0\"] = g0.values\n",
    "                cat_pair[\"_g1\"] = g1.values\n",
    "                self.cat_pair_ = cat_pair.groupby([\"_g0\", \"_g1\"], dropna=True).agg(lambda s: self._mode(s))\n",
    "            else:\n",
    "                self.cat_pair_ = pd.DataFrame()\n",
    "        else:\n",
    "            self.cat_global_ = pd.Series(dtype=\"object\")\n",
    "            self.cat_first_ = pd.DataFrame()\n",
    "            self.cat_pair_ = pd.DataFrame()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply hierarchical imputation to new data.\n",
    "            1) Convert input to DataFrame and align columns to what fit() saw.\n",
    "            2) Rebuild group keys g0, g1 from the current data.\n",
    "            3) (special case) mileage↔age bucket fallback where possible.\n",
    "            4) For each numeric column with missing values:\n",
    "                - try pair-level median (g0, g1)\n",
    "                - then brand-level median (g0)\n",
    "                - then global median\n",
    "            5) Same for categorical columns with modes.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(X).copy()\n",
    "        df = df.reindex(columns=self.feature_names_in_)\n",
    "\n",
    "        g0 = self._get_group_series(df, self.group_cols_[0])\n",
    "        g1 = None\n",
    "        if len(self.group_cols_) > 1:\n",
    "            g1 = self._get_group_series(df, self.group_cols_[1])\n",
    "\n",
    "        # NEW: audit counters\n",
    "        report = {\n",
    "            \"num_age_bucket\": 0,\n",
    "            \"num_mileage_bucket\": 0,\n",
    "            \"num_pair\": 0,\n",
    "            \"num_brand\": 0,\n",
    "            \"num_global\": 0,\n",
    "            \"cat_pair\": 0,\n",
    "            \"cat_brand\": 0,\n",
    "            \"cat_global\": 0,\n",
    "        }\n",
    "        per_col = Counter()\n",
    "\n",
    "        # numeric imputation\n",
    "        if hasattr(self, \"num_cols_\") and self.num_cols_:\n",
    "            df[self.num_cols_] = df[self.num_cols_].astype(\"float64\")\n",
    "\n",
    "            # (0) special-case: mileage from age bucket\n",
    "            if (\n",
    "                self.mileage_col in df.columns\n",
    "                and self.age_col in df.columns\n",
    "                and self.age_bin_edges_ is not None\n",
    "                and isinstance(self.mileage_by_agebin_, pd.Series)\n",
    "                and not self.mileage_by_agebin_.empty\n",
    "            ):\n",
    "                age_bins = pd.cut(df[self.age_col], self.age_bin_edges_, include_lowest=True)\n",
    "                fill_mileage = age_bins.map(self.mileage_by_agebin_)\n",
    "                mask = df[self.mileage_col].isna() & fill_mileage.notna()\n",
    "                n = int(mask.sum())\n",
    "                if n > 0:\n",
    "                    report[\"num_age_bucket\"] += n\n",
    "                    per_col[self.mileage_col] += n\n",
    "                    df.loc[mask, self.mileage_col] = fill_mileage.loc[mask].astype(\"float64\")\n",
    "\n",
    "            # (0) special-case: age from mileage bucket\n",
    "            if (\n",
    "                self.mileage_col in df.columns\n",
    "                and self.age_col in df.columns\n",
    "                and self.mileage_bin_edges_ is not None\n",
    "                and isinstance(self.age_by_mileagebin_, pd.Series)\n",
    "                and not self.age_by_mileagebin_.empty\n",
    "            ):\n",
    "                mil_bins = pd.cut(df[self.mileage_col], self.mileage_bin_edges_, include_lowest=True)\n",
    "                fill_age = mil_bins.map(self.age_by_mileagebin_)\n",
    "                mask = df[self.age_col].isna() & fill_age.notna()\n",
    "                n = int(mask.sum())\n",
    "                if n > 0:\n",
    "                    report[\"num_mileage_bucket\"] += n\n",
    "                    per_col[self.age_col] += n\n",
    "                    df.loc[mask, self.age_col] = fill_age.loc[mask].astype(\"float64\")\n",
    "\n",
    "            to_impute_num = [c for c in self.num_cols_ if df[c].isna().any()]\n",
    "\n",
    "            if to_impute_num:\n",
    "                # 1) pair-level imputation: per (g0, g1)\n",
    "                if g1 is not None and not self.num_pair_.empty:\n",
    "                    key_df = pd.DataFrame({\"_g0\": g0.values, \"_g1\": g1.values})\n",
    "                    med_df = self.num_pair_.reset_index()\n",
    "                    joined = key_df.merge(med_df, on=[\"_g0\", \"_g1\"], how=\"left\")\n",
    "\n",
    "                    for col in to_impute_num:\n",
    "                        if col not in self.num_pair_.columns:\n",
    "                            continue\n",
    "                        mask = df[col].isna() & joined[col].notna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"num_pair\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df.loc[mask, col] = joined.loc[mask, col]\n",
    "\n",
    "                # 2) first-level imputation: per g0 only\n",
    "                if not self.num_first_.empty:\n",
    "                    key1 = pd.DataFrame({\"_g0\": g0.values})\n",
    "                    med1 = self.num_first_.reset_index()\n",
    "                    joined1 = key1.merge(med1, on=\"_g0\", how=\"left\")\n",
    "\n",
    "                    for col in to_impute_num:\n",
    "                        if col not in self.num_first_.columns:\n",
    "                            continue\n",
    "                        mask = df[col].isna() & joined1[col].notna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"num_brand\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df.loc[mask, col] = joined1.loc[mask, col]\n",
    "\n",
    "                # 3) global median fallback\n",
    "                for col in to_impute_num:\n",
    "                    if col in self.num_global_:\n",
    "                        mask = df[col].isna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"num_global\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df[col] = df[col].fillna(self.num_global_[col])\n",
    "\n",
    "        # categorical imputation\n",
    "        if hasattr(self, \"cat_cols_\") and self.cat_cols_:\n",
    "            to_impute_cat = [c for c in self.cat_cols_ if df[c].isna().any()]\n",
    "\n",
    "            if to_impute_cat:\n",
    "                # 1) pair-level imputation: per (g0, g1)\n",
    "                if g1 is not None and not self.cat_pair_.empty:\n",
    "                    key_df = pd.DataFrame({\"_g0\": g0.values, \"_g1\": g1.values})\n",
    "                    mode_df = self.cat_pair_.reset_index()\n",
    "                    joined = key_df.merge(mode_df, on=[\"_g0\", \"_g1\"], how=\"left\")\n",
    "\n",
    "                    for col in to_impute_cat:\n",
    "                        if col not in self.cat_pair_.columns:\n",
    "                            continue\n",
    "                        mask = df[col].isna() & joined[col].notna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"cat_pair\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df.loc[mask, col] = joined.loc[mask, col]\n",
    "\n",
    "                # 2) first-level imputation: per g0 only\n",
    "                if not self.cat_first_.empty:\n",
    "                    key1 = pd.DataFrame({\"_g0\": g0.values})\n",
    "                    mode1 = self.cat_first_.reset_index()\n",
    "                    joined1 = key1.merge(mode1, on=\"_g0\", how=\"left\")\n",
    "\n",
    "                    for col in to_impute_cat:\n",
    "                        if col not in self.cat_first_.columns:\n",
    "                            continue\n",
    "                        mask = df[col].isna() & joined1[col].notna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"cat_brand\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df.loc[mask, col] = joined1.loc[mask, col]\n",
    "\n",
    "                # 3) global mode fallback (or fallback token)\n",
    "                for col in to_impute_cat:\n",
    "                    mask = df[col].isna()\n",
    "                    n = int(mask.sum())\n",
    "                    report[\"cat_global\"] += n\n",
    "                    per_col[col] += n\n",
    "                    df[col] = df[col].fillna(self.cat_global_.get(col, self.fallback))\n",
    "\n",
    "        # store report for later inspection\n",
    "        self.report_ = report\n",
    "        self.report_by_column_ = (\n",
    "            pd.DataFrame(per_col.items(), columns=[\"column\", \"values_filled\"])\n",
    "            .sort_values(\"values_filled\", ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            _print_section(\"GroupImputer report\")\n",
    "            print(\"Imputed Missing Values ( always try 'most similar cars' first):\\n\")\n",
    "            print(\n",
    "                f\"- Numeric (Median):   age->mileage={report['num_age_bucket']}, mileage->age={report['num_mileage_bucket']}, \"\n",
    "                f\"(brand+model)={report['num_pair']}, brand={report['num_brand']}, global={report['num_global']}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"- Categorical (Mode): (brand+model)={report['cat_pair']}, brand={report['cat_brand']}, global={report['cat_global']}\"\n",
    "            )\n",
    "            print(\"\\nTop columns affected:\")\n",
    "            _maybe_display(self.report_by_column_, max_rows=self.verbose_top_n)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"\n",
    "        Make the transformer compatible with sklearn's get feature-name.\n",
    "\n",
    "        - If called without arguments, return the original feature names seen in fit().\n",
    "        - This is mostly useful when GroupImputer is at the top of a Pipeline and\n",
    "          later steps want to introspect feature names.\n",
    "        \"\"\"\n",
    "        if input_features is None:\n",
    "            input_features = getattr(self, \"feature_names_in_\", None)\n",
    "        return np.asarray(input_features, dtype=object)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "new imputer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
