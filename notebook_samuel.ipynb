{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f009dcb-5f6f-4797-af22-fd6f7b342476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ToDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9291dc51-8250-4f36-b536-607e97da0e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# statsmodels implementieren und anschauen (linear reg solution lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52359b17-1216-4744-a528-81e23a033a68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27fff3e4-624f-4b1f-b258-fbbe7b69cb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import & load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f54ab11e-fcfc-4278-8997-89a92adf8eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import and load Data"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from data_cleaning import clean_car_dataframe\n",
    "\n",
    "df_cars_train = pd.read_csv(\"train.csv\")\n",
    "df_cars_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"MAE: {mae:.4f} | RMSE: {rmse:.4f} | R2: {r2:.4f}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "228971f7-84d2-4472-84d1-f4b91e185bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### EDA & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b60f640e-1c37-401c-bce1-290392e2be9a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exploratory Data Analysis"
    }
   },
   "outputs": [],
   "source": [
    "df_cars_train.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6fa0cfe9-be7e-4e90-ac1c-cd600bb7d13f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Numerical / Categorical Data Cleaning: with Pandas"
    }
   },
   "outputs": [],
   "source": [
    "df_cars_train = clean_car_dataframe(df_cars_train)\n",
    "df_cars_test = clean_car_dataframe(df_cars_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "82d4e036-95fd-471a-b953-d4b8ad7dd924",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Safety Check"
    }
   },
   "outputs": [],
   "source": [
    "# print all unique values of all columns of df_cars_train // df_cars_test\n",
    "\n",
    "for col in df_cars_train.columns:\n",
    "    print(col, df_cars_train[col].unique())\n",
    "\n",
    "print(\"X\"*150)\n",
    "\n",
    "for col in df_cars_test.columns:\n",
    "    print(col, df_cars_test[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b75ee214-0996-406f-b77f-9e72fa7153e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering, Split & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2df7bf7c-0ebb-4f4b-a168-5b2e2f6b21d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering"
    }
   },
   "outputs": [],
   "source": [
    "# add column age\n",
    "df_cars_train['age'] = 2025 - df_cars_train['year']\n",
    "df_cars_test['age'] = 2025 - df_cars_test['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dda482e9-4a68-438d-86c0-e0524a9b8692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split Data"
    }
   },
   "outputs": [],
   "source": [
    "X = df_cars_train.drop(columns='price')\n",
    "y = df_cars_train['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.3, \n",
    "                                                  random_state = 42, \n",
    "                                                  #stratify = y,\n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "39142374-d485-42e7-a6cb-83ecffae0091",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocessing: with Pipeline & Column Transformer"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = [\"year\", \"mileage\", \"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"hasDamage\"]\n",
    "categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "log_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=False)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"mileage\", log_transformer, [\"mileage\"]),\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# ColumnTransformer lets you apply different transformations to different feature subsets:\n",
    "#       > Numeric → impute mean + scale\n",
    "#       > Categorical → impute \"Unknown\" + OneHotEncode\n",
    "#       > Mileage → impute log-transform\n",
    "#   This is key, because numeric and categorical data need different math, you can't scale strings or one-hot encode continuous numbers.\n",
    "\n",
    "\n",
    "# Pipeline bundles preprocessing + model training:\n",
    "#     > Cross-validation applies preprocessing inside each fold (no data leakage).\n",
    "#     > The final model object (after .fit()) knows exactly how to preprocess new data.\n",
    "#     > When saving the pipeline with joblib, everything (scaler, encoder, model) is saved together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dcae03f-ed97-410e-9078-fb734f42be87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Models Setup and Baselining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d3aa5b-36ca-4dba-9592-24edd345897d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Models Setup (inkl. Prepro in Pipeline)"
    }
   },
   "outputs": [],
   "source": [
    "ridge_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor), \n",
    "    (\"model\", Ridge())\n",
    "])\n",
    "\n",
    "lasso_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor), \n",
    "    (\"model\", Lasso(max_iter=20000))\n",
    "])\n",
    "\n",
    "elastic_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=20000))\n",
    "])\n",
    "\n",
    "# GradientBoostingRegressor: baseline, has to beat Ridge/Lasso, if not something’s wrong with data preprocessing, not the model.\n",
    "gbr_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor), \n",
    "    (\"model\", GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "# HistGradientBoostingRegressor: modern and very fast, handles missing values natively (no imputation needed!). often matches or beats XGBoost/LightGBM \n",
    "hgb_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        max_depth=None, \n",
    "        learning_rate=0.05, \n",
    "        max_iter=500,           \n",
    "        l2_regularization=1.0,\n",
    "        random_state=42\n",
    "    ))\n",
    "    # possible to grid search over learning_rate, max_leaf_nodes, min_samples_leaf\n",
    "])\n",
    "\n",
    "# RandomForestRegressor: excellent general baseline ensemble, handles non-linearities well, doesn’t overfit easily but can be slow for large data\n",
    "rf_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300, \n",
    "        max_depth=None, \n",
    "        n_jobs=-1, \n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ExtraTreesRegressor: similar to RandomForest but with more randomization → often better generalization\n",
    "et_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=300, \n",
    "        max_depth=None, \n",
    "        n_jobs=-1, \n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# SVR: powerful, but slow on large data, sensitive to scaling → already handled in preprocessing\n",
    "svr_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", SVR(C=10, epsilon=0.2, kernel=\"rbf\"))\n",
    "])\n",
    "\n",
    "# StackingRegressor: stacks/blends multiple models → typically gives a small but consistent boost in leaderboard competitions\n",
    "stack_pipe = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"ridge\", ridge_pipe),\n",
    "        (\"lasso\", lasso_pipe),\n",
    "        (\"rf\", rf_pipe),\n",
    "        (\"gbr\", gbr_pipe),\n",
    "    ],\n",
    "    final_estimator=HistGradientBoostingRegressor(learning_rate=0.05, max_depth=5),\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b26348e-a5ff-4dc9-9d61-7f59d0ea335d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Baselining mean/median"
    }
   },
   "outputs": [],
   "source": [
    "mean_pred = y_train.mean()\n",
    "median_pred = y_train.median()\n",
    "\n",
    "print(\"baseline mean predictor: \")\n",
    "print_metrics(y_val, [mean_pred]*len(y_val))\n",
    "print(\"-\"*150)\n",
    "print(\"baseline median predictor: \") \n",
    "print_metrics(y_val, [median_pred]*len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b07ea302-a29d-492b-8a84-a97c6f6e86a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### HT: linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43b14f13-f7f2-44c5-a0a3-a6777cf72e91",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: Linear Models"
    }
   },
   "outputs": [],
   "source": [
    "param_grids_linear = {\n",
    "    #ridge\n",
    "    \"ridge\": {\"model__alpha\": [0.1, 1.0, 10.0]},\n",
    "\n",
    "    #lasso\n",
    "    \"lasso\": {\"model__alpha\": [0.001, 0.01, 0.1, 1.0]},\n",
    "\n",
    "    #elasticnet\n",
    "    \"elastic\": {\n",
    "        \"model__alpha\": [0.01, 0.1, 1.0],\n",
    "        \"model__l1_ratio\": [0.2, 0.5, 0.8]\n",
    "    }\n",
    "}\n",
    "\n",
    "pipes_linear = {\n",
    "    \"ridge\": ridge_pipe,\n",
    "    \"lasso\": lasso_pipe,  # iter 20000 because of convergence warning\n",
    "    \"elastic\": elastic_pipe  # iter 20000 because of convergence warning\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, pipe in pipes_linear.items():\n",
    "    print(f\"\\n Results of {name.upper()} : \")\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        pipe, param_grids_linear[name], \n",
    "        cv=cv, scoring=\"r2\", n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    val_pred = best_model.predict(X_val)\n",
    "    \n",
    "    print_metrics(y_val, val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e7dac5c-ae30-4ed6-b7ac-a87942f91697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### HT: tree Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    \"model__max_depth\": [None, 10, 20],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    rf_pipe, rf_param_grid,\n",
    "    cv=5, scoring=\"r2\",\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_best = rf_grid.best_estimator_\n",
    "rf_val_pred = rf_best.predict(X_val)\n",
    "\n",
    "print(\"Random Forest Results: \")\n",
    "print_metrics(y_val, rf_val_pred)\n",
    "\n",
    "#MAE: 1431.4111 | RMSE: 5764825.2457 | R2: 0.9379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4ff12e-07a4-47f5-8c08-43b5d8d86942",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ExtraTrees"
    }
   },
   "outputs": [],
   "source": [
    "et_param_grid = {\n",
    "    \"model__max_depth\": [None, 10, 20],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe, et_param_grid,\n",
    "    cv=5, scoring=\"r2\",\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "et_grid.fit(X_train, y_train)\n",
    "et_best = et_grid.best_estimator_\n",
    "et_val_pred = et_best.predict(X_val)\n",
    "\n",
    "print(\"ExtraTrees Results: \")\n",
    "print_metrics(y_val, et_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "151b3c01-a218-4349-ac7d-48711b9cfd30",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: GradientBoosting"
    }
   },
   "outputs": [],
   "source": [
    "gbr_param_grid = {\n",
    "    \"model__n_estimators\": [200, 500],\n",
    "    \"model__learning_rate\": [0.05, 0.1],\n",
    "    \"model__max_depth\": [3, 5]\n",
    "}\n",
    "\n",
    "gbr_grid = GridSearchCV(\n",
    "    gbr_pipe, gbr_param_grid,\n",
    "    cv=5, scoring=\"r2\",\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "gbr_grid.fit(X_train, y_train)\n",
    "gbr_best = gbr_grid.best_estimator_\n",
    "gbr_val_pred = gbr_best.predict(X_val)\n",
    "\n",
    "print(\"GradientBoosting Results: \")\n",
    "print_metrics(y_val, gbr_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5bdcf66-8612-4455-979e-a348ca38dd42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### HT: advanced Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78157989-9ef4-4b2b-a9e1-72309a89e754",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: HistGradientBoost"
    }
   },
   "outputs": [],
   "source": [
    "hgb_param_grid = {\n",
    "    \"model__learning_rate\": [0.03, 0.05, 0.1],\n",
    "    \"model__max_leaf_nodes\": [15, 31, None],\n",
    "    \"model__min_samples_leaf\": [10, 20]\n",
    "}\n",
    "\n",
    "hgb_grid = GridSearchCV(\n",
    "    hgb_pipe, hgb_param_grid,\n",
    "    cv=cv, scoring=\"r2\", n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "hgb_grid.fit(X_train, y_train)\n",
    "hgb_best = hgb_grid.best_estimator_\n",
    "\n",
    "hgb_val_pred = hgb_best.predict(X_val)\n",
    "r2_hgb, rmse_hgb, mae_hgb = print_metrics(y_val, hgb_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f487848b-789b-4084-827b-2ca4a2313bfc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: SupportVectorRegressor"
    }
   },
   "outputs": [],
   "source": [
    "svr_param_grid = {\n",
    "    \"model__C\": [1, 10, 100],\n",
    "    \"model__epsilon\": [0.1, 0.2],\n",
    "    \"model__kernel\": [\"rbf\"]\n",
    "}\n",
    "\n",
    "svr_grid = GridSearchCV(\n",
    "    svr_pipe, svr_param_grid,\n",
    "    cv=cv, scoring=\"r2\", n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "svr_grid.fit(X_train, y_train)\n",
    "svr_best = svr_grid.best_estimator_\n",
    "\n",
    "svr_val_pred = svr_best.predict(X_val)\n",
    "r2_svr, rmse_svr, mae_svr = print_metrics(y_val, svr_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0525442b-db49-45b7-ad73-ff98599eb223",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: StackingRegressor"
    }
   },
   "outputs": [],
   "source": [
    "# stackingregressor (lasso, ridge, rf, gbr)\n",
    "\n",
    "print(\"\\n Training StackingRegressor ...\")\n",
    "stack_pipe.fit(X_train, y_train)\n",
    "val_pred = stack_pipe.predict(X_val)\n",
    "r2, rmse, mae = print_metrics(y_val, val_pred)\n",
    "\n",
    "# (MAE: 1433.9603 | RMSE: 2536.9591 | R²: 0.9307)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac5b8d47-1b22-478a-9b05-bfece72b804c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluation, Logging, Kaggle, Feature Importance, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "629491ac-829c-4038-8bac-98e916cf4843",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validation Evaluation & Experiment Logging"
    }
   },
   "outputs": [],
   "source": [
    "# What/Why: Evaluate each model on the validation set (the data not used for training) and log metrics. Compare models and pick the one with best validation performance.\n",
    "\n",
    "# Benchmarking note: your professor’s “benchmarking” asks for such a log — always compare to mean baseline and to a simple linear baseline (Ridge/Lasso) before complex models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a25f58c-4c24-4460-93d4-830176016ee8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pick best Model and evaluate on test set (+ kaggle api push)"
    }
   },
   "outputs": [],
   "source": [
    "# What/Why: Only after you pick the model using the validation set do you evaluate on test — this gives an unbiased final estimate.\n",
    "\n",
    "# Why: test must never influence tuning; otherwise you leak information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5d1e3d5b-a46a-4077-b29f-d44f953c634c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Importance & Interpretation"
    }
   },
   "outputs": [],
   "source": [
    "# What/Why: Different models give different “importance” signals:\n",
    "# Lasso → coefficients (linear importance); zero → removed feature.\n",
    "# Tree ensembles (GBR) → feature_importances_ (importance in splits).\n",
    "# For rigorous interpretation, use SHAP for consistent feature attributions across models.\n",
    "\n",
    "\n",
    "# Important: yes — each model may select different features. That’s expected. Use the model type that matches your use-case:\n",
    "# If you need a sparse, interpretable linear model → use Lasso.\n",
    "# If you need best predictive power on tabular data → use ensemble/boosting and interpret via SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3cded4d1-99c8-4006-90cd-07b8c22c047c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save Pipeline for Production"
    }
   },
   "outputs": [],
   "source": [
    "# What/Why: Save the entire pipeline (preprocessing + model) so new observations are processed consistently.\n",
    "\n",
    "\"\"\"\n",
    "# (cell) Save and reload\n",
    "import joblib, os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(best_pipeline, \"models/car_price_model.pkl\")\n",
    "\n",
    "loaded = joblib.load(\"models/car_price_model.pkl\")\n",
    "# predict on new example\n",
    "new_car = pd.DataFrame([{\n",
    "    \"mileage\": 60000, \"year\": 2018, \"engine_size\": 2.0, \"horsepower\": 150, \"doors\": 4, \"owners\": 2,\n",
    "    \"brand\": \"BMW\", \"fuel_type\": \"Petrol\", \"transmission\": \"Auto\", \"color\": \"Black\",\n",
    "    \"region\": \"Urban\", \"condition\": \"Used\", \"warranty\": \"No\", \"dealer_type\": \"Independent\"\n",
    "}])\n",
    "print(\"Predicted price:\", loaded.predict(new_car)[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ebc3c64-02eb-481d-bf58-59f92e0fddcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Iterative Loop Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f20976-4956-40c7-89e7-355361e29a56",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Iterative Loop Begins"
    }
   },
   "source": [
    "The loop starts after you look at baseline performance and diagnostics:\n",
    "- Baseline → Check metrics on validation (and residuals).\n",
    "- Inspect failures / residual plots / feature importances (did a certain brand get consistently over/under predicted?)\n",
    "- Hypothesize (e.g. add interaction year * mileage, try log transform for horsepower, create age = current_year - year).\n",
    "- Implement changes in pipeline (e.g. add FunctionTransformer for log(horsepower) or PolynomialFeatures on a small set).\n",
    "- Re-run CV/hyperparameter search and evaluate again.\n",
    "- Log results, repeat.\n",
    "\n",
    "Note on feature selection: yes — different models will select different subsets. Typical approaches:\n",
    "- Use Lasso or SelectFromModel as a filter for linear pipelines.\n",
    "- Use tree-based model importances or SHAP to select features for simpler models.\n",
    "- Or let the best predictive model use all features (trees are robust to redundancy).\n",
    "\n",
    "Final notes (recommended best-practices)\n",
    "- Always fit preprocessing only on training data (pipelines do this automatically if you use them inside CV).\n",
    "- Start simple: mean baseline → Ridge/Lasso → tree-based. Use the simple models for interpretability and as sanity checks.\n",
    "- For heavy hyperparameter searches use RandomizedSearchCV or Optuna if the space is big.\n",
    "- When comparing models, report multiple metrics (R², MAE, RMSE). For price prediction MAE is often most interpretable.\n",
    "- For reproducibility, store your dataset version, random seed, code, and results log."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "notebook_samuel",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
