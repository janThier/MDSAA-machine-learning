{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33be9785-fb92-4a6c-8e31-775f782ed1d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Your work will be evaluated according to the following criteria:**\n",
    "- Project Structure and Notebook(s) Quality (4/20)\n",
    "- Data Exploration & Initial Preprocessing (4/20)\n",
    "- Regression Benchmarking and Optimization (7/20)\n",
    "- Open-Ended Section (4/20)\n",
    "- Deployment (1/20)\n",
    "- Extra Point: Have Project Be Publicly Available on GitHub (1/20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab56b787-71c4-4043-b0a8-2bb264d7236b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO Brainstorm + Implementation of Ideas for open ended Section (Several can get explored):\n",
    "#   1. Create a classification Model, that predicts if a dataset is gonna be a price outlier (outlier flag) (SAMUEL)\n",
    "#   2. Jan:\n",
    "#   3. Elias: \n",
    "#   4. Lukas: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4467ab3-7594-4e02-a19f-8c5e47fb14e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"\n",
    "    background: rgba(25, 25, 25, 0.55);\n",
    "    backdrop-filter: blur(16px) saturate(150%);\n",
    "    -webkit-backdrop-filter: blur(16px) saturate(150%);\n",
    "    border: 1px solid rgba(255, 255, 255, 0.12);\n",
    "    border-radius: 18px;\n",
    "    padding: 45px 30px;\n",
    "    text-align: center;\n",
    "    font-family: 'Inter', 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;\n",
    "    color: #e0e0e0;\n",
    "    box-shadow: 0 0 30px rgba(0, 0, 0, 0.35);\n",
    "    margin: 40px auto;\n",
    "    max-width: 800px;\n",
    "\">\n",
    "\n",
    "  <h1 style=\"\n",
    "      font-size: 2.8em;\n",
    "      font-weight: 700;\n",
    "      margin: 0 0 8px 0;\n",
    "      letter-spacing: -0.02em;\n",
    "      background: linear-gradient(90deg, #00e0ff, #9c7eff);\n",
    "      -webkit-background-clip: text;\n",
    "      -webkit-text-fill-color: transparent;\n",
    "  \">\n",
    "      Machine Learning Project\n",
    "  </h1>\n",
    "\n",
    "  <h2 style=\"\n",
    "      font-size: 1.6em;\n",
    "      font-weight: 500;\n",
    "      margin: 0 0 25px 0;\n",
    "      color: #b0b0b0;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Cars 4 You - Predicting Car Prices\n",
    "  </h2>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.25em;\n",
    "      font-weight: 500;\n",
    "      color: #c0c0c0;\n",
    "      margin-bottom: 6px;\n",
    "  \">\n",
    "      Group 5 - Lukas Belser, Samuel Braun, Elias Karle, Jan Thier\n",
    "  </p>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.05em;\n",
    "      font-weight: 400;\n",
    "      color: #8a8a8a;\n",
    "      font-style: italic;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Machine Learning End Results · 22.12.2025\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "372c87aa-b7f3-4a34-a596-bb5e584db114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Table of Contents**\n",
    "\n",
    "TODO finish toc > at the end of project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b8c7878-3db2-47d4-b6a4-c91bbd1f5c33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcea0314-f61b-48dd-93e0-540b52850d1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Process to test new features: (TODO remove before submission)\n",
    "\n",
    "1. Add new Features in cell Feature Engineering\n",
    "2. Go into Preprocessing and add the features in either num, log or cat\n",
    "3. Run the RandomSearch // Hyperparameter Tuning for at least HGB and RF and see if MAE gets improved compared to previous results\n",
    "4. If MAE gets not improved, comment in cell below Feature Engineering that you tested those features (+results?) and on which Models - and remove everything\n",
    "5. If MAE gets improved, find out via Feature Importance (Shap Values) which Feature was responsible + document it, remove the other features that have negative impact\n",
    "6. comment below the Hyperparameter Tuning cell of the model the new achieved results + all the features you used for that + the hyperparameters\n",
    "7. save the results in a new model with joblib, name it correctly\n",
    "8. push to kaggle\n",
    "9. push to GIT + document everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27fff3e4-624f-4b1f-b258-fbbe7b69cb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Import Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e6aadb-727a-4399-8888-9d6d4e593898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee943552-3eee-4e9c-bb91-481b64a2e59e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO remove pip installs before submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aa35af5d-8257-4fb6-a380-86bbbfac3244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f02b1610-84c8-46a7-8c37-980904a0cef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "008f906c-e207-4658-8e03-59bd25b8e2d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f54ab11e-fcfc-4278-8997-89a92adf8eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import and load Data"
    }
   },
   "outputs": [],
   "source": [
    "# Import and load Data # TODO remove unused imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, chi2, RFECV\n",
    "from scipy.stats import spearmanr, uniform, loguniform, randint\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, FunctionTransformer, RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from car_functions import clean_car_dataframe, GroupMedianImputer, cv_target_encode, add_price_anchor_features, print_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9737aef-d2bd-4748-89ed-9b0e2cea8637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.2 Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3da1ad2b-8ee0-48b0-917d-545c70633ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cars_train = pd.read_csv(\"train.csv\")\n",
    "df_cars_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab6500b5-94a1-4a0e-b7f6-ea9d35c787eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Kaggle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "556ffc24-70f7-48c9-96a4-3a19380f91db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Connect"
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle API Connect => everyone has to do this himself, with his own kaggle.json api token\n",
    "\n",
    "# Folder containing kaggle.json\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/Workspace/Users/20250355@novaims.unl.pt\"\n",
    "\n",
    "# Test\n",
    "!echo $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b75ee214-0996-406f-b77f-9e72fa7153e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data Cleaning, Feature Engineering, Split & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f53a2a97-4662-40a3-8b24-b238dad5138e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6fa0cfe9-be7e-4e90-ac1c-cd600bb7d13f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Numerical / Categorical Data Cleaning: with Pandas"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier Preprocessing, Missing Value Handling and Decision justifying happens here\n",
    "df_cars_train = clean_car_dataframe(df_cars_train)\n",
    "df_cars_test = clean_car_dataframe(df_cars_test)\n",
    "\n",
    "\n",
    "# Safety Check: print unique values of all columns of df_cars_train // df_cars_test to see if data cleaning worked and if there are still odd values\n",
    "for col in df_cars_train.columns:\n",
    "    print(col, df_cars_train[col].unique())\n",
    "print(\"X\"*150); print(\"-\"*150); print(\"X\"*150)\n",
    "for col in df_cars_test.columns:\n",
    "    print(col, df_cars_test[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b198720e-b5a0-46ef-8055-4ac33b852410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Feature Engineering (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "83812e71-e13e-43d4-883b-41ffd3b3466b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Base Feature Creation\n",
    "\n",
    "# Car Age: Newer cars usually have higher prices, models prefer linear features\n",
    "df_cars_train['age'] = 2020 - df_cars_train['year']\n",
    "df_cars_test['age']  = 2020 - df_cars_test['year']\n",
    "\n",
    "# Miles per Year: Normalizes mileage by age (solves multicollinearity between year and mileage)\n",
    "df_cars_train['miles_per_year'] = df_cars_train['mileage'] / df_cars_train['age'].replace({0: np.nan})\n",
    "df_cars_train['miles_per_year'] = df_cars_train['miles_per_year'].fillna(df_cars_train['mileage'])\n",
    "\n",
    "df_cars_test['miles_per_year'] = df_cars_test['mileage'] / df_cars_test['age'].replace({0: np.nan})\n",
    "df_cars_test['miles_per_year'] = df_cars_test['miles_per_year'].fillna(df_cars_test['mileage'])\n",
    "\n",
    "# Interaction Terms: Capture non-linear effects between engine and other numeric features\n",
    "df_cars_train['age_x_engine'] = df_cars_train['age'] * df_cars_train['engineSize'].fillna(0)\n",
    "df_cars_test['age_x_engine']  = df_cars_test['age']  * df_cars_test['engineSize'].fillna(0)\n",
    "\n",
    "df_cars_train['mpg_x_engine'] = df_cars_train['mpg'].fillna(0) * df_cars_train['engineSize'].fillna(0)\n",
    "df_cars_test['mpg_x_engine']  = df_cars_test['mpg'].fillna(0)  * df_cars_test['engineSize'].fillna(0)\n",
    "\n",
    "# tax per engine\n",
    "df_cars_train['tax_per_engine'] = df_cars_train['tax'] / df_cars_train['engineSize'].replace({0: np.nan})\n",
    "df_cars_train['tax_per_engine'] = df_cars_train['tax_per_engine'].fillna(df_cars_train['tax'])\n",
    "\n",
    "df_cars_test['tax_per_engine'] = df_cars_test['tax'] / df_cars_test['engineSize'].replace({0: np.nan})\n",
    "df_cars_test['tax_per_engine'] = df_cars_test['tax_per_engine'].fillna(df_cars_test['tax'])\n",
    "\n",
    "# MPG per engineSize to represent the efficiency\n",
    "df_cars_train['mpg_per_engine'] = df_cars_train['mpg'] / df_cars_train['engineSize'].replace({0: np.nan})\n",
    "df_cars_train['mpg_per_engine'] = df_cars_train['mpg_per_engine'].fillna(df_cars_train['mpg'])\n",
    "\n",
    "df_cars_test['mpg_per_engine'] = df_cars_test['mpg'] / df_cars_test['engineSize'].replace({0: np.nan})\n",
    "df_cars_test['mpg_per_engine'] = df_cars_test['mpg_per_engine'].fillna(df_cars_test['mpg'])\n",
    "\n",
    "\n",
    "# 2. Model Frequency: Popular models tend to have stable demand and prices\n",
    "model_freq = df_cars_train['model'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "df_cars_train['model_freq'] = df_cars_train['model'].map(model_freq).fillna(0.0) # TODO chat added fillna\n",
    "df_cars_test['model_freq']  = df_cars_test['model'].map(model_freq).fillna(0.0)\n",
    "\n",
    "\n",
    "# 3. Relative Age (within brand): newer/older than brand median year\n",
    "brand_median_age = df_cars_train.groupby('Brand')['age'].median().to_dict()\n",
    "\n",
    "df_cars_train['age_rel_brand'] = df_cars_train['age'] - df_cars_train['Brand'].map(brand_median_age)\n",
    "df_cars_test['age_rel_brand']  = df_cars_test['age']  - df_cars_test['Brand'].map(brand_median_age)\n",
    "\n",
    "\n",
    "# 4. CV-Safe Target Encodings (no leakage): category means\n",
    "for col, m in [('model', 100), ('Brand', 30), ('fuelType', 20), ('transmission', 20)]:\n",
    "    tr_enc, te_enc = cv_target_encode(df_cars_train, df_cars_test, col, ycol='price', m=m)\n",
    "    df_cars_train[f'{col}_te'] = tr_enc\n",
    "    df_cars_test[f'{col}_te']  = te_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e72b5602-fc3d-4397-bb66-10e7b501438a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.3 Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dda482e9-4a68-438d-86c0-e0524a9b8692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split Data"
    }
   },
   "outputs": [],
   "source": [
    "# Split Data, Stratify not necessary due to regression problem and Cross Validation later\n",
    "X = df_cars_train.drop(columns='price')\n",
    "y = df_cars_train['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.2, \n",
    "                                                  random_state = 42, \n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "456d7244-32c4-4acc-ab57-a592cfef856b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Feature Engineering (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31237b07-903b-4ca6-ba93-42d18b61096f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Brand/Model/Fuel/Transmission price anchors without leaking validation targets\n",
    "\n",
    "# We now compute price anchors using only the training fold (X_train + y_train) \n",
    "# and then map them onto X_val. This avoids the subtle leakage where validation\n",
    "# rows help define their own anchor via the global group median.\n",
    "\n",
    "# Rebuild small train/val dataframes with the target so the helper can see `price`\n",
    "train_tmp = X_train.copy()\n",
    "train_tmp[\"price\"] = y_train\n",
    "\n",
    "val_tmp = X_val.copy()\n",
    "val_tmp[\"price\"] = y_val\n",
    "\n",
    "# Compute anchors based only on training labels and map them to validation\n",
    "train_tmp, val_tmp = add_price_anchor_features(\n",
    "    train_tmp,\n",
    "    val_tmp,\n",
    "    target_col=\"price\"\n",
    ")\n",
    "\n",
    "# Drop target again: X_train/X_val now include brand_*_price + *_anchor columns\n",
    "X_train = train_tmp.drop(columns=[\"price\"])\n",
    "X_val   = val_tmp.drop(columns=[\"price\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9425a7c0-ec96-4458-a0da-8ef65acb3f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.4 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88f8b62d-f805-4b7a-a04f-82c7f29ff9f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_float_array(x):\n",
    "    \"\"\"Convert input to float array.\"\"\"\n",
    "    return np.array(x, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0912ffe3-c356-41e7-8f4b-3691fb223051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PIPELINE WITH preprocessor_orig CONTAINING ONLY ORIGINAL FEATURES\n",
    "\n",
    "\n",
    "orig_numeric_features = [\n",
    "    \"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\"\n",
    "]\n",
    "orig_categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "numeric_transformer_orig = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),    # simple global median imputation\n",
    "    (\"to_float\", FunctionTransformer(to_float_array)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer_orig = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # fill by mode instead of Unknown\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # One-hot encoding\n",
    "])\n",
    "\n",
    "preprocessor_orig = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer_orig, orig_numeric_features),\n",
    "    (\"cat\", categorical_transformer_orig, orig_categorical_features)\n",
    "])\n",
    "\n",
    "preprocessor_orig.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "39142374-d485-42e7-a6cb-83ecffae0091",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocessing: with sklearn Pipeline & Column Transformer"
    }
   },
   "outputs": [],
   "source": [
    "# PIPELINE WITH preprocessor_fe CONTAINING ENGINEERED FEATURES\n",
    "\n",
    "# Custom Written GroupMedianImputer to get Brand and Model specific Medians\n",
    "# Uses target-encoded Brand_te/model_te as grouping cols (present in numeric_features)\n",
    "group_imputer = GroupMedianImputer(group_cols=[\"Brand_te\", \"model_te\"])\n",
    "\n",
    "numeric_features = [\n",
    "    \"age\", \"age_rel_brand\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"model_freq\",\n",
    "    \"brand_med_price_anchor\", \"model_med_price_anchor\", \"brand_fuel_med_price_anchor\", \"brand_trans_med_price_anchor\",\n",
    "    \"age_x_engine\", \"mpg_x_engine\",\n",
    "    \"model_te\", \"Brand_te\", \"fuelType_te\", \"transmission_te\",\n",
    "    \"tax_per_engine\", \"mpg_per_engine\"\n",
    "]\n",
    "log_features = [\"mileage\", \"miles_per_year\"]  # TODO other num columns here better?!\n",
    "categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "# left out columns: year (age is better), hasDamage (unsure what the two values 0 and NaN mean)\n",
    "\n",
    "\n",
    "log_transformer_fe = Pipeline([\n",
    "    # Hierarchical imputation on Brand_te/model_te, then log-transform\n",
    "    (\"group_impute\", GroupMedianImputer(group_cols=[\"Brand_te\", \"model_te\"])),\n",
    "    (\"to_float\", FunctionTransformer(to_float_array)),\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=False)),  # log1p handles zeros safely\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "numeric_transformer_fe = Pipeline([\n",
    "    (\"group_impute\", GroupMedianImputer(group_cols=[\"Brand_te\", \"model_te\"])),\n",
    "    (\"to_float\", FunctionTransformer(to_float_array)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer_fe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # fill by mode instead of Unknown\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # One-hot encoding\n",
    "])\n",
    "\n",
    "# ColumnTransformer that uses all engineered features\n",
    "preprocessor_fe = ColumnTransformer([\n",
    "    (\"log\", log_transformer_fe, log_features),\n",
    "    (\"num\", numeric_transformer_fe, numeric_features),\n",
    "    (\"cat\", categorical_transformer_fe, categorical_features)\n",
    "])\n",
    "\n",
    "preprocessor_fe.fit(X_train)\n",
    "\n",
    "# EXPLANATIONS:\n",
    "# 1) Pipeline bundles preprocessing + model training:\n",
    "#       > Ensures all preprocessing happens inside cross-validation folds (no data leakage)\n",
    "#       > Keeps the entire workflow reproducible — scaling, encoding, and modeling are learned together\n",
    "#       > After .fit(), the final model automatically knows how to preprocess new unseen data\n",
    "#       > When saving with joblib, the entire preprocessing (imputers, scalers, encoders) and model are stored together\n",
    "\n",
    "# 2) The ColumnTransformer applies different transformations to subsets of features:\n",
    "#       > Numeric Features arehandled by our custom GroupMedianImputer (domain-aware filling)\n",
    "#           - Missing numeric values are imputed hierarchically:\n",
    "#           1. By (Brand, model)\n",
    "#           2. If missing model by Brand\n",
    "#           3. If missing Brand by global median\n",
    "#       > This approach captures brand/model-level patterns (e.g. BMWs have similar engine sizes)\n",
    "#       > After imputation, StandardScaler standardizes all numeric features\n",
    "#\n",
    "#       > Log Features use the same group-median imputation, followed by log1p() transformation\n",
    "#           - log1p() compresses large, skewed values (like mileage or price-related features), stabilizing variance and helping linear models perform better\n",
    "#           - StandardScaler then scales them to zero mean and unit variance\n",
    "#\n",
    "#       > Categorical Features are handled by SimpleImputer + OneHotEncoder\n",
    "#           - SimpleImputer fills missing categorical values with the most frequent (mode) value.\n",
    "#             (Alternative would be “Unknown”, but mode keeps categories realistic, e.g. most cars in a model share the same transmission)\n",
    "#           - OneHotEncoder converts each categorical label (Brand, model, etc.) into binary dummy variables\n",
    "#             This lets the model use category information numerically without implying order\n",
    "#\n",
    "# 3) Overall:\n",
    "#       > The pipeline ensures consistent preprocessing across training, validation, and test data.\n",
    "#       > It combines domain knowledge (brand/model-aware imputation) with robust numerical scaling.\n",
    "#       > Linear models (ElasticNet, Ridge, Lasso) and tree models (HistGradientBoosting, RandomForest)\n",
    "#           can now learn from the same standardized, clean, and information-rich feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba8f763f-298d-444a-80a9-0270ba842dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "130e5929-dcd7-48db-8afa-8f9f560968fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Selection with Filter Methods"
    }
   },
   "outputs": [],
   "source": [
    "X_train_proc = preprocessor_fe.transform(X_train)\n",
    "\n",
    "feature_names_all = []\n",
    "for name, trans, cols in preprocessor_fe.transformers_:\n",
    "    if name != 'remainder':\n",
    "        if hasattr(trans, 'get_feature_names_out'):\n",
    "            # for categorical OHE\n",
    "            try:\n",
    "                feature_names_all.extend(trans.get_feature_names_out(cols))\n",
    "            except:\n",
    "                feature_names_all.extend(cols)\n",
    "        else:\n",
    "            feature_names_all.extend(cols)\n",
    "\n",
    "X_df = pd.DataFrame(X_train_proc, columns=feature_names_all)\n",
    "\n",
    "\n",
    "# Variance Threshold\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "vt.fit(X_df)\n",
    "vt_deselect = [f for f, keep in zip(feature_names_all, vt.get_support()) if not keep]\n",
    "print(\"Features to deselect according to VarianceThreshold:\", vt_deselect)\n",
    "\n",
    "\n",
    "# Spearman correlation (numeric + log only)\n",
    "numeric_log = numeric_features + log_features\n",
    "spearman_deselect = []\n",
    "for f in numeric_log:\n",
    "    if f in X_df.columns:\n",
    "        corr, _ = spearmanr(X_df[f], y_train)\n",
    "        if abs(corr) <= 0.05:\n",
    "            spearman_deselect.append(f)\n",
    "print(\"Features to deselect according to Spearman correlation:\", spearman_deselect)\n",
    "\n",
    "\n",
    "# Chi2 (categorical only, must be non-negative)\n",
    "cat_cols = [c for c in X_df.columns if c not in numeric_log]\n",
    "X_cat = X_df[cat_cols].astype(float)\n",
    "chi2_vals, _ = chi2(X_cat, y_train)\n",
    "chi2_deselect = [f for f, val in zip(cat_cols, chi2_vals) if val <= 0]\n",
    "print(\"Features to deselect according to Chi²:\", chi2_deselect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d62d3f6a-baa5-40e9-913a-b99f443b5080",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Deselection for linear Models"
    }
   },
   "outputs": [],
   "source": [
    "# Numeric/log features for linear models\n",
    "linear_numeric_features = [f for f in numeric_features + log_features if f not in spearman_deselect]\n",
    "\n",
    "preprocessor_linear = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer_fe, linear_numeric_features),\n",
    "    (\"cat\", categorical_transformer_fe, categorical_features)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# => use preprocessor_linear for linear model setup; since tree models are indifferent to irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0dc6124-f736-46fb-9f73-895b930faae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Model Evaluation Metrics, Baselining, Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6297f034-638d-4db6-a681-597f20036432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definition of Function for Model Assessment"
    }
   },
   "outputs": [],
   "source": [
    "# Model evaluation metrics used throughout this analysis:\n",
    "#   MAE: Mean Absolute Error: average absolute deviation between predicted and true car prices\n",
    "#           Easy to interpret in pounds, same metric used by Kaggle competition\n",
    "#   RMSE: Root Mean Squared Error: sensitive to outliers, helps identify large prediction errors  \n",
    "#   R²: Coefficient of determination: proportion of variance explained by the model\n",
    "#           1.0 = perfect predictions, 0.0 = same as predicting mean, < 0.0 = worse than mean\n",
    "#\n",
    "# These metrics are appropriate for regression problems predicting continuous variables (car prices)\n",
    "# We define them in the method file car_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b26348e-a5ff-4dc9-9d61-7f59d0ea335d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Baselining mean/median"
    }
   },
   "outputs": [],
   "source": [
    "# Absolute basic baselining with the mean and median\n",
    "\n",
    "mean_pred = y_train.mean()\n",
    "median_pred = y_train.median()\n",
    "\n",
    "print(\"baseline mean predictor: \")\n",
    "print_metrics(y_val, [mean_pred]*len(y_val))\n",
    "# MAE: 6976.3626 | RMSE: 92839550.2849 | R2: -0.0000\n",
    "\n",
    "print(\"-\"*150)\n",
    "\n",
    "print(\"baseline median predictor: \") \n",
    "print_metrics(y_val, [median_pred]*len(y_val))\n",
    "# MAE: 6751.1604 | RMSE: 97557866.6363 | R2: -0.0508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d3aa5b-36ca-4dba-9592-24edd345897d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Models Setup (inkl. Prepro in Pipeline)"
    }
   },
   "outputs": [],
   "source": [
    "### LINEAR MODEL (ElasticNet)\n",
    "\n",
    "# Original-features pipeline\n",
    "elastic_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", ElasticNet(\n",
    "        alpha=0.01,\n",
    "        l1_ratio=0.5,\n",
    "        max_iter=30000,\n",
    "        tol=1e-4,\n",
    "        selection=\"cyclic\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Engineered-features pipeline\n",
    "elastic_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_linear), \n",
    "    (\"model\", ElasticNet(\n",
    "        alpha=0.01,\n",
    "        l1_ratio=0.5,\n",
    "        max_iter=30000,\n",
    "        tol=1e-4,\n",
    "        selection=\"cyclic\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### TREE MODELS\n",
    "\n",
    "# HistGradientBoostingRegressor\n",
    "\n",
    "hgb_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "hgb_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# RandomForestRegressor\n",
    "\n",
    "rf_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# ExtraTreesRegressor\n",
    "\n",
    "et_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "et_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### KERNEL-BASED MODEL (SVR)\n",
    "\n",
    "svr_pipe_orig = Pipeline([\n",
    "    (\"preprocess\", preprocessor_orig),\n",
    "    (\"model\", SVR(\n",
    "        kernel=\"rbf\",\n",
    "        C=10,\n",
    "        epsilon=0.1,\n",
    "        gamma=\"scale\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "svr_pipe_fe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_fe),\n",
    "    (\"model\", SVR(\n",
    "        kernel=\"rbf\",\n",
    "        C=10,\n",
    "        epsilon=0.1,\n",
    "        gamma=\"scale\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# ENSEMBLE META MODEL (Stacking)\n",
    "\n",
    "stack_pipe_orig = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"elastic_orig\", elastic_pipe_orig),\n",
    "        (\"hgb_orig\", hgb_pipe_orig),\n",
    "        (\"rf_orig\", rf_pipe_orig),\n",
    "    ],\n",
    "    final_estimator=HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42\n",
    "    ),\n",
    "    passthrough=False,   # <- was True: disable raw-X passthrough to avoid string->float error\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "stack_pipe_fe = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"elastic_fe\", elastic_pipe_fe),\n",
    "        (\"hgb_fe\", hgb_pipe_fe),\n",
    "        (\"rf_fe\", rf_pipe_fe),\n",
    "    ],\n",
    "    final_estimator=HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42\n",
    "    ),\n",
    "    passthrough=False,   # <- same here\n",
    "    n_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Model Evaluation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d9b2b569-37c5-4048-806a-1fb114dd6817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First evaluation of metrics based on original and engineered feature pipeline to decide how to proceed\n",
    "\n",
    "models_orig = {\n",
    "    \"ElasticNet_orig\": elastic_pipe_orig,\n",
    "    \"HGB_orig\": hgb_pipe_orig,\n",
    "    \"RF_orig\": rf_pipe_orig,\n",
    "    \"ET_orig\": et_pipe_orig,\n",
    "    \"SVR_orig\": svr_pipe_orig,\n",
    "    \"Stack_orig\": stack_pipe_orig,\n",
    "}\n",
    "\n",
    "models_fe = {\n",
    "    \"ElasticNet_fe\": elastic_pipe_fe,\n",
    "    \"HGB_fe\": hgb_pipe_fe,\n",
    "    \"RF_fe\": rf_pipe_fe,\n",
    "    \"ET_fe\": et_pipe_fe,\n",
    "    \"SVR_fe\": svr_pipe_fe,\n",
    "    \"Stack_fe\": stack_pipe_fe,\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in {**models_orig, **models_fe}.items():\n",
    "    print(f\"Fitting {name} ...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    rmse = mean_squared_error(y_val, y_pred)  # keep consistent with print_metrics\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"feature_set\": \"original\" if name.endswith(\"_orig\") else \"engineered\",\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "    })\n",
    "\n",
    "results_df = (\n",
    "    pd.DataFrame(results)\n",
    "      .sort_values([\"feature_set\", \"MAE\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffadcd30-bf06-4504-a3f5-a6c0e43dbd8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After a first run comparing the original feature pipeline and the engineered feature pipeline for all models, we decided to focus on RandomForest and HistGradientBoost. \n",
    "\n",
    "They seem to have the best prediction performance for now. StackingRegressor currently performs best, but since it is blending existing models, we will focus on that and reevaluatedin the end.\n",
    "\n",
    "With ExtraTrees and SVR really underperforming, we decide not to do Hyperparameter Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c880d2d-c7aa-478f-8d34-90aa6e33a172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.1 ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18042056-7ef9-489f-afc3-958fe02fbfc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ElasticNet"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: ElasticNet\n",
    "\n",
    "elastic_param_grid = {\n",
    "    \"model__alpha\": [0.001],    # also tried 0.01, 0.05, 0.1, 0.5\n",
    "    \"model__l1_ratio\": [0.9]    # also tried 0.1, 0.3, 0.5, 0.7  \n",
    "}\n",
    "\n",
    "elastic_grid = GridSearchCV(\n",
    "    elastic_pipe_fe, \n",
    "    param_grid=elastic_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "elastic_grid.fit(X_train, y_train)\n",
    "elastic_best = elastic_grid.best_estimator_\n",
    "elastic_val_pred = elastic_best.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"ElasticNet Results: \")\n",
    "print_metrics(y_val, elastic_val_pred)\n",
    "print(\"Best ElasticNet params:\", elastic_grid.best_params_)\n",
    "\n",
    "# ElasticNet Results: \n",
    "# MAE: 2353.9112 | RMSE: 13356867.7860 | R2: 0.8534\n",
    "# Best ElasticNet params: {'model__alpha': 0.001, 'model__l1_ratio': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95719624-7d97-43e5-9da4-46bb44f4ef6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wrapper Feature Selection (RFECV) for ElasticNet — uses preprocessor_linear and existing print_metrics\n",
    "\n",
    "elastic_base = ElasticNet(\n",
    "    alpha=0.001,     # from your best params\n",
    "    l1_ratio=0.9,    # from your best params\n",
    "    max_iter=30000,\n",
    "    tol=1e-4,\n",
    "    selection=\"cyclic\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rfecv_pipe_linear = Pipeline([\n",
    "    (\"preprocess\", preprocessor_linear),  # uses linear_numeric_features + categorical_features\n",
    "    (\"rfecv\", RFECV(\n",
    "        estimator=elastic_base,\n",
    "        step=10,                               # remove 10 features per iteration\n",
    "        cv=3,                                 # consistent with your tuning\n",
    "        scoring=\"neg_mean_absolute_error\",    # same metric\n",
    "        n_jobs=1,\n",
    "        min_features_to_select=10             # adjust as needed\n",
    "    )),\n",
    "    (\"model\", ElasticNet(                    # final model with same params\n",
    "        alpha=0.001,\n",
    "        l1_ratio=0.9,\n",
    "        max_iter=30000,\n",
    "        tol=1e-4,\n",
    "        selection=\"cyclic\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "rfecv_pipe_linear.fit(X_train, y_train)\n",
    "val_pred_linear_rfecv = rfecv_pipe_linear.predict(X_val)\n",
    "print(\"ElasticNet with RFECV (Wrapper Selection):\")\n",
    "print_metrics(y_val, val_pred_linear_rfecv)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "636039d8-d2f6-41d3-afa0-4d04503af34f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show selected (post-preprocessing) feature names\n",
    "pre = rfecv_pipe_linear.named_steps[\"preprocess\"]\n",
    "support_mask = rfecv_pipe_linear.named_steps[\"rfecv\"].support_\n",
    "\n",
    "feature_names_linear = (\n",
    "    linear_numeric_features\n",
    "    + list(pre.named_transformers_[\"cat\"]\n",
    "              .named_steps[\"encoder\"]\n",
    "              .get_feature_names_out(categorical_features))\n",
    ")\n",
    "\n",
    "selected_features_linear = [n for n, keep in zip(feature_names_linear, support_mask) if keep]\n",
    "print(f\"Selected {len(selected_features_linear)} features:\")\n",
    "for n in selected_features_linear:\n",
    "    print(\"-\", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23c273de-39f7-40d4-ac12-acd71371fc96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "RFECV selected 143/143 features, meaning the best cross‑validated MAE was achieved with the full post‑preprocessing feature set and dropping any feature hurt performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad5a0287-45b6-4022-8d47-cde8cc52469e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.2 HistGradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2d5effc-c1e9-4606-aa57-f0b5041d2a35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: HistGradientBoost"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to use it here and potentially use it later for a final hyperparameter tuning after feature selection again\n",
    "def hgb_hyperparameter_tuning(hgb_estimator, n_iter):\n",
    "    # hgb_param_dist_old = {\n",
    "    #     \"model__learning_rate\": uniform(0.01, 0.09),       # samples values\n",
    "    #     \"model__max_leaf_nodes\": randint(20, 120),         # tries between 20–120 leaves\n",
    "    #     \"model__min_samples_leaf\": randint(2, 20),         # samples leaf sizes between 2–20\n",
    "    #     \"model__max_iter\": randint(400, 1000),             # tries 400–1000 iterations\n",
    "    #     \"model__l2_regularization\": uniform(0.0, 1.0)      # samples small regularization values\n",
    "    # }\n",
    "\n",
    "    # optimized the parameter distributions based on previous runs to focus search space\n",
    "    hgb_param_dist = {\n",
    "        \"model__learning_rate\": [0.06389789198396824],\n",
    "        \"model__max_leaf_nodes\": [105],\n",
    "        \"model__min_samples_leaf\": [3],\n",
    "        \"model__max_iter\": [642],\n",
    "        \"model__l2_regularization\": [0.942853570557981],\n",
    "        \"model__early_stopping\": [True],\n",
    "        \"model__validation_fraction\": [0.1],\n",
    "        \"model__n_iter_no_change\": [20],\n",
    "        \"model__random_state\":[42]\n",
    "    }\n",
    "    \n",
    "\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=42)  # 3-fold for faster runtime\n",
    "\n",
    "    # Randomized search setup\n",
    "    hgb_random = RandomizedSearchCV(\n",
    "        estimator=hgb_estimator,\n",
    "        param_distributions=hgb_param_dist,\n",
    "        n_iter=n_iter,                         # number of random combinations to try\n",
    "        scoring=\"neg_mean_absolute_error\",     # optimize for MAE (primary metric)\n",
    "        cv=cv,\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        verbose=3\n",
    "    )\n",
    "\n",
    "    # Fit the search\n",
    "    hgb_random.fit(X_train, y_train)\n",
    "    return hgb_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998dab6d-7494-483c-ab9e-a075b4c75c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Don't run this cell unless you want to do hyperparameter tuning again (very time consuming)\n",
    "hgb_random = hgb_hyperparameter_tuning(hgb_pipe_fe, n_iter=1) \n",
    "# Get best model\n",
    "hgb_best = hgb_random.best_estimator_\n",
    "print(\"Best Params:\", hgb_random.best_params_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "hgb_val_pred = hgb_random.best_estimator_.predict(X_val)\n",
    "print_metrics(y_val, hgb_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70de2673-0443-4cc4-b1cd-f3813a969246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.3 RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: RandomForest\n",
    "\n",
    "# Old parameter distribution\n",
    "# rf_param_dist_old = {\n",
    "#     \"model__n_estimators\": randint(200, 600),        # number of trees\n",
    "#     \"model__max_depth\": randint(5, 40),              # depth of each tree\n",
    "#     \"model__min_samples_split\": randint(2, 10),      # min samples to split an internal node\n",
    "#     \"model__min_samples_leaf\": randint(1, 8),        # min samples per leaf\n",
    "#     \"model__max_features\": [\"sqrt\", \"log2\"],         # feature sampling strategy\n",
    "#     \"model__bootstrap\": [True, False]                # use bootstrapping or not\n",
    "# }\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "rf_param_dist = {\n",
    "    \"model__n_estimators\": [467],        \n",
    "    \"model__max_depth\": [32],              \n",
    "    \"model__min_samples_split\": [9],      \n",
    "    \"model__min_samples_leaf\": [1],        \n",
    "    \"model__max_features\": [\"sqrt\"],         \n",
    "    \"model__bootstrap\": [False]                \n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Randomized search setup\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf_pipe_fe,\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=1,                         # reduced number of random combinations to 1 because of fixed set after previous runs\n",
    "    scoring=\"neg_mean_absolute_error\", \n",
    "    cv=cv,\n",
    "    n_jobs=1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the search\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "rf_best_rand = rf_random.best_estimator_\n",
    "print(\"Best Params:\", rf_random.best_params_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "rf_val_pred = rf_best_rand.predict(X_val)\n",
    "print_metrics(y_val, rf_val_pred)\n",
    "\n",
    "# Save best model\n",
    "# joblib.dump(rf_best_rand, \"rf_best.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "980b2649-0f0d-4d52-91a2-dc569c878850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5.6 StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0525442b-db49-45b7-ad73-ff98599eb223",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: StackingRegressor"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: StackingRegressor\n",
    "\n",
    "# Old parameter distribution\n",
    "# stack_param_dist = {\n",
    "#     \"final_estimator__learning_rate\": uniform(0.02, 0.08),\n",
    "#     \"final_estimator__max_depth\": randint(3, 7),\n",
    "#     \"final_estimator__min_samples_leaf\": randint(3, 15),\n",
    "#     \"final_estimator__l2_regularization\": uniform(0.0, 1.0)\n",
    "# }\n",
    "\n",
    "# So far best parameter distribution based on previous runs to focus search space\n",
    "stack_param_dist = {\n",
    "    \"final_estimator__learning_rate\": [0.0960571445127933],\n",
    "    \"final_estimator__max_depth\": [5],\n",
    "    \"final_estimator__min_samples_leaf\": [10],\n",
    "    \"final_estimator__l2_regularization\": [0.3745401188473625]\n",
    "}\n",
    "\n",
    "stack_random = RandomizedSearchCV(\n",
    "    estimator=stack_pipe_fe,\n",
    "    param_distributions=stack_param_dist,\n",
    "    n_iter=25,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=3,                 # low CV because stacking is slow\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "stack_random.fit(X_train, y_train)\n",
    "\n",
    "stack_best = stack_random.best_estimator_\n",
    "print(\"StackingRegressor Best Params:\", stack_random.best_params_)\n",
    "\n",
    "stack_val_pred = stack_best.predict(X_val)\n",
    "print_metrics(y_val, stack_val_pred)\n",
    "\n",
    "# joblib.dump(stack_best, \"stack_best.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "852bd7bc-33d9-4ae4-a552-0850b200be11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Feature Importance of Tree Models (with SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c02463-def6-4c5c-93ce-5ca710b6eb7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  **Problem:** Current feature selection targets linear models\n",
    "  (ElasticNet), but we primarily use tree-based models (HGB,\n",
    "  RandomForest).\n",
    "\n",
    "  **Solution:** Use SHAP (SHapley Additive exPlanations) to\n",
    "  identify feature importance specifically for tree models\n",
    "\n",
    "  **Why SHAP for Trees:**\n",
    "  - Provides exact feature importance values for tree-based\n",
    "  models\n",
    "  - Tree models handle irrelevant features, but noise features\n",
    "  still impact performance\n",
    "  - Enables data-driven selection rather than statistical filter\n",
    "  methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9a6ba540-ce60-45c2-a760-a22b0974d4b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_shap_bar(top_df, title, cmap):\n",
    "    \"\"\"\n",
    "    Horizontal bar plot for SHAP feature importances.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    top_df : pd.DataFrame\n",
    "        DataFrame with columns [\"feature\", \"importance\"], already sorted and sliced.\n",
    "    title : str\n",
    "        Plot title.\n",
    "    cmap : matplotlib colormap\n",
    "        Colormap used for bar coloring (e.g. plt.cm.Blues, plt.cm.Greens).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars = ax.barh(\n",
    "        top_df[\"feature\"],\n",
    "        top_df[\"importance\"],\n",
    "        color=cmap(np.linspace(0.4, 0.9, len(top_df)))\n",
    "    )\n",
    "    ax.set_xlabel(\"Average |SHAP| value\")\n",
    "    ax.set_ylabel(\"Feature\")\n",
    "    ax.set_title(title)\n",
    "    ax.bar_label(bars, fmt=\"%.0f\", padding=3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7de8112-ad4c-4761-a868-9ed084f86174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.1 HGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cadbb5c-a5f6-4fb1-963c-70ff79b146f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 1: Baseline Performance with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "51a1844a-48ae-4a25-8f9a-5759a7d13a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_val_processed = hgb_best.named_steps[\"preprocess\"].transform(X_val)\n",
    "hgb_val_pred = hgb_best.named_steps[\"model\"].predict(X_val_processed)\n",
    "n_features_total = X_val_processed.shape[1]\n",
    "baseline_mae = mean_absolute_error(y_val, hgb_val_pred)\n",
    "\n",
    "print(\"Baseline Performance of HGB model after Hyperparameter Tuning:\\n\")\n",
    "print_metrics(y_val, hgb_val_pred)\n",
    "print(f\"\\nTotal features used: {n_features_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a381d64-0406-4daa-a147-95081efa4eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 2: SHAP Feature Importance Analysis\n",
    "\n",
    "We use SHAP's TreeExplainer to calculate feature importance values. TreeExplainer is specifically optimized for tree-based models and provides exact Shapley values efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "41d9d985-7fa1-4326-9524-57af6c84e7a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility, otherwise we get different results each time we run the notebook\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract trained model and preprocessed data\n",
    "hgb_model = hgb_best.named_steps[\"model\"]\n",
    "X_train_processed = hgb_best.named_steps[\"preprocess\"].transform(X_train)\n",
    "\n",
    "# Build feature names manually (avoid GroupMedianImputer issue)\n",
    "feature_names_all = (\n",
    "    log_features\n",
    "    + numeric_features\n",
    "    + list(\n",
    "        hgb_best.named_steps[\"preprocess\"]\n",
    "        .named_transformers_[\"cat\"]\n",
    "        .named_steps[\"encoder\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "    )\n",
    ")\n",
    "\n",
    "# TreeExplainer for HGB model\n",
    "explainer = shap.TreeExplainer(hgb_model)\n",
    "\n",
    "# Sample for speed\n",
    "sample_size = min(1000, len(X_train_processed))\n",
    "sample_indices = np.random.choice(len(X_train_processed), sample_size, replace=False)\n",
    "X_sample = X_train_processed[sample_indices]\n",
    "\n",
    "print(f\"Computing SHAP values for HGB on {sample_size} samples...\")\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# Mean absolute SHAP = global importance\n",
    "feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Create DataFrame with proper columns\n",
    "shap_importance_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names_all,\n",
    "        \"importance\": feature_importance\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 20 most important features:\")\n",
    "print(shap_importance_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "355aaca1-92eb-42d9-82d5-e76c3a8b99bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 3: Automated Feature Selection Optimization\n",
    "\n",
    "Now we systematically test different numbers of top features to find the optimal subset. We train the model with the same optimized hyperparameters but using only the most important features identified by SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4444cebb-c91d-43a4-bb57-65188afa4d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nHGB Feature Selection Analysis with different bins:\")\n",
    "\n",
    "results = []\n",
    "total_features = len(feature_names_all)\n",
    "\n",
    "# Test range — adjust as you like\n",
    "# feature_counts = list(range(5, total_features + 1))\n",
    "feature_counts = list(range(16,20))\n",
    "\n",
    "# Define bins to print # TODO bins werden nicht geprinted\n",
    "print_bins = {5, 10, 15, 20, 30, 50, 70, 100, total_features}\n",
    "\n",
    "# Use the same processed validation data\n",
    "X_val_processed = hgb_best.named_steps[\"preprocess\"].transform(X_val)\n",
    "\n",
    "\n",
    "hgb_model = hgb_best.named_steps[\"model\"]\n",
    "clean_params = hgb_model.get_params()\n",
    "\n",
    "# Track best model\n",
    "best_model = None\n",
    "best_mae = float(\"inf\")\n",
    "best_n = None\n",
    "best_features = None\n",
    "\n",
    "for n_features in feature_counts:\n",
    "    # Select top N features\n",
    "    top_features = shap_importance_df.head(n_features)[\"feature\"].tolist()\n",
    "    feature_indices = [i for i, fname in enumerate(feature_names_all) if fname in top_features]\n",
    "\n",
    "    X_train_subset = X_train_processed[:, feature_indices]\n",
    "    X_val_subset   = X_val_processed[:, feature_indices]\n",
    "\n",
    "    # Train new model on subset of features\n",
    "    hgb_selected = HistGradientBoostingRegressor(**clean_params)\n",
    "    hgb_selected.fit(X_train_subset, y_train)\n",
    "\n",
    "    pred_subset = hgb_selected.predict(X_val_subset)\n",
    "    mae_subset = mean_absolute_error(y_val, pred_subset)\n",
    "\n",
    "    # Save result\n",
    "    results.append({\"n_features\": n_features, \"mae\": mae_subset})\n",
    "\n",
    "    # Check if it's the best so far\n",
    "    if mae_subset < best_mae:\n",
    "        best_mae = mae_subset\n",
    "        best_n = n_features\n",
    "        best_model = hgb_selected\n",
    "        best_features = top_features\n",
    "\n",
    "    # Print only key bins\n",
    "    if n_features in print_bins:\n",
    "        improvement = baseline_mae - mae_subset\n",
    "        print(f\"Top {n_features:3d} features: MAE: {mae_subset:.1f} (Δ: {improvement:+.1f})\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\nOptimal feature selection results:\")\n",
    "print(f\"Best performance with {best_n} features: MAE: {best_mae:.2f}\")\n",
    "print(f\"Improvement over baseline: {baseline_mae - best_mae:+.2f} MAE\\n\")\n",
    "\n",
    "print(f\"Optimal {best_n} features for production model:\")\n",
    "for i, feat in enumerate(best_features, start=1):\n",
    "    imp = shap_importance_df.loc[shap_importance_df['feature'] == feat, 'importance'].values[0]\n",
    "    print(f\"{i:2d}. {feat:25s} ({imp:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "94cd8db0-31da-4f79-a4e1-00181450e3f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "top_df = shap_importance_df.head(top_k).iloc[::-1]\n",
    "plot_shap_bar(\n",
    "    top_df,\n",
    "    title=f\"Top {top_k} HGB features by SHAP importance\",\n",
    "    cmap=plt.cm.Blues\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1e893a1a-3c61-4f13-9fd9-35715c65d492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build the final pipeline with feature selection included\n",
    "def select_best_features(X):\n",
    "    idx = [i for i, fname in enumerate(feature_names_all) if fname in best_features]\n",
    "    return X[:, idx]\n",
    "\n",
    "hgb_final_pipe = Pipeline([\n",
    "    (\"preprocess\", hgb_best.named_steps[\"preprocess\"]),\n",
    "    (\"feature_selector\", FunctionTransformer(select_best_features, validate=False)),\n",
    "    (\"model\", best_model)\n",
    "])\n",
    "\n",
    "# Save the best model for later use\n",
    "joblib.dump(hgb_final_pipe, \"hgb_best_feature.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a16b675a-660c-46c8-8430-5b28d91b8c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.2 RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f65c9142-f110-4d6f-a31c-5764d2492fc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 1: Baseline Performance with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95adb47f-fb45-47f0-b24d-e75d5ef1c68e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the tuned RF pipeline (rf_best_rand) and compute baseline on the validation set\n",
    "X_val_processed_rf = rf_best_rand.named_steps[\"preprocess\"].transform(X_val)\n",
    "rf_val_pred = rf_best_rand.named_steps[\"model\"].predict(X_val_processed_rf)\n",
    "n_features_total_rf = X_val_processed_rf.shape[1]\n",
    "baseline_mae_rf = mean_absolute_error(y_val, rf_val_pred)\n",
    "\n",
    "print(\"Baseline Performance of RF model after Hyperparameter Tuning:\\n\")\n",
    "print_metrics(y_val, rf_val_pred)\n",
    "print(f\"\\nTotal features used: {n_features_total_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eef64537-b8f4-4f70-b060-78606e96731b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 2: SHAP Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "09d26704-08d7-4e72-b236-d44d0c2433ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility, otherwise we get different results each time we run the notebook\n",
    "np.random.seed(42)\n",
    "\n",
    "# Preprocess training data (same preprocessor as in rf_best_rand)\n",
    "X_train_processed_rf = rf_best_rand.named_steps[\"preprocess\"].transform(X_train)\n",
    "\n",
    "# Build feature names (log + numeric + OHE categories), same logic as for HGB\n",
    "feature_names_all_rf = (\n",
    "    log_features\n",
    "    + numeric_features\n",
    "    + list(\n",
    "        rf_best_rand.named_steps[\"preprocess\"]\n",
    "        .named_transformers_[\"cat\"]\n",
    "        .named_steps[\"encoder\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "    )\n",
    ")\n",
    "\n",
    "# SHAP for RandomForest\n",
    "rf_model = rf_best_rand.named_steps[\"model\"]\n",
    "explainer_rf = shap.TreeExplainer(rf_model)\n",
    "\n",
    "sample_size_rf = min(50, len(X_train_processed_rf))\n",
    "sample_idx_rf = np.random.choice(len(X_train_processed_rf), sample_size_rf, replace=False)\n",
    "X_sample_rf = X_train_processed_rf[sample_idx_rf]\n",
    "\n",
    "print(f\"Computing SHAP values for RF on {sample_size_rf} samples...\")\n",
    "shap_values_rf = explainer_rf.shap_values(X_sample_rf)\n",
    "\n",
    "# Mean absolute SHAP = global importance\n",
    "feature_importance_rf = np.abs(shap_values_rf).mean(axis=0)\n",
    "\n",
    "# Create DataFrame with proper columns\n",
    "shap_importance_df_rf = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names_all_rf,\n",
    "        \"importance\": feature_importance_rf\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 20 most important RF features:\")\n",
    "print(shap_importance_df_rf.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0af8a6-1394-464d-8e8c-d452d0138cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 3: Automated Feature Selection Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81e6107a-fdf8-4229-a971-5749c4bddf2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO Step 2 (6 min) and Step 3 (9 min) vs HGB Step 2+3 40 seconds => code needs to be refined, cant be that longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ef268bcf-3ae2-4345-a60f-68ed288d5ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nRF Feature Selection Analysis with different bins:\")\n",
    "\n",
    "results_rf = []\n",
    "\n",
    "# Use the same processed validation data\n",
    "X_val_processed_rf = rf_best_rand.named_steps[\"preprocess\"].transform(X_val)\n",
    "\n",
    "# Reuse tuned RF hyperparameters\n",
    "rf_params = {k.replace(\"model__\", \"\"): v for k, v in rf_random.best_params_.items()}\n",
    "\n",
    "# Track best model\n",
    "best_model_rf = None\n",
    "best_mae_rf = float(\"inf\")\n",
    "best_n_rf = None\n",
    "best_features_rf = None\n",
    "\n",
    "# feature_counts_rf = list(range(5, total_features + 1)) \n",
    "feature_counts_rf = list(range(16, 30)) #faster\n",
    "print_bins_rf = {5, 10, 15, 20, 30, 50, 70, 100, len(feature_names_all_rf)}\n",
    "\n",
    "for n_features in feature_counts_rf:\n",
    "    # Select top N features\n",
    "    top_features_rf = shap_importance_df_rf.head(n_features)[\"feature\"].tolist()\n",
    "    feature_indices_rf = [i for i, fname in enumerate(feature_names_all_rf) if fname in top_features_rf]\n",
    "\n",
    "    X_train_subset_rf = X_train_processed_rf[:, feature_indices_rf]\n",
    "    X_val_subset_rf   = X_val_processed_rf[:, feature_indices_rf]\n",
    "\n",
    "    # Train RF with the same tuned params\n",
    "    rf_selected = RandomForestRegressor(random_state=42, n_jobs=-1, **rf_params)\n",
    "    rf_selected.fit(X_train_subset_rf, y_train)\n",
    "    pred_subset_rf = rf_selected.predict(X_val_subset_rf)\n",
    "    mae_subset_rf = mean_absolute_error(y_val, pred_subset_rf)\n",
    "\n",
    "    # Save result\n",
    "    results_rf.append({\"n_features\": n_features, \"mae\": mae_subset_rf})\n",
    "\n",
    "    # Best so far?\n",
    "    if mae_subset_rf < best_mae_rf:\n",
    "        best_mae_rf = mae_subset_rf\n",
    "        best_n_rf = n_features\n",
    "        best_model_rf = rf_selected\n",
    "        best_features_rf = top_features_rf\n",
    "\n",
    "    # Optional summary for key bins\n",
    "    if n_features in print_bins_rf:\n",
    "        improvement_rf = baseline_mae_rf - mae_subset_rf\n",
    "        print(f\"Top {n_features:3d} features: MAE: {mae_subset_rf:.1f} (Δ: {improvement_rf:+.1f})\")\n",
    "\n",
    "results_rf_df = pd.DataFrame(results_rf)\n",
    "\n",
    "print(f\"\\nOptimal RF feature selection results:\")\n",
    "print(f\"Best performance with {best_n_rf} features: MAE: {best_mae_rf:.2f}\")\n",
    "print(f\"Improvement over baseline: {baseline_mae_rf - best_mae_rf:+.2f} MAE\\n\")\n",
    "\n",
    "print(f\"Optimal {best_n_rf} RF features for production model:\")\n",
    "for i, feat in enumerate(best_features_rf, start=1):\n",
    "    imp = shap_importance_df_rf.loc[shap_importance_df_rf['feature'] == feat, 'importance'].values[0]\n",
    "    print(f\"{i:2d}. {feat:25s} ({imp:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a5bdfcad-da5d-4543-8fce-7ab42593c972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RF SHAP bar plot\n",
    "top_k = 20\n",
    "top_df = shap_importance_df_rf.head(top_k).iloc[::-1]\n",
    "plot_shap_bar(\n",
    "    top_df,\n",
    "    title=f\"Top {top_k} RF features by SHAP importance\",\n",
    "    cmap=plt.cm.Greens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e8398f0e-737d-4ae1-8ba1-5f8f21aa211d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the best RF model for later use\n",
    "\n",
    "# Build the final RF pipeline with feature selection included\n",
    "def select_best_features_rf(X):\n",
    "    idx = [i for i, fname in enumerate(feature_names_all_rf) if fname in best_features_rf]\n",
    "    return X[:, idx]\n",
    "\n",
    "final_rf_pipe = Pipeline([\n",
    "    (\"preprocess\", rf_best_rand.named_steps[\"preprocess\"]),\n",
    "    (\"feature_selector\", FunctionTransformer(select_best_features_rf, validate=False)),\n",
    "    (\"model\", best_model_rf)\n",
    "])\n",
    "\n",
    "# joblib.dump(final_rf_pipe, \"rf_best_feature.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7fbc1f32-5a4e-4b43-a842-86ef6a7f6cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO check whether this might improve performance with big n_iter when everything else is done. If not, just remove section 7.\n",
    "hgb_random = hgb_hyperparameter_tuning(hgb_final_pipe, n_iter=1000)\n",
    "# Get best model\n",
    "hgb_best = hgb_random.best_estimator_\n",
    "print(\"Best Params:\", hgb_random.best_params_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "hgb_val_pred = hgb_random.best_estimator_.predict(X_val)\n",
    "print_metrics(y_val, hgb_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac03ca0e-2a97-46c0-8669-25ce44fc0d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Build Final Stacking Regressor to mix tuned and feature selected HGB and RF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "629b5a0e-41bc-4d94-ac6d-2834d6050bdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stack_pipe_final = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"hgb_final\", hgb_final_pipe),   # tuned HGB pipeline (preprocessor + model)\n",
    "        (\"rf_final\",  final_rf_pipe),    # tuned RF pipeline (preprocessor + model)\n",
    "    ],\n",
    "    final_estimator=LinearRegression(),  # simple, perfect for 2 base preds\n",
    "    passthrough=False,                   # meta model sees only base predictions\n",
    "    cv=5,                                # proper OOF stacking\n",
    "    n_jobs=1                             # no BrokenProcessPool on Databricks\n",
    ")\n",
    "\n",
    "stack_pipe_final.fit(X_train, y_train)\n",
    "stack_val_pred = stack_pipe_final.predict(X_val)\n",
    "print_metrics(y_val, stack_val_pred)\n",
    "\n",
    "# MAE: 1255.3112 | RMSE: 4157099.9081 | R2: 0.9544\n",
    "\n",
    "# Kaggle Score submit 1274 !! OVERFITTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f12a3b8-0b27-4615-b9bd-0e6a9d2d746d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Final SR of the tuned HGB and RF models, did not improve over the best single HGB and RF models on the validation set (MAE 1332 vs 1281/1284). \n",
    "\n",
    "Therefore we will keep the RF/HGB model => with such small difference in MAE, we further need to evaluate them both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "135acfad-1ad6-49f9-97fc-f1a81d0c3187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "927be871-4f27-4453-baac-b4f7102534d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load best Models from Joblib"
    }
   },
   "outputs": [],
   "source": [
    "# Load best Models from Joblib\n",
    "\n",
    "hgb_best_99 = joblib.load(\"hgb_best_feature.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a25f58c-4c24-4460-93d4-830176016ee8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pick best Model, predict on Test"
    }
   },
   "outputs": [],
   "source": [
    "# Pick best model and predict on test:\n",
    "\n",
    "# Build a temporary train df with the target for anchor computation\n",
    "train_tmp = X_train.copy()\n",
    "train_tmp[\"price\"] = y_train\n",
    "\n",
    "# Compute anchors based on training data and map them onto the test set\n",
    "_, df_cars_test_anchors = add_price_anchor_features(\n",
    "    train_tmp,\n",
    "    df_cars_test.copy(),\n",
    "    target_col=\"price\"\n",
    ")\n",
    "\n",
    "# Now we can safely predict on the test set with all expected columns present\n",
    "df_cars_test_anchors[\"price\"] = hgb_best_99.predict(df_cars_test_anchors)\n",
    "\n",
    "# Save submission file\n",
    "df_cars_test_anchors[\"price\"].to_csv(\"Group05_Version05.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f5b18805-e662-4807-bdb8-758df6027d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Submission with API push"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c cars4you -f Group05_Version05.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "350057a3-c771-4bf0-8fac-19b49be17e35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Score Check"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c cars4you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf75ca6e-1284-47a9-9441-b630db689f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. Load and predict model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7668090c-1fca-4f5f-9a5a-cf41e04b2acc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hgb_loaded = joblib.load(\"hgb_best_feature.pkl\")\n",
    "\n",
    "preprocessor = hgb_loaded.named_steps[\"preprocess\"]\n",
    "display(preprocessor)\n",
    "\n",
    "pred_loaded = hgb_loaded.predict(X_val)\n",
    "mae_loaded = mean_absolute_error(y_val, pred_loaded)\n",
    "print(f\"Loaded model MAE on validation set: {mae_loaded:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
