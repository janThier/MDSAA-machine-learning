{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b83a4d32-7797-4f5f-8269-7bc08d0b9431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Clone Before Using to play with Features / Hyperparameters. Use the Separate Clone for MidTermSubmission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52359b17-1216-4744-a528-81e23a033a68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d766326a-8f1d-4017-adfe-f1b69fdce297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# statsmodels review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27fff3e4-624f-4b1f-b258-fbbe7b69cb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import & load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aa35af5d-8257-4fb6-a380-86bbbfac3244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "556ffc24-70f7-48c9-96a4-3a19380f91db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Connect"
    }
   },
   "outputs": [],
   "source": [
    "### Everyone has to do this himself, with his own kaggle.json -> get it from kaggle as api token\n",
    "import os\n",
    "\n",
    "# Folder containing kaggle.json\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/Workspace/Users/20250355@novaims.unl.pt\"\n",
    "\n",
    "# Optional: test\n",
    "!echo $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f54ab11e-fcfc-4278-8997-89a92adf8eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import and load Data"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import re\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, chi2\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, FunctionTransformer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.linear_model import Ridge, Lasso, LassoCV, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from data_cleaning import clean_car_dataframe\n",
    "\n",
    "df_cars_train = pd.read_csv(\"train.csv\")\n",
    "df_cars_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "228971f7-84d2-4472-84d1-f4b91e185bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explorative Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b65f0ae-0d7a-4cdb-830a-391a209b82d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TASK I (3 Points): Descriptive Statistics, Inconsistency Check, Visual Data Explorance, Extraction of Relevant Insights, Multivariate Relationships  => Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b60f640e-1c37-401c-bce1-290392e2be9a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exploratory Data Analysis"
    }
   },
   "outputs": [],
   "source": [
    "df_cars_train.describe(include=\"all\").T\n",
    "\n",
    "# Findings:\n",
    "# column carID has no duplicates\n",
    "# column year: 1970 to 2024\n",
    "# column mileage: -58.000 to 323.000 \n",
    "# column tax: -91 to 580\n",
    "# column mpg: -43 to 470 \n",
    "# column engineSize: -0.1 to 6.6 \n",
    "# column paintQuality%: 70-100, few outliers 1.6 or 125  \n",
    "# column previousOwners: -2.3 to 6.2 \n",
    "# column hasDamage (0/nan, not sure if nan means damaged, convert to Int)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b75ee214-0996-406f-b77f-9e72fa7153e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Cleaning, Feature Engineering, Split & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b78e7ca8-ba25-4ea3-873d-4f3515281440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task II (5 Points): Clean and preprocess the dataset. \n",
    "- Missing Value handling, Outlier preprocessing + justify decisions -> in data_cleaning.py\n",
    "- Review current features and create extra features if needed + explain\n",
    "- Deal with categorical variables -> One-Hot-Encoding \n",
    "- Perform data scaling, explain reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6fa0cfe9-be7e-4e90-ac1c-cd600bb7d13f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Numerical / Categorical Data Cleaning: with Pandas"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier Preprocessing happens here\n",
    "df_cars_train = clean_car_dataframe(df_cars_train)\n",
    "df_cars_test = clean_car_dataframe(df_cars_test)\n",
    "\n",
    "\n",
    "# Safety Check: print all unique values of all columns of df_cars_train // df_cars_test to see if data cleaning worked and if there are still weird values\n",
    "for col in df_cars_train.columns:\n",
    "    print(col, df_cars_train[col].unique())\n",
    "print(\"X\"*150)\n",
    "for col in df_cars_test.columns:\n",
    "    print(col, df_cars_test[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2df7bf7c-0ebb-4f4b-a168-5b2e2f6b21d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering and Explaination\n",
    "\n",
    "# add column age: models can easier interpret linear numerical features\n",
    "df_cars_train['age'] = 2025 - df_cars_train['year']\n",
    "df_cars_test['age'] = 2025 - df_cars_test['year']\n",
    "\n",
    "# miles per year: normalizes the total mileage by how old the car is\n",
    "df_cars_train['miles_per_year'] = df_cars_train['mileage'] / df_cars_train['age'].replace({0: np.nan})\n",
    "df_cars_train['miles_per_year'] = df_cars_train['miles_per_year'].fillna(df_cars_train['mileage'])\n",
    "\n",
    "df_cars_test['miles_per_year'] = df_cars_test['mileage'] / df_cars_test['age'].replace({0: np.nan})\n",
    "df_cars_test['miles_per_year'] = df_cars_test['miles_per_year'].fillna(df_cars_test['mileage'])\n",
    "\n",
    "# model frequency: some models are more common, which means they can be cheaper (supply) or retain their values better (demand). freq shows their popularity\n",
    "model_freq = df_cars_train['model'].value_counts(normalize=True).to_dict()\n",
    "df_cars_train['model_freq'] = df_cars_train['model'].map(model_freq)\n",
    "\n",
    "model_freq = df_cars_test['model'].value_counts(normalize=True).to_dict()\n",
    "df_cars_test['model_freq'] = df_cars_test['model'].map(model_freq)\n",
    "\n",
    "# brand median price (only train): shows brand positioning (e.g. BMW > KIA)\n",
    "brand_median_price = df_cars_train.groupby('Brand')['price'].median()\n",
    "df_cars_train['brand_med_price'] = df_cars_train['Brand'].map(brand_median_price)\n",
    "\n",
    "# model median price (only train): shows model positioning (e.g. 3er > 1er)\n",
    "model_med_price = df_cars_train.groupby('model')['price'].median()\n",
    "df_cars_train['model_med_price'] = df_cars_train['model'].map(model_med_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dda482e9-4a68-438d-86c0-e0524a9b8692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split Data"
    }
   },
   "outputs": [],
   "source": [
    "X = df_cars_train.drop(columns='price')\n",
    "y = df_cars_train['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.3, \n",
    "                                                  random_state = 42, \n",
    "                                                  #stratify = y,    # if y, class proportions get preserved between train and test sets\n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "39142374-d485-42e7-a6cb-83ecffae0091",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocessing: with sklearn Pipeline & Column Transformer"
    }
   },
   "outputs": [],
   "source": [
    "# Define which columns are numeric vs categorical (mileage not in here because skewed - log)\n",
    "numeric_features = [\"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\"]\n",
    "log_features = [\"mileage\", \"miles_per_year\", \"model_freq\", \"brand_med_price\", \"model_med_price\"] # could try to test previousOwners, tax, age here\n",
    "categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "# left out columns: year (age is better), hasDamage (unsure what the two values 0 and NaN mean)\n",
    "\n",
    "\n",
    "log_transformer = Pipeline([\n",
    "    # Handling of missing numerical values with sklearn SimpleImputer (mean)\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    # Data Scaling with sklearn FunctionTransformer (for log) and StandardScaler\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=False)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    # Handling of missing numerical values with sklearn SimpleImputer (mean)\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    # Data Scaling with sklearn StandardScaler\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    # Handling of missing categorical values with sklearn SimpleImputer (Unknown)\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "    # Deal with Categorical Variables with sklearn OneHotEncoder:\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)) \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    # Apply the preprocessing steps to the data\n",
    "    (\"mileage\", log_transformer, [\"mileage\"]), # log because mileage is skewed\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "\n",
    "# Fit preprocessor on training data - avoid data leakage\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ColumnTransformer lets you apply different transformations to different feature subsets:\n",
    "#       > Numeric → impute mean + scale\n",
    "#       > Categorical → impute \"Unknown\" + OneHotEncode\n",
    "#       > Mileage → impute log-transform\n",
    "#   This is key, because numeric and categorical data need different math, you can't scale strings or one-hot encode continuous numbers.\n",
    "\n",
    "\n",
    "# Pipeline bundles preprocessing + model training:\n",
    "#     > Cross-validation applies preprocessing inside each fold (no data leakage).\n",
    "#     > The final model object (after .fit()) knows exactly how to preprocess new data.\n",
    "#     > When saving the pipeline with joblib, everything (scaler, encoder, model) is saved together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba8f763f-298d-444a-80a9-0270ba842dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71e9a293-7457-4767-8a26-7c930f7dbad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task III (3 Points): Define and Implement a clear and unambiguous strategy for feature selection. Use the methods discussed in the course. Present and justify your final selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d989a4b-077d-47bd-b4bb-c4528628888f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Model independent Filter Methods:\n",
    "- Remove constant numerical variables with VarianceThreshold (manual)\n",
    "- Check highly correlated numerical variables and keep one with Spearman (manual)\n",
    "- Remove unindependent categorical variables with Chi2\n",
    "\n",
    "Model dependent Wrapper Methods:\n",
    "- RFE LR / RFE SVR for linear Models: Ridge, Lasso, ElasticNet, SVM\n",
    "- Feature Importance for tree Models: DecisionTrees, RandomForest, GradientBoosting => trees are unsensitive to irrelevant features but doing feature importance and remove some can reduce dimensionality\n",
    "- L1 Regularization for Neural Networks: MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "130e5929-dcd7-48db-8afa-8f9f560968fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Selection with Filter Methods"
    }
   },
   "outputs": [],
   "source": [
    "X_train_proc = preprocessor.transform(X_train)\n",
    "\n",
    "feature_names_all = []\n",
    "for name, trans, cols in preprocessor.transformers_:\n",
    "    if name != 'remainder':\n",
    "        if hasattr(trans, 'get_feature_names_out'):\n",
    "            # for categorical OHE\n",
    "            try:\n",
    "                feature_names_all.extend(trans.get_feature_names_out(cols))\n",
    "            except:\n",
    "                feature_names_all.extend(cols)\n",
    "        else:\n",
    "            feature_names_all.extend(cols)\n",
    "\n",
    "X_df = pd.DataFrame(X_train_proc, columns=feature_names_all)\n",
    "\n",
    "\n",
    "# Variance Threshold\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "vt.fit(X_df)\n",
    "vt_deselect = [f for f, keep in zip(feature_names_all, vt.get_support()) if not keep]\n",
    "print(\"Features to deselect according to VarianceThreshold:\", vt_deselect)\n",
    "\n",
    "\n",
    "# Spearman correlation (numeric + log only)\n",
    "numeric_log = numeric_features + log_features\n",
    "spearman_deselect = []\n",
    "for f in numeric_log:\n",
    "    if f in X_df.columns:\n",
    "        corr, _ = spearmanr(X_df[f], y_train)\n",
    "        if abs(corr) <= 0.05:\n",
    "            spearman_deselect.append(f)\n",
    "print(\"Features to deselect according to Spearman correlation:\", spearman_deselect)\n",
    "\n",
    "\n",
    "# Chi2 (categorical only, must be non-negative)\n",
    "cat_cols = [c for c in X_df.columns if c not in numeric_log]\n",
    "X_cat = X_df[cat_cols].astype(float)\n",
    "chi2_vals, _ = chi2(X_cat, y_train)\n",
    "chi2_deselect = [f for f, val in zip(cat_cols, chi2_vals) if val <= 0]\n",
    "print(\"Features to deselect according to Chi²:\", chi2_deselect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d62d3f6a-baa5-40e9-913a-b99f443b5080",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Deselection for linear Models"
    }
   },
   "outputs": [],
   "source": [
    "# Numeric/log features for linear models\n",
    "linear_numeric_features = [f for f in numeric_features + log_features if f not in spearman_deselect]\n",
    "\n",
    "preprocessor_linear = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, linear_numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# => use preprocessor_linear for linear model setup; since tree models are indifferent to irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dcae03f-ed97-410e-9078-fb734f42be87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Models Setup and Baselining (with SKLEARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06207ec2-af0d-489f-a23d-467019791499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TASK IV (4 Points): Build a simple model and assess the performance\n",
    "- Identify the type of problem and select the relevant algorithms\n",
    "- Select one Model Assessment Strategy to use throughout your work. Which metrics are you using to evaluate your model and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6297f034-638d-4db6-a681-597f20036432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definition of Function for Model Assessment"
    }
   },
   "outputs": [],
   "source": [
    "# Following metrics are used for model evaluation:\n",
    "#\n",
    "#   MAE: Average absolute deviation between predicted and true car prices, easy to interpret, kaggle competition uses same metric\n",
    "#   RMSE: Root mean squared error, helps to see if large errors on same values were made, therefore sensitive to outliers\n",
    "#   R2: Proportion of variance explained by the model, 1 = perfect, 0 = same as predicting mean, < 0 = worse than mean baseline\n",
    "#\n",
    "# Because our task is a regression problem and we are predicting a continuous variable (car price)\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"MAE: {mae:.4f} | RMSE: {rmse:.4f} | R2: {r2:.4f}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d3aa5b-36ca-4dba-9592-24edd345897d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Models Setup (inkl. Prepro in Pipeline)"
    }
   },
   "outputs": [],
   "source": [
    "### LINEAR MODEL\n",
    "# ElasticNet\n",
    "elastic_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_linear),\n",
    "    (\"model\", ElasticNet(\n",
    "        max_iter=20000,\n",
    "        selection=\"random\",\n",
    "        warm_start=False,  # set True only if iteratively tuning manually\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### TREE MODELS\n",
    "# GradientBoostingRegressor: baseline, has to beat ElasticNet, if not something’s wrong with data preprocessing, not the model.\n",
    "gbr_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor), \n",
    "    (\"model\", GradientBoostingRegressor(loss='absolute_error'))\n",
    "])\n",
    "\n",
    "# HistGradientBoostingRegressor: modern and very fast, handles missing values natively (no imputation needed!). often matches or beats XGBoost/LightGBM \n",
    "hgb_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5,  # regularize slightly to prevent overfit, try 0.1, 0.5, 1.0\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# RandomForestRegressor: excellent general baseline ensemble, handles non-linearities well, doesn’t overfit easily but can be slow for large data\n",
    "rf_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300, \n",
    "        max_depth=None, \n",
    "        n_jobs=-1, \n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ExtraTreesRegressor: similar to RandomForest but with more randomization => often better generalization\n",
    "et_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=300, \n",
    "        max_depth=None, \n",
    "        n_jobs=-1, \n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### KERNEL BASED MODEL\n",
    "# SVR: powerful, but slow on large data, sensitive to scaling => already handled in preprocessing\n",
    "svr_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", SVR(C=10, epsilon=0.2, kernel=\"rbf\"))\n",
    "])\n",
    "\n",
    "\n",
    "### ENSEMBLE META MODEL\n",
    "# StackingRegressor: stacks/blends multiple models => typically gives a small but consistent boost in leaderboard competitions\n",
    "stack_pipe = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"el\", elastic_pipe),\n",
    "        (\"hgb\", hgb_pipe),\n",
    "    ],\n",
    "    passthrough=False   # True can sometimes help but increases overfitting risk\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b26348e-a5ff-4dc9-9d61-7f59d0ea335d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Baselining mean/median"
    }
   },
   "outputs": [],
   "source": [
    "mean_pred = y_train.mean()\n",
    "median_pred = y_train.median()\n",
    "\n",
    "print(\"baseline mean predictor: \")\n",
    "print_metrics(y_val, [mean_pred]*len(y_val))\n",
    "# MAE: 6976.3626 | RMSE: 92839550.2849 | R2: -0.0000\n",
    "\n",
    "print(\"-\"*150)\n",
    "\n",
    "print(\"baseline median predictor: \") \n",
    "print_metrics(y_val, [median_pred]*len(y_val))\n",
    "# MAE: 6751.1604 | RMSE: 97557866.6363 | R2: -0.0508"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "18042056-7ef9-489f-afc3-958fe02fbfc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ElasticNet"
    }
   },
   "outputs": [],
   "source": [
    "elastic_param_grid = {\n",
    "    \"model__alpha\": [0.001, 0.01, 0.05, 0.1, 0.5],\n",
    "    \"model__l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9]     \n",
    "}\n",
    "\n",
    "elastic_grid = GridSearchCV(\n",
    "    elastic_pipe, \n",
    "    param_grid=elastic_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "elastic_grid.fit(X_train, y_train)\n",
    "elastic_best = elastic_grid.best_estimator_\n",
    "elastic_val_pred = elastic_best.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"ElasticNet Results: \")\n",
    "print_metrics(y_val, elastic_val_pred)\n",
    "print(\"Best ElasticNet params:\", elastic_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e7dac5c-ae30-4ed6-b7ac-a87942f91697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### HT: tree Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    \"model__max_depth\": [None, 10, 20],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    rf_pipe, rf_param_grid,\n",
    "    cv=5, scoring=\"r2\",\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_best = rf_grid.best_estimator_\n",
    "rf_val_pred = rf_best.predict(X_val)\n",
    "\n",
    "print(\"Random Forest Results: \")\n",
    "print_metrics(y_val, rf_val_pred)\n",
    "\n",
    "# 1st Try: MAE: 1431.4111 | RMSE: 5764825.2457 | R2: 0.9379 with max_depth= None, 10, 20; max_features= sqrt, log2; cv = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6f4ff12e-07a4-47f5-8c08-43b5d8d86942",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ExtraTrees"
    }
   },
   "outputs": [],
   "source": [
    "et_param_grid = {\n",
    "    \"model__max_depth\": [None, 10, 20],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe, et_param_grid,\n",
    "    cv=5, scoring=\"r2\",\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "et_grid.fit(X_train, y_train)\n",
    "et_best = et_grid.best_estimator_\n",
    "et_val_pred = et_best.predict(X_val)\n",
    "\n",
    "print(\"ExtraTrees Results: \")\n",
    "print_metrics(y_val, et_val_pred)\n",
    "\n",
    "# 1st Try: MAE: 1438.3138 | RMSE: 5865741.8358 | R2: 0.9368 with max_depth= None, 10, 20; max_features= sqrt, log2; cv = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "151b3c01-a218-4349-ac7d-48711b9cfd30",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: GradientBoosting"
    }
   },
   "outputs": [],
   "source": [
    "gbr_param_grid = {\n",
    "    \"model__n_estimators\": [500],\n",
    "    \"model__learning_rate\": [0.05],\n",
    "    \"model__max_depth\": [5], \"model__loss\": ['squared_error']\n",
    "}\n",
    "\n",
    "gbr_grid = GridSearchCV(\n",
    "    gbr_pipe, gbr_param_grid,\n",
    "    cv=5, scoring=\"r2\",\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "gbr_grid.fit(X_train, y_train)\n",
    "gbr_best = gbr_grid.best_estimator_\n",
    "gbr_val_pred = gbr_best.predict(X_val)\n",
    "\n",
    "print(\"GradientBoosting Results: \")\n",
    "print_metrics(y_val, gbr_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "78157989-9ef4-4b2b-a9e1-72309a89e754",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: HistGradientBoost"
    }
   },
   "outputs": [],
   "source": [
    "hgb_param_grid = {\n",
    "    \"model__learning_rate\": [0.07], # also tried: 0.02, 0.04, 0.06, 0.1\n",
    "    \"model__max_leaf_nodes\": [60], # also tried: 15, 25, 31, 50\n",
    "    \"model__min_samples_leaf\": [8], # also tried: 5, 10, 15, 20\n",
    "    \"model__max_iter\": [1000] # also tried: 500, 800\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "hgb_grid = GridSearchCV(\n",
    "    estimator=hgb_pipe,\n",
    "    param_grid=hgb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",  # optimize MAE\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "hgb_grid.fit(X_train, y_train)\n",
    "hgb_best_1 = hgb_grid.best_estimator_\n",
    "\n",
    "hgb_val_pred = hgb_best_1.predict(X_val)\n",
    "print_metrics(y_val, hgb_val_pred)\n",
    "\n",
    "# Best Parameters: {'model__learning_rate': 0.06, 'model__max_iter': 800, 'model__max_leaf_nodes': 50, 'model__min_samples_leaf': 5}\n",
    "# MAE: 1304.7611 | RMSE: 4503446.5247 | R2: 0.9515\n",
    "\n",
    "# Save model for later use\n",
    "joblib.dump(hgb_best_1, \"hgb_best_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9f283585-b396-4c60-a4bc-ec12b2f42c01",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TEST PARAMETERS HBG"
    }
   },
   "outputs": [],
   "source": [
    "hgb_param_grid = {\n",
    "    \"model__learning_rate\": [0.07],\n",
    "    \"model__max_leaf_nodes\": [65],\n",
    "    \"model__min_samples_leaf\": [8],\n",
    "    \"model__max_iter\": [1200]\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "hgb_grid = GridSearchCV(\n",
    "    estimator=hgb_pipe,\n",
    "    param_grid=hgb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",  # optimize MAE\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "hgb_grid.fit(X_train, y_train)\n",
    "hgb_best_2 = hgb_grid.best_estimator_\n",
    "\n",
    "hgb_val_pred = hgb_best_2.predict(X_val)\n",
    "print_metrics(y_val, hgb_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5bdcf66-8612-4455-979e-a348ca38dd42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### HT: advanced Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f487848b-789b-4084-827b-2ca4a2313bfc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: SupportVectorRegressor"
    }
   },
   "outputs": [],
   "source": [
    "svr_param_grid = {\n",
    "    \"model__C\": [1, 10, 100],\n",
    "    \"model__epsilon\": [0.1, 0.2],\n",
    "    \"model__kernel\": [\"rbf\"]\n",
    "}\n",
    "\n",
    "svr_grid = GridSearchCV(\n",
    "    svr_pipe, svr_param_grid,\n",
    "    cv=cv, scoring=\"r2\", n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "svr_grid.fit(X_train, y_train)\n",
    "svr_best = svr_grid.best_estimator_\n",
    "\n",
    "svr_val_pred = svr_best.predict(X_val)\n",
    "r2_svr, rmse_svr, mae_svr = print_metrics(y_val, svr_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0525442b-db49-45b7-ad73-ff98599eb223",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: StackingRegressor"
    }
   },
   "outputs": [],
   "source": [
    "# stackingregressor (elasticnet + histgradientboost)\n",
    "\n",
    "param_grid_stack = {\n",
    "    \"final_estimator__learning_rate\": [0.03, 0.05, 0.08],\n",
    "    \"final_estimator__max_depth\": [3, 5, 7],\n",
    "    \"final_estimator__min_samples_leaf\": [5, 10, 20]\n",
    "}\n",
    "\n",
    "stack_grid = GridSearchCV(\n",
    "    stack_pipe,\n",
    "    param_grid=param_grid_stack,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=3,            # lower CV since it’s meta-tuning\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "stack_grid.fit(X_train, y_train)\n",
    "stack_best = stack_grid.best_estimator_\n",
    "\n",
    "val_pred = stack_best.predict(X_val)\n",
    "print(\"Best params:\", stack_grid.best_params_)\n",
    "print_metrics(y_val, val_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "135acfad-1ad6-49f9-97fc-f1a81d0c3187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb68e758-d7b1-4f80-9dc4-ed985f721c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Extra Task (1 Point): Be in the Top 5 Groups on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "927be871-4f27-4453-baac-b4f7102534d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load hgb_best_1 from joblib\n",
    "hgb_best_1 = joblib.load(\"hgb_best_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6a25f58c-4c24-4460-93d4-830176016ee8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pick best Model, predict on Test"
    }
   },
   "outputs": [],
   "source": [
    "# Pick best model and predict on test:\n",
    "df_cars_test['price'] = hgb_best_1.predict(df_cars_test)\n",
    "\n",
    "df_cars_test['price'].to_csv('submission.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "448c5790-0cfd-4544-b244-a0e3b79acf8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b18805-e662-4807-bdb8-758df6027d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Submission with API push"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c cars4you -f submission.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350057a3-c771-4bf0-8fac-19b49be17e35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Score Check"
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle Score Check\n",
    "!kaggle competitions submissions -c cars4you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "228368c1-cec9-4df8-9171-8cefeb426783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1e3d5b-a46a-4077-b29f-d44f953c634c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Importance & Interpretation"
    }
   },
   "outputs": [],
   "source": [
    "# What/Why: Different models give different “importance” signals:\n",
    "# Lasso → coefficients (linear importance); zero → removed feature.\n",
    "# Tree ensembles (GBR) → feature_importances_ (importance in splits).\n",
    "# For rigorous interpretation, use SHAP for consistent feature attributions across models.\n",
    "\n",
    "\n",
    "# Important: yes — each model may select different features. That’s expected. Use the model type that matches your use-case:\n",
    "# If you need a sparse, interpretable linear model → use Lasso.\n",
    "# If you need best predictive power on tabular data → use ensemble/boosting and interpret via SHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26243df4-e6bb-4275-b46e-9070512317a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Save Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cded4d1-99c8-4006-90cd-07b8c22c047c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save Pipeline for Production"
    }
   },
   "outputs": [],
   "source": [
    "# What/Why: Save the entire pipeline (preprocessing + model) so new observations are processed consistently.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# (cell) Save and reload\n",
    "import joblib, os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(best_pipeline, \"models/car_price_model.pkl\")\n",
    "\n",
    "loaded = joblib.load(\"models/car_price_model.pkl\")\n",
    "# predict on new example\n",
    "new_car = pd.DataFrame([{\n",
    "    \"mileage\": 60000, \"year\": 2018, \"engine_size\": 2.0, \"horsepower\": 150, \"doors\": 4, \"owners\": 2,\n",
    "    \"brand\": \"BMW\", \"fuel_type\": \"Petrol\", \"transmission\": \"Auto\", \"color\": \"Black\",\n",
    "    \"region\": \"Urban\", \"condition\": \"Used\", \"warranty\": \"No\", \"dealer_type\": \"Independent\"\n",
    "}])\n",
    "print(\"Predicted price:\", loaded.predict(new_car)[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ebc3c64-02eb-481d-bf58-59f92e0fddcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Iterative Loop Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f20976-4956-40c7-89e7-355361e29a56",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Iterative Loop Begins"
    }
   },
   "source": [
    "The loop starts after you look at baseline performance and diagnostics:\n",
    "- Baseline → Check metrics on validation (and residuals).\n",
    "- Inspect failures / residual plots / feature importances (did a certain brand get consistently over/under predicted?)\n",
    "- Hypothesize (e.g. add interaction year * mileage, try log transform for horsepower, create age = current_year - year).\n",
    "- Implement changes in pipeline (e.g. add FunctionTransformer for log(horsepower) or PolynomialFeatures on a small set).\n",
    "- Re-run CV/hyperparameter search and evaluate again.\n",
    "- Log results, repeat.\n",
    "\n",
    "Note on feature selection: yes — different models will select different subsets. Typical approaches:\n",
    "- Use Lasso or SelectFromModel as a filter for linear pipelines.\n",
    "- Use tree-based model importances or SHAP to select features for simpler models.\n",
    "- Or let the best predictive model use all features (trees are robust to redundancy).\n",
    "\n",
    "Final notes (recommended best-practices)\n",
    "- Always fit preprocessing only on training data (pipelines do this automatically if you use them inside CV).\n",
    "- Start simple: mean baseline → Ridge/Lasso → tree-based. Use the simple models for interpretability and as sanity checks.\n",
    "- For heavy hyperparameter searches use RandomizedSearchCV or Optuna if the space is big.\n",
    "- When comparing models, report multiple metrics (R², MAE, RMSE). For price prediction MAE is often most interpretable.\n",
    "- For reproducibility, store your dataset version, random seed, code, and results log."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
