{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b83a4d32-7797-4f5f-8269-7bc08d0b9431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_Clone before play/engineer with Features / Hyperparameters_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcea0314-f61b-48dd-93e0-540b52850d1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Process to test new features:\n",
    "\n",
    "1. Add new Features in cell Feature Engineering\n",
    "2. Go into Preprocessing and add the features in either num, log or cat\n",
    "3. Run the RandomSearch // Hyperparameter Tuning for at least HGB and RF and see if MAE gets improved compared to previous results\n",
    "4. If MAE gets not improved, comment in cell below Feature Engineering that you tested those features (+results?) and on which Models - and remove everything\n",
    "5. If MAE gets improved, find out via Feature Importance (Shap Values) which Feature was responsible + document it, remove the other features that have negative impact\n",
    "6. comment below the Hyperparameter Tuning cell of the model the new achieved results + all the features you used for that + the hyperparameters\n",
    "7. save the results in a new model with joblib, name it correctly\n",
    "8. push to kaggle\n",
    "9. push to GIT + document everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52359b17-1216-4744-a528-81e23a033a68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Project Cars4you (Group 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fff3e4-624f-4b1f-b258-fbbe7b69cb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import & load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f54ab11e-fcfc-4278-8997-89a92adf8eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import and load Data"
    }
   },
   "outputs": [],
   "source": [
    "# Import and load Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, chi2\n",
    "from scipy.stats import spearmanr, uniform, loguniform, randint\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.linear_model import Ridge, Lasso, LassoCV, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from data_cleaning import clean_car_dataframe\n",
    "\n",
    "df_cars_train = pd.read_csv(\"train.csv\")\n",
    "df_cars_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aa35af5d-8257-4fb6-a380-86bbbfac3244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "556ffc24-70f7-48c9-96a4-3a19380f91db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Connect"
    }
   },
   "outputs": [],
   "source": [
    "### Everyone has to do this himself, with his own kaggle.json -> get it from kaggle as api token\n",
    "# Kaggle API Connect\n",
    "\n",
    "# Folder containing kaggle.json\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/Workspace/Users/20250355@novaims.unl.pt\"\n",
    "\n",
    "# Test\n",
    "!echo $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75ee214-0996-406f-b77f-9e72fa7153e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Cleaning, Feature Engineering, Split & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78e7ca8-ba25-4ea3-873d-4f3515281440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task II (5 Points): Clean and preprocess the dataset. \n",
    "- Missing Value handling, Outlier preprocessing + justify decisions -> in data_cleaning.py\n",
    "- Review current features and create extra features if needed + explain -> in Feature Engineering\n",
    "- Deal with categorical variables -> In One-Hot-Encoding \n",
    "- Perform data scaling, explain reasoning -> In Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6fa0cfe9-be7e-4e90-ac1c-cd600bb7d13f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Numerical / Categorical Data Cleaning: with Pandas"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand ['VW' 'Toyota' 'Audi' 'Ford' 'BMW' 'Skoda' 'Opel' 'Mercedes' 'Hyundai' nan]\n",
      "model ['golf' 'yaris' 'q2' 'fiesta' '2 series' '3 series' 'a3' 'octavia'\n",
      " 'passat' 'focus' 'insignia' 'a class' 'q3' 'fabia' 'ka+' 'glc class'\n",
      " 'i30' 'c class' 'polo' 'e class' 'q5' 'up' 'c-hr' 'mokka' 'corsa' 'astra'\n",
      " 'tt' '5 series' 'aygo' '4 series' nan 'ecosport' 'tucson' 'x-class'\n",
      " 'cl class' 'ix20' 'i20' 'a1' 'auris' 'sharan' 'adam' 'x3' 'a8'\n",
      " 'gls class' 'b-max' 'a4' 'kona' 'i10' 's-max' 'x2' 'crossland x' 'tiguan'\n",
      " 'a5' 'gle class' 'zafira' 'ioniq' 'a6' 'mondeo' 'yeti' 'x1' 'scala'\n",
      " 's class' '1 series' 'kamiq' 'kuga' 'q7' 'gla class' 'arteon' 'sl class'\n",
      " 'santa fe' 'grandland x' 'rav4' 'touran' 'corolla' 'b class' 'kodiaq'\n",
      " 'v class' 'superb' 'combo life' 'beetle' 'm3' 'x4' 'ix35' 'm4' 'z4' 'x5'\n",
      " 'meriva' 'verso' 'cls class' 'c-max' 'puma' 'i40' '6 series' 'karoq' 'a7'\n",
      " 'land cruiser' 'edge' 'x6' '8 series' 'scirocco' 'z3' 'hilux' 'amarok'\n",
      " '7 series' 'avensis' 'm class' 'r8' 'antara' 'q8' 'x7' 'g class'\n",
      " 'glb class' 'm5' 'vectra' 'm6']\n",
      "year <IntegerArray>\n",
      "[2016, 2019, 2018, 2014, 2017, 2020, 2013, <NA>, 2015, 2023, 2011, 2012, 2003,\n",
      " 2009, 2007, 2005, 2004, 2010, 2008, 2024, 2006, 2001, 2000, 2002, 2022, 1996,\n",
      " 1998, 1970, 1999, 1997]\n",
      "Length: 30, dtype: Int64\n",
      "price [22290 13790 24990 ... 33606 16809 25785]\n",
      "transmission ['Semi-Auto' 'Manual' 'Automatic' nan 'Unknown' 'Other']\n",
      "mileage <IntegerArray>\n",
      "[ 28421,   4589,   3624,   9102,   1000, 101153,  21396, 116750,  30339,\n",
      "  10700,\n",
      " ...\n",
      "  31488,  22296,  16098,   1177,   1027,   9475,  41914,  13613,  52134,\n",
      "  69072]\n",
      "Length: 34800, dtype: Int64\n",
      "fuelType ['Petrol' 'Diesel' 'Hybrid' nan 'Other' 'Electric']\n",
      "tax <IntegerArray>\n",
      "[<NA>,  145,   30,   20,  150,  160,    0,  125,  200,  250,  135,  300,  205,\n",
      "  260,  326,  325,  265,  305,  303,  235,  299,  240,  165,  140,  540,  155,\n",
      "  220,  120,  330,  290,  327,  270,  115,  185,  316,  570,  555,  277,   10,\n",
      "  315,  190,  230,  294,  195,  580,  130,  565,  210,  280,  255,  520,  295,\n",
      "  245,  110,  535]\n",
      "Length: 55, dtype: Int64\n",
      "mpg <IntegerArray>\n",
      "[  11,   47,   40,   65,   42,   60,   68,   62,   67,   54,   57, <NA>,   56,\n",
      "   72,   44,   46,   32,   48,   74,   43,   78,   51,   50,   28,   64,   61,\n",
      "   41,   52,   53,   35,   80,   37,   49,   45,   34,   29,   76,   58,   39,\n",
      "   55,   83,   33,   70,   30,   36,   38,   69,   66,    9,   85,   89,  134,\n",
      "   27,   88,  100,   31,   20,   22,   24,   94,    5,    6,   21,   86,   23,\n",
      "   26,  113,   81,   16,  117,   92,    8,   25,  110,  148,  141,  122,   14,\n",
      "   19,   17,  135,  128]\n",
      "Length: 82, dtype: Int64\n",
      "engineSize [2.  1.5 1.  1.4 1.6 nan 1.2 3.  1.8 2.1 1.3 0.7 2.3 1.1 4.  1.7 2.2 2.5\n",
      " 2.6 2.8 3.7 4.2 3.2 2.9 2.4 2.7 5.2 5.  4.4 6.2 3.8 1.9 0.6 6.6 5.5 3.5\n",
      " 4.7 3.6 4.3 5.4 4.5 4.1 6. ]\n",
      "paintQuality <IntegerArray>\n",
      "[<NA>,   97,   74,   75,   85,   94,   87,   83,   95,   80,   77,   90,   91,\n",
      "   71,   82,   81,   79,   72,   88,   96,   84,   70,   86,   92,   73,   78,\n",
      "   93,   89,   99,   76,   98]\n",
      "Length: 31, dtype: Int64\n",
      "previousOwners <IntegerArray>\n",
      "[4, 1, <NA>, 3, 0, 2, 6]\n",
      "Length: 7, dtype: Int64\n",
      "hasDamage <IntegerArray>\n",
      "[0, <NA>]\n",
      "Length: 2, dtype: Int64\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Brand ['Hyundai' 'VW' 'BMW' 'Opel' 'Ford' 'Mercedes' 'Skoda' 'Toyota' 'Audi' nan]\n",
      "model ['i30' 'tiguan' '2 series' 'grandland x' '1 series' 'fiesta' 'x1'\n",
      " 'b class' 'focus' 'superb' '5 series' 'c class' 'up' 'aygo' 'golf' nan\n",
      " 'land cruiser' 'tt' 'adam' 'zafira' 'e class' '3 series' 'ix20' 'a4'\n",
      " 'yaris' 'passat' 'i10' 'mokka' 'ecosport' '4 series' 'a7' 'corsa' 'kuga'\n",
      " 'q2' 'm4' 'a class' 'rav4' 'fabia' 'insignia' 'a1' 'x6' 'meriva'\n",
      " 'octavia' 'auris' 'x-class' 'astra' 'v class' 'polo' 'karoq' 'q5'\n",
      " 'tucson' 'a3' 'sl class' 'corolla' 'ka+' 'x3' 'i40' 'i20' 'kamiq' 'ix35'\n",
      " 'crossland x' 'q3' 'gla class' 'cls class' 'x2' 'kodiaq' 'glc class'\n",
      " 'mondeo' 'touran' 'cl class' 'x5' 'verso' 'a5' 's class' 'scirocco' 'x7'\n",
      " 'b-max' '8 series' 'a6' 'santa fe' 'gle class' 'beetle' 'x4' 'sharan'\n",
      " 'c-max' 'm6' 'a8' 'kona' 'yeti' 'q7' 'c-hr' 'm5' 'avensis' 'ioniq'\n",
      " 'amarok' 'z4' 'm3' 'arteon' 'scala' 'hilux' 'puma' 's-max' 'edge'\n",
      " 'combo life' '7 series' 'm class' 'glb class' '6 series' 'r8' 'gls class'\n",
      " 'antara' 'q8' 'g class' 'vectra' 'z3']\n",
      "year <IntegerArray>\n",
      "[2022, 2017, 2016, 2019, 2018, 2011, 2015, <NA>, 2023, 2014, 2020, 2013, 2010,\n",
      " 2024, 2008, 2012, 2009, 2007, 2004, 2002, 2006, 2000, 2003, 1997, 2001, 1999,\n",
      " 1998, 2005, 1991, 1996]\n",
      "Length: 30, dtype: Int64\n",
      "transmission ['Automatic' 'Semi-Auto' 'Manual' 'Unknown' nan 'Other']\n",
      "mileage <IntegerArray>\n",
      "[30700,  <NA>, 36792,  5533,  9058, 29626, 57717, 14005, 68274, 20632,\n",
      " ...\n",
      " 69505, 20648, 27127,  5970,  1522, 19812, 10297, 27575,  8297, 11071]\n",
      "Length: 20133, dtype: Int64\n",
      "fuelType ['Petrol' 'Diesel' 'Hybrid' nan 'Other' 'Electric']\n",
      "tax <IntegerArray>\n",
      "[ 205,  150,  125,  145,  200,  300, <NA>,  160,   20,    0,   30,  240,  135,\n",
      "  260,  299,  570,  235,  555,  326,  165,  140,  327,  265,  305,  325,  294,\n",
      "  277,  316,   10,  330,  115,  195,  565,  120,  303,  290,  515,  245,  580,\n",
      "  315,  220,  280,  190,  540,  230,  130,  185,  270]\n",
      "Length: 48, dtype: Int64\n",
      "mpg <IntegerArray>\n",
      "[  41,   38,   51,   44,   65,   58,   47,   64,   62,   61,   94,   69,   39,\n",
      "   30,   57,   53, <NA>,   40,   67,   50,   68,   76,   49,   42,  100,   86,\n",
      "   78,   48,   37,   54,   60,   32,   43,   52,   45,   34,   29,   72,   35,\n",
      "   74,   36,   55,   28,   20,   56,   81,   31,   46,  110,   70,   24,   33,\n",
      "   66,    9,  134,   26,   83,   88,   25,   85,   21,   27,   92,   80,   16,\n",
      "   11,  141,   14,   22,   23,   89,  148,    8,  117,    5,   18,  113,  122]\n",
      "Length: 78, dtype: Int64\n",
      "engineSize [1.6 2.  1.5 1.2 1.  nan 2.1 3.  2.8 1.4 2.5 1.8 2.3 1.7 1.1 4.  2.9 2.7\n",
      " 1.3 3.2 2.2 5.5 3.8 3.7 6.3 4.4 3.5 2.4 4.2 5.  2.6 5.2 6.2 0.7 1.9 4.7\n",
      " 0.6 5.4 6.  4.1 6.6]\n",
      "paintQuality <IntegerArray>\n",
      "[<NA>,   94,   77,   71,   99,   73,   78,   76,   90,   70,   75,   79,   98,\n",
      "   88,   97,   85,   89,   92,   80,   91,   84,   82,   86,   74,   87,   72,\n",
      "   93,   83,   96,   81,   95]\n",
      "Length: 31, dtype: Int64\n",
      "previousOwners <IntegerArray>\n",
      "[3, 2, 1, 4, 0, <NA>, 6]\n",
      "Length: 7, dtype: Int64\n",
      "hasDamage <IntegerArray>\n",
      "[0, <NA>]\n",
      "Length: 2, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# Outlier Preprocessing, Missing Value Handling and Decision justifying happens here\n",
    "df_cars_train = clean_car_dataframe(df_cars_train)\n",
    "df_cars_test = clean_car_dataframe(df_cars_test)\n",
    "\n",
    "\n",
    "# Safety Check: print unique values of all columns of df_cars_train // df_cars_test to see if data cleaning worked and if there are still odd values\n",
    "for col in df_cars_train.columns:\n",
    "    print(col, df_cars_train[col].unique())\n",
    "print(\"X\"*150)\n",
    "print(\"-\"*150)\n",
    "print(\"X\"*150)\n",
    "for col in df_cars_test.columns:\n",
    "    print(col, df_cars_test[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2df7bf7c-0ebb-4f4b-a168-5b2e2f6b21d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering and Explaination\n",
    "\n",
    "# add column age: models can easier interpret linear numerical features\n",
    "df_cars_train['age'] = 2025 - df_cars_train['year']\n",
    "df_cars_test['age'] = 2025 - df_cars_test['year']\n",
    "\n",
    "\n",
    "# miles per year: normalizes the total mileage by how old the car is\n",
    "df_cars_train['miles_per_year'] = df_cars_train['mileage'] / df_cars_train['age'].replace({0: np.nan})\n",
    "df_cars_train['miles_per_year'] = df_cars_train['miles_per_year'].fillna(df_cars_train['mileage'])\n",
    "\n",
    "df_cars_test['miles_per_year'] = df_cars_test['mileage'] / df_cars_test['age'].replace({0: np.nan})\n",
    "df_cars_test['miles_per_year'] = df_cars_test['miles_per_year'].fillna(df_cars_test['mileage'])\n",
    "\n",
    "\n",
    "# model frequency: some models are more common, which means they can be cheaper (supply) or retain their values better (demand). freq shows their popularity\n",
    "model_freq = df_cars_train['model'].value_counts(normalize=True).to_dict()\n",
    "df_cars_train['model_freq'] = df_cars_train['model'].map(model_freq)\n",
    "df_cars_test['model_freq'] = df_cars_test['model'].map(model_freq)\n",
    "\n",
    "\n",
    "# brand median price (only train): shows brand positioning (e.g. BMW > KIA)\n",
    "brand_median_price = df_cars_train.groupby('Brand')['price'].median()\n",
    "df_cars_train['brand_med_price'] = df_cars_train['Brand'].map(brand_median_price)\n",
    "\n",
    "\n",
    "# model median price (only train): shows model positioning (e.g. 3er > 1er)\n",
    "model_med_price = df_cars_train.groupby('model')['price'].median()\n",
    "df_cars_train['model_med_price'] = df_cars_train['model'].map(model_med_price)\n",
    "\n",
    "\n",
    "# brand anchor (market position) \n",
    "brand_median_price = df_cars_train.groupby(\"Brand\")[\"price\"].median().to_dict()\n",
    "overall_mean_price = df_cars_train[\"price\"].mean()\n",
    "\n",
    "df_cars_train[\"brand_anchor\"] = df_cars_train[\"Brand\"].map(brand_median_price) / overall_mean_price\n",
    "df_cars_test[\"brand_anchor\"]  = df_cars_test[\"Brand\"].map(brand_median_price) / overall_mean_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dda482e9-4a68-438d-86c0-e0524a9b8692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split Data"
    }
   },
   "outputs": [],
   "source": [
    "# Split Data, Stratify not necessary due to regression problem and Cross Validation later\n",
    "X = df_cars_train.drop(columns='price')\n",
    "y = df_cars_train['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.3, \n",
    "                                                  random_state = 42, \n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c92786c5-7746-459e-9834-e7c36f01c22c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write GroupMedianImputer"
    }
   },
   "outputs": [],
   "source": [
    "# Write custom GroupMedianImputer to impute missing values on a model, brand level and not only global (with SimpleImputer)\n",
    "class GroupMedianImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Impute missing numeric values using hierarchical medians:\n",
    "    1. By (Brand, model)\n",
    "    2. If model missing → by Brand\n",
    "    3. Fallback → global median\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, group_cols=[\"Brand\", \"model\"]):\n",
    "        self.group_cols = group_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        self.feature_names_in_ = X.columns\n",
    "\n",
    "        # Step 1 — model-level medians\n",
    "        if all(c in X.columns for c in self.group_cols):\n",
    "            self.medians_ = X.groupby(self.group_cols).median(numeric_only=True)\n",
    "        else:\n",
    "            self.medians_ = pd.DataFrame()\n",
    "\n",
    "        # Step 2 — brand-level medians\n",
    "        if \"Brand\" in X.columns:\n",
    "            self.brand_medians_ = X.groupby(\"Brand\").median(numeric_only=True)\n",
    "        else:\n",
    "            self.brand_medians_ = pd.DataFrame()\n",
    "\n",
    "        # Step 3 — global medians\n",
    "        self.global_median_ = X.median(numeric_only=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        for col in X.columns:\n",
    "            if X[col].isna().sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # (Brand, model) level\n",
    "            if all(c in X.columns for c in self.group_cols):\n",
    "                X[col] = X.apply(\n",
    "                    lambda r: self.medians_.loc[(r[\"Brand\"], r[\"model\"]), col]\n",
    "                    if (r[\"Brand\"], r[\"model\"]) in self.medians_.index and pd.isna(r[col])\n",
    "                    else r[col],\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "            # Brand-only fallback\n",
    "            if \"Brand\" in X.columns:\n",
    "                X[col] = X.apply(\n",
    "                    lambda r: self.brand_medians_.loc[r[\"Brand\"], col]\n",
    "                    if r[\"Brand\"] in self.brand_medians_.index and pd.isna(r[col])\n",
    "                    else r[col],\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "            # Global fallback\n",
    "            X[col] = X[col].fillna(self.global_median_[col])\n",
    "\n",
    "        return X.values  # sklearn expects ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39142374-d485-42e7-a6cb-83ecffae0091",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocessing: with sklearn Pipeline & Column Transformer"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' EXPLAINATIONS '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing: with sklearn Pipeline & Column Transformer\n",
    "\n",
    "group_imputer = GroupMedianImputer(group_cols=[\"Brand\", \"model\"])\n",
    "\n",
    "numeric_features = [\"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"brand_anchor\"]\n",
    "log_features = [\"mileage\", \"miles_per_year\", \"model_freq\", \"brand_med_price\", \"model_med_price\"] # could try to test previousOwners, age here; tax, mpg didnt work\n",
    "categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "# left out columns: year (age is better), hasDamage (unsure what the two values 0 and NaN mean)\n",
    "\n",
    "\n",
    "log_transformer = Pipeline([\n",
    "    (\"group_impute\", GroupMedianImputer(group_cols=[\"Brand\", \"model\"])),  # Handling of missing numerical values with GroupMedianImputer\n",
    "    (\"to_float\", FunctionTransformer(lambda x: np.array(x, dtype=float))),\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=False)), # log1p handles zeros safely\n",
    "    (\"scaler\", StandardScaler()) #  # Data Scaling with sklearn StandardScaler\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"group_impute\", GroupMedianImputer(group_cols=[\"Brand\", \"model\"])),\n",
    "    (\"to_float\", FunctionTransformer(lambda x: np.array(x, dtype=float))),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), # fill by mode instead of Unknown (a diesel 3er BMW is probably a diesel)\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)) # Deal with Categorical Variables with sklearn OneHotEncoder\n",
    "])\n",
    "\n",
    "# Apply the preprocessing steps to the data with ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"log\", log_transformer, log_features),\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "\n",
    "# Fit preprocessor on training data - avoid data leakage\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" EXPLAINATIONS \"\"\"\n",
    "# 1) Pipeline bundles preprocessing + model training:\n",
    "#       > Ensures all preprocessing happens inside cross-validation folds (no data leakage)\n",
    "#       > Keeps the entire workflow reproducible — scaling, encoding, and modeling are learned together\n",
    "#       > After .fit(), the final model automatically knows how to preprocess new unseen data\n",
    "#       > When saving with joblib, the entire preprocessing (imputers, scalers, encoders) and model are stored together\n",
    "\n",
    "# 2) The ColumnTransformer applies different transformations to subsets of features:\n",
    "#       > Numeric Features arehandled by our custom GroupMedianImputer (domain-aware filling)\n",
    "#           - Missing numeric values are imputed hierarchically:\n",
    "#           1. By (Brand, model)\n",
    "#           2. If missing model by Brand\n",
    "#           3. If missing Brand by global median\n",
    "#       > This approach captures brand/model-level patterns (e.g. BMWs have similar engine sizes)\n",
    "#       > After imputation, StandardScaler standardizes all numeric features\n",
    "#\n",
    "#       > Log Features use the same group-median imputation, followed by log1p() transformation\n",
    "#           - log1p() compresses large, skewed values (like mileage or price-related features), stabilizing variance and helping linear models perform better\n",
    "#           - StandardScaler then scales them to zero mean and unit variance\n",
    "#\n",
    "#       > Categorical Features are handled by SimpleImputer + OneHotEncoder\n",
    "#           - SimpleImputer fills missing categorical values with the most frequent (mode) value.\n",
    "#             (Alternative would be “Unknown”, but mode keeps categories realistic, e.g. most cars in a model share the same transmission)\n",
    "#           - OneHotEncoder converts each categorical label (Brand, model, etc.) into binary dummy variables\n",
    "#             This lets the model use category information numerically without implying order\n",
    "#\n",
    "# 3) Overall:\n",
    "#       > The pipeline ensures consistent preprocessing across training, validation, and test data.\n",
    "#       > It combines domain knowledge (brand/model-aware imputation) with robust numerical scaling.\n",
    "#       > Linear models (ElasticNet, Ridge, Lasso) and tree models (HistGradientBoosting, RandomForest)\n",
    "#           can now learn from the same standardized, clean, and information-rich feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8f763f-298d-444a-80a9-0270ba842dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e9a293-7457-4767-8a26-7c930f7dbad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task III (3 Points): Define and Implement a clear and unambiguous strategy for feature selection. Use the methods **discussed in the course**. Present and justify your final selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d989a4b-077d-47bd-b4bb-c4528628888f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Model independent Filter Methods:\n",
    "- Remove constant numerical variables with VarianceThreshold (manual)\n",
    "- Check highly correlated numerical variables and keep one with Spearman (manual)\n",
    "- Remove unindependent categorical variables with Chi2\n",
    "\n",
    "Model dependent Wrapper Methods:\n",
    "- RFE LR / RFE SVR for linear Models: Ridge, Lasso, ElasticNet, SVM\n",
    "- Feature Importance for tree Models: DecisionTrees, RandomForest, GradientBoosting => trees are unsensitive to irrelevant features but doing feature importance and remove some can reduce dimensionality\n",
    "- L1 Regularization for Neural Networks: MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "130e5929-dcd7-48db-8afa-8f9f560968fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Selection with Filter Methods"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to deselect according to VarianceThreshold: []\n",
      "Features to deselect according to Spearman correlation: ['paintQuality', 'previousOwners']\n",
      "Features to deselect according to Chi²: []\n"
     ]
    }
   ],
   "source": [
    "X_train_proc = preprocessor.transform(X_train)\n",
    "\n",
    "feature_names_all = []\n",
    "for name, trans, cols in preprocessor.transformers_:\n",
    "    if name != 'remainder':\n",
    "        if hasattr(trans, 'get_feature_names_out'):\n",
    "            # for categorical OHE\n",
    "            try:\n",
    "                feature_names_all.extend(trans.get_feature_names_out(cols))\n",
    "            except:\n",
    "                feature_names_all.extend(cols)\n",
    "        else:\n",
    "            feature_names_all.extend(cols)\n",
    "\n",
    "X_df = pd.DataFrame(X_train_proc, columns=feature_names_all)\n",
    "\n",
    "\n",
    "# Variance Threshold\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "vt.fit(X_df)\n",
    "vt_deselect = [f for f, keep in zip(feature_names_all, vt.get_support()) if not keep]\n",
    "print(\"Features to deselect according to VarianceThreshold:\", vt_deselect)\n",
    "\n",
    "\n",
    "# Spearman correlation (numeric + log only)\n",
    "numeric_log = numeric_features + log_features\n",
    "spearman_deselect = []\n",
    "for f in numeric_log:\n",
    "    if f in X_df.columns:\n",
    "        corr, _ = spearmanr(X_df[f], y_train)\n",
    "        if abs(corr) <= 0.05:\n",
    "            spearman_deselect.append(f)\n",
    "print(\"Features to deselect according to Spearman correlation:\", spearman_deselect)\n",
    "\n",
    "\n",
    "# Chi2 (categorical only, must be non-negative)\n",
    "cat_cols = [c for c in X_df.columns if c not in numeric_log]\n",
    "X_cat = X_df[cat_cols].astype(float)\n",
    "chi2_vals, _ = chi2(X_cat, y_train)\n",
    "chi2_deselect = [f for f, val in zip(cat_cols, chi2_vals) if val <= 0]\n",
    "print(\"Features to deselect according to Chi²:\", chi2_deselect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d62d3f6a-baa5-40e9-913a-b99f443b5080",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Deselection for linear Models"
    }
   },
   "outputs": [],
   "source": [
    "# Numeric/log features for linear models\n",
    "linear_numeric_features = [f for f in numeric_features + log_features if f not in spearman_deselect]\n",
    "\n",
    "preprocessor_linear = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, linear_numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# => use preprocessor_linear for linear model setup; since tree models are indifferent to irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Model Feature Selection with SHAP\n",
    "\n",
    "  **Problem:** Current feature selection targets linear models\n",
    "  (ElasticNet), but we primarily use tree-based models (HGB,\n",
    "  RandomForest).\n",
    "\n",
    "  **Solution:** Use SHAP (SHapley Additive exPlanations) to\n",
    "  identify feature importance specifically for tree models and\n",
    "  optimize our 145-feature dataset.\n",
    "\n",
    "  **Why SHAP for Trees:**\n",
    "  - Provides exact feature importance values for tree-based\n",
    "  models\n",
    "  - Tree models handle irrelevant features, but noise features\n",
    "  still impact performance\n",
    "  - Enables data-driven selection rather than statistical filter\n",
    "  methods\n",
    "  - Automated and reproducible process for production models\n",
    "\n",
    "  **Methodology:** Train baseline → Calculate SHAP importance →\n",
    "  Test feature subsets → Recommend optimal configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Baseline Performance with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE PERFORMANCE WITH OPTIMIZED HYPERPARAMETERS ===\n",
      "Baseline HGB Performance (optimized hyperparameters, all features):\n",
      "MAE: 1323.7920 | RMSE: 4484463.9519 | R2: 0.9517\n",
      "Using 145 features\n"
     ]
    }
   ],
   "source": [
    "# Import SHAP and define helper function\n",
    "import shap\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"MAE: {mae:.4f} | RMSE: {rmse:.4f} | R2: {r2:.4f}\")\n",
    "    return\n",
    "\n",
    "# Train baseline model with optimized hyperparameters\n",
    "print(\"=== BASELINE PERFORMANCE WITH OPTIMIZED HYPERPARAMETERS ===\")\n",
    "\n",
    "# Use best hyperparameters from previous hyperparameter tuning\n",
    "# sklearn 1.6.1\n",
    "best_params = {\n",
    "      'l2_regularization': 0.942853570557981,\n",
    "      'learning_rate': 0.06389789198396824,\n",
    "      'max_iter': 642,\n",
    "      'max_leaf_nodes': 105,\n",
    "      'min_samples_leaf': 3,\n",
    "      'random_state': 42\n",
    "  }\n",
    "\n",
    "# sklearn 1.4.2\n",
    "# best_params = {\n",
    "#     'learning_rate': 0.05704595464437946,\n",
    "#     'max_leaf_nodes': 109,\n",
    "#     'min_samples_leaf': 11,\n",
    "#     'max_iter': 414,\n",
    "#     'l2_regularization': 0.49379559636439074,\n",
    "#     'random_state': 42\n",
    "# }\n",
    "\n",
    "hgb_baseline = HistGradientBoostingRegressor(**best_params)\n",
    "\n",
    "# Fit baseline model\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "hgb_baseline.fit(X_train_processed, y_train)\n",
    "baseline_pred = hgb_baseline.predict(X_val_processed)\n",
    "\n",
    "print(\"Baseline HGB Performance (optimized hyperparameters, all features):\")\n",
    "print_metrics(y_val, baseline_pred)\n",
    "print(f\"Using {X_train_processed.shape[1]} features\")\n",
    "\n",
    "# Store baseline for comparison\n",
    "baseline_mae = mean_absolute_error(y_val, baseline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: SHAP Feature Importance Analysis\n",
    "\n",
    "We use SHAP's TreeExplainer to calculate feature importance values. TreeExplainer is specifically optimized for tree-based models and provides exact Shapley values efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CALCULATING SHAP VALUES ===\n",
      "Computing SHAP values for 1000 samples...\n",
      "Top 20 most important features:\n",
      "                    feature   importance\n",
      "4           model_med_price  2798.250661\n",
      "5                       age  2271.378566\n",
      "136     transmission_Manual  1656.190012\n",
      "0                   mileage  1557.877704\n",
      "8                engineSize  1511.617460\n",
      "7                       mpg   798.939048\n",
      "3           brand_med_price   781.774598\n",
      "1            miles_per_year   191.858515\n",
      "2                model_freq   145.477741\n",
      "6                       tax   123.331385\n",
      "144         fuelType_Petrol   118.094618\n",
      "142         fuelType_Hybrid   106.727948\n",
      "62              model_focus   102.024200\n",
      "14               Brand_Ford    76.184813\n",
      "140         fuelType_Diesel    46.629148\n",
      "138  transmission_Semi-Auto    32.958380\n",
      "12               Brand_Audi    31.773527\n",
      "135  transmission_Automatic    30.133471\n",
      "18              Brand_Skoda    26.802926\n",
      "57            model_e class    25.986541\n",
      "\n",
      "Bottom 10 least important features:\n",
      "               feature  importance\n",
      "91            model_m6         0.0\n",
      "86  model_land cruiser         0.0\n",
      "75           model_i40         0.0\n",
      "65     model_glb class         0.0\n",
      "50         model_c-max         0.0\n",
      "47        model_beetle         0.0\n",
      "43       model_avensis         0.0\n",
      "39        model_antara         0.0\n",
      "16      Brand_Mercedes         0.0\n",
      "72           model_i10         0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate SHAP values for feature importance analysis\n",
    "print(\"=== CALCULATING SHAP VALUES ===\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Use TreeExplainer for fast computation with tree models\n",
    "explainer = shap.TreeExplainer(hgb_baseline)\n",
    "\n",
    "# Calculate SHAP values on a sample (for speed) - REPRODUCIBLE\n",
    "sample_size = min(1000, len(X_train_processed))\n",
    "sample_indices = np.random.choice(len(X_train_processed), sample_size, replace=False)\n",
    "X_sample = X_train_processed[sample_indices]\n",
    "\n",
    "print(f\"Computing SHAP values for {sample_size} samples...\")\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# Calculate feature importance (mean absolute SHAP values)\n",
    "feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "shap_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names_all,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "print(shap_importance_df.head(20))\n",
    "\n",
    "print(f\"\\nBottom 10 least important features:\")\n",
    "print(shap_importance_df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Automated Feature Selection Optimization\n",
    "\n",
    "Now we systematically test different numbers of top features to find the optimal subset. We train the model with the same optimized hyperparameters but using only the most important features identified by SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AUTOMATED FEATURE SELECTION ANALYSIS ===\n",
      "Top  5 features → MAE: 1615.8 (change: -292.0)\n",
      "Top 10 features → MAE: 1360.6 (change: -36.8)\n",
      "Top 15 features → MAE: 1299.8 (change: +23.9)\n",
      "Top 20 features → MAE: 1302.3 (change: +21.5)\n",
      "Top 30 features → MAE: 1316.7 (change: +7.1)\n",
      "Top 50 features → MAE: 1318.8 (change: +5.0)\n",
      "Top 70 features → MAE: 1316.5 (change: +7.3)\n",
      "Top 100 features → MAE: 1364.1 (change: -40.3)\n",
      "All 145 features → MAE: 1323.8 (baseline)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate different feature subset sizes (5-145)\n",
    "feature_counts = [5, 10, 15, 20, 30, 50, 70, 100, 145]\n",
    "results = []\n",
    "\n",
    "print(\"=== AUTOMATED FEATURE SELECTION ANALYSIS ===\")\n",
    "\n",
    "for n_features in feature_counts:\n",
    "    if n_features == 145:\n",
    "        # Use baseline result for all features\n",
    "        mae_subset = baseline_mae\n",
    "        print(f\"All {n_features} features → MAE: {mae_subset:.1f} (baseline)\")\n",
    "    else:\n",
    "        # Select top N features based on SHAP importance\n",
    "        top_features = shap_importance_df.head(n_features)[\"feature\"].tolist()\n",
    "        feature_indices = [i for i, fname in enumerate(feature_names_all) if fname in top_features]\n",
    "\n",
    "        # Create subset datasets\n",
    "        X_train_subset = X_train_processed[:, feature_indices]\n",
    "        X_val_subset = X_val_processed[:, feature_indices]\n",
    "\n",
    "        # Train model with selected features using same optimized hyperparameters\n",
    "        hgb_selected = HistGradientBoostingRegressor(**best_params)\n",
    "        hgb_selected.fit(X_train_subset, y_train)\n",
    "        pred_subset = hgb_selected.predict(X_val_subset)\n",
    "        mae_subset = mean_absolute_error(y_val, pred_subset)\n",
    "\n",
    "        improvement = baseline_mae - mae_subset\n",
    "        print(f\"Top {n_features:2d} features → MAE: {mae_subset:.1f} (change: {improvement:+.1f})\")\n",
    "    \n",
    "    # Store results for optimization analysis\n",
    "    results.append({\n",
    "        'n_features': n_features,\n",
    "        'mae': mae_subset,\n",
    "        'improvement': baseline_mae - mae_subset\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AUTOMATED FEATURE SELECTION ANALYSIS ===\n",
      "Top 10 features → MAE: 1360.6 (change: -36.8)\n",
      "Top 11 features → MAE: 1320.4 (change: +3.4)\n",
      "Top 12 features → MAE: 1310.6 (change: +13.2)\n",
      "Top 13 features → MAE: 1317.3 (change: +6.5)\n",
      "Top 14 features → MAE: 1346.4 (change: -22.7)\n",
      "Top 15 features → MAE: 1299.8 (change: +23.9)\n",
      "Top 16 features → MAE: 1322.9 (change: +0.9)\n",
      "Top 17 features → MAE: 1303.3 (change: +20.5)\n",
      "Top 18 features → MAE: 1302.1 (change: +21.7)\n",
      "Top 19 features → MAE: 1295.2 (change: +28.6)\n",
      "Top 20 features → MAE: 1302.3 (change: +21.5)\n",
      "Top 21 features → MAE: 1299.7 (change: +24.1)\n",
      "Top 22 features → MAE: 1335.6 (change: -11.8)\n",
      "Top 23 features → MAE: 1312.1 (change: +11.7)\n",
      "Top 24 features → MAE: 1343.0 (change: -19.2)\n",
      "Top 25 features → MAE: 1337.2 (change: -13.4)\n",
      "Top 26 features → MAE: 1343.5 (change: -19.7)\n",
      "Top 27 features → MAE: 1334.8 (change: -11.0)\n",
      "Top 28 features → MAE: 1356.5 (change: -32.7)\n",
      "Top 29 features → MAE: 1332.2 (change: -8.5)\n",
      "Top 30 features → MAE: 1316.7 (change: +7.1)\n",
      "\n",
      "=== OPTIMAL FEATURE SELECTION RESULTS ===\n",
      "Best performance: 19 features\n",
      "Best MAE: 1295.2\n",
      "Improvement over baseline: 28.6 MAE\n",
      "\n",
      "Optimal 19 features for production model:\n",
      " 1. model_med_price      (importance: 2798.3)\n",
      " 2. age                  (importance: 2271.4)\n",
      " 3. transmission_Manual  (importance: 1656.2)\n",
      " 4. mileage              (importance: 1557.9)\n",
      " 5. engineSize           (importance: 1511.6)\n",
      " 6. mpg                  (importance: 798.9)\n",
      " 7. brand_med_price      (importance: 781.8)\n",
      " 8. miles_per_year       (importance: 191.9)\n",
      " 9. model_freq           (importance: 145.5)\n",
      "10. tax                  (importance: 123.3)\n",
      "11. fuelType_Petrol      (importance: 118.1)\n",
      "12. fuelType_Hybrid      (importance: 106.7)\n",
      "13. model_focus          (importance: 102.0)\n",
      "14. Brand_Ford           (importance: 76.2)\n",
      "15. fuelType_Diesel      (importance: 46.6)\n",
      "... and 4 more features\n",
      "\n",
      "=== FINAL RECOMMENDATION ===\n",
      "Current baseline (145 features): 1323.8 MAE\n",
      "Optimized model (19 features): 1295.2 MAE\n",
      "Improvement: 28.6 MAE better\n",
      "Recommendation: Use top 19 features for final submission\n"
     ]
    }
   ],
   "source": [
    "# Evaluate different feature subset sizes in detail (10-30)\n",
    "feature_counts = [10, 11, 12, 13, 14, 15, 16, 17,\n",
    "  18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "results = []\n",
    "\n",
    "print(\"=== AUTOMATED FEATURE SELECTION ANALYSIS ===\")\n",
    "\n",
    "for n_features in feature_counts:\n",
    "    if n_features == 145:\n",
    "        # Use baseline result for all features\n",
    "        mae_subset = baseline_mae\n",
    "        print(f\"All {n_features} features → MAE: {mae_subset:.1f} (baseline)\")\n",
    "    else:\n",
    "        # Select top N features based on SHAP importance\n",
    "        top_features = shap_importance_df.head(n_features)[\"feature\"].tolist()\n",
    "        feature_indices = [i for i, fname in enumerate(feature_names_all) if fname in top_features]\n",
    "\n",
    "        # Create subset datasets\n",
    "        X_train_subset = X_train_processed[:, feature_indices]\n",
    "        X_val_subset = X_val_processed[:, feature_indices]\n",
    "\n",
    "        # Train model with selected features using same optimized hyperparameters\n",
    "        hgb_selected = HistGradientBoostingRegressor(**best_params)\n",
    "        hgb_selected.fit(X_train_subset, y_train)\n",
    "        pred_subset = hgb_selected.predict(X_val_subset)\n",
    "        mae_subset = mean_absolute_error(y_val, pred_subset)\n",
    "\n",
    "        improvement = baseline_mae - mae_subset\n",
    "        print(f\"Top {n_features:2d} features → MAE: {mae_subset:.1f} (change: {improvement:+.1f})\")\n",
    "    \n",
    "    # Store results for optimization analysis\n",
    "    results.append({\n",
    "        'n_features': n_features,\n",
    "        'mae': mae_subset,\n",
    "        'improvement': baseline_mae - mae_subset\n",
    "    })\n",
    "\n",
    "# Find optimal feature configuration\n",
    "results_df = pd.DataFrame(results)\n",
    "best_result = results_df.loc[results_df['mae'].idxmin()]\n",
    "\n",
    "print(f\"\\n=== OPTIMAL FEATURE SELECTION RESULTS ===\")\n",
    "print(f\"Best performance: {best_result['n_features']:.0f} features\")\n",
    "print(f\"Best MAE: {best_result['mae']:.1f}\")\n",
    "print(f\"Improvement over baseline: {best_result['improvement']:.1f} MAE\")\n",
    "\n",
    "# Display optimal feature set\n",
    "optimal_n = int(best_result['n_features'])\n",
    "optimal_features = shap_importance_df.head(optimal_n)['feature'].tolist()\n",
    "\n",
    "print(f\"\\nOptimal {optimal_n} features for production model:\")\n",
    "for i, feat in enumerate(optimal_features[:15]):\n",
    "    importance = shap_importance_df.iloc[i]['importance']\n",
    "    print(f\"{i+1:2d}. {feat:20s} (importance: {importance:.1f})\")\n",
    "if optimal_n > 15:\n",
    "    print(f\"... and {optimal_n-15} more features\")\n",
    "\n",
    "print(f\"\\n=== FINAL RECOMMENDATION ===\")\n",
    "print(f\"Current baseline (145 features): {baseline_mae:.1f} MAE\")\n",
    "print(f\"Optimized model ({optimal_n} features): {best_result['mae']:.1f} MAE\")\n",
    "\n",
    "if best_result['improvement'] > 0:\n",
    "    print(f\"Improvement: {best_result['improvement']:.1f} MAE better\")\n",
    "    print(f\"Recommendation: Use top {optimal_n} features for final submission\")\n",
    "else:\n",
    "    print(f\"Result: Feature selection shows minimal impact\")\n",
    "    print(f\"Recommendation: Use all features (preprocessing already well-optimized)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all engineered features that didn't positively affect MAE for HGB or RF at all here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06207ec2-af0d-489f-a23d-467019791499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TASK IV (4 Points): Build a simple model and assess the performance\n",
    "- Identify the type of problem and select the relevant algorithms\n",
    "- Select one Model Assessment Strategy to use throughout your work. Which metrics are you using to evaluate your model and why?\n",
    "\n",
    "\n",
    "=> Tip from lecturer: Use RandomSearch instead of GridSearchCV, set a wider Range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6297f034-638d-4db6-a681-597f20036432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definition of Function for Model Assessment"
    }
   },
   "outputs": [],
   "source": [
    "# Model evaluation metrics used throughout this analysis:\n",
    "#\n",
    "#   > MAE: Mean Absolute Error - average absolute deviation between predicted and true car prices\n",
    "#          Easy to interpret in pounds, same metric used by Kaggle competition\n",
    "#   > RMSE: Root Mean Squared Error - sensitive to outliers, helps identify large prediction errors  \n",
    "#   > R²: Coefficient of determination - proportion of variance explained by the model\n",
    "#         1.0 = perfect predictions, 0.0 = same as predicting mean, < 0.0 = worse than mean\n",
    "#\n",
    "# These metrics are appropriate for regression problems predicting continuous variables (car prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b26348e-a5ff-4dc9-9d61-7f59d0ea335d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Baselining mean/median"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline mean predictor: \n",
      "MAE: 6976.3626 | RMSE: 92839550.2849 | R2: -0.0000\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "baseline median predictor: \n",
      "MAE: 6751.1604 | RMSE: 97557866.6363 | R2: -0.0508\n"
     ]
    }
   ],
   "source": [
    "# Absolute basic baselining with the mean and median\n",
    "\n",
    "mean_pred = y_train.mean()\n",
    "median_pred = y_train.median()\n",
    "\n",
    "print(\"baseline mean predictor: \")\n",
    "print_metrics(y_val, [mean_pred]*len(y_val))\n",
    "# MAE: 6976.3626 | RMSE: 92839550.2849 | R2: -0.0000\n",
    "\n",
    "print(\"-\"*150)\n",
    "\n",
    "print(\"baseline median predictor: \") \n",
    "print_metrics(y_val, [median_pred]*len(y_val))\n",
    "# MAE: 6751.1604 | RMSE: 97557866.6363 | R2: -0.0508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "04d3aa5b-36ca-4dba-9592-24edd345897d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Models Setup (inkl. Prepro in Pipeline)"
    }
   },
   "outputs": [],
   "source": [
    "# Models Setup (inkl. Prepro in Pipeline)\n",
    "\n",
    "### LINEAR MODEL\n",
    "\n",
    "# ElasticNet\n",
    "elastic_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_linear),\n",
    "    (\"model\", ElasticNet(\n",
    "        alpha=0.01,            # mild regularization to stabilize if many features\n",
    "        l1_ratio=0.5,          # balanced L1/L2, can grid-search\n",
    "        max_iter=30000,        # allow more convergence iterations\n",
    "        tol=1e-4,              # stricter tolerance often improves accuracy\n",
    "        selection=\"cyclic\",    # usually converges faster than random\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### TREE MODELS\n",
    "\n",
    "# HistGradientBoostingRegressor: modern and very fast, handles missing values natively (no imputation needed!). often matches or beats XGBoost/LightGBM \n",
    "hgb_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5, # regularize slightly to prevent overfit, > 0.5 does not seem to work\n",
    "        random_state=42  \n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# RandomForestRegressor: excellent general baseline ensemble, handles non-linearities well, doesn’t overfit easily but can be slow for large data\n",
    "rf_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ExtraTreesRegressor: similar to RandomForest but with more randomization => often better generalization\n",
    "et_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=400,          \n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### KERNEL BASED MODEL\n",
    "\n",
    "# SVR: powerful, but slow on large data, sensitive to scaling => already handled in preprocessing\n",
    "svr_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", SVR(\n",
    "        kernel=\"rbf\",\n",
    "        C=10,\n",
    "        epsilon=0.1,     # slightly tighter margin\n",
    "        gamma=\"scale\"    # default: 1 / (n_features * X.var())\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### ENSEMBLE META MODEL\n",
    "\n",
    "# StackingRegressor: stacks/blends multiple models => typically gives a small but consistent boost in leaderboard competitions\n",
    "stack_pipe = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"elastic\", elastic_pipe),\n",
    "        (\"hgb\", hgb_pipe),\n",
    "        (\"rf\", rf_pipe),\n",
    "    ],\n",
    "    final_estimator=HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42\n",
    "    ),\n",
    "    passthrough=True,     # allow meta-model to see raw inputs too\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hyperparameter Tuning, Evaluation and Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c880d2d-c7aa-478f-8d34-90aa6e33a172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "18042056-7ef9-489f-afc3-958fe02fbfc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ElasticNet"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: ElasticNet\n",
    "\n",
    "elastic_param_grid = {\n",
    "    \"model__alpha\": [0.001],    # also tried 0.01, 0.05, 0.1, 0.5\n",
    "    \"model__l1_ratio\": [0.9]    # also tried 0.1, 0.3, 0.5, 0.7  \n",
    "}\n",
    "\n",
    "elastic_grid = GridSearchCV(\n",
    "    elastic_pipe, \n",
    "    param_grid=elastic_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "elastic_grid.fit(X_train, y_train)\n",
    "elastic_best = elastic_grid.best_estimator_\n",
    "elastic_val_pred = elastic_best.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"ElasticNet Results: \")\n",
    "print_metrics(y_val, elastic_val_pred)\n",
    "print(\"Best ElasticNet params:\", elastic_grid.best_params_)\n",
    "\n",
    "#ElasticNet Results: \n",
    "#MAE: 2543.7302 | RMSE: 16690880.0888 | R2: 0.8202\n",
    "#Best ElasticNet params: {'model__alpha': 0.001, 'model__l1_ratio': 0.9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5a0287-45b6-4022-8d47-cde8cc52469e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### HistGradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2d5effc-c1e9-4606-aa57-f0b5041d2a35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: HistGradientBoost"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "[CV] END model__l2_regularization=0.3745401188473625, model__learning_rate=0.09556428757689245, model__max_iter=506, model__max_leaf_nodes=91, model__min_samples_leaf=8; total time=  14.5s\n",
      "[CV] END model__l2_regularization=0.3745401188473625, model__learning_rate=0.09556428757689245, model__max_iter=506, model__max_leaf_nodes=91, model__min_samples_leaf=8; total time=  15.5s\n",
      "[CV] END model__l2_regularization=0.3745401188473625, model__learning_rate=0.09556428757689245, model__max_iter=506, model__max_leaf_nodes=91, model__min_samples_leaf=8; total time=  18.3s\n",
      "[CV] END model__l2_regularization=0.14286681792194078, model__learning_rate=0.06857996256539675, model__max_iter=708, model__max_leaf_nodes=21, model__min_samples_leaf=13; total time=  18.5s\n",
      "[CV] END model__l2_regularization=0.14286681792194078, model__learning_rate=0.06857996256539675, model__max_iter=708, model__max_leaf_nodes=21, model__min_samples_leaf=13; total time=  21.2s\n",
      "[CV] END model__l2_regularization=0.14286681792194078, model__learning_rate=0.06857996256539675, model__max_iter=708, model__max_leaf_nodes=21, model__min_samples_leaf=13; total time=  21.5s\n",
      "[CV] END model__l2_regularization=0.04666566321361543, model__learning_rate=0.09763799669573131, model__max_iter=589, model__max_leaf_nodes=81, model__min_samples_leaf=16; total time=  14.7s\n",
      "[CV] END model__l2_regularization=0.9385527090157502, model__learning_rate=0.01007008892569129, model__max_iter=676, model__max_leaf_nodes=52, model__min_samples_leaf=13; total time=  42.4s\n",
      "[CV] END model__l2_regularization=0.44583275285359114, model__learning_rate=0.018997742423620262, model__max_iter=858, model__max_leaf_nodes=107, model__min_samples_leaf=5; total time=  43.4s\n",
      "[CV] END model__l2_regularization=0.9385527090157502, model__learning_rate=0.01007008892569129, model__max_iter=676, model__max_leaf_nodes=52, model__min_samples_leaf=13; total time=  43.3s\n",
      "[CV] END model__l2_regularization=0.04666566321361543, model__learning_rate=0.09763799669573131, model__max_iter=589, model__max_leaf_nodes=81, model__min_samples_leaf=16; total time=  14.9s\n",
      "[CV] END model__l2_regularization=0.9385527090157502, model__learning_rate=0.01007008892569129, model__max_iter=676, model__max_leaf_nodes=52, model__min_samples_leaf=13; total time=  43.4s\n",
      "[CV] END model__l2_regularization=0.44583275285359114, model__learning_rate=0.018997742423620262, model__max_iter=858, model__max_leaf_nodes=107, model__min_samples_leaf=5; total time=  59.0s\n",
      "[CV] END model__l2_regularization=0.04666566321361543, model__learning_rate=0.09763799669573131, model__max_iter=589, model__max_leaf_nodes=81, model__min_samples_leaf=16; total time=  23.1s\n",
      "[CV] END model__l2_regularization=0.44583275285359114, model__learning_rate=0.018997742423620262, model__max_iter=858, model__max_leaf_nodes=107, model__min_samples_leaf=5; total time=  59.8s\n",
      "[CV] END model__l2_regularization=0.6183860093330873, model__learning_rate=0.044421579214044646, model__max_iter=643, model__max_leaf_nodes=83, model__min_samples_leaf=4; total time=  32.8s\n",
      "[CV] END model__l2_regularization=0.8599404067363206, model__learning_rate=0.07122767847290017, model__max_iter=566, model__max_leaf_nodes=37, model__min_samples_leaf=5; total time=  21.4s\n",
      "[CV] END model__l2_regularization=0.9422017556848528, model__learning_rate=0.06069593960609854, model__max_iter=745, model__max_leaf_nodes=72, model__min_samples_leaf=3; total time=  22.1s\n",
      "[CV] END model__l2_regularization=0.8599404067363206, model__learning_rate=0.07122767847290017, model__max_iter=566, model__max_leaf_nodes=37, model__min_samples_leaf=5; total time=  24.7s\n",
      "[CV] END model__l2_regularization=0.8599404067363206, model__learning_rate=0.07122767847290017, model__max_iter=566, model__max_leaf_nodes=37, model__min_samples_leaf=5; total time=  24.2s\n",
      "[CV] END model__l2_regularization=0.6183860093330873, model__learning_rate=0.044421579214044646, model__max_iter=643, model__max_leaf_nodes=83, model__min_samples_leaf=4; total time=  27.0s\n",
      "[CV] END model__l2_regularization=0.6116531604882809, model__learning_rate=0.010635967469774566, model__max_iter=960, model__max_leaf_nodes=110, model__min_samples_leaf=11; total time= 1.3min\n",
      "[CV] END model__l2_regularization=0.6183860093330873, model__learning_rate=0.044421579214044646, model__max_iter=643, model__max_leaf_nodes=83, model__min_samples_leaf=4; total time=  40.4s\n",
      "[CV] END model__l2_regularization=0.6116531604882809, model__learning_rate=0.010635967469774566, model__max_iter=960, model__max_leaf_nodes=110, model__min_samples_leaf=11; total time= 1.4min\n",
      "[CV] END model__l2_regularization=0.6116531604882809, model__learning_rate=0.010635967469774566, model__max_iter=960, model__max_leaf_nodes=110, model__min_samples_leaf=11; total time= 1.4min\n",
      "[CV] END model__l2_regularization=0.034388521115218396, model__learning_rate=0.09183883618709038, model__max_iter=961, model__max_leaf_nodes=23, model__min_samples_leaf=3; total time=  21.0s\n",
      "[CV] END model__l2_regularization=0.9422017556848528, model__learning_rate=0.06069593960609854, model__max_iter=745, model__max_leaf_nodes=72, model__min_samples_leaf=3; total time=  33.6s\n",
      "[CV] END model__l2_regularization=0.6842330265121569, model__learning_rate=0.049613724436564116, model__max_iter=854, model__max_leaf_nodes=63, model__min_samples_leaf=9; total time=  37.3s\n",
      "[CV] END model__l2_regularization=0.6842330265121569, model__learning_rate=0.049613724436564116, model__max_iter=854, model__max_leaf_nodes=63, model__min_samples_leaf=9; total time=  38.3s\n",
      "[CV] END model__l2_regularization=0.6842330265121569, model__learning_rate=0.049613724436564116, model__max_iter=854, model__max_leaf_nodes=63, model__min_samples_leaf=9; total time=  40.7s\n",
      "[CV] END model__l2_regularization=0.9422017556848528, model__learning_rate=0.06069593960609854, model__max_iter=745, model__max_leaf_nodes=72, model__min_samples_leaf=3; total time=  46.2s\n",
      "[CV] END model__l2_regularization=0.034388521115218396, model__learning_rate=0.09183883618709038, model__max_iter=961, model__max_leaf_nodes=23, model__min_samples_leaf=3; total time=  30.9s\n",
      "[CV] END model__l2_regularization=0.7751328233611146, model__learning_rate=0.09455490474077702, model__max_iter=669, model__max_leaf_nodes=114, model__min_samples_leaf=17; total time=  26.6s\n",
      "[CV] END model__l2_regularization=0.034388521115218396, model__learning_rate=0.09183883618709038, model__max_iter=961, model__max_leaf_nodes=23, model__min_samples_leaf=3; total time=  45.7s\n",
      "[CV] END model__l2_regularization=0.7751328233611146, model__learning_rate=0.09455490474077702, model__max_iter=669, model__max_leaf_nodes=114, model__min_samples_leaf=17; total time=  32.2s\n",
      "[CV] END model__l2_regularization=0.3265407688058354, model__learning_rate=0.061339957696485946, model__max_iter=651, model__max_leaf_nodes=81, model__min_samples_leaf=9; total time=  41.9s\n",
      "[CV] END model__l2_regularization=0.7751328233611146, model__learning_rate=0.09455490474077702, model__max_iter=669, model__max_leaf_nodes=114, model__min_samples_leaf=17; total time=  47.8s\n",
      "[CV] END model__l2_regularization=0.3265407688058354, model__learning_rate=0.061339957696485946, model__max_iter=651, model__max_leaf_nodes=81, model__min_samples_leaf=9; total time=  47.2s\n",
      "[CV] END model__l2_regularization=0.4251558744912447, model__learning_rate=0.028714749658136994, model__max_iter=876, model__max_leaf_nodes=82, model__min_samples_leaf=19; total time= 1.2min\n",
      "[CV] END model__l2_regularization=0.3265407688058354, model__learning_rate=0.061339957696485946, model__max_iter=651, model__max_leaf_nodes=81, model__min_samples_leaf=9; total time=  46.9s\n",
      "[CV] END model__l2_regularization=0.8445338486781514, model__learning_rate=0.07725880991236427, model__max_iter=452, model__max_leaf_nodes=43, model__min_samples_leaf=14; total time=  40.0s\n",
      "[CV] END model__l2_regularization=0.8445338486781514, model__learning_rate=0.07725880991236427, model__max_iter=452, model__max_leaf_nodes=43, model__min_samples_leaf=14; total time=  38.1s\n",
      "[CV] END model__l2_regularization=0.8445338486781514, model__learning_rate=0.07725880991236427, model__max_iter=452, model__max_leaf_nodes=43, model__min_samples_leaf=14; total time=  34.8s\n",
      "[CV] END model__l2_regularization=0.4251558744912447, model__learning_rate=0.028714749658136994, model__max_iter=876, model__max_leaf_nodes=82, model__min_samples_leaf=19; total time= 1.5min\n",
      "[CV] END model__l2_regularization=0.14092422497476265, model__learning_rate=0.08219772826786356, model__max_iter=464, model__max_leaf_nodes=108, model__min_samples_leaf=8; total time=  21.5s\n",
      "[CV] END model__l2_regularization=0.14092422497476265, model__learning_rate=0.08219772826786356, model__max_iter=464, model__max_leaf_nodes=108, model__min_samples_leaf=8; total time=  24.0s\n",
      "[CV] END model__l2_regularization=0.4251558744912447, model__learning_rate=0.028714749658136994, model__max_iter=876, model__max_leaf_nodes=82, model__min_samples_leaf=19; total time= 1.5min\n",
      "[CV] END model__l2_regularization=0.14092422497476265, model__learning_rate=0.08219772826786356, model__max_iter=464, model__max_leaf_nodes=108, model__min_samples_leaf=8; total time=  32.3s\n",
      "[CV] END model__l2_regularization=0.19884240408880516, model__learning_rate=0.07402077574737849, model__max_iter=818, model__max_leaf_nodes=52, model__min_samples_leaf=6; total time=  24.9s\n",
      "[CV] END model__l2_regularization=0.926300878513349, model__learning_rate=0.068596932295175, model__max_iter=427, model__max_leaf_nodes=26, model__min_samples_leaf=10; total time=  24.1s\n",
      "[CV] END model__l2_regularization=0.926300878513349, model__learning_rate=0.068596932295175, model__max_iter=427, model__max_leaf_nodes=26, model__min_samples_leaf=10; total time=  24.0s\n",
      "[CV] END model__l2_regularization=0.19884240408880516, model__learning_rate=0.07402077574737849, model__max_iter=818, model__max_leaf_nodes=52, model__min_samples_leaf=6; total time=  35.9s\n",
      "[CV] END model__l2_regularization=0.926300878513349, model__learning_rate=0.068596932295175, model__max_iter=427, model__max_leaf_nodes=26, model__min_samples_leaf=10; total time=  24.4s\n",
      "[CV] END model__l2_regularization=0.19884240408880516, model__learning_rate=0.07402077574737849, model__max_iter=818, model__max_leaf_nodes=52, model__min_samples_leaf=6; total time=  32.4s\n",
      "[CV] END model__l2_regularization=0.4234014807063696, model__learning_rate=0.04553933663580127, model__max_iter=871, model__max_leaf_nodes=82, model__min_samples_leaf=12; total time=  50.7s\n",
      "[CV] END model__l2_regularization=0.4234014807063696, model__learning_rate=0.04553933663580127, model__max_iter=871, model__max_leaf_nodes=82, model__min_samples_leaf=12; total time=  54.1s\n",
      "[CV] END model__l2_regularization=0.6232981268275579, model__learning_rate=0.03978082223673843, model__max_iter=447, model__max_leaf_nodes=42, model__min_samples_leaf=6; total time=  31.8s\n",
      "[CV] END model__l2_regularization=0.4234014807063696, model__learning_rate=0.04553933663580127, model__max_iter=871, model__max_leaf_nodes=82, model__min_samples_leaf=12; total time= 1.1min\n",
      "[CV] END model__l2_regularization=0.7296061783380641, model__learning_rate=0.06738017242196918, model__max_iter=946, model__max_leaf_nodes=84, model__min_samples_leaf=4; total time=  26.8s\n",
      "[CV] END model__l2_regularization=0.6232981268275579, model__learning_rate=0.03978082223673843, model__max_iter=447, model__max_leaf_nodes=42, model__min_samples_leaf=6; total time=  32.9s\n",
      "[CV] END model__l2_regularization=0.6232981268275579, model__learning_rate=0.03978082223673843, model__max_iter=447, model__max_leaf_nodes=42, model__min_samples_leaf=6; total time=  32.7s\n",
      "[CV] END model__l2_regularization=0.7296061783380641, model__learning_rate=0.06738017242196918, model__max_iter=946, model__max_leaf_nodes=84, model__min_samples_leaf=4; total time=  31.7s\n",
      "[CV] END model__l2_regularization=0.1195942459383017, model__learning_rate=0.07419203085006955, model__max_iter=404, model__max_leaf_nodes=109, model__min_samples_leaf=15; total time=  25.0s\n",
      "[CV] END model__l2_regularization=0.1195942459383017, model__learning_rate=0.07419203085006955, model__max_iter=404, model__max_leaf_nodes=109, model__min_samples_leaf=15; total time=  33.4s\n",
      "[CV] END model__l2_regularization=0.7296061783380641, model__learning_rate=0.06738017242196918, model__max_iter=946, model__max_leaf_nodes=84, model__min_samples_leaf=4; total time=  39.7s\n",
      "[CV] END model__l2_regularization=0.1195942459383017, model__learning_rate=0.07419203085006955, model__max_iter=404, model__max_leaf_nodes=109, model__min_samples_leaf=15; total time=  32.6s\n",
      "[CV] END model__l2_regularization=0.49379559636439074, model__learning_rate=0.05704595464437946, model__max_iter=414, model__max_leaf_nodes=109, model__min_samples_leaf=11; total time=  34.3s\n",
      "[CV] END model__l2_regularization=0.49379559636439074, model__learning_rate=0.05704595464437946, model__max_iter=414, model__max_leaf_nodes=109, model__min_samples_leaf=11; total time=  34.1s\n",
      "[CV] END model__l2_regularization=0.49379559636439074, model__learning_rate=0.05704595464437946, model__max_iter=414, model__max_leaf_nodes=109, model__min_samples_leaf=11; total time=  39.7s\n",
      "[CV] END model__l2_regularization=0.907566473926093, model__learning_rate=0.032436300623398744, model__max_iter=570, model__max_leaf_nodes=48, model__min_samples_leaf=5; total time=  41.2s\n",
      "[CV] END model__l2_regularization=0.907566473926093, model__learning_rate=0.032436300623398744, model__max_iter=570, model__max_leaf_nodes=48, model__min_samples_leaf=5; total time=  41.3s\n",
      "[CV] END model__l2_regularization=0.4393365018657701, model__learning_rate=0.028154728210185657, model__max_iter=640, model__max_leaf_nodes=71, model__min_samples_leaf=5; total time=  53.6s\n",
      "[CV] END model__l2_regularization=0.907566473926093, model__learning_rate=0.032436300623398744, model__max_iter=570, model__max_leaf_nodes=48, model__min_samples_leaf=5; total time=  41.3s\n",
      "[CV] END model__l2_regularization=0.942853570557981, model__learning_rate=0.06389789198396824, model__max_iter=642, model__max_leaf_nodes=105, model__min_samples_leaf=3; total time=  38.6s\n",
      "[CV] END model__l2_regularization=0.4393365018657701, model__learning_rate=0.028154728210185657, model__max_iter=640, model__max_leaf_nodes=71, model__min_samples_leaf=5; total time=  53.9s\n",
      "[CV] END model__l2_regularization=0.4393365018657701, model__learning_rate=0.028154728210185657, model__max_iter=640, model__max_leaf_nodes=71, model__min_samples_leaf=5; total time=  53.9s\n",
      "[CV] END model__l2_regularization=0.942853570557981, model__learning_rate=0.06389789198396824, model__max_iter=642, model__max_leaf_nodes=105, model__min_samples_leaf=3; total time=  38.3s\n",
      "[CV] END model__l2_regularization=0.942853570557981, model__learning_rate=0.06389789198396824, model__max_iter=642, model__max_leaf_nodes=105, model__min_samples_leaf=3; total time=  34.0s\n",
      "[CV] END model__l2_regularization=0.6243540481337932, model__learning_rate=0.03660703172539426, model__max_iter=427, model__max_leaf_nodes=63, model__min_samples_leaf=12; total time=  34.4s\n",
      "[CV] END model__l2_regularization=0.6243540481337932, model__learning_rate=0.03660703172539426, model__max_iter=427, model__max_leaf_nodes=63, model__min_samples_leaf=12; total time=  35.2s\n",
      "[CV] END model__l2_regularization=0.2721322493846353, model__learning_rate=0.06829211084872261, model__max_iter=658, model__max_leaf_nodes=89, model__min_samples_leaf=9; total time=  29.5s\n",
      "[CV] END model__l2_regularization=0.6243540481337932, model__learning_rate=0.03660703172539426, model__max_iter=427, model__max_leaf_nodes=63, model__min_samples_leaf=12; total time=  34.9s\n",
      "[CV] END model__l2_regularization=0.2721322493846353, model__learning_rate=0.06829211084872261, model__max_iter=658, model__max_leaf_nodes=89, model__min_samples_leaf=9; total time=  29.9s\n",
      "[CV] END model__l2_regularization=0.2721322493846353, model__learning_rate=0.06829211084872261, model__max_iter=658, model__max_leaf_nodes=89, model__min_samples_leaf=9; total time=  28.3s\n",
      "[CV] END model__l2_regularization=0.534089419375442, model__learning_rate=0.05363469742230849, model__max_iter=624, model__max_leaf_nodes=70, model__min_samples_leaf=11; total time=  25.4s\n",
      "[CV] END model__l2_regularization=0.32434502100527396, model__learning_rate=0.020987915923060603, model__max_iter=872, model__max_leaf_nodes=81, model__min_samples_leaf=2; total time=  47.0s\n",
      "[CV] END model__l2_regularization=0.534089419375442, model__learning_rate=0.05363469742230849, model__max_iter=624, model__max_leaf_nodes=70, model__min_samples_leaf=11; total time=  38.2s\n",
      "[CV] END model__l2_regularization=0.534089419375442, model__learning_rate=0.05363469742230849, model__max_iter=624, model__max_leaf_nodes=70, model__min_samples_leaf=11; total time=  26.5s\n",
      "[CV] END model__l2_regularization=0.32434502100527396, model__learning_rate=0.020987915923060603, model__max_iter=872, model__max_leaf_nodes=81, model__min_samples_leaf=2; total time=  56.7s\n",
      "[CV] END model__l2_regularization=0.32434502100527396, model__learning_rate=0.020987915923060603, model__max_iter=872, model__max_leaf_nodes=81, model__min_samples_leaf=2; total time=  56.2s\n",
      "Best Params: {'model__l2_regularization': np.float64(0.942853570557981), 'model__learning_rate': np.float64(0.06389789198396824), 'model__max_iter': 642, 'model__max_leaf_nodes': 105, 'model__min_samples_leaf': 3}\n",
      "MAE: 1303.5851 | RMSE: 4411007.7758 | R2: 0.9525\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning: HistGradientBoost\n",
    "\n",
    "hgb_param_dist = {\n",
    "    \"model__learning_rate\": uniform(0.01, 0.09),       # samples values between 0.01–0.10\n",
    "    \"model__max_leaf_nodes\": randint(20, 120),         # tries between 20–120 leaves\n",
    "    \"model__min_samples_leaf\": randint(2, 20),         # samples leaf sizes between 2–20\n",
    "    \"model__max_iter\": randint(400, 1000),             # tries 400–1000 iterations\n",
    "    \"model__l2_regularization\": uniform(0.0, 1.0)      # samples small regularization values\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)  # 3-fold for faster runtime\n",
    "\n",
    "# Randomized search setup\n",
    "hgb_random = RandomizedSearchCV(\n",
    "    estimator=hgb_pipe,\n",
    "    param_distributions=hgb_param_dist,\n",
    "    n_iter=30,                         # number of random combinations to try\n",
    "    scoring=\"neg_mean_absolute_error\", # optimize for MAE\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the search\n",
    "hgb_random.fit(X_train, y_train)\n",
    "\n",
    "# Get best model\n",
    "hgb_best_rand = hgb_random.best_estimator_\n",
    "print(\"Best Params:\", hgb_random.best_params_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "hgb_val_pred = hgb_random.best_estimator_.predict(X_val)\n",
    "print_metrics(y_val, hgb_val_pred)\n",
    "\n",
    "# Save model for later use\n",
    "#joblib.dump(hgb_XYZ, \"hgb_XYZ\") # neuen namen vergeben und speichern wenn besser als hgb_best_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b449fd7-2c96-44b4-a9f0-2603c7e1667e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Results of current best HGB (hgb_best_1.pk)\n",
    "\n",
    "# Params: {'model__l2_regularization': 0.49379559636439074, 'model__learning_rate': 0.05704595464437946, 'model__max_iter': 414, 'model__max_leaf_nodes': 109, 'model__min_samples_leaf': 11}\n",
    "# MAE: 1326.7973 | RMSE: 4663930.5059 | R2: 0.9498\n",
    "\n",
    "# Features:\n",
    "# numeric_features = [\"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"brand_anchor\"]\n",
    "# log_features = [\"mileage\", \"miles_per_year\", \"model_freq\", \"brand_med_price\", \"model_med_price\"] # could try to test previousOwners, age here; tax, mpg didnt work\n",
    "# categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "78157989-9ef4-4b2b-a9e1-72309a89e754",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "OUTDATED: HT: HistGradientBoost"
    }
   },
   "outputs": [],
   "source": [
    "# OUTDATED HT: HistGradientBoost - with GridSearch - but good to quickly check something\n",
    "\n",
    "hgb_param_grid = {\n",
    "    \"model__learning_rate\": [0.06], # also tried: 0.02, 0.04, 0.05, 0.1\n",
    "    \"model__max_leaf_nodes\": [50], # also tried: 15, 25, 31, 60\n",
    "    \"model__min_samples_leaf\": [5], # also tried: 8, 10, 15, 20\n",
    "    \"model__max_iter\": [800] # also tried: 500, 1000\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "hgb_grid = GridSearchCV(\n",
    "    estimator=hgb_pipe,\n",
    "    param_grid=hgb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",  # optimize MAE\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "hgb_grid.fit(X_train, y_train)\n",
    "hgb_best_2 = hgb_grid.best_estimator_\n",
    "\n",
    "hgb_val_pred = hgb_best_2.predict(X_val)\n",
    "print_metrics(y_val, hgb_val_pred)\n",
    "\n",
    "# Best Parameters: {'model__learning_rate': 0.06, 'model__max_iter': 800, 'model__max_leaf_nodes': 50, 'model__min_samples_leaf': 5}\n",
    "# MAE: 1304.7611 | RMSE: 4503446.5247 | R2: 0.9515 \n",
    "# joblib.dump(hgb_best_1, \"hgb_best_1.pkl\") => save current best model\n",
    "\n",
    "\n",
    "# Save model for later use\n",
    "#joblib.dump(hgb_XYZ, \"hgb_XYZ\") # neuen namen vergeben und speichern wenn besser als hgb_best_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70de2673-0443-4cc4-b1cd-f3813a969246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: RandomForest\n",
    "\n",
    "rf_param_dist = {\n",
    "    \"model__n_estimators\": randint(200, 600),        # number of trees\n",
    "    \"model__max_depth\": randint(5, 40),              # depth of each tree\n",
    "    \"model__min_samples_split\": randint(2, 10),      # min samples to split an internal node\n",
    "    \"model__min_samples_leaf\": randint(1, 8),        # min samples per leaf\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\"],         # feature sampling strategy\n",
    "    \"model__bootstrap\": [True, False]                # use bootstrapping or not\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Randomized search setup\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf_pipe,\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=30,                         # number of random combinations\n",
    "    scoring=\"neg_mean_absolute_error\", \n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the search\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "rf_best_rand = rf_random.best_estimator_\n",
    "print(\"Best Params:\", rf_random.best_params_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "rf_val_pred = rf_best_rand.predict(X_val)\n",
    "print_metrics(y_val, rf_val_pred)\n",
    "\n",
    "# Save best model\n",
    "# joblib.dump(rf_best_rand, \"rf_best.pkl\")\n",
    "\n",
    "\n",
    "# Best Params: {'model__bootstrap': False, 'model__max_depth': 32, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 9, 'model__n_estimators': 467}\n",
    "# MAE: 1403.6263 | RMSE: 5529283.1606 | R2: 0.9404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9ab1516-d5e9-4b85-9ef5-c5556053c548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Results of current best RF > Samuel\n",
    "\n",
    "# Best Params: {'model__bootstrap': False, 'model__max_depth': 32, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 9, 'model__n_estimators': 467}\n",
    "# MAE: 1403.6263 | RMSE: 5529283.1606 | R2: 0.9404\n",
    "\n",
    "# Features:\n",
    "# numeric_features = [\"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"brand_anchor\"]\n",
    "# log_features = [\"mileage\", \"miles_per_year\", \"model_freq\", \"brand_med_price\", \"model_med_price\"] # could try to test previousOwners, age here; tax, mpg didnt work\n",
    "# categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6d3b92-efc6-4d8a-b3e1-cb6aff36b84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6f4ff12e-07a4-47f5-8c08-43b5d8d86942",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ExtraTrees"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: ExtraTrees\n",
    "\n",
    "et_param_dist = {\n",
    "    \"model__n_estimators\": randint(200, 600),\n",
    "    \"model__max_depth\": randint(5, 40),\n",
    "    \"model__min_samples_split\": randint(2, 10),\n",
    "    \"model__min_samples_leaf\": randint(1, 8),\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"model__bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "et_random = RandomizedSearchCV(\n",
    "    estimator=et_pipe,\n",
    "    param_distributions=et_param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "et_random.fit(X_train, y_train)\n",
    "\n",
    "et_best = et_random.best_estimator_\n",
    "print(\"ExtraTrees Best Params:\", et_random.best_params_)\n",
    "\n",
    "et_val_pred = et_best.predict(X_val)\n",
    "print_metrics(y_val, et_val_pred)\n",
    "\n",
    "# joblib.dump(et_best, \"et_best.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f1dea5a-14a8-40c9-bfe4-1cbb96e207a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Results of current best ET > Samuel\n",
    "\n",
    "# ExtraTrees Best Params: {'model__bootstrap': False, 'model__max_depth': 32, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 9, \n",
    "# 'model__n_estimators': 467}\n",
    "# MAE: 1631.3539 | RMSE: 7549459.7557 | R2: 0.9187\n",
    "\n",
    "# Features:\n",
    "# numeric_features = [\"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"brand_anchor\"]\n",
    "# log_features = [\"mileage\", \"miles_per_year\", \"model_freq\", \"brand_med_price\", \"model_med_price\"] # could try to test previousOwners, age here; tax, mpg didnt work\n",
    "# categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a6e6e70-8430-4ef7-bbdf-27602c253f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### SupportVectorRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f487848b-789b-4084-827b-2ca4a2313bfc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: SupportVectorRegressor"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: SupportVectorRegressor\n",
    "\n",
    "svr_param_dist = {\n",
    "    \"model__C\": loguniform(1e-1, 1e3),          # wide search over regularization\n",
    "    \"model__epsilon\": uniform(0.05, 0.3),       # small-margin tolerance\n",
    "    \"model__kernel\": [\"rbf\"],                   # best general-purpose kernel\n",
    "    \"model__gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "svr_random = RandomizedSearchCV(\n",
    "    estimator=svr_pipe,\n",
    "    param_distributions=svr_param_dist,\n",
    "    n_iter=25,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "svr_random.fit(X_train, y_train)\n",
    "\n",
    "svr_best = svr_random.best_estimator_\n",
    "print(\"SVR Best Params:\", svr_random.best_params_)\n",
    "\n",
    "svr_val_pred = svr_best.predict(X_val)\n",
    "print_metrics(y_val, svr_val_pred)\n",
    "\n",
    "# joblib.dump(svr_best, \"svr_best.pkl\")\n",
    "\n",
    "# MAE: 1683.6556 | RMSE: 10282674.8643 | R2: 0.8892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "73408a5c-9acf-4658-9f5f-4c4a430b731c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Results of current best SVR > Samuel\n",
    "\n",
    "# MAE: 1683.6556 | RMSE: 10282674.8643 | R2: 0.8892\n",
    "\n",
    "# Features:\n",
    "# numeric_features = [\"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"brand_anchor\"]\n",
    "# log_features = [\"mileage\", \"miles_per_year\", \"model_freq\", \"brand_med_price\", \"model_med_price\"] # could try to test previousOwners, age here; tax, mpg didnt work\n",
    "# categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "980b2649-0f0d-4d52-91a2-dc569c878850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0525442b-db49-45b7-ad73-ff98599eb223",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: StackingRegressor"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: StackingRegressor\n",
    "\n",
    "stack_param_dist = {\n",
    "    \"final_estimator__learning_rate\": uniform(0.02, 0.08),\n",
    "    \"final_estimator__max_depth\": randint(3, 7),\n",
    "    \"final_estimator__min_samples_leaf\": randint(3, 15),\n",
    "    \"final_estimator__l2_regularization\": uniform(0.0, 1.0)\n",
    "}\n",
    "\n",
    "stack_random = RandomizedSearchCV(\n",
    "    estimator=stack_pipe,\n",
    "    param_distributions=stack_param_dist,\n",
    "    n_iter=25,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=3,                 # low CV because stacking is slow\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "stack_random.fit(X_train, y_train)\n",
    "\n",
    "stack_best = stack_random.best_estimator_\n",
    "print(\"StackingRegressor Best Params:\", stack_random.best_params_)\n",
    "\n",
    "stack_val_pred = stack_best.predict(X_val)\n",
    "print_metrics(y_val, stack_val_pred)\n",
    "\n",
    "# joblib.dump(stack_best, \"stack_best.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a77338ae-128f-46b2-932e-d2349944bbe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Results of current best SVR\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135acfad-1ad6-49f9-97fc-f1a81d0c3187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb68e758-d7b1-4f80-9dc4-ed985f721c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Extra Task (1 Point): Be in the Top 5 Groups on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "927be871-4f27-4453-baac-b4f7102534d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load best Models from Joblib"
    }
   },
   "outputs": [],
   "source": [
    "# Load best Models from Joblib\n",
    "\n",
    "# hgb_best_1\n",
    "hgb_best_1 = joblib.load(\"hgb_best_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a25f58c-4c24-4460-93d4-830176016ee8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pick best Model, predict on Test"
    }
   },
   "outputs": [],
   "source": [
    "# Pick best model and predict on test:\n",
    "df_cars_test['price'] = hgb_best_1.predict(df_cars_test)\n",
    "\n",
    "df_cars_test['price'].to_csv('Group05_VersionXX.csv', index=True) # currently version 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f5b18805-e662-4807-bdb8-758df6027d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Submission with API push"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c cars4you -f Group05_VersionXX.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350057a3-c771-4bf0-8fac-19b49be17e35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Score Check"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c cars4you"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
