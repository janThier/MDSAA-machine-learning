{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b83a4d32-7797-4f5f-8269-7bc08d0b9431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_Clone Before Using to play with Features / Hyperparameters_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52359b17-1216-4744-a528-81e23a033a68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fff3e4-624f-4b1f-b258-fbbe7b69cb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import & load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aa35af5d-8257-4fb6-a380-86bbbfac3244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "556ffc24-70f7-48c9-96a4-3a19380f91db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Connect"
    }
   },
   "outputs": [],
   "source": [
    "### Everyone has to do this himself, with his own kaggle.json -> get it from kaggle as api token\n",
    "import os\n",
    "\n",
    "# Folder containing kaggle.json\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/Workspace/Users/20250355@novaims.unl.pt\"\n",
    "\n",
    "# Optional: test\n",
    "!echo $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f54ab11e-fcfc-4278-8997-89a92adf8eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import and load Data"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, chi2\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, FunctionTransformer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.linear_model import Ridge, Lasso, LassoCV, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from data_cleaning import clean_car_dataframe\n",
    "\n",
    "df_cars_train = pd.read_csv(\"train.csv\")\n",
    "df_cars_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228971f7-84d2-4472-84d1-f4b91e185bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explorative Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b65f0ae-0d7a-4cdb-830a-391a209b82d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TASK I (3 Points): Descriptive Statistics, Inconsistency Check, Visual Data Explorance, Extraction of Relevant Insights, Multivariate Relationships  => Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "00193525-37c5-4d69-bc00-605d8fb49f7d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Decriptive Statistics"
    }
   },
   "outputs": [],
   "source": [
    "# Overview of structure and data types\n",
    "df_cars_train.info()\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate rows: {df_cars_train.duplicated().sum()}\") # 0\n",
    "\n",
    "# Check for duplicate carID\n",
    "print(f\"\\nDuplicate carID: {df_cars_train['carID'].duplicated().sum()}\") # 0\n",
    "\n",
    "# Check for null values in column CarID\n",
    "print(f\"\\nNull values in column carID: {df_cars_train['carID'].isnull().sum()}\") # 0\n",
    "\n",
    "# Findings:\n",
    "#   - missing values in every column except carID\n",
    "#   - year, mpg, previousOwners, hasDamage as float seems weird\n",
    "#   - no duplicates in all columns, no null values in carID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b60f640e-1c37-401c-bce1-290392e2be9a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inconsistency Check"
    }
   },
   "outputs": [],
   "source": [
    "# print exact unique values of df_cars_train before doing describe\n",
    "for col in df_cars_train.columns:\n",
    "    print(col, df_cars_train[col].unique())\n",
    "\n",
    "# describe all\n",
    "df_cars_train.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "36e654d9-0893-4966-90eb-7cf33f4c62b6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Univariate Distributions"
    }
   },
   "outputs": [],
   "source": [
    "# Univariate Exploration of relevant Columns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "num_cols = ['price','mileage','tax','mpg','engineSize','paintQuality%','previousOwners','year']\n",
    "\n",
    "# colors\n",
    "hist_color = '#1f77b4'   # dark blue\n",
    "box_color = '#ff7f0e'    # warm orange\n",
    "\n",
    "# figure: 2 features per row (4 plots per row = hist + boxplot per feature)\n",
    "n_features = len(num_cols)\n",
    "n_rows = int(np.ceil(n_features / 2))\n",
    "fig, axes = plt.subplots(n_rows, 4, figsize=(16, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    # histogram\n",
    "    sns.histplot(df_cars_train[col], bins=25, color=hist_color, kde=False, ax=axes[i*2])\n",
    "    axes[i*2].set_xlabel('')\n",
    "    axes[i*2].set_ylabel('')\n",
    "    axes[i*2].set_title(f'{col}', fontsize=11, pad=12)\n",
    "\n",
    "    # boxplot (vertical)\n",
    "    sns.boxplot(y=df_cars_train[col], ax=axes[i*2 + 1], color=box_color)\n",
    "    axes[i*2 + 1].set_xlabel('')\n",
    "    axes[i*2 + 1].set_ylabel('')\n",
    "    axes[i*2 + 1].set_title(f'{col}', fontsize=11, pad=12)\n",
    "\n",
    "# hide any unused axes\n",
    "for j in range(i*2 + 2, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.suptitle('univariate distributions of numerical features', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87c827ae-6704-4616-8782-1d11f7385da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "##### Findings after Descriptive Statistics and Inconsistency Check\n",
    "\n",
    "carID:  \n",
    "- sequential numeric identifier, ranges from **0–75,972**  \n",
    "- no duplicates expected, used only as index/key  \n",
    "\n",
    "brand:  \n",
    "- **72 unique brands** with severe spelling and capitalization inconsistencies (**Ford**, **ford**, **FOR**, **ord**, **For**, etc.)  \n",
    "- contains partial or truncated names (**w**, **MW**, **Ope**, **Mercede**) → heavy cleaning required  \n",
    "- **Ford dominates (~15k entries)**, followed by **BMW**, **VW**, **Mercedes**, **Toyota**, **Audi**, **Skoda**, **Hyundai**  \n",
    "- strong class imbalance → use frequency encoding or median price per brand later  \n",
    "\n",
    "model:  \n",
    "- **735 unique entries** with inconsistent formatting, spacing, capitalization, and partial strings  \n",
    "- duplicates of same model under variations (e.g. *“focus”*, *“ FOCUS”*, *“ Focu”*, *“Focus”*)  \n",
    "- many small typos, truncated or malformed entries (*“Focu”*, *“EcoSpor”*, *“Gol”*, *“Yeti Outdoo”*, etc.)  \n",
    "- heavy normalization needed → strip whitespace, lowercase, and fix common truncations  \n",
    "- dominated by popular models like **Ford Focus**, **VW Golf**, **Vauxhall Astra/Corsa**, **Skoda Octavia**  \n",
    "\n",
    "year:  \n",
    "- values range from **1970–2024**, mean ≈ **2017**  \n",
    "- years after **2020** are unrealistic (future registration) → invalid entries  \n",
    "- decimals in year (e.g. **2023.367**) → data corruption, round to nearest int  \n",
    "- older outliers before **2000** rare, likely classic or miscoded entries  \n",
    "- can derive **age = 2020 - year** for modeling  \n",
    "\n",
    "transmission:  \n",
    "- **40 distinct entries**, mostly spelling variants of *manual*, *automatic*, *semi-auto*  \n",
    "- common corruptions: *manua*, *anual*, *semi-aut*, *utomatic*, *nknow*, etc.  \n",
    "- some leading/trailing spaces (*' manual '*, *' Manual '*)  \n",
    "- categories should be reduced to clean labels: **manual**, **automatic**, **semi-auto**, **unknown**  \n",
    "\n",
    "mileage:  \n",
    "- range **–58,540 → 323,000**, mean ≈ **23k** → negatives invalid  \n",
    "- missing values around **1.5k**  \n",
    "- strong right skew, some extreme outliers >**250k miles**  \n",
    "- negative or zero values should be filtered or replaced with abs()  \n",
    "\n",
    "tax:  \n",
    "- range **–91 → 580**, mean ≈ **120** → invalid negatives present  \n",
    "- normal values cluster around **125–145**  \n",
    "- decimals and small negatives appear due to calculation/entry errors  \n",
    "- likely strong right skew → a few cars taxed over **500**  \n",
    "- needs capping and replacement for negatives  \n",
    "\n",
    "mpg:  \n",
    "- range **–43 → 470**, mean ≈ **55** → negative and extreme outliers exist  \n",
    "- typical real range **30–70 mpg**, but some values like **470** unrealistic  \n",
    "- invalid entries indicate unit mix-up or input noise  \n",
    "- expected inverse relation with **engine size** and **price**  \n",
    "\n",
    "engineSize:  \n",
    "- range **–0.1 → 6.6L**, mean ≈ **1.66L**, std ≈ **0.57**  \n",
    "- several decimals and negative/zero values → invalid  \n",
    "- expected valid range **0.6–6.0L**  \n",
    "- most cars between **1.2–2.0L** → compact to mid-size engines  \n",
    "- positive correlation with **price** and **tax**  \n",
    "\n",
    "paintQuality%:  \n",
    "- range **1.6 → 125.6%**, mean ≈ **64.6%**  \n",
    "- values above **100%** unrealistic → scaling error  \n",
    "- some extremely low values (**≈1–3%**) indicate outliers or noise  \n",
    "- most cars between **50–80%** → average paint quality  \n",
    "\n",
    "previousOwners:  \n",
    "- range **–2.3 → 6.26**, mean ≈ **2**  \n",
    "- negative values invalid → likely placeholder or encoding issue  \n",
    "- most between **0–3** → typical secondhand ownership distribution  \n",
    "- outliers >**6** likely data entry errors or mis-scaling  \n",
    "- likely negative correlation with **price**  \n",
    "\n",
    "hasDamage:  \n",
    "- only values are **0 and NaN** → no variation  \n",
    "- unclear if NaN means damaged → convert to int and verify meaning  \n",
    "- likely nonfunctional feature → **drop**  \n",
    "\n",
    "price (target):  \n",
    "- range **£450–£159,999**, mean ≈ **£16.9k**, median ≈ **£14.7k** → right-skewed  \n",
    "- typical cars priced **£10k–£21k**, few luxury outliers inflate mean  \n",
    "- consistent integer values, no missing or obvious anomalies  \n",
    "- strong dependence expected on **mileage**, **year**, **engine size**, and **brand**\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0d9d97a2-8ecc-41e2-81e0-abe470940a9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Correlations"
    }
   },
   "outputs": [],
   "source": [
    "# pearson and spearman correlation\n",
    "\n",
    "num_cols = ['price','mileage','tax','mpg','engineSize','paintQuality%','previousOwners','year']\n",
    "\n",
    "corr_pearson = df_cars_train[num_cols].corr(method='pearson', numeric_only=True).round(2)\n",
    "corr_spearman = df_cars_train[num_cols].corr(method='spearman', numeric_only=True).round(2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "mask = np.triu(np.ones_like(corr_spearman, dtype=bool))\n",
    "\n",
    "sns.heatmap(corr_pearson, mask=mask, annot=True, cmap='coolwarm', center=0, fmt='.2f',\n",
    "            linewidths=0.5, ax=axes[0], cbar=False)\n",
    "axes[0].set_title('pearson correlation (linear)')\n",
    "\n",
    "sns.heatmap(corr_spearman, mask=mask, annot=True, cmap='coolwarm', center=0, fmt='.2f',\n",
    "            linewidths=0.5, ax=axes[1], cbar_kws={'label': 'correlation strength'})\n",
    "axes[1].set_title('spearman correlation (monotonic)')\n",
    "\n",
    "plt.suptitle('comparison of pearson vs spearman correlation', fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8efa37e9-9ab5-472e-8514-8e7b6b832be4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Why we decided to use Spearman correlation:\n",
    "\n",
    "- the numeric variables (price, mileage, mpg, tax, engineSize) are **non-normally distributed and contain outliers**, causing linear measures like pearson to distort correlation strength.  \n",
    "- **spearman** evaluates *monotonic* relationships based on rank order rather than exact linearity, making it **robust to skewness and outliers**.  \n",
    "- this allows us to correctly capture the direction and magnitude of real-world trends, such as price decreasing with mileage, even if not perfectly linear.  \n",
    "- after cleaning and scaling the data, we will revisit **pearson correlation** for linear modeling checks, but **spearman is more appropriate for initial EDA**.\n",
    "\n",
    "##### Findings after correlation:\n",
    "\n",
    "- price shows **strong positive correlation with year (0.60)** → newer cars are priced higher  \n",
    "- price is **positively correlated with engineSize (0.56)** → larger engines increase car value  \n",
    "- price is **moderately negatively correlated with mileage (–0.51)** → more driven cars lose value  \n",
    "- mpg correlates **negatively with price (–0.39)** → efficient cars are typically smaller and cheaper  \n",
    "- tax has a **moderate positive correlation (0.31)** with price → more expensive cars often have higher taxes  \n",
    "- year and mileage have a **very strong negative correlation (–0.78)** → newer cars have lower mileage  \n",
    "- mpg and tax are **strongly negatively correlated (–0.55)** → efficient cars usually taxed less  \n",
    "- engineSize and mpg **correlate negatively (–0.20) → larger engines are less fuel-efficient  \n",
    "- paintQuality% and previousOwners show **near-zero correlations** with all other variables → low predictive relevance  \n",
    "- overall, **price mainly depends on year, engineSize, mileage, and mpg**, which align with intuitive market behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a5042c-f3b6-4071-a655-3a8e91fa6402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Multivariate Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ce9ca3c-079c-4ca1-b962-1982bbb812bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "##### Findings after Multivariate Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75ee214-0996-406f-b77f-9e72fa7153e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Cleaning, Feature Engineering, Split & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78e7ca8-ba25-4ea3-873d-4f3515281440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task II (5 Points): Clean and preprocess the dataset. \n",
    "- Missing Value handling, Outlier preprocessing + justify decisions -> in data_cleaning.py\n",
    "- Review current features and create extra features if needed + explain\n",
    "- Deal with categorical variables -> One-Hot-Encoding \n",
    "- Perform data scaling, explain reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6fa0cfe9-be7e-4e90-ac1c-cd600bb7d13f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Numerical / Categorical Data Cleaning: with Pandas"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier Preprocessing happens here\n",
    "df_cars_train = clean_car_dataframe(df_cars_train)\n",
    "df_cars_test = clean_car_dataframe(df_cars_test)\n",
    "\n",
    "\n",
    "# Safety Check: print all unique values of all columns of df_cars_train // df_cars_test to see if data cleaning worked and if there are still weird values\n",
    "for col in df_cars_train.columns:\n",
    "    print(col, df_cars_train[col].unique())\n",
    "print(\"X\"*150)\n",
    "for col in df_cars_test.columns:\n",
    "    print(col, df_cars_test[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2df7bf7c-0ebb-4f4b-a168-5b2e2f6b21d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering and Explaination\n",
    "\n",
    "# add column age: models can easier interpret linear numerical features\n",
    "df_cars_train['age'] = 2025 - df_cars_train['year']\n",
    "df_cars_test['age'] = 2025 - df_cars_test['year']\n",
    "\n",
    "# miles per year: normalizes the total mileage by how old the car is\n",
    "df_cars_train['miles_per_year'] = df_cars_train['mileage'] / df_cars_train['age'].replace({0: np.nan})\n",
    "df_cars_train['miles_per_year'] = df_cars_train['miles_per_year'].fillna(df_cars_train['mileage'])\n",
    "\n",
    "df_cars_test['miles_per_year'] = df_cars_test['mileage'] / df_cars_test['age'].replace({0: np.nan})\n",
    "df_cars_test['miles_per_year'] = df_cars_test['miles_per_year'].fillna(df_cars_test['mileage'])\n",
    "\n",
    "# model frequency: some models are more common, which means they can be cheaper (supply) or retain their values better (demand). freq shows their popularity\n",
    "model_freq = df_cars_train['model'].value_counts(normalize=True).to_dict()\n",
    "df_cars_train['model_freq'] = df_cars_train['model'].map(model_freq)\n",
    "\n",
    "model_freq = df_cars_test['model'].value_counts(normalize=True).to_dict()\n",
    "df_cars_test['model_freq'] = df_cars_test['model'].map(model_freq)\n",
    "\n",
    "# brand median price (only train): shows brand positioning (e.g. BMW > KIA)\n",
    "brand_median_price = df_cars_train.groupby('Brand')['price'].median()\n",
    "df_cars_train['brand_med_price'] = df_cars_train['Brand'].map(brand_median_price)\n",
    "\n",
    "# model median price (only train): shows model positioning (e.g. 3er > 1er)\n",
    "model_med_price = df_cars_train.groupby('model')['price'].median()\n",
    "df_cars_train['model_med_price'] = df_cars_train['model'].map(model_med_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dda482e9-4a68-438d-86c0-e0524a9b8692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split Data"
    }
   },
   "outputs": [],
   "source": [
    "X = df_cars_train.drop(columns='price')\n",
    "y = df_cars_train['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.3, \n",
    "                                                  random_state = 42, \n",
    "                                                  #stratify = y,    # if y, class proportions get preserved between train and test sets\n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "39142374-d485-42e7-a6cb-83ecffae0091",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocessing: with sklearn Pipeline & Column Transformer"
    }
   },
   "outputs": [],
   "source": [
    "# Define which columns are numeric vs categorical (mileage not in here because skewed - log)\n",
    "numeric_features = [\"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\"]\n",
    "log_features = [\"mileage\", \"miles_per_year\", \"model_freq\", \"brand_med_price\", \"model_med_price\"] # could try to test previousOwners, age here, tax, mpg didnt work\n",
    "categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "\n",
    "# left out columns: year (age is better), hasDamage (unsure what the two values 0 and NaN mean)\n",
    "\n",
    "\n",
    "log_transformer = Pipeline([\n",
    "    # Handling of missing numerical values with sklearn SimpleImputer (mean)\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    # Data Scaling with sklearn FunctionTransformer (for log) and StandardScaler\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=False)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    # Handling of missing numerical values with sklearn SimpleImputer (mean)\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    # Data Scaling with sklearn StandardScaler\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    # Handling of missing categorical values with sklearn SimpleImputer (Unknown)\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "    # Deal with Categorical Variables with sklearn OneHotEncoder:\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)) \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    # Apply the preprocessing steps to the data\n",
    "    (\"mileage\", log_transformer, [\"mileage\"]), # log because mileage is skewed\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "\n",
    "# Fit preprocessor on training data - avoid data leakage\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ColumnTransformer lets you apply different transformations to different feature subsets:\n",
    "#       > Numeric → impute mean + scale\n",
    "#       > Categorical → impute \"Unknown\" + OneHotEncode\n",
    "#       > Mileage → impute log-transform\n",
    "#   This is key, because numeric and categorical data need different math, you can't scale strings or one-hot encode continuous numbers.\n",
    "\n",
    "\n",
    "# Pipeline bundles preprocessing + model training:\n",
    "#     > Cross-validation applies preprocessing inside each fold (no data leakage).\n",
    "#     > The final model object (after .fit()) knows exactly how to preprocess new data.\n",
    "#     > When saving the pipeline with joblib, everything (scaler, encoder, model) is saved together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8f763f-298d-444a-80a9-0270ba842dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e9a293-7457-4767-8a26-7c930f7dbad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task III (3 Points): Define and Implement a clear and unambiguous strategy for feature selection. Use the methods discussed in the course. Present and justify your final selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d989a4b-077d-47bd-b4bb-c4528628888f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Model independent Filter Methods:\n",
    "- Remove constant numerical variables with VarianceThreshold (manual)\n",
    "- Check highly correlated numerical variables and keep one with Spearman (manual)\n",
    "- Remove unindependent categorical variables with Chi2\n",
    "\n",
    "Model dependent Wrapper Methods:\n",
    "- RFE LR / RFE SVR for linear Models: Ridge, Lasso, ElasticNet, SVM\n",
    "- Feature Importance for tree Models: DecisionTrees, RandomForest, GradientBoosting => trees are unsensitive to irrelevant features but doing feature importance and remove some can reduce dimensionality\n",
    "- L1 Regularization for Neural Networks: MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "130e5929-dcd7-48db-8afa-8f9f560968fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Selection with Filter Methods"
    }
   },
   "outputs": [],
   "source": [
    "X_train_proc = preprocessor.transform(X_train)\n",
    "\n",
    "feature_names_all = []\n",
    "for name, trans, cols in preprocessor.transformers_:\n",
    "    if name != 'remainder':\n",
    "        if hasattr(trans, 'get_feature_names_out'):\n",
    "            # for categorical OHE\n",
    "            try:\n",
    "                feature_names_all.extend(trans.get_feature_names_out(cols))\n",
    "            except:\n",
    "                feature_names_all.extend(cols)\n",
    "        else:\n",
    "            feature_names_all.extend(cols)\n",
    "\n",
    "X_df = pd.DataFrame(X_train_proc, columns=feature_names_all)\n",
    "\n",
    "\n",
    "# Variance Threshold\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "vt.fit(X_df)\n",
    "vt_deselect = [f for f, keep in zip(feature_names_all, vt.get_support()) if not keep]\n",
    "print(\"Features to deselect according to VarianceThreshold:\", vt_deselect)\n",
    "\n",
    "\n",
    "# Spearman correlation (numeric + log only)\n",
    "numeric_log = numeric_features + log_features\n",
    "spearman_deselect = []\n",
    "for f in numeric_log:\n",
    "    if f in X_df.columns:\n",
    "        corr, _ = spearmanr(X_df[f], y_train)\n",
    "        if abs(corr) <= 0.05:\n",
    "            spearman_deselect.append(f)\n",
    "print(\"Features to deselect according to Spearman correlation:\", spearman_deselect)\n",
    "\n",
    "\n",
    "# Chi2 (categorical only, must be non-negative)\n",
    "cat_cols = [c for c in X_df.columns if c not in numeric_log]\n",
    "X_cat = X_df[cat_cols].astype(float)\n",
    "chi2_vals, _ = chi2(X_cat, y_train)\n",
    "chi2_deselect = [f for f, val in zip(cat_cols, chi2_vals) if val <= 0]\n",
    "print(\"Features to deselect according to Chi²:\", chi2_deselect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d62d3f6a-baa5-40e9-913a-b99f443b5080",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Deselection for linear Models"
    }
   },
   "outputs": [],
   "source": [
    "# Numeric/log features for linear models\n",
    "linear_numeric_features = [f for f in numeric_features + log_features if f not in spearman_deselect]\n",
    "\n",
    "preprocessor_linear = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, linear_numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# => use preprocessor_linear for linear model setup; since tree models are indifferent to irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dcae03f-ed97-410e-9078-fb734f42be87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Models Setup and Baselining (with SKLEARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06207ec2-af0d-489f-a23d-467019791499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TASK IV (4 Points): Build a simple model and assess the performance\n",
    "- Identify the type of problem and select the relevant algorithms\n",
    "- Select one Model Assessment Strategy to use throughout your work. Which metrics are you using to evaluate your model and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6297f034-638d-4db6-a681-597f20036432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definition of Function for Model Assessment"
    }
   },
   "outputs": [],
   "source": [
    "# Following metrics are used for model evaluation:\n",
    "#\n",
    "#   MAE: Average absolute deviation between predicted and true car prices, easy to interpret, kaggle competition uses same metric\n",
    "#   RMSE: Root mean squared error, helps to see if large errors on same values were made, therefore sensitive to outliers\n",
    "#   R2: Proportion of variance explained by the model, 1 = perfect, 0 = same as predicting mean, < 0 = worse than mean baseline\n",
    "#\n",
    "# Because our task is a regression problem and we are predicting a continuous variable (car price)\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"MAE: {mae:.4f} | RMSE: {rmse:.4f} | R2: {r2:.4f}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d3aa5b-36ca-4dba-9592-24edd345897d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Models Setup (inkl. Prepro in Pipeline)"
    }
   },
   "outputs": [],
   "source": [
    "### LINEAR MODEL\n",
    "# ElasticNet\n",
    "elastic_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_linear),\n",
    "    (\"model\", ElasticNet(\n",
    "        max_iter=20000,\n",
    "        selection=\"random\",\n",
    "        warm_start=False,  # set True only if iteratively tuning manually\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### TREE MODELS\n",
    "# GradientBoostingRegressor: baseline, has to beat ElasticNet, if not something’s wrong with data preprocessing, not the model.\n",
    "gbr_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor), \n",
    "    (\"model\", GradientBoostingRegressor(loss='absolute_error'))\n",
    "])\n",
    "\n",
    "# HistGradientBoostingRegressor: modern and very fast, handles missing values natively (no imputation needed!). often matches or beats XGBoost/LightGBM \n",
    "hgb_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5,  # regularize slightly to prevent overfit, > 0.5 does not seem to work\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# RandomForestRegressor: excellent general baseline ensemble, handles non-linearities well, doesn’t overfit easily but can be slow for large data\n",
    "rf_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300, \n",
    "        max_depth=None, \n",
    "        n_jobs=-1, \n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ExtraTreesRegressor: similar to RandomForest but with more randomization => often better generalization\n",
    "et_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=300, \n",
    "        max_depth=None, \n",
    "        n_jobs=-1, \n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### KERNEL BASED MODEL\n",
    "# SVR: powerful, but slow on large data, sensitive to scaling => already handled in preprocessing\n",
    "svr_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", SVR(C=10, epsilon=0.2, kernel=\"rbf\"))\n",
    "])\n",
    "\n",
    "\n",
    "### ENSEMBLE META MODEL\n",
    "# StackingRegressor: stacks/blends multiple models => typically gives a small but consistent boost in leaderboard competitions\n",
    "stack_pipe = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"el\", elastic_pipe),\n",
    "        (\"hgb\", hgb_pipe),\n",
    "    ],\n",
    "    passthrough=False   # True can sometimes help but increases overfitting risk\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b26348e-a5ff-4dc9-9d61-7f59d0ea335d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Baselining mean/median"
    }
   },
   "outputs": [],
   "source": [
    "mean_pred = y_train.mean()\n",
    "median_pred = y_train.median()\n",
    "\n",
    "print(\"baseline mean predictor: \")\n",
    "print_metrics(y_val, [mean_pred]*len(y_val))\n",
    "# MAE: 6976.3626 | RMSE: 92839550.2849 | R2: -0.0000\n",
    "\n",
    "print(\"-\"*150)\n",
    "\n",
    "print(\"baseline median predictor: \") \n",
    "print_metrics(y_val, [median_pred]*len(y_val))\n",
    "# MAE: 6751.1604 | RMSE: 97557866.6363 | R2: -0.0508"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18042056-7ef9-489f-afc3-958fe02fbfc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ElasticNet"
    }
   },
   "outputs": [],
   "source": [
    "elastic_param_grid = {\n",
    "    \"model__alpha\": [0.001, 0.01, 0.05, 0.1, 0.5],\n",
    "    \"model__l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9]     \n",
    "}\n",
    "\n",
    "elastic_grid = GridSearchCV(\n",
    "    elastic_pipe, \n",
    "    param_grid=elastic_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "elastic_grid.fit(X_train, y_train)\n",
    "elastic_best = elastic_grid.best_estimator_\n",
    "elastic_val_pred = elastic_best.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"ElasticNet Results: \")\n",
    "print_metrics(y_val, elastic_val_pred)\n",
    "print(\"Best ElasticNet params:\", elastic_grid.best_params_)\n",
    "\n",
    "# MAE: 2540.2806 | RMSE: 16642911.3643 | R2: 0.8207\n",
    "# Best ElasticNet params: {'model__alpha': 0.001, 'model__l1_ratio': 0.9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e7dac5c-ae30-4ed6-b7ac-a87942f91697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### HT: tree Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    \"model__max_depth\": [None, 10, 20],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    rf_pipe, rf_param_grid,\n",
    "    cv=5, scoring=\"r2\",\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_best = rf_grid.best_estimator_\n",
    "rf_val_pred = rf_best.predict(X_val)\n",
    "\n",
    "print(\"Random Forest Results: \")\n",
    "print_metrics(y_val, rf_val_pred)\n",
    "\n",
    "# 1st Try: MAE: 1431.4111 | RMSE: 5764825.2457 | R2: 0.9379 with max_depth= None, 10, 20; max_features= sqrt, log2; cv = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6f4ff12e-07a4-47f5-8c08-43b5d8d86942",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ExtraTrees"
    }
   },
   "outputs": [],
   "source": [
    "et_param_grid = {\n",
    "    \"model__max_depth\": [None, 10, 20],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(\n",
    "    et_pipe, et_param_grid,\n",
    "    cv=5, scoring=\"r2\",\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "et_grid.fit(X_train, y_train)\n",
    "et_best = et_grid.best_estimator_\n",
    "et_val_pred = et_best.predict(X_val)\n",
    "\n",
    "print(\"ExtraTrees Results: \")\n",
    "print_metrics(y_val, et_val_pred)\n",
    "\n",
    "# 1st Try: MAE: 1438.3138 | RMSE: 5865741.8358 | R2: 0.9368 with max_depth= None, 10, 20; max_features= sqrt, log2; cv = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "151b3c01-a218-4349-ac7d-48711b9cfd30",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: GradientBoosting"
    }
   },
   "outputs": [],
   "source": [
    "gbr_param_grid = {\n",
    "    \"model__n_estimators\": [500],\n",
    "    \"model__learning_rate\": [0.05],\n",
    "    \"model__max_depth\": [5], \"model__loss\": ['squared_error']\n",
    "}\n",
    "\n",
    "gbr_grid = GridSearchCV(\n",
    "    gbr_pipe, gbr_param_grid,\n",
    "    cv=5, scoring=\"r2\",\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "gbr_grid.fit(X_train, y_train)\n",
    "gbr_best = gbr_grid.best_estimator_\n",
    "gbr_val_pred = gbr_best.predict(X_val)\n",
    "\n",
    "print(\"GradientBoosting Results: \")\n",
    "print_metrics(y_val, gbr_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "78157989-9ef4-4b2b-a9e1-72309a89e754",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: HistGradientBoost"
    }
   },
   "outputs": [],
   "source": [
    "hgb_param_grid = {\n",
    "    \"model__learning_rate\": [0.06], # also tried: 0.02, 0.04, 0.05, 0.1\n",
    "    \"model__max_leaf_nodes\": [50], # also tried: 15, 25, 31, 60\n",
    "    \"model__min_samples_leaf\": [5], # also tried: 8, 10, 15, 20\n",
    "    \"model__max_iter\": [800] # also tried: 500, 1000\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "hgb_grid = GridSearchCV(\n",
    "    estimator=hgb_pipe,\n",
    "    param_grid=hgb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",  # optimize MAE\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "hgb_grid.fit(X_train, y_train)\n",
    "hgb_best_1 = hgb_grid.best_estimator_\n",
    "\n",
    "hgb_val_pred = hgb_best_1.predict(X_val)\n",
    "print_metrics(y_val, hgb_val_pred)\n",
    "\n",
    "# Best Parameters: {'model__learning_rate': 0.06, 'model__max_iter': 800, 'model__max_leaf_nodes': 50, 'model__min_samples_leaf': 5}\n",
    "# MAE: 1304.7611 | RMSE: 4503446.5247 | R2: 0.9515 \n",
    "# joblib.dump(hgb_best_1, \"hgb_best_1.pkl\")\n",
    "\n",
    "# Save model for later use\n",
    "joblib.dump(hgb_XYZ, \"hgb_XYZ\") # neuen namen vergeben und speichern wenn besser als hgb_best_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9f283585-b396-4c60-a4bc-ec12b2f42c01",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TEST PARAMETERS HBG"
    }
   },
   "outputs": [],
   "source": [
    "hgb_param_grid = {\n",
    "    \"model__learning_rate\": [0.06, 0.07, 0.08],\n",
    "    \"model__max_leaf_nodes\": [45, 50, 55],\n",
    "    \"model__min_samples_leaf\": [4, 5, 6],\n",
    "    \"model__max_iter\": [700, 800, 900]\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "hgb_grid = GridSearchCV(\n",
    "    estimator=hgb_pipe,\n",
    "    param_grid=hgb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",  # optimize MAE\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "hgb_grid.fit(X_train, y_train)\n",
    "hgb_best_2 = hgb_grid.best_estimator_\n",
    "\n",
    "hgb_val_pred = hgb_best_2.predict(X_val)\n",
    "print_metrics(y_val, hgb_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5bdcf66-8612-4455-979e-a348ca38dd42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### HT: advanced Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f487848b-789b-4084-827b-2ca4a2313bfc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: SupportVectorRegressor"
    }
   },
   "outputs": [],
   "source": [
    "svr_param_grid = {\n",
    "    \"model__C\": [1, 10, 100],\n",
    "    \"model__epsilon\": [0.1, 0.2],\n",
    "    \"model__kernel\": [\"rbf\"]\n",
    "}\n",
    "\n",
    "svr_grid = GridSearchCV(\n",
    "    svr_pipe, svr_param_grid,\n",
    "    cv=cv, scoring=\"r2\", n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "svr_grid.fit(X_train, y_train)\n",
    "svr_best = svr_grid.best_estimator_\n",
    "\n",
    "svr_val_pred = svr_best.predict(X_val)\n",
    "r2_svr, rmse_svr, mae_svr = print_metrics(y_val, svr_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0525442b-db49-45b7-ad73-ff98599eb223",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: StackingRegressor"
    }
   },
   "outputs": [],
   "source": [
    "# stackingregressor (elasticnet + histgradientboost)\n",
    "\n",
    "param_grid_stack = {\n",
    "    \"final_estimator__learning_rate\": [0.03, 0.05, 0.08],\n",
    "    \"final_estimator__max_depth\": [3, 5, 7],\n",
    "    \"final_estimator__min_samples_leaf\": [5, 10, 20]\n",
    "}\n",
    "\n",
    "stack_grid = GridSearchCV(\n",
    "    stack_pipe,\n",
    "    param_grid=param_grid_stack,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=3,            # lower CV since it’s meta-tuning\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "stack_grid.fit(X_train, y_train)\n",
    "stack_best = stack_grid.best_estimator_\n",
    "\n",
    "val_pred = stack_best.predict(X_val)\n",
    "print(\"Best params:\", stack_grid.best_params_)\n",
    "print_metrics(y_val, val_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135acfad-1ad6-49f9-97fc-f1a81d0c3187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb68e758-d7b1-4f80-9dc4-ed985f721c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Extra Task (1 Point): Be in the Top 5 Groups on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "927be871-4f27-4453-baac-b4f7102534d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load hgb_best_1 from joblib\n",
    "hgb_best_1 = joblib.load(\"hgb_best_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6a25f58c-4c24-4460-93d4-830176016ee8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pick best Model, predict on Test"
    }
   },
   "outputs": [],
   "source": [
    "# Pick best model and predict on test:\n",
    "df_cars_test['price'] = hgb_best_1.predict(df_cars_test)\n",
    "\n",
    "df_cars_test['price'].to_csv('submission.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448c5790-0cfd-4544-b244-a0e3b79acf8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b18805-e662-4807-bdb8-758df6027d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Submission with API push"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c cars4you -f submission.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350057a3-c771-4bf0-8fac-19b49be17e35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Score Check"
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle Score Check\n",
    "!kaggle competitions submissions -c cars4you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228368c1-cec9-4df8-9171-8cefeb426783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1e3d5b-a46a-4077-b29f-d44f953c634c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Importance & Interpretation"
    }
   },
   "outputs": [],
   "source": [
    "# What/Why: Different models give different “importance” signals:\n",
    "# Lasso → coefficients (linear importance); zero → removed feature.\n",
    "# Tree ensembles (GBR) → feature_importances_ (importance in splits).\n",
    "# For rigorous interpretation, use SHAP for consistent feature attributions across models.\n",
    "\n",
    "\n",
    "# Important: yes — each model may select different features. That’s expected. Use the model type that matches your use-case:\n",
    "# If you need a sparse, interpretable linear model → use Lasso.\n",
    "# If you need best predictive power on tabular data → use ensemble/boosting and interpret via SHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ebc3c64-02eb-481d-bf58-59f92e0fddcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Iterative Loop Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f20976-4956-40c7-89e7-355361e29a56",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Iterative Loop Begins"
    }
   },
   "source": [
    "The loop starts after you look at baseline performance and diagnostics:\n",
    "- Baseline → Check metrics on validation (and residuals).\n",
    "- Inspect failures / residual plots / feature importances (did a certain brand get consistently over/under predicted?)\n",
    "- Hypothesize (e.g. add interaction year * mileage, try log transform for horsepower, create age = current_year - year).\n",
    "- Implement changes in pipeline (e.g. add FunctionTransformer for log(horsepower) or PolynomialFeatures on a small set).\n",
    "- Re-run CV/hyperparameter search and evaluate again.\n",
    "- Log results, repeat.\n",
    "\n",
    "Note on feature selection: yes — different models will select different subsets. Typical approaches:\n",
    "- Use Lasso or SelectFromModel as a filter for linear pipelines.\n",
    "- Use tree-based model importances or SHAP to select features for simpler models.\n",
    "- Or let the best predictive model use all features (trees are robust to redundancy).\n",
    "\n",
    "Final notes (recommended best-practices)\n",
    "- Always fit preprocessing only on training data (pipelines do this automatically if you use them inside CV).\n",
    "- Start simple: mean baseline → Ridge/Lasso → tree-based. Use the simple models for interpretability and as sanity checks.\n",
    "- For heavy hyperparameter searches use RandomizedSearchCV or Optuna if the space is big.\n",
    "- When comparing models, report multiple metrics (R², MAE, RMSE). For price prediction MAE is often most interpretable.\n",
    "- For reproducibility, store your dataset version, random seed, code, and results log."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
