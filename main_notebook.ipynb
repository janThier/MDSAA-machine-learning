{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4467ab3-7594-4e02-a19f-8c5e47fb14e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"\n",
    "    background: rgba(25, 25, 25, 0.55);\n",
    "    backdrop-filter: blur(16px) saturate(150%);\n",
    "    -webkit-backdrop-filter: blur(16px) saturate(150%);\n",
    "    border: 1px solid rgba(255, 255, 255, 0.12);\n",
    "    border-radius: 18px;\n",
    "    padding: 45px 30px;\n",
    "    text-align: center;\n",
    "    font-family: 'Inter', 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;\n",
    "    color: #e0e0e0;\n",
    "    box-shadow: 0 0 30px rgba(0, 0, 0, 0.35);\n",
    "    margin: 40px auto;\n",
    "    max-width: 800px;\n",
    "\">\n",
    "\n",
    "  <h1 style=\"\n",
    "      font-size: 2.8em;\n",
    "      font-weight: 700;\n",
    "      margin: 0 0 8px 0;\n",
    "      letter-spacing: -0.02em;\n",
    "      background: linear-gradient(90deg, #00e0ff, #9c7eff);\n",
    "      -webkit-background-clip: text;\n",
    "      -webkit-text-fill-color: transparent;\n",
    "  \">\n",
    "      Machine Learning Project\n",
    "  </h1>\n",
    "\n",
    "  <h2 style=\"\n",
    "      font-size: 1.6em;\n",
    "      font-weight: 500;\n",
    "      margin: 0 0 25px 0;\n",
    "      color: #b0b0b0;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Cars price prediction\n",
    "  </h2>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.25em;\n",
    "      font-weight: 500;\n",
    "      color: #c0c0c0;\n",
    "      margin-bottom: 6px;\n",
    "  \">\n",
    "      Group 5\n",
    "  </p>\n",
    "\n",
    "  <p style=\"\n",
    "      font-size: 1.05em;\n",
    "      font-weight: 400;\n",
    "      color: #8a8a8a;\n",
    "      font-style: italic;\n",
    "      letter-spacing: 0.5px;\n",
    "  \">\n",
    "      Machine Learning Part 1 · 2025 / 2026\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "372c87aa-b7f3-4a34-a596-bb5e584db114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Table of Contents**\n",
    "\n",
    "- [0. Context and Metadata](#0-context)  \n",
    "  - [0.1 Project Overview](#0.1-overview)  \n",
    "  - [0.2 Team Members](#0.2-team)\n",
    "\n",
    "- [1. Import Packages and Data](#1-import)  \n",
    "  - [1.1 Import Required Packages](#1.1-packages)  \n",
    "  - [1.2 Load Datasets](#1.2-data)\n",
    "  - [1.3 Kaggle Setup]()\n",
    "\n",
    "TODO finish toc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcea0314-f61b-48dd-93e0-540b52850d1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Process to test new features: (TODO remove before submission)\n",
    "\n",
    "1. Add new Features in cell Feature Engineering\n",
    "2. Go into Preprocessing and add the features in either num, log or cat\n",
    "3. Run the RandomSearch // Hyperparameter Tuning for at least HGB and RF and see if MAE gets improved compared to previous results\n",
    "4. If MAE gets not improved, comment in cell below Feature Engineering that you tested those features (+results?) and on which Models - and remove everything\n",
    "5. If MAE gets improved, find out via Feature Importance (Shap Values) which Feature was responsible + document it, remove the other features that have negative impact\n",
    "6. comment below the Hyperparameter Tuning cell of the model the new achieved results + all the features you used for that + the hyperparameters\n",
    "7. save the results in a new model with joblib, name it correctly\n",
    "8. push to kaggle\n",
    "9. push to GIT + document everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27fff3e4-624f-4b1f-b258-fbbe7b69cb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Import Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e6aadb-727a-4399-8888-9d6d4e593898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aa35af5d-8257-4fb6-a380-86bbbfac3244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f54ab11e-fcfc-4278-8997-89a92adf8eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import and load Data"
    }
   },
   "outputs": [],
   "source": [
    "# Import and load Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, chi2, RFECV\n",
    "from scipy.stats import spearmanr, uniform, loguniform, randint\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.linear_model import Ridge, Lasso, LassoCV, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from data_cleaning import clean_car_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9737aef-d2bd-4748-89ed-9b0e2cea8637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.2 Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3da1ad2b-8ee0-48b0-917d-545c70633ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cars_train = pd.read_csv(\"train.csv\")\n",
    "df_cars_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab6500b5-94a1-4a0e-b7f6-ea9d35c787eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Kaggle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "556ffc24-70f7-48c9-96a4-3a19380f91db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Connect"
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle API Connect => everyone has to do this himself, with his own kaggle.json api token\n",
    "\n",
    "# Folder containing kaggle.json\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/Workspace/Users/20250355@novaims.unl.pt\"\n",
    "\n",
    "# Test\n",
    "!echo $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b75ee214-0996-406f-b77f-9e72fa7153e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data Cleaning, Feature Engineering, Split & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b78e7ca8-ba25-4ea3-873d-4f3515281440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task II (5 Points): Clean and preprocess the dataset. \n",
    "- Missing Value handling, Outlier preprocessing + justify decisions -> in data_cleaning.py\n",
    "- Review current features and create extra features if needed + explain -> in Feature Engineering\n",
    "- Deal with categorical variables -> In One-Hot-Encoding \n",
    "- Perform data scaling, explain reasoning -> In Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6fa0cfe9-be7e-4e90-ac1c-cd600bb7d13f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Numerical / Categorical Data Cleaning: with Pandas"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier Preprocessing, Missing Value Handling and Decision justifying happens here\n",
    "df_cars_train = clean_car_dataframe(df_cars_train)\n",
    "df_cars_test = clean_car_dataframe(df_cars_test)\n",
    "\n",
    "\n",
    "# Safety Check: print unique values of all columns of df_cars_train // df_cars_test to see if data cleaning worked and if there are still odd values\n",
    "for col in df_cars_train.columns:\n",
    "    print(col, df_cars_train[col].unique())\n",
    "print(\"X\"*150)\n",
    "print(\"-\"*150)\n",
    "print(\"X\"*150)\n",
    "for col in df_cars_test.columns:\n",
    "    print(col, df_cars_test[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2df7bf7c-0ebb-4f4b-a168-5b2e2f6b21d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering and Explaination\n",
    "\n",
    "# add column age: models can easier interpret linear numerical features\n",
    "df_cars_train['age'] = 2025 - df_cars_train['year']\n",
    "df_cars_test['age'] = 2025 - df_cars_test['year']\n",
    "\n",
    "\n",
    "# miles per year: normalizes the total mileage by how old the car is (solves year vs. mileage multicollinearity found in EDA)\n",
    "df_cars_train['miles_per_year'] = df_cars_train['mileage'] / df_cars_train['age'].replace({0: np.nan})\n",
    "df_cars_train['miles_per_year'] = df_cars_train['miles_per_year'].fillna(df_cars_train['mileage'])\n",
    "\n",
    "df_cars_test['miles_per_year'] = df_cars_test['mileage'] / df_cars_test['age'].replace({0: np.nan})\n",
    "df_cars_test['miles_per_year'] = df_cars_test['miles_per_year'].fillna(df_cars_test['mileage'])\n",
    "\n",
    "\n",
    "# model frequency: some models are more common, which means they can be cheaper (supply) or retain their values better (demand). freq shows their popularity\n",
    "model_freq = df_cars_train['model'].value_counts(normalize=True).to_dict()\n",
    "df_cars_train['model_freq'] = df_cars_train['model'].map(model_freq)\n",
    "df_cars_test['model_freq'] = df_cars_test['model'].map(model_freq)\n",
    "\n",
    "\n",
    "# brand median price (only train): shows brand positioning (e.g. BMW > KIA)\n",
    "brand_median_price = df_cars_train.groupby('Brand')['price'].median()\n",
    "df_cars_train['brand_med_price'] = df_cars_train['Brand'].map(brand_median_price)\n",
    "df_cars_test[\"brand_med_price\"] = df_cars_test[\"Brand\"].map(brand_median_price)\n",
    "\n",
    "\n",
    "# model median price (only train): shows model positioning (e.g. 3er > 1er)\n",
    "model_med_price = df_cars_train.groupby('model')['price'].median()\n",
    "df_cars_train['model_med_price'] = df_cars_train['model'].map(model_med_price)\n",
    "df_cars_test[\"model_med_price\"] = df_cars_test[\"model\"].map(model_med_price)\n",
    "\n",
    "# brand anchor (market position) \n",
    "brand_median_price = df_cars_train.groupby(\"Brand\")[\"price\"].median().to_dict()\n",
    "overall_mean_price = df_cars_train[\"price\"].mean()\n",
    "\n",
    "df_cars_train[\"brand_anchor\"] = df_cars_train[\"Brand\"].map(brand_median_price) / overall_mean_price\n",
    "df_cars_test[\"brand_anchor\"]  = df_cars_test[\"Brand\"].map(brand_median_price) / overall_mean_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e0e5b3fd-063b-4c4f-896e-fc0aedb73a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Put all engineered features that didn't positively affect MAE for HGB or RF at all here:\n",
    "# # tax per engine ~Jan\n",
    "# df_cars_train['tax_per_engine'] = df_cars_train['tax'] / df_cars_train['engineSize'].replace({0: np.nan})\n",
    "# df_cars_train['tax_per_engine'] = df_cars_train['tax_per_engine'].fillna(df_cars_train['tax'])\n",
    "\n",
    "# df_cars_test['tax_per_engine'] = df_cars_test['tax'] / df_cars_test['engineSize'].replace({0: np.nan})\n",
    "# df_cars_test['tax_per_engine'] = df_cars_test['tax_per_engine'].fillna(df_cars_test['tax'])\n",
    "\n",
    "# # efficiency index ~Jan\n",
    "# df_cars_train['efficiency_index'] = df_cars_train['mpg'] / df_cars_train['engineSize'].replace({0: np.nan})\n",
    "# df_cars_train['efficiency_index'] = df_cars_train['efficiency_index'].fillna(df_cars_train['mpg'])\n",
    "\n",
    "# df_cars_test['efficiency_index'] = df_cars_test['mpg'] / df_cars_test['engineSize'].replace({0: np.nan})\n",
    "# df_cars_test['efficiency_index'] = df_cars_test['efficiency_index'].fillna(df_cars_test['mpg'])\n",
    "\n",
    "# # mpg per engine ~Jan\n",
    "# df_cars_train['mpg_per_engine'] = df_cars_train['mpg'] / df_cars_train['engineSize'].replace({0: np.nan})\n",
    "# df_cars_train['mpg_per_engine'] = df_cars_train['mpg_per_engine'].fillna(df_cars_train['mpg'])\n",
    "\n",
    "# df_cars_test['mpg_per_engine'] = df_cars_test['mpg'] / df_cars_test['engineSize'].replace({0: np.nan})\n",
    "# df_cars_test['mpg_per_engine'] = df_cars_test['mpg_per_engine'].fillna(df_cars_test['mpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dda482e9-4a68-438d-86c0-e0524a9b8692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split Data"
    }
   },
   "outputs": [],
   "source": [
    "# Split Data, Stratify not necessary due to regression problem and Cross Validation later\n",
    "X = df_cars_train.drop(columns='price')\n",
    "y = df_cars_train['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.3, \n",
    "                                                  random_state = 42, \n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c92786c5-7746-459e-9834-e7c36f01c22c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write GroupMedianImputer"
    }
   },
   "outputs": [],
   "source": [
    "# Write custom GroupMedianImputer to impute missing values on a model, brand level and not only global (with SimpleImputer)\n",
    "class GroupMedianImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Impute missing numeric values using hierarchical medians:\n",
    "    1. By (Brand, model)\n",
    "    2. If model missing → by Brand\n",
    "    3. Fallback → global median\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, group_cols=[\"Brand\", \"model\"]):\n",
    "        self.group_cols = group_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        self.feature_names_in_ = X.columns\n",
    "\n",
    "        # Step 1 — model-level medians\n",
    "        if all(c in X.columns for c in self.group_cols):\n",
    "            self.medians_ = X.groupby(self.group_cols).median(numeric_only=True)\n",
    "        else:\n",
    "            self.medians_ = pd.DataFrame()\n",
    "\n",
    "        # Step 2 — brand-level medians\n",
    "        if \"Brand\" in X.columns:\n",
    "            self.brand_medians_ = X.groupby(\"Brand\").median(numeric_only=True)\n",
    "        else:\n",
    "            self.brand_medians_ = pd.DataFrame()\n",
    "\n",
    "        # Step 3 — global medians\n",
    "        self.global_median_ = X.median(numeric_only=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        for col in X.columns:\n",
    "            if X[col].isna().sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # (Brand, model) level\n",
    "            if all(c in X.columns for c in self.group_cols):\n",
    "                X[col] = X.apply(\n",
    "                    lambda r: self.medians_.loc[(r[\"Brand\"], r[\"model\"]), col]\n",
    "                    if (r[\"Brand\"], r[\"model\"]) in self.medians_.index and pd.isna(r[col])\n",
    "                    else r[col],\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "            # Brand-only fallback\n",
    "            if \"Brand\" in X.columns:\n",
    "                X[col] = X.apply(\n",
    "                    lambda r: self.brand_medians_.loc[r[\"Brand\"], col]\n",
    "                    if r[\"Brand\"] in self.brand_medians_.index and pd.isna(r[col])\n",
    "                    else r[col],\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "            # Global fallback\n",
    "            X[col] = X[col].fillna(self.global_median_[col])\n",
    "\n",
    "        return X.values  # sklearn expects ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "39142374-d485-42e7-a6cb-83ecffae0091",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocessing: with sklearn Pipeline & Column Transformer"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing: with sklearn Pipeline & Column Transformer\n",
    "\n",
    "group_imputer = GroupMedianImputer(group_cols=[\"Brand\", \"model\"])\n",
    "\n",
    "numeric_features = [\"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"brand_anchor\"]\n",
    "log_features = [\"mileage\", \"miles_per_year\", \"model_freq\", \"brand_med_price\", \"model_med_price\"] # could try to test previousOwners, age here; tax, mpg didnt work\n",
    "categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "# left out columns: year (age is better), hasDamage (unsure what the two values 0 and NaN mean)\n",
    "\n",
    "def to_float_array(x):\n",
    "    \"\"\"Convert input to float array.\"\"\"\n",
    "    return np.array(x, dtype=float)\n",
    "\n",
    "log_transformer = Pipeline([\n",
    "    (\"group_impute\", GroupMedianImputer(group_cols=[\"Brand\", \"model\"])),  # Handling of missing numerical values with GroupMedianImputer\n",
    "    (\"to_float\", FunctionTransformer(to_float_array)),\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=False)), # log1p handles zeros safely\n",
    "    (\"scaler\", StandardScaler()) #  # Data Scaling with sklearn StandardScaler\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"group_impute\", GroupMedianImputer(group_cols=[\"Brand\", \"model\"])),\n",
    "    (\"to_float\", FunctionTransformer(to_float_array)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), # fill by mode instead of Unknown (a diesel 3er BMW is probably a diesel)\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)) # Deal with Categorical Variables with sklearn OneHotEncoder\n",
    "])\n",
    "\n",
    "# Apply the preprocessing steps to the data with ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"log\", log_transformer, log_features),\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "\n",
    "# Fit preprocessor on training data - avoid data leakage\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" EXPLAINATIONS \"\"\"\n",
    "# 1) Pipeline bundles preprocessing + model training:\n",
    "#       > Ensures all preprocessing happens inside cross-validation folds (no data leakage)\n",
    "#       > Keeps the entire workflow reproducible — scaling, encoding, and modeling are learned together\n",
    "#       > After .fit(), the final model automatically knows how to preprocess new unseen data\n",
    "#       > When saving with joblib, the entire preprocessing (imputers, scalers, encoders) and model are stored together\n",
    "\n",
    "# 2) The ColumnTransformer applies different transformations to subsets of features:\n",
    "#       > Numeric Features arehandled by our custom GroupMedianImputer (domain-aware filling)\n",
    "#           - Missing numeric values are imputed hierarchically:\n",
    "#           1. By (Brand, model)\n",
    "#           2. If missing model by Brand\n",
    "#           3. If missing Brand by global median\n",
    "#       > This approach captures brand/model-level patterns (e.g. BMWs have similar engine sizes)\n",
    "#       > After imputation, StandardScaler standardizes all numeric features\n",
    "#\n",
    "#       > Log Features use the same group-median imputation, followed by log1p() transformation\n",
    "#           - log1p() compresses large, skewed values (like mileage or price-related features), stabilizing variance and helping linear models perform better\n",
    "#           - StandardScaler then scales them to zero mean and unit variance\n",
    "#\n",
    "#       > Categorical Features are handled by SimpleImputer + OneHotEncoder\n",
    "#           - SimpleImputer fills missing categorical values with the most frequent (mode) value.\n",
    "#             (Alternative would be “Unknown”, but mode keeps categories realistic, e.g. most cars in a model share the same transmission)\n",
    "#           - OneHotEncoder converts each categorical label (Brand, model, etc.) into binary dummy variables\n",
    "#             This lets the model use category information numerically without implying order\n",
    "#\n",
    "# 3) Overall:\n",
    "#       > The pipeline ensures consistent preprocessing across training, validation, and test data.\n",
    "#       > It combines domain knowledge (brand/model-aware imputation) with robust numerical scaling.\n",
    "#       > Linear models (ElasticNet, Ridge, Lasso) and tree models (HistGradientBoosting, RandomForest)\n",
    "#           can now learn from the same standardized, clean, and information-rich feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba8f763f-298d-444a-80a9-0270ba842dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71e9a293-7457-4767-8a26-7c930f7dbad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task III (3 Points): Define and Implement a clear and unambiguous strategy for feature selection. Use the methods **discussed in the course**. Present and justify your final selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d989a4b-077d-47bd-b4bb-c4528628888f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Model independent Filter Methods:\n",
    "- Remove constant numerical variables with VarianceThreshold\n",
    "- Check highly correlated numerical variables and keep one with Spearman\n",
    "- Remove unindependent categorical variables with Chi2\n",
    "\n",
    "Model dependent Wrapper Methods:\n",
    "- RFE LR / RFE SVR for linear Models: ElasticNet, SVM\n",
    "- Feature Importance for tree Models: RandomForest, HistGradientBoosting (see at X.XX Feature Importance (with SHAP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "130e5929-dcd7-48db-8afa-8f9f560968fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Selection with Filter Methods"
    }
   },
   "outputs": [],
   "source": [
    "X_train_proc = preprocessor.transform(X_train)\n",
    "\n",
    "feature_names_all = []\n",
    "for name, trans, cols in preprocessor.transformers_:\n",
    "    if name != 'remainder':\n",
    "        if hasattr(trans, 'get_feature_names_out'):\n",
    "            # for categorical OHE\n",
    "            try:\n",
    "                feature_names_all.extend(trans.get_feature_names_out(cols))\n",
    "            except:\n",
    "                feature_names_all.extend(cols)\n",
    "        else:\n",
    "            feature_names_all.extend(cols)\n",
    "\n",
    "X_df = pd.DataFrame(X_train_proc, columns=feature_names_all)\n",
    "\n",
    "\n",
    "# Variance Threshold\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "vt.fit(X_df)\n",
    "vt_deselect = [f for f, keep in zip(feature_names_all, vt.get_support()) if not keep]\n",
    "print(\"Features to deselect according to VarianceThreshold:\", vt_deselect)\n",
    "\n",
    "\n",
    "# Spearman correlation (numeric + log only)\n",
    "numeric_log = numeric_features + log_features\n",
    "spearman_deselect = []\n",
    "for f in numeric_log:\n",
    "    if f in X_df.columns:\n",
    "        corr, _ = spearmanr(X_df[f], y_train)\n",
    "        if abs(corr) <= 0.05:\n",
    "            spearman_deselect.append(f)\n",
    "print(\"Features to deselect according to Spearman correlation:\", spearman_deselect)\n",
    "\n",
    "\n",
    "# Chi2 (categorical only, must be non-negative)\n",
    "cat_cols = [c for c in X_df.columns if c not in numeric_log]\n",
    "X_cat = X_df[cat_cols].astype(float)\n",
    "chi2_vals, _ = chi2(X_cat, y_train)\n",
    "chi2_deselect = [f for f, val in zip(cat_cols, chi2_vals) if val <= 0]\n",
    "print(\"Features to deselect according to Chi²:\", chi2_deselect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d62d3f6a-baa5-40e9-913a-b99f443b5080",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Deselection for linear Models"
    }
   },
   "outputs": [],
   "source": [
    "# Numeric/log features for linear models\n",
    "linear_numeric_features = [f for f in numeric_features + log_features if f not in spearman_deselect]\n",
    "\n",
    "preprocessor_linear = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, linear_numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# => use preprocessor_linear for linear model setup; since tree models are indifferent to irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0dc6124-f736-46fb-9f73-895b930faae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Model Evaluation Metrics, Baselining, Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06207ec2-af0d-489f-a23d-467019791499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TASK IV (4 Points): Build a simple model and assess the performance\n",
    "- Identify the type of problem and select the relevant algorithms\n",
    "- Select one Model Assessment Strategy to use throughout your work. Which metrics are you using to evaluate your model and why?\n",
    "\n",
    "\n",
    "=> Tip from lecturer: Use RandomSearch instead of GridSearchCV, set a wider Range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6297f034-638d-4db6-a681-597f20036432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definition of Function for Model Assessment"
    }
   },
   "outputs": [],
   "source": [
    "# Model evaluation metrics used throughout this analysis:\n",
    "#\n",
    "#   > MAE: Mean Absolute Error - average absolute deviation between predicted and true car prices\n",
    "#          Easy to interpret in pounds, same metric used by Kaggle competition\n",
    "#   > RMSE: Root Mean Squared Error - sensitive to outliers, helps identify large prediction errors  \n",
    "#   > R²: Coefficient of determination - proportion of variance explained by the model\n",
    "#         1.0 = perfect predictions, 0.0 = same as predicting mean, < 0.0 = worse than mean\n",
    "#\n",
    "# These metrics are appropriate for regression problems predicting continuous variables (car prices)\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred) #, squared=False) # TODO had to comment this because it lead to an error ~Jan\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"MAE: {mae:.4f} | RMSE: {rmse:.4f} | R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b26348e-a5ff-4dc9-9d61-7f59d0ea335d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Baselining mean/median"
    }
   },
   "outputs": [],
   "source": [
    "# Absolute basic baselining with the mean and median\n",
    "\n",
    "mean_pred = y_train.mean()\n",
    "median_pred = y_train.median()\n",
    "\n",
    "print(\"baseline mean predictor: \")\n",
    "print_metrics(y_val, [mean_pred]*len(y_val))\n",
    "# MAE: 6976.3626 | RMSE: 92839550.2849 | R2: -0.0000\n",
    "\n",
    "print(\"-\"*150)\n",
    "\n",
    "print(\"baseline median predictor: \") \n",
    "print_metrics(y_val, [median_pred]*len(y_val))\n",
    "# MAE: 6751.1604 | RMSE: 97557866.6363 | R2: -0.0508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "04d3aa5b-36ca-4dba-9592-24edd345897d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Models Setup (inkl. Prepro in Pipeline)"
    }
   },
   "outputs": [],
   "source": [
    "# Models Setup (inkl. Prepro in Pipeline)\n",
    "\n",
    "### LINEAR MODEL\n",
    "\n",
    "# ElasticNet\n",
    "elastic_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_linear),\n",
    "    (\"model\", ElasticNet(\n",
    "        alpha=0.01,            # mild regularization to stabilize if many features\n",
    "        l1_ratio=0.5,          # balanced L1/L2, can grid-search\n",
    "        max_iter=30000,        # allow more convergence iterations\n",
    "        tol=1e-4,              # stricter tolerance often improves accuracy\n",
    "        selection=\"cyclic\",    # usually converges faster than random\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### TREE MODELS\n",
    "\n",
    "# HistGradientBoostingRegressor: modern and very fast, handles missing values natively (no imputation needed!). often matches or beats XGBoost/LightGBM \n",
    "hgb_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5, # regularize slightly to prevent overfit, > 0.5 does not seem to work\n",
    "        random_state=42  \n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# RandomForestRegressor: excellent general baseline ensemble, handles non-linearities well, doesn’t overfit easily but can be slow for large data\n",
    "rf_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ExtraTreesRegressor: similar to RandomForest but with more randomization => often better generalization\n",
    "et_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=400,          \n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### KERNEL BASED MODEL\n",
    "\n",
    "# SVR: powerful, but slow on large data, sensitive to scaling => already handled in preprocessing\n",
    "svr_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", SVR(\n",
    "        kernel=\"rbf\",\n",
    "        C=10,\n",
    "        epsilon=0.1,     # slightly tighter margin\n",
    "        gamma=\"scale\"    # default: 1 / (n_features * X.var())\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### ENSEMBLE META MODEL\n",
    "\n",
    "# StackingRegressor: stacks/blends multiple models => typically gives a small but consistent boost in leaderboard competitions\n",
    "stack_pipe = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"elastic\", elastic_pipe),\n",
    "        (\"hgb\", hgb_pipe),\n",
    "        (\"rf\", rf_pipe),\n",
    "    ],\n",
    "    final_estimator=HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42\n",
    "    ),\n",
    "    passthrough=True,     # allow meta-model to see raw inputs too\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Hyperparameter Tuning and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c880d2d-c7aa-478f-8d34-90aa6e33a172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "18042056-7ef9-489f-afc3-958fe02fbfc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ElasticNet"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: ElasticNet\n",
    "\n",
    "elastic_param_grid = {\n",
    "    \"model__alpha\": [0.001],    # also tried 0.01, 0.05, 0.1, 0.5\n",
    "    \"model__l1_ratio\": [0.9]    # also tried 0.1, 0.3, 0.5, 0.7  \n",
    "}\n",
    "\n",
    "elastic_grid = GridSearchCV(\n",
    "    elastic_pipe, \n",
    "    param_grid=elastic_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "elastic_grid.fit(X_train, y_train)\n",
    "elastic_best = elastic_grid.best_estimator_\n",
    "elastic_val_pred = elastic_best.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"ElasticNet Results: \")\n",
    "print_metrics(y_val, elastic_val_pred)\n",
    "print(\"Best ElasticNet params:\", elastic_grid.best_params_)\n",
    "\n",
    "#ElasticNet Results: \n",
    "#MAE: 2543.7302 | RMSE: 16690880.0888 | R2: 0.8202\n",
    "#Best ElasticNet params: {'model__alpha': 0.001, 'model__l1_ratio': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95719624-7d97-43e5-9da4-46bb44f4ef6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wrapper Feature Selection (RFECV) for ElasticNet — uses preprocessor_linear and existing print_metrics\n",
    "\n",
    "elastic_base = ElasticNet(\n",
    "    alpha=0.001,     # from your best params\n",
    "    l1_ratio=0.9,    # from your best params\n",
    "    max_iter=30000,\n",
    "    tol=1e-4,\n",
    "    selection=\"cyclic\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rfecv_pipe_linear = Pipeline([\n",
    "    (\"preprocess\", preprocessor_linear),  # uses linear_numeric_features + categorical_features\n",
    "    (\"rfecv\", RFECV(\n",
    "        estimator=elastic_base,\n",
    "        step=10,                               # remove 10 features per iteration\n",
    "        cv=3,                                 # consistent with your tuning\n",
    "        scoring=\"neg_mean_absolute_error\",    # same metric\n",
    "        n_jobs=-1,\n",
    "        min_features_to_select=10             # adjust as needed\n",
    "    )),\n",
    "    (\"model\", ElasticNet(                    # final model with same params\n",
    "        alpha=0.001,\n",
    "        l1_ratio=0.9,\n",
    "        max_iter=30000,\n",
    "        tol=1e-4,\n",
    "        selection=\"cyclic\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "rfecv_pipe_linear.fit(X_train, y_train)\n",
    "val_pred_linear_rfecv = rfecv_pipe_linear.predict(X_val)\n",
    "print(\"ElasticNet with RFECV (Wrapper Selection):\")\n",
    "print_metrics(y_val, val_pred_linear_rfecv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "636039d8-d2f6-41d3-afa0-4d04503af34f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show selected (post-preprocessing) feature names\n",
    "pre = rfecv_pipe_linear.named_steps[\"preprocess\"]\n",
    "support_mask = rfecv_pipe_linear.named_steps[\"rfecv\"].support_\n",
    "\n",
    "feature_names_linear = (\n",
    "    linear_numeric_features\n",
    "    + list(pre.named_transformers_[\"cat\"]\n",
    "              .named_steps[\"encoder\"]\n",
    "              .get_feature_names_out(categorical_features))\n",
    ")\n",
    "\n",
    "selected_features_linear = [n for n, keep in zip(feature_names_linear, support_mask) if keep]\n",
    "print(f\"Selected {len(selected_features_linear)} features:\")\n",
    "for n in selected_features_linear:\n",
    "    print(\"-\", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23c273de-39f7-40d4-ac12-acd71371fc96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "RFECV selected 143/143 features, meaning the best cross‑validated MAE was achieved with the full post‑preprocessing feature set and dropping any feature hurt performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad5a0287-45b6-4022-8d47-cde8cc52469e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### HistGradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a2d5effc-c1e9-4606-aa57-f0b5041d2a35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: HistGradientBoost"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: HistGradientBoost\n",
    "\n",
    "hgb_param_dist = {\n",
    "    \"model__learning_rate\": uniform(0.01, 0.09),       # samples values between 0.01–0.10\n",
    "    \"model__max_leaf_nodes\": randint(20, 120),         # tries between 20–120 leaves\n",
    "    \"model__min_samples_leaf\": randint(2, 20),         # samples leaf sizes between 2–20\n",
    "    \"model__max_iter\": randint(400, 1000),             # tries 400–1000 iterations\n",
    "    \"model__l2_regularization\": uniform(0.0, 1.0)      # samples small regularization values\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)  # 3-fold for faster runtime\n",
    "\n",
    "# Randomized search setup\n",
    "hgb_random = RandomizedSearchCV(\n",
    "    estimator=hgb_pipe,\n",
    "    param_distributions=hgb_param_dist,\n",
    "    n_iter=30,                         # number of random combinations to try\n",
    "    scoring=\"neg_mean_absolute_error\", # optimize for MAE\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the search\n",
    "hgb_random.fit(X_train, y_train)\n",
    "\n",
    "# Get best model\n",
    "hgb_best = hgb_random.best_estimator_\n",
    "print(\"Best Params:\", hgb_random.best_params_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "hgb_val_pred = hgb_random.best_estimator_.predict(X_val)\n",
    "print_metrics(y_val, hgb_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8b449fd7-2c96-44b4-a9f0-2603c7e1667e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Results of current best HGB (hgb_best_1.pk)\n",
    "\n",
    "# Params: {'model__l2_regularization': 0.942853570557981, 'model__learning_rate': 0.06389789198396824, 'model__max_iter': 642, 'model__max_leaf_nodes': 105, 'model__min_samples_leaf': 3}\n",
    "# MAE: 1303.5851 | RMSE: 4411007.7758 | R2: 0.9525\n",
    "\n",
    "# Features:\n",
    "# numeric_features = [\"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"brand_anchor\"]\n",
    "# log_features = [\"mileage\", \"miles_per_year\", \"model_freq\", \"brand_med_price\", \"model_med_price\"] # could try to test previousOwners, age here; tax, mpg didnt work\n",
    "# categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "\n",
    "# After Feature Selection with SHAP Values:\n",
    "\n",
    "# MAE: 1293.87\n",
    "# Features:\n",
    "# 1. model_med_price           (2791.0)\n",
    "# 2. age                       (2263.5)\n",
    "# 3. transmission_Manual       (1644.6)\n",
    "# 4. mileage                   (1566.7)\n",
    "# 5. engineSize                (1497.4)\n",
    "# 6. mpg                       (816.1)\n",
    "# 7. brand_med_price           (792.1)\n",
    "# 8. miles_per_year            (209.0)\n",
    "# 9. model_freq                (152.6)\n",
    "# 10. tax                       (132.1)\n",
    "# 11. fuelType_Petrol           (120.9)\n",
    "# 12. fuelType_Hybrid           (108.8)\n",
    "# 13. model_focus               (99.4)\n",
    "# 14. Brand_Ford                (82.1)\n",
    "# 15. fuelType_Diesel           (58.0)\n",
    "# 16. previousOwners            (36.3)\n",
    "# 17. transmission_Automatic    (33.5)\n",
    "# 18. transmission_Semi-Auto    (33.4)\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415bf18f-bdcc-458f-9e77-f0b6913c7175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # For quick things to skip the 16 minute randomsearch\n",
    "# hgb_best = Pipeline([\n",
    "#     (\"preprocess\", preprocessor),\n",
    "#     (\"model\", HistGradientBoostingRegressor(\n",
    "#         learning_rate=0.06389789198396824,\n",
    "#         max_leaf_nodes=105,\n",
    "#         min_samples_leaf=3,\n",
    "#         max_iter=642,\n",
    "#         l2_regularization=0.942853570557981,\n",
    "#         early_stopping=True,\n",
    "#         validation_fraction=0.1,\n",
    "#         n_iter_no_change=20,\n",
    "#         random_state=42\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "# # Fit on training data\n",
    "# hgb_best.fit(X_train, y_train) # TODO dont we have to comment this out or are you just not running this cell all the time? ~Jan - just dont run it -sam\n",
    "\n",
    "# # Predict on validation set\n",
    "# hgb_val_pred = hgb_best.predict(X_val)\n",
    "\n",
    "# # Evaluate\n",
    "# print(\"Final HGB Model Evaluation (Best Params)\")\n",
    "# print_metrics(y_val, hgb_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70de2673-0443-4cc4-b1cd-f3813a969246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: RandomForest\n",
    "\n",
    "rf_param_dist = {\n",
    "    \"model__n_estimators\": randint(200, 600),        # number of trees\n",
    "    \"model__max_depth\": randint(5, 40),              # depth of each tree\n",
    "    \"model__min_samples_split\": randint(2, 10),      # min samples to split an internal node\n",
    "    \"model__min_samples_leaf\": randint(1, 8),        # min samples per leaf\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\"],         # feature sampling strategy\n",
    "    \"model__bootstrap\": [True, False]                # use bootstrapping or not\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Randomized search setup\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf_pipe,\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=30,                         # number of random combinations\n",
    "    scoring=\"neg_mean_absolute_error\", \n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the search\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "rf_best_rand = rf_random.best_estimator_\n",
    "print(\"Best Params:\", rf_random.best_params_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "rf_val_pred = rf_best_rand.predict(X_val)\n",
    "print_metrics(y_val, rf_val_pred)\n",
    "\n",
    "# Save best model\n",
    "# joblib.dump(rf_best_rand, \"rf_best.pkl\")\n",
    "\n",
    "\n",
    "# Best Params: {'model__bootstrap': False, 'model__max_depth': 32, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 9, 'model__n_estimators': 467}\n",
    "# MAE: 1403.6263 | RMSE: 5529283.1606 | R2: 0.9404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e9ab1516-d5e9-4b85-9ef5-c5556053c548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Results of current best RF > Samuel\n",
    "\n",
    "# Best Params: {'model__bootstrap': False, 'model__max_depth': 32, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 9, 'model__n_estimators': 467}\n",
    "# MAE: 1403.6263 | RMSE: 5529283.1606 | R2: 0.9404\n",
    "\n",
    "# Features:\n",
    "# numeric_features = [\"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"brand_anchor\"]\n",
    "# log_features = [\"mileage\", \"miles_per_year\", \"model_freq\", \"brand_med_price\", \"model_med_price\"] # could try to test previousOwners, age here; tax, mpg didnt work\n",
    "# categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae6d3b92-efc6-4d8a-b3e1-cb6aff36b84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6f4ff12e-07a4-47f5-8c08-43b5d8d86942",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ExtraTrees"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: ExtraTrees\n",
    "\n",
    "et_param_dist = {\n",
    "    \"model__n_estimators\": randint(200, 600),\n",
    "    \"model__max_depth\": randint(5, 40),\n",
    "    \"model__min_samples_split\": randint(2, 10),\n",
    "    \"model__min_samples_leaf\": randint(1, 8),\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"model__bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "et_random = RandomizedSearchCV(\n",
    "    estimator=et_pipe,\n",
    "    param_distributions=et_param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "et_random.fit(X_train, y_train)\n",
    "\n",
    "et_best = et_random.best_estimator_\n",
    "print(\"ExtraTrees Best Params:\", et_random.best_params_)\n",
    "\n",
    "et_val_pred = et_best.predict(X_val)\n",
    "print_metrics(y_val, et_val_pred)\n",
    "\n",
    "# joblib.dump(et_best, \"et_best.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1f1dea5a-14a8-40c9-bfe4-1cbb96e207a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Results of current best ET > Samuel\n",
    "\n",
    "# ExtraTrees Best Params: {'model__bootstrap': False, 'model__max_depth': 32, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 9, \n",
    "# 'model__n_estimators': 467}\n",
    "# MAE: 1631.3539 | RMSE: 7549459.7557 | R2: 0.9187\n",
    "\n",
    "# Features:\n",
    "# numeric_features = [\"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"brand_anchor\"]\n",
    "# log_features = [\"mileage\", \"miles_per_year\", \"model_freq\", \"brand_med_price\", \"model_med_price\"] # could try to test previousOwners, age here; tax, mpg didnt work\n",
    "# categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a6e6e70-8430-4ef7-bbdf-27602c253f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### SupportVectorRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f487848b-789b-4084-827b-2ca4a2313bfc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: SupportVectorRegressor"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: SupportVectorRegressor\n",
    "\n",
    "svr_param_dist = {\n",
    "    \"model__C\": loguniform(1e-1, 1e3),          # wide search over regularization\n",
    "    \"model__epsilon\": uniform(0.05, 0.3),       # small-margin tolerance\n",
    "    \"model__kernel\": [\"rbf\"],                   # best general-purpose kernel\n",
    "    \"model__gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "svr_random = RandomizedSearchCV(\n",
    "    estimator=svr_pipe,\n",
    "    param_distributions=svr_param_dist,\n",
    "    n_iter=25,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "svr_random.fit(X_train, y_train)\n",
    "\n",
    "svr_best = svr_random.best_estimator_\n",
    "print(\"SVR Best Params:\", svr_random.best_params_)\n",
    "\n",
    "svr_val_pred = svr_best.predict(X_val)\n",
    "print_metrics(y_val, svr_val_pred)\n",
    "\n",
    "# joblib.dump(svr_best, \"svr_best.pkl\")\n",
    "\n",
    "# MAE: 1683.6556 | RMSE: 10282674.8643 | R2: 0.8892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "73408a5c-9acf-4658-9f5f-4c4a430b731c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Results of current best SVR > Samuel\n",
    "\n",
    "# MAE: 1683.6556 | RMSE: 10282674.8643 | R2: 0.8892\n",
    "\n",
    "# Features:\n",
    "# numeric_features = [\"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"brand_anchor\"]\n",
    "# log_features = [\"mileage\", \"miles_per_year\", \"model_freq\", \"brand_med_price\", \"model_med_price\"] # could try to test previousOwners, age here; tax, mpg didnt work\n",
    "# categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "980b2649-0f0d-4d52-91a2-dc569c878850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0525442b-db49-45b7-ad73-ff98599eb223",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: StackingRegressor"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: StackingRegressor\n",
    "\n",
    "stack_param_dist = {\n",
    "    \"final_estimator__learning_rate\": uniform(0.02, 0.08),\n",
    "    \"final_estimator__max_depth\": randint(3, 7),\n",
    "    \"final_estimator__min_samples_leaf\": randint(3, 15),\n",
    "    \"final_estimator__l2_regularization\": uniform(0.0, 1.0)\n",
    "}\n",
    "\n",
    "stack_random = RandomizedSearchCV(\n",
    "    estimator=stack_pipe,\n",
    "    param_distributions=stack_param_dist,\n",
    "    n_iter=25,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=3,                 # low CV because stacking is slow\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "stack_random.fit(X_train, y_train)\n",
    "\n",
    "stack_best = stack_random.best_estimator_\n",
    "print(\"StackingRegressor Best Params:\", stack_random.best_params_)\n",
    "\n",
    "stack_val_pred = stack_best.predict(X_val)\n",
    "print_metrics(y_val, stack_val_pred)\n",
    "\n",
    "# joblib.dump(stack_best, \"stack_best.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a77338ae-128f-46b2-932e-d2349944bbe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Results of current best SVR\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "852bd7bc-33d9-4ae4-a552-0850b200be11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Feature Importance of Tree Models (with SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c02463-def6-4c5c-93ce-5ca710b6eb7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  **Problem:** Current feature selection targets linear models\n",
    "  (ElasticNet), but we primarily use tree-based models (HGB,\n",
    "  RandomForest).\n",
    "\n",
    "  **Solution:** Use SHAP (SHapley Additive exPlanations) to\n",
    "  identify feature importance specifically for tree models\n",
    "\n",
    "  **Why SHAP for Trees:**\n",
    "  - Provides exact feature importance values for tree-based\n",
    "  models\n",
    "  - Tree models handle irrelevant features, but noise features\n",
    "  still impact performance\n",
    "  - Enables data-driven selection rather than statistical filter\n",
    "  methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7de8112-ad4c-4761-a868-9ed084f86174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### HGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cadbb5c-a5f6-4fb1-963c-70ff79b146f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 1: Baseline Performance with Optimized Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a1844a-48ae-4a25-8f9a-5759a7d13a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_val_processed = hgb_best.named_steps[\"preprocess\"].transform(X_val)\n",
    "hgb_val_pred = hgb_best.named_steps[\"model\"].predict(X_val_processed)\n",
    "n_features_total = X_val_processed.shape[1]\n",
    "baseline_mae = mean_absolute_error(y_val, hgb_val_pred)\n",
    "\n",
    "print(\"Baseline Performance of HGB model after Hyperparameter Tuning:\\n\")\n",
    "print_metrics(y_val, hgb_val_pred)\n",
    "print(f\"\\nTotal features used: {n_features_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a381d64-0406-4daa-a147-95081efa4eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 2: SHAP Feature Importance Analysis**\n",
    "\n",
    "We use SHAP's TreeExplainer to calculate feature importance values. TreeExplainer is specifically optimized for tree-based models and provides exact Shapley values efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "41d9d985-7fa1-4326-9524-57af6c84e7a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract trained model and preprocessed data\n",
    "hgb_model = hgb_best.named_steps[\"model\"]\n",
    "X_train_processed = hgb_best.named_steps[\"preprocess\"].transform(X_train)\n",
    "\n",
    "# Build feature names manually (avoid GroupMedianImputer issue)\n",
    "feature_names_all = (\n",
    "    log_features\n",
    "    + numeric_features\n",
    "    + list(\n",
    "        hgb_best.named_steps[\"preprocess\"]\n",
    "        .named_transformers_[\"cat\"]\n",
    "        .named_steps[\"encoder\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "    )\n",
    ")\n",
    "\n",
    "# TreeExplainer for HGB model\n",
    "explainer = shap.TreeExplainer(hgb_model)\n",
    "\n",
    "# Sample for speed\n",
    "sample_size = min(1000, len(X_train_processed))\n",
    "sample_indices = np.random.choice(len(X_train_processed), sample_size, replace=False)\n",
    "X_sample = X_train_processed[sample_indices]\n",
    "\n",
    "print(f\"Computing SHAP values for {sample_size} samples...\")\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# Mean absolute SHAP = global importance\n",
    "feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Create DataFrame with proper columns\n",
    "shap_importance_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names_all,\n",
    "        \"importance\": feature_importance\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 20 most important features:\")\n",
    "print(shap_importance_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94cd8db0-31da-4f79-a4e1-00181450e3f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "top_df = shap_importance_df.head(top_k).iloc[::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.barh(top_df[\"feature\"], top_df[\"importance\"],\n",
    "               color=plt.cm.Blues(np.linspace(0.4, 0.9, len(top_df))))\n",
    "\n",
    "ax.set_xlabel(\"Average |SHAP| value\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "ax.set_title(f\"Top {top_k} HGB features by SHAP importance\")\n",
    "ax.bar_label(bars, fmt=\"%.0f\", padding=3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "355aaca1-92eb-42d9-82d5-e76c3a8b99bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 3: Automated Feature Selection Optimization**\n",
    "\n",
    "Now we systematically test different numbers of top features to find the optimal subset. We train the model with the same optimized hyperparameters but using only the most important features identified by SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4444cebb-c91d-43a4-bb57-65188afa4d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nFeature Selection Analysis with different bins:\")\n",
    "\n",
    "results = []\n",
    "total_features = len(feature_names_all)\n",
    "\n",
    "# test every single feature count\n",
    "# feature_counts = list(range(5, total_features + 1)) \n",
    "feature_counts = list(range(16, 20))\n",
    "\n",
    "# define which bins to *print*\n",
    "print_bins = {5, 10, 15, 20, 30, 50, 70, 100, total_features}\n",
    "\n",
    "# use the same processed validation data\n",
    "X_val_processed = hgb_best.named_steps[\"preprocess\"].transform(X_val)\n",
    "clean_params = {k.replace(\"model__\", \"\"): v for k, v in hgb_random.best_params_.items()}\n",
    "\n",
    "# track best model\n",
    "best_model = None\n",
    "best_mae = float(\"inf\")\n",
    "best_n = None\n",
    "best_features = None\n",
    "\n",
    "for n_features in feature_counts:\n",
    "    # Select top N features\n",
    "    top_features = shap_importance_df.head(n_features)[\"feature\"].tolist()\n",
    "    feature_indices = [i for i, fname in enumerate(feature_names_all) if fname in top_features]\n",
    "\n",
    "    X_train_subset = X_train_processed[:, feature_indices]\n",
    "    X_val_subset   = X_val_processed[:, feature_indices]\n",
    "\n",
    "    # Train model\n",
    "    hgb_selected = HistGradientBoostingRegressor(**clean_params, random_state=42)\n",
    "    hgb_selected.fit(X_train_subset, y_train)\n",
    "    pred_subset = hgb_selected.predict(X_val_subset)\n",
    "    mae_subset = mean_absolute_error(y_val, pred_subset)\n",
    "\n",
    "    # Save result\n",
    "    results.append({\"n_features\": n_features, \"mae\": mae_subset})\n",
    "\n",
    "    # Check if it's the best so far\n",
    "    if mae_subset < best_mae:\n",
    "        best_mae = mae_subset\n",
    "        best_n = n_features\n",
    "        best_model = hgb_selected\n",
    "        best_features = top_features\n",
    "\n",
    "    # print only the key bins\n",
    "    if n_features in print_bins:\n",
    "        improvement = baseline_mae - mae_subset\n",
    "        print(f\"Top {n_features:3d} features: MAE: {mae_subset:.1f} (Δ: {improvement:+.1f})\")\n",
    "\n",
    "# convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\nOptimal feature selection results:\")\n",
    "print(f\"Best performance with {best_n} features: MAE: {best_mae:.2f}\")\n",
    "print(f\"Improvement over baseline: {baseline_mae - best_mae:+.2f} MAE\\n\")\n",
    "\n",
    "print(f\"Optimal {best_n} features for production model:\")\n",
    "for i, feat in enumerate(best_features, start=1):\n",
    "    imp = shap_importance_df.loc[shap_importance_df['feature'] == feat, 'importance'].values[0]\n",
    "    print(f\"{i:2d}. {feat:25s} ({imp:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e893a1a-3c61-4f13-9fd9-35715c65d492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the best model for later use\n",
    "# Build the final pipeline with feature selection included\n",
    "def select_best_features(X):\n",
    "    idx = [i for i, fname in enumerate(feature_names_all) if fname in best_features]\n",
    "    return X[:, idx]\n",
    "\n",
    "final_pipe = Pipeline([\n",
    "    (\"preprocess\", hgb_best.named_steps[\"preprocess\"]),\n",
    "    (\"feature_selector\", FunctionTransformer(select_best_features, validate=False)),\n",
    "    (\"model\", best_model)\n",
    "])\n",
    "\n",
    "joblib.dump(final_pipe, \"hgb_best_feature.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a16b675a-660c-46c8-8430-5b28d91b8c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f65c9142-f110-4d6f-a31c-5764d2492fc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 1: Baseline Performance with Optimized Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95adb47f-fb45-47f0-b24d-e75d5ef1c68e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the tuned RF pipeline (rf_best_rand) and compute baseline on the validation set\n",
    "X_val_processed_rf = rf_best_rand.named_steps[\"preprocess\"].transform(X_val)\n",
    "rf_val_pred = rf_best_rand.named_steps[\"model\"].predict(X_val_processed_rf)\n",
    "baseline_mae_rf = mean_absolute_error(y_val, rf_val_pred)\n",
    "\n",
    "print(\"Baseline Performance of RF model after Hyperparameter Tuning:\\n\")\n",
    "print_metrics(y_val, rf_val_pred)\n",
    "\n",
    "# total features? TODO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eef64537-b8f4-4f70-b060-78606e96731b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 2: SHAP Feature Importance Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09d26704-08d7-4e72-b236-d44d0c2433ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess training data (same preprocessor as in rf_best_rand)\n",
    "X_train_processed_rf = rf_best_rand.named_steps[\"preprocess\"].transform(X_train)\n",
    "\n",
    "# Build feature names (log + numeric + OHE categories), same logic as for HGB\n",
    "feature_names_all_rf = (\n",
    "    log_features\n",
    "    + numeric_features\n",
    "    + list(\n",
    "        rf_best_rand.named_steps[\"preprocess\"]\n",
    "        .named_transformers_[\"cat\"]\n",
    "        .named_steps[\"encoder\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "    )\n",
    ")\n",
    "\n",
    "# SHAP for RandomForest (regression)\n",
    "rf_model = rf_best_rand.named_steps[\"model\"]\n",
    "explainer_rf = shap.TreeExplainer(rf_model)\n",
    "\n",
    "# Sample up to 1000 rows\n",
    "np.random.seed(42)\n",
    "sample_size_rf = min(50, len(X_train_processed_rf))\n",
    "sample_idx_rf = np.random.choice(len(X_train_processed_rf), sample_size_rf, replace=False)\n",
    "X_sample_rf = X_train_processed_rf[sample_idx_rf]\n",
    "\n",
    "print(f\"Computing SHAP values for RF on {sample_size_rf} samples...\")\n",
    "shap_values_rf = explainer_rf.shap_values(X_sample_rf)\n",
    "\n",
    "# Global importance = mean absolute SHAP\n",
    "feature_importance_rf = np.abs(shap_values_rf).mean(axis=0)\n",
    "\n",
    "shap_importance_df_rf = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names_all_rf,\n",
    "        \"importance\": feature_importance_rf\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 20 most important RF features:\")\n",
    "print(shap_importance_df_rf.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0af8a6-1394-464d-8e8c-d452d0138cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 3: Automated Feature Selection Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef268bcf-3ae2-4345-a60f-68ed288d5ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nRF Feature Selection Analysis with different bins:\")\n",
    "\n",
    "results_rf = []\n",
    "\n",
    "# Use the same processed validation data\n",
    "X_val_processed_rf = rf_best_rand.named_steps[\"preprocess\"].transform(X_val)\n",
    "\n",
    "# Reuse tuned RF hyperparameters (strip 'model__' as we instantiate the bare estimator)\n",
    "rf_params = {k.replace(\"model__\", \"\"): v for k, v in rf_random.best_params_.items()}\n",
    "\n",
    "# Track best model\n",
    "best_model_rf = None\n",
    "best_mae_rf = float(\"inf\")\n",
    "best_n_rf = None\n",
    "best_features_rf = None\n",
    "\n",
    "# Candidate feature counts (mirror HGB style; adjust if you want to sweep wider)\n",
    "# feature_counts_rf = list(range(5, total_features + 1)) \n",
    "feature_counts_rf = list(range(16, 30)) #faster\n",
    "print_bins_rf = {5, 10, 15, 20, 30, 50, 70, 100, len(feature_names_all_rf)}\n",
    "\n",
    "for n_features in feature_counts_rf:\n",
    "    # Select top N features\n",
    "    top_features_rf = shap_importance_df_rf.head(n_features)[\"feature\"].tolist()\n",
    "    feature_indices_rf = [i for i, fname in enumerate(feature_names_all_rf) if fname in top_features_rf]\n",
    "\n",
    "    X_train_subset_rf = X_train_processed_rf[:, feature_indices_rf]\n",
    "    X_val_subset_rf   = X_val_processed_rf[:, feature_indices_rf]\n",
    "\n",
    "    # Train RF with the same tuned params\n",
    "    rf_selected = RandomForestRegressor(random_state=42, n_jobs=-1, **rf_params)\n",
    "    rf_selected.fit(X_train_subset_rf, y_train)\n",
    "    pred_subset_rf = rf_selected.predict(X_val_subset_rf)\n",
    "    mae_subset_rf = mean_absolute_error(y_val, pred_subset_rf)\n",
    "\n",
    "    # Save result\n",
    "    results_rf.append({\"n_features\": n_features, \"mae\": mae_subset_rf})\n",
    "\n",
    "    # Best so far?\n",
    "    if mae_subset_rf < best_mae_rf:\n",
    "        best_mae_rf = mae_subset_rf\n",
    "        best_n_rf = n_features\n",
    "        best_model_rf = rf_selected\n",
    "        best_features_rf = top_features_rf\n",
    "\n",
    "    # Optional summary for key bins\n",
    "    if n_features in print_bins_rf:\n",
    "        improvement_rf = baseline_mae_rf - mae_subset_rf\n",
    "        print(f\"Top {n_features:3d} features: MAE: {mae_subset_rf:.1f} (Δ: {improvement_rf:+.1f})\")\n",
    "\n",
    "results_rf_df = pd.DataFrame(results_rf)\n",
    "\n",
    "print(f\"\\nOptimal RF feature selection results:\")\n",
    "print(f\"Best performance with {best_n_rf} features: MAE: {best_mae_rf:.2f}\")\n",
    "print(f\"Improvement over baseline: {baseline_mae_rf - best_mae_rf:+.2f} MAE\\n\")\n",
    "\n",
    "print(f\"Optimal {best_n_rf} RF features for production model:\")\n",
    "for i, feat in enumerate(best_features_rf, start=1):\n",
    "    imp = shap_importance_df_rf.loc[shap_importance_df_rf['feature'] == feat, 'importance'].values[0]\n",
    "    print(f\"{i:2d}. {feat:25s} ({imp:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5bdfcad-da5d-4543-8fce-7ab42593c972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RF SHAP bar plot\n",
    "top_k = 20\n",
    "top_df = shap_importance_df_rf.head(top_k).iloc[::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.barh(\n",
    "    top_df[\"feature\"],\n",
    "    top_df[\"importance\"],\n",
    "    color=plt.cm.Greens(np.linspace(0.4, 0.9, len(top_df)))  # andere Farbe für RF\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Average |SHAP| value\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "ax.set_title(f\"Top {top_k} RF features by SHAP importance\")\n",
    "ax.bar_label(bars, fmt=\"%.0f\", padding=3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8398f0e-737d-4ae1-8ba1-5f8f21aa211d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the best RF model for later use\n",
    "\n",
    "# Build the final RF pipeline with feature selection included\n",
    "def select_best_features_rf(X):\n",
    "    idx = [i for i, fname in enumerate(feature_names_all_rf) if fname in best_features_rf]\n",
    "    return X[:, idx]\n",
    "\n",
    "final_rf_pipe = Pipeline([\n",
    "    (\"preprocess\", rf_best_rand.named_steps[\"preprocess\"]),\n",
    "    (\"feature_selector\", FunctionTransformer(select_best_features_rf, validate=False)),\n",
    "    (\"model\", best_model_rf)\n",
    "])\n",
    "\n",
    "# joblib.dump(final_rf_pipe, \"rf_best_feature.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f1f6d7f-0eed-48c6-865e-c0a248d4fd1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Final hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fbc1f32-5a4e-4b43-a842-86ef6a7f6cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO ~J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "135acfad-1ad6-49f9-97fc-f1a81d0c3187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb68e758-d7b1-4f80-9dc4-ed985f721c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Extra Task (1 Point): Be in the Top 5 Groups on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "927be871-4f27-4453-baac-b4f7102534d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load best Models from Joblib"
    }
   },
   "outputs": [],
   "source": [
    "# Load best Models from Joblib\n",
    "\n",
    "hgb_best_99 = joblib.load(\"hgb_best_feature.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a25f58c-4c24-4460-93d4-830176016ee8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pick best Model, predict on Test"
    }
   },
   "outputs": [],
   "source": [
    "# Pick best model and predict on test:\n",
    "\n",
    "df_cars_test['price'] = hgb_best_99.predict(df_cars_test)\n",
    "\n",
    "df_cars_test['price'].to_csv('Group05_Version03.csv', index=True) # currently version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b18805-e662-4807-bdb8-758df6027d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Submission with API push"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c cars4you -f Group05_Version03.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350057a3-c771-4bf0-8fac-19b49be17e35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Score Check"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c cars4you"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
