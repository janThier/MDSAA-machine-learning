{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b83a4d32-7797-4f5f-8269-7bc08d0b9431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_Clone before play/engineer with Features / Hyperparameters_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52359b17-1216-4744-a528-81e23a033a68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Project Cars4you (Group 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fff3e4-624f-4b1f-b258-fbbe7b69cb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import & load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f54ab11e-fcfc-4278-8997-89a92adf8eff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import and load Data"
    }
   },
   "outputs": [],
   "source": [
    "# Import and load Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, chi2\n",
    "from scipy.stats import spearmanr, uniform, loguniform, randint\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.linear_model import Ridge, Lasso, LassoCV, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from data_cleaning import clean_car_dataframe\n",
    "\n",
    "df_cars_train = pd.read_csv(\"train.csv\")\n",
    "df_cars_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa35af5d-8257-4fb6-a380-86bbbfac3244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "556ffc24-70f7-48c9-96a4-3a19380f91db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Connect"
    }
   },
   "outputs": [],
   "source": [
    "### Everyone has to do this himself, with his own kaggle.json -> get it from kaggle as api token\n",
    "# Kaggle API Connect\n",
    "\n",
    "# Folder containing kaggle.json\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/Workspace/Users/20250355@novaims.unl.pt\"\n",
    "\n",
    "# Test\n",
    "!echo $KAGGLE_CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75ee214-0996-406f-b77f-9e72fa7153e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Cleaning, Feature Engineering, Split & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78e7ca8-ba25-4ea3-873d-4f3515281440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task II (5 Points): Clean and preprocess the dataset. \n",
    "- Missing Value handling, Outlier preprocessing + justify decisions -> in data_cleaning.py\n",
    "- Review current features and create extra features if needed + explain -> in Feature Engineering\n",
    "- Deal with categorical variables -> In One-Hot-Encoding \n",
    "- Perform data scaling, explain reasoning -> In Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6fa0cfe9-be7e-4e90-ac1c-cd600bb7d13f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Numerical / Categorical Data Cleaning: with Pandas"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier Preprocessing, Missing Value Handling and Decision justifying happens here\n",
    "df_cars_train = clean_car_dataframe(df_cars_train)\n",
    "df_cars_test = clean_car_dataframe(df_cars_test)\n",
    "\n",
    "\n",
    "# Safety Check: print unique values of all columns of df_cars_train // df_cars_test to see if data cleaning worked and if there are still odd values\n",
    "for col in df_cars_train.columns:\n",
    "    print(col, df_cars_train[col].unique())\n",
    "print(\"X\"*150)\n",
    "print(\"-\"*150)\n",
    "print(\"X\"*150)\n",
    "for col in df_cars_test.columns:\n",
    "    print(col, df_cars_test[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2df7bf7c-0ebb-4f4b-a168-5b2e2f6b21d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering and Explaination\n",
    "\n",
    "# add column age: models can easier interpret linear numerical features\n",
    "df_cars_train['age'] = 2025 - df_cars_train['year']\n",
    "df_cars_test['age'] = 2025 - df_cars_test['year']\n",
    "\n",
    "\n",
    "# miles per year: normalizes the total mileage by how old the car is\n",
    "df_cars_train['miles_per_year'] = df_cars_train['mileage'] / df_cars_train['age'].replace({0: np.nan})\n",
    "df_cars_train['miles_per_year'] = df_cars_train['miles_per_year'].fillna(df_cars_train['mileage'])\n",
    "\n",
    "df_cars_test['miles_per_year'] = df_cars_test['mileage'] / df_cars_test['age'].replace({0: np.nan})\n",
    "df_cars_test['miles_per_year'] = df_cars_test['miles_per_year'].fillna(df_cars_test['mileage'])\n",
    "\n",
    "\n",
    "# model frequency: some models are more common, which means they can be cheaper (supply) or retain their values better (demand). freq shows their popularity\n",
    "model_freq = df_cars_train['model'].value_counts(normalize=True).to_dict()\n",
    "df_cars_train['model_freq'] = df_cars_train['model'].map(model_freq)\n",
    "\n",
    "model_freq = df_cars_test['model'].value_counts(normalize=True).to_dict()\n",
    "df_cars_test['model_freq'] = df_cars_test['model'].map(model_freq)\n",
    "\n",
    "\n",
    "# brand median price (only train): shows brand positioning (e.g. BMW > KIA)\n",
    "brand_median_price = df_cars_train.groupby('Brand')['price'].median()\n",
    "df_cars_train['brand_med_price'] = df_cars_train['Brand'].map(brand_median_price)\n",
    "\n",
    "\n",
    "# model median price (only train): shows model positioning (e.g. 3er > 1er)\n",
    "model_med_price = df_cars_train.groupby('model')['price'].median()\n",
    "df_cars_train['model_med_price'] = df_cars_train['model'].map(model_med_price)\n",
    "\n",
    "\n",
    "# brand anchor (market position) \n",
    "brand_median_price = df_cars_train.groupby(\"Brand\")[\"price\"].median().to_dict()\n",
    "overall_mean_price = df_cars_train[\"price\"].mean()\n",
    "\n",
    "df_cars_train[\"brand_anchor\"] = df_cars_train[\"Brand\"].map(brand_median_price) / overall_mean_price\n",
    "df_cars_test[\"brand_anchor\"]  = df_cars_test[\"Brand\"].map(brand_median_price) / overall_mean_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dda482e9-4a68-438d-86c0-e0524a9b8692",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split Data"
    }
   },
   "outputs": [],
   "source": [
    "# Split Data, Stratify not necessary due to regression problem and Cross Validation later\n",
    "X = df_cars_train.drop(columns='price')\n",
    "y = df_cars_train['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.3, \n",
    "                                                  random_state = 42, \n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c92786c5-7746-459e-9834-e7c36f01c22c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write GroupMedianImputer"
    }
   },
   "outputs": [],
   "source": [
    "# Write custom GroupMedianImputer to impute missing values on a model, brand level and not only global (with SimpleImputer)\n",
    "class GroupMedianImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Impute missing numeric values using hierarchical medians:\n",
    "    1. By (Brand, model)\n",
    "    2. If model missing → by Brand\n",
    "    3. Fallback → global median\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, group_cols=[\"Brand\", \"model\"]):\n",
    "        self.group_cols = group_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        self.feature_names_in_ = X.columns\n",
    "\n",
    "        # Step 1 — model-level medians\n",
    "        if all(c in X.columns for c in self.group_cols):\n",
    "            self.medians_ = X.groupby(self.group_cols).median(numeric_only=True)\n",
    "        else:\n",
    "            self.medians_ = pd.DataFrame()\n",
    "\n",
    "        # Step 2 — brand-level medians\n",
    "        if \"Brand\" in X.columns:\n",
    "            self.brand_medians_ = X.groupby(\"Brand\").median(numeric_only=True)\n",
    "        else:\n",
    "            self.brand_medians_ = pd.DataFrame()\n",
    "\n",
    "        # Step 3 — global medians\n",
    "        self.global_median_ = X.median(numeric_only=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        for col in X.columns:\n",
    "            if X[col].isna().sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # (Brand, model) level\n",
    "            if all(c in X.columns for c in self.group_cols):\n",
    "                X[col] = X.apply(\n",
    "                    lambda r: self.medians_.loc[(r[\"Brand\"], r[\"model\"]), col]\n",
    "                    if (r[\"Brand\"], r[\"model\"]) in self.medians_.index and pd.isna(r[col])\n",
    "                    else r[col],\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "            # Brand-only fallback\n",
    "            if \"Brand\" in X.columns:\n",
    "                X[col] = X.apply(\n",
    "                    lambda r: self.brand_medians_.loc[r[\"Brand\"], col]\n",
    "                    if r[\"Brand\"] in self.brand_medians_.index and pd.isna(r[col])\n",
    "                    else r[col],\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "            # Global fallback\n",
    "            X[col] = X[col].fillna(self.global_median_[col])\n",
    "\n",
    "        return X.values  # sklearn expects ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "39142374-d485-42e7-a6cb-83ecffae0091",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocessing: with sklearn Pipeline & Column Transformer"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing: with sklearn Pipeline & Column Transformer\n",
    "\n",
    "group_imputer = GroupMedianImputer(group_cols=[\"Brand\", \"model\"])\n",
    "\n",
    "numeric_features = [\"age\", \"tax\", \"mpg\", \"engineSize\", \"paintQuality\", \"previousOwners\", \"brand_anchor\"]\n",
    "log_features = [\"mileage\", \"miles_per_year\", \"model_freq\", \"brand_med_price\", \"model_med_price\"] # could try to test previousOwners, age here; tax, mpg didnt work\n",
    "categorical_features = [\"Brand\", \"model\", \"transmission\", \"fuelType\"]\n",
    "\n",
    "# left out columns: year (age is better), hasDamage (unsure what the two values 0 and NaN mean)\n",
    "\n",
    "\n",
    "log_transformer = Pipeline([\n",
    "    (\"group_impute\", GroupMedianImputer(group_cols=[\"Brand\", \"model\"])),  # Handling of missing numerical values with GroupMedianImputer\n",
    "    (\"to_float\", FunctionTransformer(lambda x: np.array(x, dtype=float))),\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=False)), # log1p handles zeros safely\n",
    "    (\"scaler\", StandardScaler()) #  # Data Scaling with sklearn StandardScaler\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"group_impute\", GroupMedianImputer(group_cols=[\"Brand\", \"model\"])),\n",
    "    (\"to_float\", FunctionTransformer(lambda x: np.array(x, dtype=float))),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), # fill by mode instead of Unknown (a diesel 3er BMW is probably a diesel)\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)) # Deal with Categorical Variables with sklearn OneHotEncoder\n",
    "])\n",
    "\n",
    "# Apply the preprocessing steps to the data with ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"log\", log_transformer, log_features),\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "\n",
    "# Fit preprocessor on training data - avoid data leakage\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" EXPLAINATIONS \"\"\"\n",
    "# 1) Pipeline bundles preprocessing + model training:\n",
    "#       > Ensures all preprocessing happens inside cross-validation folds (no data leakage)\n",
    "#       > Keeps the entire workflow reproducible — scaling, encoding, and modeling are learned together\n",
    "#       > After .fit(), the final model automatically knows how to preprocess new unseen data\n",
    "#       > When saving with joblib, the entire preprocessing (imputers, scalers, encoders) and model are stored together\n",
    "\n",
    "# 2) The ColumnTransformer applies different transformations to subsets of features:\n",
    "#       > Numeric Features arehandled by our custom GroupMedianImputer (domain-aware filling)\n",
    "#           - Missing numeric values are imputed hierarchically:\n",
    "#           1. By (Brand, model)\n",
    "#           2. If missing model by Brand\n",
    "#           3. If missing Brand by global median\n",
    "#       > This approach captures brand/model-level patterns (e.g. BMWs have similar engine sizes)\n",
    "#       > After imputation, StandardScaler standardizes all numeric features\n",
    "#\n",
    "#       > Log Features use the same group-median imputation, followed by log1p() transformation\n",
    "#           - log1p() compresses large, skewed values (like mileage or price-related features), stabilizing variance and helping linear models perform better\n",
    "#           - StandardScaler then scales them to zero mean and unit variance\n",
    "#\n",
    "#       > Categorical Features are handled by SimpleImputer + OneHotEncoder\n",
    "#           - SimpleImputer fills missing categorical values with the most frequent (mode) value.\n",
    "#             (Alternative would be “Unknown”, but mode keeps categories realistic, e.g. most cars in a model share the same transmission)\n",
    "#           - OneHotEncoder converts each categorical label (Brand, model, etc.) into binary dummy variables\n",
    "#             This lets the model use category information numerically without implying order\n",
    "#\n",
    "# 3) Overall:\n",
    "#       > The pipeline ensures consistent preprocessing across training, validation, and test data.\n",
    "#       > It combines domain knowledge (brand/model-aware imputation) with robust numerical scaling.\n",
    "#       > Linear models (ElasticNet, Ridge, Lasso) and tree models (HistGradientBoosting, RandomForest)\n",
    "#           can now learn from the same standardized, clean, and information-rich feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8f763f-298d-444a-80a9-0270ba842dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e9a293-7457-4767-8a26-7c930f7dbad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task III (3 Points): Define and Implement a clear and unambiguous strategy for feature selection. Use the methods **discussed in the course**. Present and justify your final selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d989a4b-077d-47bd-b4bb-c4528628888f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Model independent Filter Methods:\n",
    "- Remove constant numerical variables with VarianceThreshold (manual)\n",
    "- Check highly correlated numerical variables and keep one with Spearman (manual)\n",
    "- Remove unindependent categorical variables with Chi2\n",
    "\n",
    "Model dependent Wrapper Methods:\n",
    "- RFE LR / RFE SVR for linear Models: Ridge, Lasso, ElasticNet, SVM\n",
    "- Feature Importance for tree Models: DecisionTrees, RandomForest, GradientBoosting => trees are unsensitive to irrelevant features but doing feature importance and remove some can reduce dimensionality\n",
    "- L1 Regularization for Neural Networks: MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "130e5929-dcd7-48db-8afa-8f9f560968fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Selection with Filter Methods"
    }
   },
   "outputs": [],
   "source": [
    "X_train_proc = preprocessor.transform(X_train)\n",
    "\n",
    "feature_names_all = []\n",
    "for name, trans, cols in preprocessor.transformers_:\n",
    "    if name != 'remainder':\n",
    "        if hasattr(trans, 'get_feature_names_out'):\n",
    "            # for categorical OHE\n",
    "            try:\n",
    "                feature_names_all.extend(trans.get_feature_names_out(cols))\n",
    "            except:\n",
    "                feature_names_all.extend(cols)\n",
    "        else:\n",
    "            feature_names_all.extend(cols)\n",
    "\n",
    "X_df = pd.DataFrame(X_train_proc, columns=feature_names_all)\n",
    "\n",
    "\n",
    "# Variance Threshold\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "vt.fit(X_df)\n",
    "vt_deselect = [f for f, keep in zip(feature_names_all, vt.get_support()) if not keep]\n",
    "print(\"Features to deselect according to VarianceThreshold:\", vt_deselect)\n",
    "\n",
    "\n",
    "# Spearman correlation (numeric + log only)\n",
    "numeric_log = numeric_features + log_features\n",
    "spearman_deselect = []\n",
    "for f in numeric_log:\n",
    "    if f in X_df.columns:\n",
    "        corr, _ = spearmanr(X_df[f], y_train)\n",
    "        if abs(corr) <= 0.05:\n",
    "            spearman_deselect.append(f)\n",
    "print(\"Features to deselect according to Spearman correlation:\", spearman_deselect)\n",
    "\n",
    "\n",
    "# Chi2 (categorical only, must be non-negative)\n",
    "cat_cols = [c for c in X_df.columns if c not in numeric_log]\n",
    "X_cat = X_df[cat_cols].astype(float)\n",
    "chi2_vals, _ = chi2(X_cat, y_train)\n",
    "chi2_deselect = [f for f, val in zip(cat_cols, chi2_vals) if val <= 0]\n",
    "print(\"Features to deselect according to Chi²:\", chi2_deselect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d62d3f6a-baa5-40e9-913a-b99f443b5080",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Deselection for linear Models"
    }
   },
   "outputs": [],
   "source": [
    "# Numeric/log features for linear models\n",
    "linear_numeric_features = [f for f in numeric_features + log_features if f not in spearman_deselect]\n",
    "\n",
    "preprocessor_linear = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, linear_numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# => use preprocessor_linear for linear model setup; since tree models are indifferent to irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dcae03f-ed97-410e-9078-fb734f42be87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Models Setup and Baselining (with SKLEARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06207ec2-af0d-489f-a23d-467019791499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TASK IV (4 Points): Build a simple model and assess the performance\n",
    "- Identify the type of problem and select the relevant algorithms\n",
    "- Select one Model Assessment Strategy to use throughout your work. Which metrics are you using to evaluate your model and why?\n",
    "\n",
    "\n",
    "=> Tip from lecturer: Use RandomSearch instead of GridSearchCV, set a wider Range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6297f034-638d-4db6-a681-597f20036432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definition of Function for Model Assessment"
    }
   },
   "outputs": [],
   "source": [
    "# Following metrics are used for model evaluation:\n",
    "#\n",
    "#   > MAE: Average absolute deviation between predicted and true car prices, easy to interpret, kaggle competition uses same metric\n",
    "#   > RMSE: Root mean squared error, helps to see if large errors on same values were made, therefore sensitive to outliers\n",
    "#   > R2: Proportion of variance explained by the model, 1 = perfect, 0 = same as predicting mean, < 0 = worse than mean baseline\n",
    "#\n",
    "# Because our task is a regression problem and we are predicting a continuous variable (car price)\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"MAE: {mae:.4f} | RMSE: {rmse:.4f} | R2: {r2:.4f}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b26348e-a5ff-4dc9-9d61-7f59d0ea335d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Baselining mean/median"
    }
   },
   "outputs": [],
   "source": [
    "# Absolute basic baselining with the mean and median\n",
    "\n",
    "mean_pred = y_train.mean()\n",
    "median_pred = y_train.median()\n",
    "\n",
    "print(\"baseline mean predictor: \")\n",
    "print_metrics(y_val, [mean_pred]*len(y_val))\n",
    "# MAE: 6976.3626 | RMSE: 92839550.2849 | R2: -0.0000\n",
    "\n",
    "print(\"-\"*150)\n",
    "\n",
    "print(\"baseline median predictor: \") \n",
    "print_metrics(y_val, [median_pred]*len(y_val))\n",
    "# MAE: 6751.1604 | RMSE: 97557866.6363 | R2: -0.0508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "04d3aa5b-36ca-4dba-9592-24edd345897d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Models Setup (inkl. Prepro in Pipeline)"
    }
   },
   "outputs": [],
   "source": [
    "# Models Setup (inkl. Prepro in Pipeline)\n",
    "\n",
    "### LINEAR MODEL\n",
    "\n",
    "# ElasticNet\n",
    "elastic_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor_linear),\n",
    "    (\"model\", ElasticNet(\n",
    "        alpha=0.01,            # mild regularization to stabilize if many features\n",
    "        l1_ratio=0.5,          # balanced L1/L2, can grid-search\n",
    "        max_iter=30000,        # allow more convergence iterations\n",
    "        tol=1e-4,              # stricter tolerance often improves accuracy\n",
    "        selection=\"cyclic\",    # usually converges faster than random\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### TREE MODELS\n",
    "\n",
    "# HistGradientBoostingRegressor: modern and very fast, handles missing values natively (no imputation needed!). often matches or beats XGBoost/LightGBM \n",
    "hgb_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=20,\n",
    "        l2_regularization=0.5, # regularize slightly to prevent overfit, > 0.5 does not seem to work\n",
    "        random_state=42  \n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# RandomForestRegressor: excellent general baseline ensemble, handles non-linearities well, doesn’t overfit easily but can be slow for large data\n",
    "rf_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ExtraTreesRegressor: similar to RandomForest but with more randomization => often better generalization\n",
    "et_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", ExtraTreesRegressor(\n",
    "        n_estimators=400,          \n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### KERNEL BASED MODEL\n",
    "\n",
    "# SVR: powerful, but slow on large data, sensitive to scaling => already handled in preprocessing\n",
    "svr_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", SVR(\n",
    "        kernel=\"rbf\",\n",
    "        C=10,\n",
    "        epsilon=0.1,     # slightly tighter margin\n",
    "        gamma=\"scale\"    # default: 1 / (n_features * X.var())\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "### ENSEMBLE META MODEL\n",
    "\n",
    "# StackingRegressor: stacks/blends multiple models => typically gives a small but consistent boost in leaderboard competitions\n",
    "stack_pipe = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"elastic\", elastic_pipe),\n",
    "        (\"hgb\", hgb_pipe),\n",
    "        (\"rf\", rf_pipe),\n",
    "    ],\n",
    "    final_estimator=HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        l2_regularization=0.5,\n",
    "        random_state=42\n",
    "    ),\n",
    "    passthrough=True,     # allow meta-model to see raw inputs too\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cc12b6-a13e-42b2-82bc-eb50b8871586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hyperparameter Tuning, Evaluation and Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c880d2d-c7aa-478f-8d34-90aa6e33a172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "18042056-7ef9-489f-afc3-958fe02fbfc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ElasticNet"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: ElasticNet\n",
    "\n",
    "elastic_param_grid = {\n",
    "    \"model__alpha\": [0.001],    # also tried 0.01, 0.05, 0.1, 0.5\n",
    "    \"model__l1_ratio\": [0.9]    # also tried 0.1, 0.3, 0.5, 0.7  \n",
    "}\n",
    "\n",
    "elastic_grid = GridSearchCV(\n",
    "    elastic_pipe, \n",
    "    param_grid=elastic_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "elastic_grid.fit(X_train, y_train)\n",
    "elastic_best = elastic_grid.best_estimator_\n",
    "elastic_val_pred = elastic_best.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"ElasticNet Results: \")\n",
    "print_metrics(y_val, elastic_val_pred)\n",
    "print(\"Best ElasticNet params:\", elastic_grid.best_params_)\n",
    "\n",
    "#ElasticNet Results: \n",
    "#MAE: 2543.7302 | RMSE: 16690880.0888 | R2: 0.8202\n",
    "#Best ElasticNet params: {'model__alpha': 0.001, 'model__l1_ratio': 0.9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5a0287-45b6-4022-8d47-cde8cc52469e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### HistGradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a2d5effc-c1e9-4606-aa57-f0b5041d2a35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: HistGradientBoost"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: HistGradientBoost\n",
    "\n",
    "hgb_param_dist = {\n",
    "    \"model__learning_rate\": uniform(0.01, 0.09),       # samples values between 0.01–0.10\n",
    "    \"model__max_leaf_nodes\": randint(20, 120),         # tries between 20–120 leaves\n",
    "    \"model__min_samples_leaf\": randint(2, 20),         # samples leaf sizes between 2–20\n",
    "    \"model__max_iter\": randint(400, 1000),             # tries 400–1000 iterations\n",
    "    \"model__l2_regularization\": uniform(0.0, 1.0)      # samples small regularization values\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)  # 3-fold for faster runtime\n",
    "\n",
    "# Randomized search setup\n",
    "hgb_random = RandomizedSearchCV(\n",
    "    estimator=hgb_pipe,\n",
    "    param_distributions=hgb_param_dist,\n",
    "    n_iter=30,                         # number of random combinations to try\n",
    "    scoring=\"neg_mean_absolute_error\", # optimize for MAE\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the search\n",
    "hgb_random.fit(X_train, y_train)\n",
    "\n",
    "# Get best model\n",
    "hgb_best_rand = hgb_random.best_estimator_\n",
    "print(\"Best Params:\", hgb_random.best_params_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "hgb_val_pred = hgb_best_rand.predict(X_val)\n",
    "print_metrics(y_val, hgb_val_pred)\n",
    "\n",
    "# Best Params: {'model__l2_regularization': 0.49379559636439074, 'model__learning_rate': 0.05704595464437946, 'model__max_iter': 414\n",
    "# 'model__max_leaf_nodes': 109, 'model__min_samples_leaf': 11}\n",
    "# MAE: 1326.7973 | RMSE: 4663930.5059 | R2: 0.9498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "78157989-9ef4-4b2b-a9e1-72309a89e754",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "OUTDATED: HT: HistGradientBoost"
    }
   },
   "outputs": [],
   "source": [
    "# OUTDATED HT: HistGradientBoost - with GridSearch - but good to quickly check something\n",
    "\n",
    "hgb_param_grid = {\n",
    "    \"model__learning_rate\": [0.06], # also tried: 0.02, 0.04, 0.05, 0.1\n",
    "    \"model__max_leaf_nodes\": [50], # also tried: 15, 25, 31, 60\n",
    "    \"model__min_samples_leaf\": [5], # also tried: 8, 10, 15, 20\n",
    "    \"model__max_iter\": [800] # also tried: 500, 1000\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "hgb_grid = GridSearchCV(\n",
    "    estimator=hgb_pipe,\n",
    "    param_grid=hgb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",  # optimize MAE\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "hgb_grid.fit(X_train, y_train)\n",
    "hgb_best_2 = hgb_grid.best_estimator_\n",
    "\n",
    "hgb_val_pred = hgb_best_2.predict(X_val)\n",
    "print_metrics(y_val, hgb_val_pred)\n",
    "\n",
    "# Best Parameters: {'model__learning_rate': 0.06, 'model__max_iter': 800, 'model__max_leaf_nodes': 50, 'model__min_samples_leaf': 5}\n",
    "# MAE: 1304.7611 | RMSE: 4503446.5247 | R2: 0.9515 \n",
    "# joblib.dump(hgb_best_1, \"hgb_best_1.pkl\") => save current best model\n",
    "\n",
    "\n",
    "# Save model for later use\n",
    "#joblib.dump(hgb_XYZ, \"hgb_XYZ\") # neuen namen vergeben und speichern wenn besser als hgb_best_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70de2673-0443-4cc4-b1cd-f3813a969246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43d239bb-a9dc-4106-a275-5dc1ef215e88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: RandomForest"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: RandomForest\n",
    "\n",
    "rf_param_dist = {\n",
    "    \"model__n_estimators\": randint(200, 600),        # number of trees\n",
    "    \"model__max_depth\": randint(5, 40),              # depth of each tree\n",
    "    \"model__min_samples_split\": randint(2, 10),      # min samples to split an internal node\n",
    "    \"model__min_samples_leaf\": randint(1, 8),        # min samples per leaf\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\"],         # feature sampling strategy\n",
    "    \"model__bootstrap\": [True, False]                # use bootstrapping or not\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Randomized search setup\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf_pipe,\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=30,                         # number of random combinations\n",
    "    scoring=\"neg_mean_absolute_error\", \n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the search\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "rf_best_rand = rf_random.best_estimator_\n",
    "print(\"Best Params:\", rf_random.best_params_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "rf_val_pred = rf_best_rand.predict(X_val)\n",
    "print_metrics(y_val, rf_val_pred)\n",
    "\n",
    "# Save best model\n",
    "# joblib.dump(rf_best_rand, \"rf_best.pkl\")\n",
    "\n",
    "\n",
    "# Best Params: {'model__bootstrap': False, 'model__max_depth': 32, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 9, 'model__n_estimators': 467}\n",
    "# MAE: 1403.6263 | RMSE: 5529283.1606 | R2: 0.9404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6d3b92-efc6-4d8a-b3e1-cb6aff36b84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6f4ff12e-07a4-47f5-8c08-43b5d8d86942",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: ExtraTrees"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: ExtraTrees\n",
    "\n",
    "et_param_dist = {\n",
    "    \"model__n_estimators\": randint(200, 600),\n",
    "    \"model__max_depth\": randint(5, 40),\n",
    "    \"model__min_samples_split\": randint(2, 10),\n",
    "    \"model__min_samples_leaf\": randint(1, 8),\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"model__bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "et_random = RandomizedSearchCV(\n",
    "    estimator=et_pipe,\n",
    "    param_distributions=et_param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "et_random.fit(X_train, y_train)\n",
    "\n",
    "et_best = et_random.best_estimator_\n",
    "print(\"ExtraTrees Best Params:\", et_random.best_params_)\n",
    "\n",
    "et_val_pred = et_best.predict(X_val)\n",
    "print_metrics(y_val, et_val_pred)\n",
    "\n",
    "# joblib.dump(et_best, \"et_best.pkl\")\n",
    "\n",
    "# ExtraTrees Best Params: {'model__bootstrap': False, 'model__max_depth': 32, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 9, \n",
    "# 'model__n_estimators': 467}\n",
    "# MAE: 1631.3539 | RMSE: 7549459.7557 | R2: 0.9187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a6e6e70-8430-4ef7-bbdf-27602c253f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### SupportVectorRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f487848b-789b-4084-827b-2ca4a2313bfc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: SupportVectorRegressor"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: SupportVectorRegressor\n",
    "\n",
    "svr_param_dist = {\n",
    "    \"model__C\": loguniform(1e-1, 1e3),          # wide search over regularization\n",
    "    \"model__epsilon\": uniform(0.05, 0.3),       # small-margin tolerance\n",
    "    \"model__kernel\": [\"rbf\"],                   # best general-purpose kernel\n",
    "    \"model__gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "svr_random = RandomizedSearchCV(\n",
    "    estimator=svr_pipe,\n",
    "    param_distributions=svr_param_dist,\n",
    "    n_iter=25,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "svr_random.fit(X_train, y_train)\n",
    "\n",
    "svr_best = svr_random.best_estimator_\n",
    "print(\"SVR Best Params:\", svr_random.best_params_)\n",
    "\n",
    "svr_val_pred = svr_best.predict(X_val)\n",
    "print_metrics(y_val, svr_val_pred)\n",
    "\n",
    "# joblib.dump(svr_best, \"svr_best.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "980b2649-0f0d-4d52-91a2-dc569c878850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0525442b-db49-45b7-ad73-ff98599eb223",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning: StackingRegressor"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning: StackingRegressor\n",
    "\n",
    "stack_param_dist = {\n",
    "    \"final_estimator__learning_rate\": uniform(0.02, 0.08),\n",
    "    \"final_estimator__max_depth\": randint(3, 7),\n",
    "    \"final_estimator__min_samples_leaf\": randint(3, 15),\n",
    "    \"final_estimator__l2_regularization\": uniform(0.0, 1.0)\n",
    "}\n",
    "\n",
    "stack_random = RandomizedSearchCV(\n",
    "    estimator=stack_pipe,\n",
    "    param_distributions=stack_param_dist,\n",
    "    n_iter=25,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=3,                 # low CV because stacking is slow\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "stack_random.fit(X_train, y_train)\n",
    "\n",
    "stack_best = stack_random.best_estimator_\n",
    "print(\"StackingRegressor Best Params:\", stack_random.best_params_)\n",
    "\n",
    "stack_val_pred = stack_best.predict(X_val)\n",
    "print_metrics(y_val, stack_val_pred)\n",
    "\n",
    "# joblib.dump(stack_best, \"stack_best.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135acfad-1ad6-49f9-97fc-f1a81d0c3187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb68e758-d7b1-4f80-9dc4-ed985f721c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Extra Task (1 Point): Be in the Top 5 Groups on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "927be871-4f27-4453-baac-b4f7102534d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load best Models from Joblib"
    }
   },
   "outputs": [],
   "source": [
    "# Load best Models from Joblib\n",
    "\n",
    "# hgb_best_1\n",
    "hgb_best_1 = joblib.load(\"hgb_best_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6a25f58c-4c24-4460-93d4-830176016ee8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pick best Model, predict on Test"
    }
   },
   "outputs": [],
   "source": [
    "# Pick best model and predict on test:\n",
    "df_cars_test['price'] = hgb_best_1.predict(df_cars_test)\n",
    "\n",
    "df_cars_test['price'].to_csv('submission.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b18805-e662-4807-bdb8-758df6027d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Submission with API push"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c cars4you -f submission.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350057a3-c771-4bf0-8fac-19b49be17e35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kaggle Score Check"
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle Score Check\n",
    "!kaggle competitions submissions -c cars4you"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
