{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Approaches worth mentioning that were tried but finally not used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The following Sections contain descriptions and the code of the approaches that we tried but decided not to use in the final clean notebook due to MAE performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Outlier Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "\n",
    "**Our approach:**\n",
    "- We treat outliers as a **data quality + robustness problem**, not as a reason to delete rare cars.\n",
    "- We keep the process **leakage-safe** by implementing outlier handling as an sklearn transformer inside the pipeline (thresholds learned on training folds only).\n",
    "- We explicitly separate:\n",
    "  - **logical inconsistencies** found in the EDA (handled deterministically in `CarDataCleaner`),\n",
    "  - vs. **statistical extremes** (handled in `OutlierHandler`).\n",
    "- **Identification** of outliers\n",
    "  - **Voting of robust univariate detectors:**\n",
    "    - **Tukey IQR fences (1.5×IQR)**\n",
    "      - Flags a value if it lies outside:\n",
    "        - `Q1 − 1.5·IQR` or `Q3 + 1.5·IQR`\n",
    "      - Strength: non-parametric, robust, widely used baseline (boxplot rule).\n",
    "    - **Modified Z-score using Median Absolute Deviation (MAD)**\n",
    "      - Robust alternative to z-scores:\n",
    "        - uses the **median** instead of mean\n",
    "        - uses **MAD** instead of standard deviation\n",
    "      - Typical threshold: `|modified_z| > 3.5`\n",
    "      - Strength: less sensitive to extreme tails than mean/std-based z-scores.\n",
    "    - **Voting rule** (for robustness):\n",
    "      - A value is treated as an outlier only if both methods agree (`min_votes=2`).\n",
    "      - This reduces false positives compared to using only IQR fences on skewed distributions.\n",
    "- **Treatment of Outliers:**\n",
    "  - **Winsorization** (clip extreme values):\n",
    "    - We keep every car in the dataset (no row deletion).\n",
    "    - We reduce the influence of extreme values while still preserving information and rank order in the feature.\n",
    "    - We **clip** flagged values to conservative bounds (`action=\"clip\"`):\n",
    "      - For each numeric feature we compute robust lower/upper bounds (from IQR and MAD-based thresholds).\n",
    "      - Values outside those bounds are replaced by the nearest bound (winsorization).\n",
    "- **Benefits** of this approach:\n",
    "  - Keeps rare cars (no row deletion).\n",
    "  - Avoids replacing informative extremes with typical medians (which can hurt tree models).\n",
    "  - Stabilizes downstream steps (imputation, feature engineering, scaling) without collapsing signal into missingness.\n",
    "- **Model-family sensitivity:** \n",
    "  - winsorization is especially helpful for linear/SVR models (reduces leverage points) and remains safe for trees\n",
    "  - we keep one unified default pipeline for comparability.\n",
    "\n",
    "<u>Place in the pipe</u>: \n",
    "- Before imputation to use original distribution for identifying the outliers (otherwise we would inflate the distributions with the imputed values)\n",
    "- Then in imputation, fill the original gaps based on a distribution that does not includes the massive outliers (skewing the mean/median)    \n",
    "  -> kill the outliers first (set to NaN) so the imputation for everyone becomes cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Further tried but unused techniques:\n",
    "- **Drop outside 1.5*IQR:** We decided against the classical “drop rows outside 1.5×IQR” because of:\n",
    "    - The classical 1.5×IQR boxplot rule (Tukey fences) is a strong baseline, but real-world car variables (especially mileage) are often skewed / heavy-tailed, which can over-flag valid high values.\n",
    "    - Dropping rows removes rare but valid cars (e.g., very high mileage vehicles), which is undesirable for production.\n",
    "- **NaN into Imputation:** Set outliers found by Voting of robust univariate detectors to NaN and impute later with (`action=\"nan\"`), but this significantly hurt the best averaged CV MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The way it was called in the pipeline:\n",
    "preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    (\"outliers\", OutlierHandler(\n",
    "        cols=[c for c in orig_numeric_features if c != \"mileage\"],      # only original numeric features here, no mileage because of log transform later\n",
    "        methods=(\"iqr\", \"mod_z\"),                                       # robust voting\n",
    "        min_votes=2,                                                    # outlier if both methods agree\n",
    "        iqr_k=1.5,\n",
    "        z_thresh=3.5,\n",
    "        action=\"clip\",                                                   \n",
    "        verbose=False,\n",
    "    )),\n",
    "    # [Unused Group Imputer Handling]\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", (enc_transf_scale)),\n",
    "    (\"fs\", (fs_pipe)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    ('debug_after_outliers', DebugTransformer('AFTER OUTLIER HANDLING', show_data=show_data, y_data_profiling=y_data_profiling)),\n",
    "])\n",
    "\n",
    "# We call fit_tranform here on the entire training data to just visualize the result. The insights from here are not used for anything else in model decisions so it's not leakage\n",
    "X_result = debug_preprocessor_pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Group Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**Our approach:**\n",
    "- We use a custom **hierarchical GroupImputer** to impute missing values in a way that matches the structure of the car market.\n",
    "- Instead of imputing from the full dataset only (global statistics), we first try to impute from **the most similar cars**:\n",
    "  - same `brand` and same `model` (closest peer group),\n",
    "  - otherwise same `brand`,\n",
    "  - otherwise the global dataset.\n",
    "- This is more realistic than a single global median because many variables (e.g., `engineSize`, `mpg`, `tax`) are strongly segment-dependent.\n",
    "\n",
    "**Leakage safety:**\n",
    "- The `GroupImputer` is implemented as an sklearn transformer in the pipeline.\n",
    "- Therefore, during cross-validation it learns all medians/modes **only on the training fold** in `fit()` and applies them to the validation fold in `transform()` (no leakage).\n",
    "\n",
    "---\n",
    "\n",
    "##### Place in the pipe\n",
    "\n",
    "> `CarDataCleaner` → `OutlierHandler` → `GroupImputer` → `CarFeatureEngineer` → `encoding/scaling` → `FS` → `model`\n",
    "\n",
    "**Justification:**\n",
    "- Imputation must happen on **original features** first, because feature engineering creates ratios/interactions (e.g., `miles_per_year`, `engine_per_mpg`) that would otherwise explode or become undefined when inputs are missing.\n",
    "- We impute **before** feature engineering to ensure engineered features are computed on complete, consistent base variables.\n",
    "\n",
    "---\n",
    "\n",
    "##### Why medians/modes:\n",
    "\n",
    "- **Median** is robust to skewed distributions (common in `mileage`, `tax`) and less sensitive to extreme values than the mean.\n",
    "- **Mode** is the natural robust default for categorical variables.\n",
    "\n",
    "---\n",
    "\n",
    "##### Implementation notes:\n",
    "\n",
    "- `group_cols` are used only to define groups; they themselves are **not imputed**.\n",
    "- The transformer is deterministic: ties in categorical mode are handled consistently (pandas `.mode()` → first entry).\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Hierarchical imputer for numeric + categorical features.\n",
    "\n",
    "    Idea:\n",
    "    ----\n",
    "    We have to  compute the median value for the train dataset and fill the missing values in train, validation and test set with the median from the train dataset.\n",
    "    For each row with a missing value, fill it using statistics from \"similar\" rows first, and only fall back to global statistics if needed.\n",
    "\n",
    "    Hierarchy for numeric columns (num_cols):\n",
    "        1) median per (group_cols[0], group_cols[1])    > we use brand, model\n",
    "        2) median per group_cols[0]                     > we use brand\n",
    "        3) global median across all rows\n",
    "\n",
    "    Hierarchy for categorical columns (cat_cols):\n",
    "        1) mode per (group_cols[0], group_cols[1])      > we use brand, model\n",
    "        2) mode per group_cols[0]                       > we use brand\n",
    "        3) global mode across all rows\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - `group_cols` are used only to define groups; they themselves are not imputed.\n",
    "    - `num_cols` and `cat_cols` can be given explicitly (lists of column names). If None, they are inferred from the dtypes in `fit`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, group_cols=(\"brand\", \"model\"), num_cols=None, cat_cols=None, fallback=\"__MISSING__\", verbose=False, verbose_top_n=10):\n",
    "        self.group_cols = group_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.fallback = fallback\n",
    "        self.verbose = verbose\n",
    "        self.verbose_top_n = verbose_top_n\n",
    "\n",
    "    # helpers\n",
    "    def _mode(self, s: pd.Series):\n",
    "        \"\"\"\n",
    "        Deterministic mode helper.\n",
    "\n",
    "        - Compute the most frequent non-null value.\n",
    "        - If multiple values tie, Series.mode() returns them in order, we take .iloc[0].\n",
    "        - If there is no valid mode (all NaN), return fallback token.\n",
    "        \"\"\"\n",
    "        m = s.mode(dropna=True)\n",
    "        if not m.empty:\n",
    "            return m.iloc[0]\n",
    "        return self.fallback\n",
    "\n",
    "    def _get_group_series(self, df: pd.DataFrame, col_name: str) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Get the FIRST physical column with the given label from df.\n",
    "\n",
    "        Reason\n",
    "        ------\n",
    "        - In some workflows, df.columns can contain duplicate labels\n",
    "          (e.g. \"brand\" appearing twice after some operations).\n",
    "        - df[\"brand\"] would then raise \"Grouper for 'brand' not 1-dimensional\".\n",
    "        - By using np.where(df.columns == col_name)[0] we get *positions* and\n",
    "          explicitly pick the first one.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError if no column with that name exists.\n",
    "        \"\"\"\n",
    "        matches = np.where(df.columns == col_name)[0]\n",
    "        if len(matches) == 0:\n",
    "            raise ValueError(f\"GroupImputer: grouping column '{col_name}' not found in data.\")\n",
    "        return df.iloc[:, matches[0]]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn the group-level and global statistics from the training data.\n",
    "\n",
    "        Steps\n",
    "        -----\n",
    "        1) Convert X to DataFrame and remember the original column order.\n",
    "        2) Resolve which columns are numeric/categorical to impute.\n",
    "        3) Build group keys (g0, g1) from group_cols (e.g. brand, model).\n",
    "        4) For numeric columns:\n",
    "            - compute global medians\n",
    "            - medians per g0 (e.g. per brand)\n",
    "            - medians per (g0, g1) (e.g. per brand+model)\n",
    "        5) For categorical columns:\n",
    "            - global modes\n",
    "            - modes per g0\n",
    "            - modes per (g0, g1)\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.DataFrame(X).copy()\n",
    "        self.feature_names_in_ = df.columns.to_list()\n",
    "\n",
    "        # group_cols must contain at least one column name\n",
    "        if self.group_cols is None or len(self.group_cols) == 0:\n",
    "            raise ValueError(\"GroupImputer: at least one group column must be specified.\")\n",
    "\n",
    "        self.group_cols_ = list(self.group_cols)\n",
    "\n",
    "        # Determine numeric columns to impute (internal num_cols_)\n",
    "        if self.num_cols is None:\n",
    "            # If not specified: all numeric columns except the group columns\n",
    "            num_cols_all = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "            self.num_cols_ = [c for c in num_cols_all if c not in self.group_cols_]\n",
    "        else:\n",
    "            # If specified: keep only those that exist in df\n",
    "            self.num_cols_ = [c for c in self.num_cols if c in df.columns]\n",
    "\n",
    "        # Determine categorical columns to impute (internal cat_cols_)\n",
    "        if self.cat_cols is None:\n",
    "            # If not specified: all non-group, non-numeric columns\n",
    "            self.cat_cols_ = [c for c in df.columns if c not in self.group_cols_ + self.num_cols_]\n",
    "        else:\n",
    "            # If specified: keep only those that exist in df\n",
    "            self.cat_cols_ = [c for c in self.cat_cols if c in df.columns]\n",
    "\n",
    "        # Build group key series based on the current df\n",
    "        # g0 = first grouping column (e.g. brand)\n",
    "        g0 = self._get_group_series(df, self.group_cols_[0])\n",
    "\n",
    "        # g1 = second grouping column (e.g. model), optional\n",
    "        g1 = None\n",
    "        if len(self.group_cols_) > 1:\n",
    "            g1 = self._get_group_series(df, self.group_cols_[1])\n",
    "\n",
    "        # numeric statistics\n",
    "        if self.num_cols_:\n",
    "            # Extract the numeric columns to impute\n",
    "            num_df = df[self.num_cols_].copy()\n",
    "\n",
    "            # 3) Global median per numeric column (fallback for any group with no stats)\n",
    "            self.num_global_ = num_df.median(numeric_only=True)\n",
    "\n",
    "            # 2) Median per first-level group (g0, e.g. brand)\n",
    "            num_first = num_df.copy()\n",
    "            num_first[\"_g0\"] = g0.values  # temporary group key column\n",
    "            self.num_first_ = num_first.groupby(\"_g0\", dropna=True).median(numeric_only=True)\n",
    "\n",
    "            # 1) Median per pair (g0, g1), e.g. (brand, model)\n",
    "            if g1 is not None:\n",
    "                num_pair = num_df.copy()\n",
    "                num_pair[\"_g0\"] = g0.values\n",
    "                num_pair[\"_g1\"] = g1.values\n",
    "                self.num_pair_ = num_pair.groupby([\"_g0\", \"_g1\"], dropna=True).median(numeric_only=True)\n",
    "            else:\n",
    "                self.num_pair_ = pd.DataFrame()\n",
    "        else:\n",
    "            self.num_global_ = pd.Series(dtype=\"float64\")\n",
    "            self.num_first_ = pd.DataFrame()\n",
    "            self.num_pair_ = pd.DataFrame()\n",
    "\n",
    "        # categorical statistics\n",
    "        if self.cat_cols_:\n",
    "            cat_df = df[self.cat_cols_].copy()\n",
    "\n",
    "            # 3) Global mode per categorical column\n",
    "            self.cat_global_ = pd.Series({c: self._mode(cat_df[c]) for c in self.cat_cols_}, dtype=\"object\")\n",
    "\n",
    "            # 2) Mode per first-level group (g0)\n",
    "            cat_first = cat_df.copy()\n",
    "            cat_first[\"_g0\"] = g0.values\n",
    "            self.cat_first_ = cat_first.groupby(\"_g0\", dropna=True).agg(lambda s: self._mode(s))\n",
    "\n",
    "            # 1) Mode per pair (g0, g1)\n",
    "            if g1 is not None:\n",
    "                cat_pair = cat_df.copy()\n",
    "                cat_pair[\"_g0\"] = g0.values\n",
    "                cat_pair[\"_g1\"] = g1.values\n",
    "                self.cat_pair_ = cat_pair.groupby([\"_g0\", \"_g1\"], dropna=True).agg(lambda s: self._mode(s))\n",
    "            else:\n",
    "                self.cat_pair_ = pd.DataFrame()\n",
    "        else:\n",
    "            self.cat_global_ = pd.Series(dtype=\"object\")\n",
    "            self.cat_first_ = pd.DataFrame()\n",
    "            self.cat_pair_ = pd.DataFrame()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply hierarchical imputation to new data.\n",
    "            1) Convert input to DataFrame and align columns to what fit() saw.\n",
    "            2) Rebuild group keys g0, g1 from the current data.\n",
    "            3) For each numeric column with missing values:\n",
    "                - try pair-level median (g0, g1)\n",
    "                - then brand-level median (g0)\n",
    "                - then global median\n",
    "            4) Same for categorical columns with modes.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(X).copy()\n",
    "        df = df.reindex(columns=self.feature_names_in_)\n",
    "\n",
    "        g0 = self._get_group_series(df, self.group_cols_[0])\n",
    "        g1 = None\n",
    "        if len(self.group_cols_) > 1:\n",
    "            g1 = self._get_group_series(df, self.group_cols_[1])\n",
    "\n",
    "        # NEW: audit counters\n",
    "        report = {\"num_pair\": 0, \"num_brand\": 0, \"num_global\": 0, \"cat_pair\": 0, \"cat_brand\": 0, \"cat_global\": 0}\n",
    "        per_col = Counter()\n",
    "\n",
    "        # numeric imputation\n",
    "        if hasattr(self, \"num_cols_\") and self.num_cols_:\n",
    "            df[self.num_cols_] = df[self.num_cols_].astype(\"float64\")\n",
    "            to_impute_num = [c for c in self.num_cols_ if df[c].isna().any()]\n",
    "\n",
    "            if to_impute_num:\n",
    "                # 1) pair-level imputation: per (g0, g1)\n",
    "                if g1 is not None and not self.num_pair_.empty:\n",
    "                    key_df = pd.DataFrame({\"_g0\": g0.values, \"_g1\": g1.values})\n",
    "                    med_df = self.num_pair_.reset_index()\n",
    "                    joined = key_df.merge(med_df, on=[\"_g0\", \"_g1\"], how=\"left\")\n",
    "\n",
    "                    for col in to_impute_num:\n",
    "                        if col not in self.num_pair_.columns:\n",
    "                            continue\n",
    "                        mask = df[col].isna() & joined[col].notna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"num_pair\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df.loc[mask, col] = joined.loc[mask, col]\n",
    "\n",
    "                # 2) first-level imputation: per g0 only\n",
    "                if not self.num_first_.empty:\n",
    "                    key1 = pd.DataFrame({\"_g0\": g0.values})\n",
    "                    med1 = self.num_first_.reset_index()\n",
    "                    joined1 = key1.merge(med1, on=\"_g0\", how=\"left\")\n",
    "\n",
    "                    for col in to_impute_num:\n",
    "                        if col not in self.num_first_.columns:\n",
    "                            continue\n",
    "                        mask = df[col].isna() & joined1[col].notna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"num_brand\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df.loc[mask, col] = joined1.loc[mask, col]\n",
    "\n",
    "                # 3) global median fallback\n",
    "                for col in to_impute_num:\n",
    "                    if col in self.num_global_:\n",
    "                        mask = df[col].isna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"num_global\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df[col] = df[col].fillna(self.num_global_[col])\n",
    "\n",
    "        # categorical imputation\n",
    "        if hasattr(self, \"cat_cols_\") and self.cat_cols_:\n",
    "            to_impute_cat = [c for c in self.cat_cols_ if df[c].isna().any()]\n",
    "\n",
    "            if to_impute_cat:\n",
    "                # 1) pair-level imputation: per (g0, g1)\n",
    "                if g1 is not None and not self.cat_pair_.empty:\n",
    "                    key_df = pd.DataFrame({\"_g0\": g0.values, \"_g1\": g1.values})\n",
    "                    mode_df = self.cat_pair_.reset_index()\n",
    "                    joined = key_df.merge(mode_df, on=[\"_g0\", \"_g1\"], how=\"left\")\n",
    "\n",
    "                    for col in to_impute_cat:\n",
    "                        if col not in self.cat_pair_.columns:\n",
    "                            continue\n",
    "                        mask = df[col].isna() & joined[col].notna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"cat_pair\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df.loc[mask, col] = joined.loc[mask, col]\n",
    "\n",
    "                # 2) first-level imputation: per g0 only\n",
    "                if not self.cat_first_.empty:\n",
    "                    key1 = pd.DataFrame({\"_g0\": g0.values})\n",
    "                    mode1 = self.cat_first_.reset_index()\n",
    "                    joined1 = key1.merge(mode1, on=\"_g0\", how=\"left\")\n",
    "\n",
    "                    for col in to_impute_cat:\n",
    "                        if col not in self.cat_first_.columns:\n",
    "                            continue\n",
    "                        mask = df[col].isna() & joined1[col].notna()\n",
    "                        n = int(mask.sum())\n",
    "                        report[\"cat_brand\"] += n\n",
    "                        per_col[col] += n\n",
    "                        df.loc[mask, col] = joined1.loc[mask, col]\n",
    "\n",
    "                # 3) global mode fallback (or fallback token)\n",
    "                for col in to_impute_cat:\n",
    "                    mask = df[col].isna()\n",
    "                    n = int(mask.sum())\n",
    "                    report[\"cat_global\"] += n\n",
    "                    per_col[col] += n\n",
    "                    df[col] = df[col].fillna(self.cat_global_.get(col, self.fallback))\n",
    "\n",
    "        # store report for later inspection\n",
    "        self.report_ = report\n",
    "        self.report_by_column_ = (\n",
    "            pd.DataFrame(per_col.items(), columns=[\"column\", \"values_filled\"])\n",
    "            .sort_values(\"values_filled\", ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            _print_section(\"GroupImputer report\")\n",
    "            print(\"Imputed Missing Values ( always try 'most similar cars' first):\\n\")\n",
    "            print(f\"- Numeric (Median):   (brand+model)={report['num_pair']}, brand={report['num_brand']}, global={report['num_global']}\")\n",
    "            print(f\"- Categorical (Mode): (brand+model)={report['cat_pair']}, brand={report['cat_brand']}, global={report['cat_global']}\")\n",
    "            print(\"\\nTop columns affected:\")\n",
    "            _maybe_display(self.report_by_column_, max_rows=self.verbose_top_n)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"\n",
    "        Make the transformer compatible with sklearn's get feature-name.\n",
    "\n",
    "        - If called without arguments, return the original feature names seen in fit().\n",
    "        - This is mostly useful when GroupImputer is at the top of a Pipeline and\n",
    "          later steps want to introspect feature names.\n",
    "        \"\"\"\n",
    "        if input_features is None:\n",
    "            input_features = getattr(self, \"feature_names_in_\", None)\n",
    "        return np.asarray(input_features, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The way it was called in the pipeline:\n",
    "preprocessor_pipe = Pipeline([\n",
    "    (\"clean\", CarDataCleaner(handle_electric=\"other\", set_carid_index=False, use_fuzzy=True)),\n",
    "    # [Unused Outlier Handling]\n",
    "    # [Unused Group Imputer Handling]\n",
    "    (\"group_imputer\", GroupImputer(\n",
    "        group_cols=(\"brand\", \"model\"),\n",
    "        num_cols=orig_numeric_features,                                 # We have to use the original features here because the others are engineered in the next step\n",
    "        # bools_cols=orig_boolean\n",
    "        cat_cols=orig_categorical_features,                             # We have to use the original features here because the others are engineered in the next step\n",
    "        fallback=\"__MISSING__\",\n",
    "    )),\n",
    "    (\"fe\", CarFeatureEngineer(ref_year=2020)),\n",
    "    (\"ct\", (enc_transf_scale)),\n",
    "    (\"fs\", (fs_pipe)),\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
